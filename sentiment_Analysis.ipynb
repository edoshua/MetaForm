{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c56cf39",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054c4dd2",
   "metadata": {},
   "source": [
    "## Dataset'in Import Edilmesi\n",
    "\n",
    "Bütün çalışma boyunca, Kaggle'ın 1-5 arası puanlama skoru bilgisine sahip Amazon Fine Food Reviews dataseti üzerinde işlemler yaptım."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1476b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:/Users/edouc/Desktop/py/Reviews.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388ef21",
   "metadata": {},
   "source": [
    "### Preparing and Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef3846a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import STOPWORDS\n",
    "def clear_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    txt = \"\".join(u for u in text if u not in string.punctuation)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    txt = txt.lower()\n",
    "\n",
    "    #removing stopwords\n",
    "    txt = txt.split()\n",
    "    txt = [word for word in txt if word not in STOPWORDS]\n",
    "    txt = ' '.join(txt)\n",
    "    \n",
    "    # Lemmatization\n",
    "    txt = txt.split()\n",
    "\n",
    "    txt = [lemmatizer.lemmatize(word) for word in txt]\n",
    "    txt = ' '.join(txt)\n",
    "    \n",
    "    return txt\n",
    "\n",
    "df['Text'] = df['Text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "687e5b28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    bought several vitality canned dog food produc...\n",
       "1    product arrived labeled jumbo salted peanutsth...\n",
       "2    confection around century light pillowy citrus...\n",
       "3    looking secret ingredient robitussin believe f...\n",
       "4    great taffy great price wide assortment yummy ...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5d26c",
   "metadata": {},
   "source": [
    "# 1-) Logistic Regression - SGD - Naive Bayes Classifier\n",
    "\n",
    "Bu çalışmayı hazırlarken öncelikle benzer çalışmalara, kullanılan kütüphanelere ve classifier modellerine göz attım. Nihai hedefimiz, eğittiğimiz modelin puansız bir durumu puanlaması olduğu için öncelikle fine-grained sentiment analysis üzerinde çalışmaya odaklandım."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c69dc4",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "712d2235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-bb04d8db3c56>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['random_number'] = np.random.randn(len(index))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# random split train(%80) and test(%20) data\n",
    "index = df.index\n",
    "df['random_number'] = np.random.randn(len(index))\n",
    "train = df[df['random_number'] <= 0.8]\n",
    "test = df[df['random_number'] > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ef8417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "train_matrix = vectorizer.fit_transform(train['Text'])\n",
    "test_matrix = vectorizer.transform(test['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3281d480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3372203",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_matrix\n",
    "X_test = test_matrix\n",
    "y_train = train['Score']\n",
    "y_test = test['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c69d1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eceddbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ad858fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7558,  1573,   907,   407,   830],\n",
       "       [  682,  1757,   648,   249,   315],\n",
       "       [  479,   898,  3025,  1030,   734],\n",
       "       [  218,   365,  1152,  4885,  2312],\n",
       "       [ 2111,  1782,  3245, 10643, 72574]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find accuracy, precision, recall:\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c39bbdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.67      0.68     11275\n",
      "           2       0.28      0.48      0.35      3651\n",
      "           3       0.34      0.49      0.40      6166\n",
      "           4       0.28      0.55      0.37      8932\n",
      "           5       0.95      0.80      0.87     90355\n",
      "\n",
      "    accuracy                           0.75    120379\n",
      "   macro avg       0.51      0.60      0.53    120379\n",
      "weighted avg       0.82      0.75      0.77    120379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc686d7",
   "metadata": {},
   "source": [
    "### SGD Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dccc279f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#training classification model with linear classifier with SGD training.\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e0acc45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.66      0.66      0.66     10921\n",
      "           2       0.16      0.58      0.25      1744\n",
      "           3       0.25      0.56      0.34      3982\n",
      "           4       0.18      0.56      0.27      5401\n",
      "           5       0.97      0.76      0.85     98331\n",
      "\n",
      "    accuracy                           0.73    120379\n",
      "   macro avg       0.44      0.63      0.48    120379\n",
      "weighted avg       0.87      0.73      0.79    120379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = sgd.predict(X_test)\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b0e4d",
   "metadata": {},
   "source": [
    "### Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bf4e584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab8d2701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.63      0.60      0.62     11550\n",
      "           2       0.14      0.58      0.23      1559\n",
      "           3       0.24      0.48      0.32      4438\n",
      "           4       0.35      0.44      0.39     13908\n",
      "           5       0.92      0.79      0.85     88924\n",
      "\n",
      "    accuracy                           0.72    120379\n",
      "   macro avg       0.46      0.58      0.48    120379\n",
      "weighted avg       0.79      0.72      0.75    120379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = nb.predict(X_test)\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee9f5d9",
   "metadata": {},
   "source": [
    "Bu üç yöntemle oluşturduğumuz modellerde accuracy değerlerinin:\n",
    "\n",
    "**- Logistic Regression:**  0.75\n",
    "\n",
    "**- SGD Classifier:** 0.73\n",
    "\n",
    "**- Naive-Bayes:** 0.72\n",
    "\n",
    "şeklinde olduğunu görüyoruz. Bu değerler ne kadar düşük değerler olmasa da zaman zaman yanlış sonuçlar almamıza sebep olabiliyor. Bunun denemesini yapacak olursak, yine amazon üzerinden  alınmış 2 puanlık bir incelemenin prediction sonuçlarının farklılık gösterdiğini ve yanlış sonuç verebildiğini aşağıda görüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba7b64d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Logistic Regression: 2\n",
      "\n",
      "- SGD Classifier: 3\n",
      "\n",
      "- Naive-Bayes: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "prediction = \"We were pretty disappointed with this shampoo and conditioner. Had high hopes for it, and it did not deliver.The smell was okay.It was harsh and stripping on the hair follicles.Left our hair with that squeaky clean feeling, but not in a good way, more in a removed all the oil from your hair kinda away.Even in using the conditioner, my hair still looked frizzy. And I do not have frizzy hair! Mine is fine, thin, and straight. If anything I usually border on having my hair be too silky and need things to volumize it.Just cannot recommend this product.\"\n",
    "prediction = vectorizer.transform([prediction]).toarray()\n",
    "result_lr = lr.predict(prediction)[0]\n",
    "result_sgd = sgd.predict(prediction)[0]\n",
    "result_nb = nb.predict(prediction)[0]\n",
    "\n",
    "print(\"\"\"\n",
    "- Logistic Regression: {}\n",
    "\n",
    "- SGD Classifier: {}\n",
    "\n",
    "- Naive-Bayes: {}\n",
    "\"\"\".format(result_lr ,result_sgd,result_nb ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9899b61c",
   "metadata": {},
   "source": [
    "# 2-) Sadece Positive/Negative Prediction Üzerinde Çalışma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbc5ec2",
   "metadata": {},
   "source": [
    "Model eğitimini ve prediction'ı yalnızca pozitif/negatif olgusuna odaklanacak şekilde düzenlediğimizde, accuracy değerlerinin nasıl değişeceğini görmek için sonrasında çalışmamı bu yönde ilerlettim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac126940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Nötr ifadeleri temizlemek için 3 puanlık incelemeleri kaldırıp yüksek puanı pozitif, düşük puanı negatif şeklinde değerlendirdim.\n",
    "df = df[df['Score'] != 3]\n",
    "df['sentiment'] = df['Score'].apply(lambda rating : +1 if rating > 3 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5fa36793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanutsth...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around century light pillowy citrus...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  sentiment\n",
       "0  bought several vitality canned dog food produc...          1\n",
       "1  product arrived labeled jumbo salted peanutsth...         -1\n",
       "2  confection around century light pillowy citrus...          1\n",
       "3  looking secret ingredient robitussin believe f...         -1\n",
       "4  great taffy great price wide assortment yummy ...          1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfNew = df[['Text','sentiment']]\n",
    "dfNew.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1adf61e",
   "metadata": {},
   "source": [
    "Bu şekilde aynı üç model üzerinde çalıştığımızda;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "702f04ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Classification Report;\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.74      0.85      0.79     15048\n",
      "           1       0.98      0.95      0.96     96293\n",
      "\n",
      "    accuracy                           0.94    111341\n",
      "   macro avg       0.86      0.90      0.88    111341\n",
      "weighted avg       0.94      0.94      0.94    111341\n",
      "\n",
      "SGD Classifier Classification Report;\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.68      0.87      0.76     13606\n",
      "           1       0.98      0.94      0.96     97735\n",
      "\n",
      "    accuracy                           0.93    111341\n",
      "   macro avg       0.83      0.91      0.86    111341\n",
      "weighted avg       0.94      0.93      0.94    111341\n",
      "\n",
      "Naive-Bayes Classifier Classification Report;\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.67      0.77      0.72     15150\n",
      "           1       0.96      0.94      0.95     96191\n",
      "\n",
      "    accuracy                           0.92    111341\n",
      "   macro avg       0.82      0.86      0.83    111341\n",
      "weighted avg       0.92      0.92      0.92    111341\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random split train and test data\n",
    "index = df.index\n",
    "df['random_number'] = np.random.randn(len(index))\n",
    "train = df[df['random_number'] <= 0.8]\n",
    "test = df[df['random_number'] > 0.8]\n",
    "\n",
    "# count vectorizer:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "train_matrix = vectorizer.fit_transform(train['Text'])\n",
    "test_matrix = vectorizer.transform(test['Text'])\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "X_train = train_matrix\n",
    "X_test = test_matrix\n",
    "y_train = train['sentiment']\n",
    "y_test = test['sentiment']\n",
    "\n",
    "lr.fit(X_train,y_train)\n",
    "\n",
    "predictions = lr.predict(X_test)\n",
    "\n",
    "# find accuracy, precision, recall:\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)\n",
    "print('Logistic Regression Classification Report;\\n')\n",
    "print(classification_report(predictions,y_test))\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#training classification model with linear classifier with SGD training.\n",
    "sgd = SGDClassifier()\n",
    "sgd.fit(X_train, y_train) \n",
    "\n",
    "predictions = sgd.predict(X_test)\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)\n",
    "print('SGD Classifier Classification Report;\\n')\n",
    "print(classification_report(predictions,y_test))\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "predictions = nb.predict(X_test)\n",
    "new = np.asarray(y_test)\n",
    "confusion_matrix(predictions,y_test)\n",
    "print('Naive-Bayes Classifier Classification Report;\\n')\n",
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b086ba4",
   "metadata": {},
   "source": [
    "Accuracy değerlerinin önceki duruma göre çok daha yüksek olduğunu görebiliyoruz.\n",
    "\n",
    "**- Logistic Regression:** 0.94\n",
    "\n",
    "**- SGD Classifier:** 0.93\n",
    "\n",
    "**- Naive-Bayes:** 0.92\n",
    "\n",
    "Predicton sonuçlarına baktığımızda ise 5'lik sistemde puanı 2, yani negatif olan yorumun üç modelde de sonucunun negatif olduğunu aşağıda görüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f18c6d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Logistic Regression: -1\n",
      "\n",
      "- SGD Classifier: -1\n",
      "\n",
      "- Naive-Bayes: -1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "prediction = \"We were pretty disappointed with this shampoo and conditioner. Had high hopes for it, and it did not deliver.The smell was okay.It was harsh and stripping on the hair follicles.Left our hair with that squeaky clean feeling, but not in a good way, more in a removed all the oil from your hair kinda away.Even in using the conditioner, my hair still looked frizzy. And I do not have frizzy hair! Mine is fine, thin, and straight. If anything I usually border on having my hair be too silky and need things to volumize it.Just cannot recommend this product.\"\n",
    "prediction = vectorizer.transform([prediction]).toarray()\n",
    "result_lr = lr.predict(prediction)[0]\n",
    "result_sgd = sgd.predict(prediction)[0]\n",
    "result_nb = nb.predict(prediction)[0]\n",
    "\n",
    "print(\"\"\"\n",
    "- Logistic Regression: {}\n",
    "\n",
    "- SGD Classifier: {}\n",
    "\n",
    "- Naive-Bayes: {}\n",
    "\"\"\".format(result_lr ,result_sgd,result_nb ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5607766",
   "metadata": {},
   "source": [
    "# 3-) Kendi Text Classifier Modelimizi Oluşturma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8055ee3d",
   "metadata": {},
   "source": [
    "Önceki iki çalışmada tam olarak istediğim verimi alamadığım için manuel olarak bir model oluşturmak için araştırmalar yaptım ve örnekleri inceledim. Ancak bu kısımda fine-grained dataset üzerinde çalıştığımda yine istediğim verimi alamadım. Bu sebeple iki farklı model oluşturmayı denedim.\n",
    "\n",
    "**MODEL 1**       \n",
    "\n",
    "Bu modelde 0.97 gibi oldukça yüksek bir accuracy değeri elde ettim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25fd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanutsth...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around century light pillowy citrus...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score  sentiment\n",
       "0  bought several vitality canned dog food produc...      5          1\n",
       "1  product arrived labeled jumbo salted peanutsth...      1          0\n",
       "2  confection around century light pillowy citrus...      4          1\n",
       "3  looking secret ingredient robitussin believe f...      2          0\n",
       "4  great taffy great price wide assortment yummy ...      5          1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = df[['Text','Score','sentiment']]\n",
    "print(new_df.shape)\n",
    "new_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f6e3353",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = new_df['sentiment'].values.tolist()\n",
    "data = new_df['Text'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "347fd121",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(len(data) * 0.80)\n",
    "x_train, x_test = data[:cutoff], data[cutoff:]\n",
    "y_train, y_test = target[:cutoff], target[cutoff:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3faaae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "num_words = 10000\n",
    "tokenizer = Tokenizer(num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecad80e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef50f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef2d4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "791bd515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "max_tokens = int(max_tokens)\n",
    "max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e7a0814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9603985439718228"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(num_tokens < max_tokens) / len(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "350565d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517b5409",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22b85f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_string(tokens):\n",
    "    words = [inverse_map[token] for token in tokens if token!=0]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d931aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dense, GRU, Embedding\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "embedding_size = 50\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='embedding_layer'))\n",
    "\n",
    "model.add(GRU(units=16, return_sequences=True))\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98c285ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 115, 50)           500000    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 115, 16)           3264      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 115, 8)            624       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 504,061\n",
      "Trainable params: 504,061\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4a2180c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1644/1644 [==============================] - 478s 285ms/step - loss: 0.2318 - accuracy: 0.9177\n",
      "Epoch 2/5\n",
      "1644/1644 [==============================] - 467s 284ms/step - loss: 0.1496 - accuracy: 0.9453\n",
      "Epoch 3/5\n",
      "1644/1644 [==============================] - 453s 275ms/step - loss: 0.1254 - accuracy: 0.9556\n",
      "Epoch 4/5\n",
      "1644/1644 [==============================] - 450s 274ms/step - loss: 0.1069 - accuracy: 0.9631\n",
      "Epoch 5/5\n",
      "1644/1644 [==============================] - 248s 151ms/step - loss: 0.0910 - accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d4a6011580>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.array(y_train)\n",
    "model.fit(x_train_pad, y_train, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d978f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9790913]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Amazon üzerindeki 4 puanlık bir yorumun oluşturduğumuz model ile elde edilen prediction değeri\n",
    "prediction = [\"I love this product! I expected it to be much bigger though. For my third grade students it is a little too small, but we make it work. If it was larger it would be much better.\"]\n",
    "pred_token = tokenizer.texts_to_sequences(prediction)\n",
    "padded_token = pad_sequences(pred_token, maxlen=max_tokens)\n",
    "padded_token.shape\n",
    "model.predict(padded_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3fe306",
   "metadata": {},
   "source": [
    "**MODEL 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2025b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(525814, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Score</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrived labeled jumbo salted peanutsth...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around century light pillowy citrus...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Score  sentiment\n",
       "0  bought several vitality canned dog food produc...      5          1\n",
       "1  product arrived labeled jumbo salted peanutsth...      1          0\n",
       "2  confection around century light pillowy citrus...      4          1\n",
       "3  looking secret ingredient robitussin believe f...      2          0\n",
       "4  great taffy great price wide assortment yummy ...      5          1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "new_df = df[['Text','Score','sentiment']]\n",
    "print(new_df.shape)\n",
    "new_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e95dd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, ..., 0, 0, 0], dtype=int64),\n",
       " Int64Index([1, 0], dtype='int64'))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_label = df.sentiment.factorize()\n",
    "sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8945508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df.Text.values\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "encoded_docs = tokenizer.texts_to_sequences(values)\n",
    "padded_sequence = pad_sequences(encoded_docs, maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f5c79bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 200, 16)           3501872   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_9 (Spatial (None, 200, 16)           0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 100)               46800     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 3,548,773\n",
      "Trainable params: 3,548,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 16\n",
    "model = Sequential() \n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200) )\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])  \n",
    "print(model.summary()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1c8bd07f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1644/1644 [==============================] - 1550s 943ms/step - loss: 0.1849 - accuracy: 0.9277 - val_loss: 0.1733 - val_accuracy: 0.9320\n",
      "Epoch 2/5\n",
      "1644/1644 [==============================] - 1723s 1s/step - loss: 0.1812 - accuracy: 0.9293 - val_loss: 0.1710 - val_accuracy: 0.9327\n",
      "Epoch 3/5\n",
      "1644/1644 [==============================] - 1591s 968ms/step - loss: 0.1750 - accuracy: 0.9319 - val_loss: 0.1691 - val_accuracy: 0.9340\n",
      "Epoch 4/5\n",
      "1644/1644 [==============================] - 1387s 844ms/step - loss: 0.1710 - accuracy: 0.9331 - val_loss: 0.1697 - val_accuracy: 0.9347\n",
      "Epoch 5/5\n",
      "1644/1644 [==============================] - 1383s 841ms/step - loss: 0.1675 - accuracy: 0.9352 - val_loss: 0.1643 - val_accuracy: 0.9363\n"
     ]
    }
   ],
   "source": [
    "model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=5, batch_size=256);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "becf05e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34768367]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Amazon üzerindeki 2 puanlık bir yorumun oluşturduğumuz model ile elde edilen prediction değeri\n",
    "text = [\"We were pretty disappointed with this shampoo and conditioner. Had high hopes for it, and it did not deliver.The smell was okay.It was harsh and stripping on the hair follicles.Left our hair with that squeaky clean feeling, but not in a good way, more in a removed all the oil from your hair kinda away.Even in using the conditioner, my hair still looked frizzy. And I do not have frizzy hair! Mine is fine, thin, and straight. If anything I usually border on having my hair be too silky and need things to volumize it.Just cannot recommend this product.\"]\n",
    "tokens = tokenizer.texts_to_sequences([text])\n",
    "padded_tokens = pad_sequences(tokens,maxlen=max_tokens)\n",
    "model.predict(padded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb0a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

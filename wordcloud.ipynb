{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05138f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\edouc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01ea097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MongoDB connection\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db = client.wikidb\n",
    "collection = db['wikipedia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dba4e0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing all text content in the database in a list\n",
    "query = collection.find({},{'text':1, '_id':0})\n",
    "liste = list(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeecd97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   nAn agent-based model  ABM  is a computational model for simulating the actions and interactions of autonomous agents  both individual or collective entities such as organizations or groups  in order to understand the behavior of a system and what governs its outcomes  It combines elements of game theory  complex systems  emergence  computational sociology  multi-agent systems  and evolutionary programming  Monte Carlo methods are used to understand the stochasticity of these models   Particularly within ecology  ABMs are also called individual-based models  IBMs   1  A review of recent literature on individual-based models  agent-based models  and multiagent systems shows that ABMs are used in many scientific domains including biology  ecology and social science  2  Agent-based modeling is related to  but distinct from  the concept of multi-agent systems or multi-agent simulation in that the goal of ABM is to search for explanatory insight into the collective behavior of agents obeying simple rules  typically in natural systems  rather than in designing agents or solving specific practical or engineering problems  2     Agent-based models are a kind of microscale model 3  that simulate the simultaneous operations and interactions of multiple agents in an attempt to re-create and predict the appearance of complex phenomena  The process is one of emergence  which some express as \"the whole is greater than the sum of its parts\"  In other words  higher-level system properties emerge from the interactions of lower-level subsystems  Or  macro-scale state changes emerge from micro-scale agent behaviors  Or  simple behaviors  meaning rules followed by agents  generate complex behaviors  meaning state changes at the whole system level      Individual agents are typically characterized as boundedly rational  presumed to be acting in what they perceive as their own interests  such as reproduction  economic benefit  or social status  4  using heuristics or simple decision-making rules  ABM agents may experience \"learning\"  adaptation  and reproduction  5     Most agent-based models are composed of   1  numerous agents specified at various scales  typically referred to as agent-granularity    2  decision-making heuristics   3  learning rules or adaptive processes   4  an interaction topology  and  5  an environment  ABMs are typically implemented as computer simulations  either as custom software  or via ABM toolkits  and this software can be then used to test how changes in individual behaviors will affect the system  s emerging overall behavior     The idea of agent-based modeling was developed as a relatively simple concept in the late 1940s  Since it requires computation-intensive procedures  it did not become widespread until the 1990s     The history of the agent-based model can be traced back to the Von Neumann machine  a theoretical machine capable of reproduction  The device von Neumann proposed would follow precisely detailed instructions to fashion a copy of itself  The concept was then built upon by von Neumann  s friend Stanislaw Ulam  also a mathematician  Ulam suggested that the machine be built on paper  as a collection of cells on a grid  The idea intrigued von Neumann  who drew it upcreating the first of the devices later termed cellular automata  nAnother advance was introduced by the mathematician John Conway   He constructed the well-known Game of Life  Unlike von Neumann  s machine  Conway  s Game of Life operated by simple rules in a virtual world in the form of a 2-dimensional checkerboard     The Simula programming language  developed in the mid 1960s and widely implemented by the early 1970s  was the first framework for automating step-by-step agent simulations     One of the earliest agent-based models in concept was Thomas Schelling  s segregation model  6  which was discussed in his paper \"Dynamic Models of Segregation\" in 1971   Though Schelling originally used coins and graph paper rather than computers  his models embodied the basic concept of agent-based models as autonomous agents interacting in a shared environment with an observed aggregate  emergent outcome     In the early 1980s  Robert Axelrod hosted a tournament of Prisoner  s Dilemma strategies and had them interact in an agent-based manner to determine a winner   Axelrod would go on to develop many other agent-based models in the field of political science that examine phenomena from ethnocentrism to the dissemination of culture  7  nBy the late 1980s  Craig Reynolds   work on flocking models contributed to the development of some of the first biological agent-based models that contained social characteristics  He tried to model the reality of lively biological agents  known as artificial life  a term coined by Christopher Langton     The first use of the word \"agent\" and a definition as it is currently used today is hard to track down   One candidate appears to be John Holland and John H  Miller  s 1991 paper \"Artificial Adaptive Agents in Economic Theory\"  8  based on an earlier conference presentation of theirs     At the same time  during the 1980s  social scientists  mathematicians  operations researchers  and a scattering of people from other disciplines developed Computational and Mathematical Organization Theory  CMOT    This field grew as a special interest group of The Institute of Management Sciences  TIMS  and its sister society  the Operations Research Society of America  ORSA      The 1990s were especially notable for the expansion of ABM within the social sciences  one notable effort was the large-scale ABM  Sugarscape  developed by nJoshua M  Epstein and Robert Axtell to simulate and explore the role of social phenomena such as seasonal migrations  pollution  sexual reproduction  combat  and transmission of disease and even culture  9  Other notable 1990s developments included Carnegie Mellon University  s Kathleen Carley ABM  10  to explore the co-evolution of social networks and culture  nDuring this 1990s timeframe Nigel Gilbert published the first textbook on Social Simulation  Simulation for the social scientist  1999  and established a journal from the perspective of social sciences  the Journal of Artificial Societies and Social Simulation  JASSS   Other than JASSS  agent-based models of any discipline are within scope of SpringerOpen journal Complex Adaptive Systems Modeling  CASM   11     Through the mid-1990s  the social sciences thread of ABM began to focus on such issues as designing effective teams  understanding the communication required for organizational effectiveness  and the behavior of social networks   CMOTlater renamed Computational Analysis of Social and Organizational Systems  CASOS incorporated more and more agent-based modeling   Samuelson  2000  is a good brief overview of the early history  12  and Samuelson  2005  and Samuelson and Macal  2006  trace the more recent developments  13  14     In the late 1990s  the merger of TIMS and ORSA to form INFORMS  and the move by INFORMS from two meetings each year to one  helped to spur the CMOT group to form a separate society  the North American Association for Computational Social and Organizational Sciences  NAACSOS    Kathleen Carley was a major contributor  especially to models of social networks  obtaining National Science Foundation funding for the annual conference and serving as the first President of NAACSOS   She was succeeded by David Sallach of the University of Chicago and Argonne National Laboratory  and then by Michael Prietula of Emory University  At about the same time NAACSOS began  the European Social Simulation Association  ESSA  and the Pacific Asian Association for Agent-Based Approach in Social Systems Science  PAAA   counterparts of NAACSOS  were organized   As of 2013  these three organizations collaborate internationally   The First World Congress on Social Simulation was held under their joint sponsorship in Kyoto  Japan  in August 2006  citation needed  The Second World Congress was held in the northern Virginia suburbs of Washington  D C   in July 2008  with George Mason University taking the lead role in local arrangements     More recently  Ron Sun developed methods for basing agent-based simulation on models of human cognition  known as cognitive social simulation  15  Bill McKelvey  Suzanne Lohmann  Dario Nardi  Dwight Read and others at UCLA have also made significant contributions in organizational behavior and decision-making   Since 2001  UCLA has arranged a conference at Lake Arrowhead  California  that has become another major gathering point for practitioners in this field  citation needed     Most computational modeling research describes systems in equilibrium or as moving between equilibria  Agent-based modeling  however  using simple rules  can result in different sorts of complex and interesting behavior   The three ideas central to agent-based models are agents as objects  emergence  and complexity     Agent-based models consist of dynamically interacting rule-based agents  The systems within which they interact can create real-world-like complexity   Typically agents are nsituated in space and time and reside in networks or in lattice-like neighborhoods  The location of the agents and their responsive behavior are encoded in algorithmic form in computer programs   In some cases  though not always  the agents may be considered as intelligent and purposeful   In ecological ABM  often referred to as \"individual-based models\" in ecology   agents may  for example  be trees in forest  and would not be considered intelligent  although they may be \"purposeful\" in the sense of optimizing access to a resource  such as water   nThe modeling process is best described as inductive  The modeler makes those assumptions thought most relevant to the situation at hand and then watches phenomena emerge from the agents   interactions  Sometimes that result is an equilibrium  Sometimes it is an emergent pattern  Sometimes  however  it is an unintelligible mangle     In some ways  agent-based models complement traditional analytic methods  Where analytic methods enable humans to characterize the equilibria of a system  agent-based models allow the possibility of generating those equilibria  This generative contribution may be the most mainstream of the potential benefits of agent-based modeling  Agent-based models can explain the emergence of higher-order patternsnetwork structures of terrorist organizations and the Internet  power-law distributions in the sizes of traffic jams  wars  and stock-market crashes  and social segregation that persists despite populations of tolerant people  Agent-based models also can be used to identify lever points  defined as moments in time in which interventions have extreme consequences  and to distinguish among types of path dependency    \"Rather than focusing on stable states  many models consider a system s robustnessthe ways that complex systems adapt to internal and external pressures so as to maintain their functionalities  The task of harnessing that complexity requires consideration of the agents themselvestheir diversity  connectedness  and level of interactions  n\"   Recent work on the Modeling and simulation of Complex Adaptive Systems has demonstrated the need for combining agent-based and complex network based models  16  17  18  describe a framework consisting of four levels of developing models of complex adaptive systems described using several example multidisciplinary case studies     Other methods of describing agent-based models include code templates 19  and text-based methods such as the ODD  Overview  Design concepts  and Design Details  protocol  20     The role of the environment where agents live  both macro and micro  21  is also becoming an important factor in agent-based modelling and simulation work  Simple environment affords simple agents  but complex environments generates diversity of behaviour  22     One strength of agent-based modelling is its ability to mediate information flow between scales  When additional details about an agent are needed  a researcher can integrate it with models describing the extra details  When one is interested in the emergent behaviours demonstrated by the agent population  they can combine the agent-based model with a continuum model describing population dynamics  For example  in a study about CD4+ T cells  a key cell type in the adaptive immune system   23  the researchers modelled biological phenomena occurring at different spatial  intracellular  cellular  and systemic   temporal  and organizational scales  signal transduction  gene regulation  metabolism  cellular behaviors  and cytokine transport   In the resulting modular model  signal transduction and gene regulation are described by a logical model  metabolism by constraint-based models  cell population dynamics are described by an agent-based model  and systemic cytokine concentrations by ordinary differential equations  In this multi-scale model  the agent-based model occupies the central place and orchestrates every stream of information flow between scales     We live in a very complex world where we face complex phenomena such as the formation of social norms and emergence of new disruptive technologies  To better understand such phenomena  social scientists often use a reductionism approach where they reduce complex systems to lower-lever variables and model the relationships among them through a scheme of equations such as partial differential equation  PDE  citation needed   This approach that is called equation-based modeling  EBM  has some basic weaknesses in modeling real complex systems  EBMs emphasize nonrealistic assumptions  such as unbounded rationality and perfect information  while adaptability  evolvability  and network effects go unaddressed citation needed   In tackling deficiencies of reductionism  the framework of complex adaptive systems  CAS  has proven very influential in the past two decades citation needed   In contrast to reductionism  in the CAS framework  complex phenomena are studied in an organic manner where their agents are supposed to be both boundedly rational and adaptive citation needed   As a powerful methodology for CAS modeling  agent-based modeling  ABM  has gained a growing popularity among academics and practitioners  ABMs show how agents simple behavioral rules and their local interactions at micro-scale can generate surprisingly complex patterns at macro-scale  24     Agent-based modeling has been used extensively in biology  including the analysis of the spread of epidemics  25  and the threat of biowarfare  biological applications including population dynamics  26  stochastic gene expression  27  plant-animal interactions  28   vegetation ecology  29  landscape diversity  30  sociobiology  31  the growth and decline of ancient civilizations  evolution of ethnocentric behavior  32  forced displacement/migration  33  language choice dynamics  34  cognitive modeling  and biomedical applications including modeling 3D breast tissue formation/morphogenesis  35  the effects of ionizing radiation on mammary stem cell subpopulation dynamics  36  inflammation  37  38   nand the human immune system  39  Agent-based models have also been used for developing decision support systems such as for breast cancer  40  Agent-based models are increasingly being used to model pharmacological systems in early stage and pre-clinical research to aid in drug development and gain insights into biological systems that would not be possible a priori  41  Military applications have also been evaluated  42  Moreover  agent-based models have been recently employed to study molecular-level biological systems  43  44  45     Agent-based models now complement traditional compartmental models  the usual type of epidemiological models  ABMs have been shown to be superior to compartmental models in regard to the accuracy of predictions  46  47  Recently  ABMs such as CovidSim by epidemiologist Neil Ferguson  have been used to inform public health  nonpharmaceutical  interventions against the spread of SARS-CoV-2  48  Epidemiological ABMs have been criticized for simplifying and unrealistic assumptions  49  50  Still  they can be useful in informing decisions regarding mitigation and suppression measures in cases when ABMs are accurately calibrated  51         Agent-based models have been used since the mid-1990s to solve a variety of business and technology problems  Examples of applications include marketing  54  organizational behaviour and cognition  55  team working  56  supply chain optimization and logistics  modeling of consumer behavior  including word of mouth  social network effects  distributed computing  workforce management  and portfolio management  They have also been used to analyze traffic congestion  57     Recently  agent based modelling and simulation has been applied to various domains such as studying the impact of publication venues by researchers in the computer science domain  journals versus conferences   58  In addition  ABMs have been used to simulate information delivery in ambient assisted environments  59  A November 2016 article in arXiv analyzed an agent based simulation of posts spread in Facebook  60  In the domain of peer-to-peer  ad hoc and other self-organizing and complex networks  the usefulness of agent based modeling and simulation has been shown  61  The use of a computer science-based formal specification framework coupled with wireless sensor networks and an agent-based simulation has recently been demonstrated  62     Agent based evolutionary search or algorithm is a new research topic for solving complex optimization problems  63     Prior to  and in the wake of the 2008 financial crisis  interest has grown in ABMs as possible tools for economic analysis  64  65  ABMs do not assume the economy can achieve equilibrium and \"representative agents\" are replaced by agents with diverse  dynamic  and interdependent behavior including herding  ABMs take a \"bottom-up\" approach and can generate extremely complex and volatile simulated economies  ABMs can represent unstable systems with crashes and booms that develop out of non-linear  disproportionate  responses to proportionally small changes  66  A July 2010 article in The Economist looked at ABMs as alternatives to DSGE models  66  The journal Nature also encouraged agent-based modeling with an editorial that suggested ABMs can do a better job of representing financial markets and other economic complexities than standard models 67  along with an essay by J  Doyne Farmer and Duncan Foley that argued ABMs could fulfill both the desires of Keynes to represent a complex economy and of Robert Lucas to construct models based on microfoundations  68  Farmer and Foley pointed to progress that has been made using ABMs to model parts of an economy  but argued for the creation of a very large model that incorporates low level models  69  By modeling a complex system of analysts based on three distinct behavioral profiles  imitating  anti-imitating  and indifferent  financial markets were simulated to high accuracy  Results showed a correlation between network morphology and the stock market index  70  However  the ABM approach has been criticized for its lack of robustness between models  where similar models can yield very different results  71  72     ABMs have been deployed in architecture and urban planning to evaluate design and to simulate pedestrian flow in the urban environment 73  and the examination of public policy applications to land-use  74  There is also a growing field of socio-economic analysis of infrastructure investment impact using ABM  s ability to discern systemic impacts upon a socio-economic network  75     The agent-directed simulation  ADS  metaphor distinguishes between two categories  namely \"Systems for Agents\" and \"Agents for Systems \" 76  Systems for Agents  sometimes referred to as agents systems  are systems implementing agents for the use in engineering  human and social dynamics  military applications  and others  Agents for Systems are divided in two subcategories  Agent-supported systems deal with the use of agents as a support facility to enable computer assistance in problem solving or enhancing cognitive capabilities  Agent-based systems focus on the use of agents for the generation of model behavior in a system evaluation  system studies and analyses      Hallerbach et al  discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents  77  Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars  78  79  It simulates traffic interactions between human drivers  pedestrians and automated vehicles  People  s behavior is imitated by artificial agents based on data of real human behavior  The basic idea of using agent-based modeling to understand self-driving cars was discussed as early as 2003  80     Many ABM frameworks are designed for serial von-Neumann computer architectures  limiting the speed and scalability of implemented models  Since emergent behavior in large-scale ABMs is dependent of population size  81  scalability restrictions may hinder model validation  82  Such limitations have mainly been addressed using distributed computing  with frameworks such as Repast HPC 83  specifically dedicated to these type of implementations  While such approaches map well to cluster and supercomputer architectures  issues related to communication and synchronization  84  85  as well as deployment complexity  86  remain potential obstacles for their widespread adoption     A recent development is the use of data-parallel algorithms on Graphics Processing Units GPUs for ABM simulation  81  87  88  The extreme memory bandwidth combined with the sheer number crunching power of multi-processor GPUs has enabled simulation of millions of agents at tens of frames per second     Since Agent-Based Modeling is more of a modeling framework than a particular piece of software or platform  it has often been used in conjunction with other modeling forms  For instance  agent-based models have also been combined with Geographic Information Systems  GIS   This provides a useful combination where the ABM serves as a process model and the GIS system can provide a model of pattern  89  Similarly  Social Network Analysis  SNA  tools and agent-based models are sometimes integrated  where the ABM is used to simulate the dynamics on the network while the SNA tool models and analyzes the network of interactions  90     Verification and validation  V&V  of simulation models is extremely important  91  92  Verification involves making sure the implemented model matches the conceptual model  whereas validation ensures that the implemented model has some relationship to the real-world  Face validation  sensitivity analysis  calibration  and statistical validation are different aspects of validation  93  A discrete-event simulation framework approach for the validation of agent-based systems has been proposed  94  A comprehensive resource on empirical validation of agent-based models can be found here  95     As an example of V&V technique  consider VOMAS  virtual overlay multi-agent system   96  a software engineering based approach  where a virtual overlay multi-agent system is developed alongside the agent-based model  Muazi et al  also provide an example of using VOMAS for verification and validation of a forest fire simulation model  97  98  Another software engineering method  i e  Test-Driven Development has been adapted to for agent-based model validation  99  This approach has another advantage that allows an automatic validation using unit test tools     In mathematics  an ordinary differential equation  ODE  is a differential equation containing one or more functions of one independent variable and the derivatives of those functions  1  The term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable  2     A linear differential equation is a differential equation that is defined by a linear polynomial in the unknown function and its derivatives  that is an equation of the form     where                               a                      0                                   x                          displaystyle a_ 0  x                                         a                      n                                   x                          displaystyle a_ n  x     and                     b                 x                          displaystyle b x     are arbitrary differentiable functions that do not need to be linear  and                                y                                                              y                                   n                                                displaystyle y     ldots  y^       are the successive derivatives of the unknown function y of the variable x     Among ordinary differential equations  linear differential equations play a prominent role for several reasons  Most elementary and special functions that are encountered in physics and applied mathematics are solutions of linear differential equations  see Holonomic function   When physical phenomena are modeled with non-linear equations  they are generally approximated by linear differential equations for an easier solution  The few non-linear ODEs that can be solved explicitly are generally solved by transforming the equation into an equivalent linear ODE  see  for example Riccati equation      Some ODEs can be solved explicitly in terms of known functions and integrals  When that is not possible  the equation for computing the Taylor series of the solutions may be useful  For applied problems  numerical methods for ordinary differential equations can supply an approximation of the solution     Ordinary differential equations  ODEs  arise in many contexts of mathematics and social and natural sciences  Mathematical descriptions of change use differentials and derivatives  Various differentials  derivatives  and functions become related via equations  such that a differential equation is a result that describes dynamically changing phenomena  evolution  and variation  Often  quantities are defined as the rate of change of other quantities  for example  derivatives of displacement with respect to time   or gradients of quantities  which is how they enter differential equations     Specific mathematical fields include geometry and analytical mechanics  Scientific fields include much of physics and astronomy  celestial mechanics   meteorology  weather modeling   chemistry  reaction rates   3  biology  infectious diseases  genetic variation   ecology and population modeling  population competition   economics  stock trends  interest rates and the market equilibrium price changes      Many mathematicians have studied differential equations and contributed to the field  including Newton  Leibniz  the Bernoulli family  Riccati   Clairaut  d  Alembert  and Euler     A simple example is Newton  s second law of motion  the relationship between the displacement x and the time t of an object under the force F  is given by the differential equation    which constrains the motion of a particle of constant mass m  In general  F is a function of the position x t  of the particle at time t  The unknown function x t  appears on both sides of the differential equation  and is indicated in the notation F x t    4  5  6  7     In what follows  let y be a dependent variable and x an independent variable  and y = f x  is an unknown function of x  The notation for differentiation varies depending upon the author and upon which notation is most useful for the task at hand  In this context  the Leibniz  s notation  dy/dx  d2y/dx2    dny/dxn  is more useful for differentiation and integration  whereas Lagrange  s notation  y  y    y   is more useful for representing derivatives of any order compactly  and Newton  s notation                                                                  y                                                                                                 y                                                                                   y                                                                                                                 displaystyle     dot  y      ddot  y      overset       y       is often used in physics for representing derivatives of low order with respect to time     Given F  a function of x  y  and derivatives of y  Then an equation of the form    is called an explicit ordinary differential equation of order  8  9     More generally  an implicit ordinary differential equation of order takes the form  10     There are further classifications     The general solution to a linear equation can be written as y = yc + yp     A number of coupled differential equations form a system of equations  If y is a vector whose elements are functions  y x  =  y1 x   y2 x       ym x    and F is a vector-valued function of y and its derivatives  then    is an explicit system of ordinary differential equations of order and dimension m  In column vector form     These are not necessarily linear  The implicit analogue is     where 0 =  0  0       0  is the zero vector  In matrix form    For a system of the form                               F                                                 x                                       y                                                                   y                                                                             =                  0                         displaystyle   mathbf  F    left x   mathbf  y     mathbf  y      right =   boldsymbol  0       some sources also require that the Jacobian matrix                                                                                       F                                           x                                             u                                                           v                                                                                                 v                                                             displaystyle    frac    partial   mathbf  F   x   mathbf  u     mathbf  v       partial   mathbf  v        be non-singular in order to call this an implicit ODE  system   an implicit ODE system satisfying this Jacobian non-singularity condition can be transformed into an explicit ODE system  In the same sources  implicit ODE systems with a singular Jacobian are termed differential algebraic equations  DAEs   This distinction is not merely one of terminology  DAEs have fundamentally different characteristics and are generally more involved to solve than  nonsingular  ODE systems  14  15  16  Presumably for additional derivatives  the Hessian matrix and so forth are also assumed non-singular according to this scheme  citation needed  although note that any ODE of order greater than one can be  and usually is  rewritten as system of ODEs of first order  17  which makes the Jacobian singularity criterion sufficient for this taxonomy to be comprehensive at all orders     The behavior of a system of ODEs can be visualized through the use of a phase portrait     Given a differential equation    a function u  I  R  R  where I is an interval  is called a solution or integral curve for F  if u is n-times differentiable on I  and    Given two solutions u  J  R  R and v  I  R  R  u is called an extension of v if I  J and    A solution that has no extension is called a maximal solution  A solution defined on all of R is called a global solution     A general solution of an nth-order equation is a solution containing arbitrary independent constants of integration   A particular solution is derived from the general solution by setting the constants to particular values  often chosen to fulfill set   initial conditions or boundary conditions    18   A singular solution is a solution that cannot be obtained by assigning definite values to the arbitrary constants in the general solution  19     In the context of linear ODE  the terminology particular solution can also refer to any solution of the ODE  not necessarily satisfying the initial conditions   which is then added to the homogeneous solution  a general solution of the homogeneous ODE   which then forms a general solution of the original ODE  This is the terminology used in the guessing method section in this article  and is frequently used when discussing the method of undetermined coefficients and variation of parameters     The theory of singular solutions of ordinary and partial differential equations was a subject of research from the time of Leibniz  but only since the middle of the nineteenth century has it received special attention  A valuable but little-known work on the subject is that of Houtain  1854   Darboux  from 1873  was a leader in the theory  and in the geometric interpretation of these solutions he opened a field worked by various writers  notably Casorati and Cayley  To the latter is due  1872  the theory of singular solutions of differential equations of the first order as accepted circa 1900     The primitive attempt in dealing with differential equations had in view a reduction to quadratures  As it had been the hope of eighteenth-century algebraists to find a method for solving the general equation of the nth degree  so it was the hope of analysts to find a general method for integrating any differential equation  Gauss  1799  showed  however  that complex differential equations require complex numbers  Hence  analysts began to substitute the study of functions  thus opening a new and fertile field  Cauchy was the first to appreciate the importance of this view   Thereafter  the real question was no longer whether a solution is possible by means of known functions or their integrals  but whether a given differential equation suffices for the definition of a function of the independent variable or variables  and  if so  what are the characteristic properties     Two memoirs by Fuchs 20  inspired a novel approach  subsequently elaborated by Thom and Frobenius  Collet was a prominent contributor beginning in 1869  His method for integrating a non-linear system was communicated to Bertrand in 1868  Clebsch  1873  attacked the theory along lines parallel to those in his theory of Abelian integrals  As the latter can be classified according to the properties of the fundamental curve that remains unchanged under a rational transformation  Clebsch proposed to classify the transcendent functions defined by differential equations according to the invariant properties of the corresponding surfaces f = 0 under rational one-to-one transformations     From 1870  Sophus Lie  s work put the theory of differential equations on a better foundation  He showed that the integration theories of the older mathematicians can  using Lie groups  be referred to a common source  and that ordinary differential equations that admit the same infinitesimal transformations present comparable integration difficulties  He also emphasized the subject of transformations of contact     Lie  s group theory of differential equations has been certified  namely   1  that it unifies the many ad hoc methods known for solving differential equations  and  2  that it provides powerful new ways to find solutions  The theory has applications to both ordinary and partial differential equations  21     A general solution approach uses the symmetry property of differential equations  the continuous infinitesimal transformations of solutions to solutions  Lie theory   Continuous group theory  Lie algebras  and differential geometry are used to understand the structure of linear and nonlinear  partial  differential equations for generating integrable equations  to find its Lax pairs  recursion operators  Bcklund transform  and finally finding exact analytic solutions to DE     Symmetry methods have been applied to differential equations that arise in mathematics  physics  engineering  and other disciplines     SturmLiouville theory is a theory of a special type of second order linear ordinary differential equation  Their solutions are based on eigenvalues and corresponding eigenfunctions of linear operators defined via second-order homogeneous linear equations  The problems are identified as Sturm-Liouville Problems  SLP  and are named after J C F  Sturm and J  Liouville  who studied them in the mid-1800s  SLPs have an infinite number of eigenvalues  and the corresponding eigenfunctions form a complete  orthogonal set  which makes orthogonal expansions possible  This is a key idea in applied mathematics  physics  and engineering  22  SLPs are also useful in the analysis of certain partial differential equations     There are several theorems that establish existence and uniqueness of solutions to initial value problems involving ODEs both locally and globally  The two main theorems are    In their basic form both of these theorems only guarantee local results  though the latter can be extended to give a global result  for example  if the conditions of Grnwall  s inequality are met     Also  uniqueness theorems like the Lipschitz one above do not apply to DAE systems  which may have multiple solutions stemming from their  non-linear  algebraic part alone  23     The theorem can be stated simply as follows  24  For the equation and initial value problem     if F and F/y are continuous in a closed rectangle    in the x-y plane  where a and b are real  symbolically  a  b    and  denotes the cartesian product  square brackets denote closed intervals  then there is an interval    for some h   where the solution to the above equation and initial value problem can be found  That is  there is a solution and it is unique  Since there is no restriction on F to be linear  this applies to non-linear equations that take the form F x  y   and it can also be applied to systems of equations     When the hypotheses of the PicardLindelf theorem are satisfied  then local existence and uniqueness can be extended to a global result  More precisely  25     For each initial condition  x0  y0  there exists a unique maximum  possibly infinite  open interval    such that any solution that satisfies this initial condition is a restriction of the solution that satisfies this initial condition with domain                               I                      max                                   displaystyle I_   max          In the case that                               x                                                                                 displaystyle x_   pm    neq   pm   infty      there are exactly two possibilities    where  is the open set in which F is defined  and                                                                                                                              displaystyle   partial    bar    Omega       is its boundary     Note that the maximum domain of the solution    This means that F x  y  = y2  which is C1 and therefore locally Lipschitz continuous  satisfying the PicardLindelf theorem     Even in such a simple setting  the maximum domain of solution cannot be all                               R                         displaystyle   mathbb  R      since the solution is    which has maximum domain     This shows clearly that the maximum interval may depend on the initial conditions  The domain of y could be taken as being                               R                                           x                      0                          +        1                  /                          y                      0                                                     displaystyle   mathbb  R    setminus  x_ 0 +1/y_ 0       but this would lead to a domain that is not an interval  so that the side opposite to the initial condition would be disconnected from the initial condition  and therefore not uniquely determined by it     The maximum domain is not                               R                         displaystyle   mathbb  R      because    which is one of the two possible cases according to the above theorem     Differential equations can usually be solved more easily if the order of the equation can be reduced     Any explicit differential equation of order     can be written as a system of first-order differential equations by defining a new family of unknown functions    for i = 1  2       The n-dimensional system of first-order coupled differential equations is then    more compactly in vector notation     where    Some differential equations have solutions that can be written in an exact and closed form  Several important classes are given here     In the table below  P x   Q x   P y   Q y   and M x y   N x y  are any integrable functions of x  y  and b and c are real given constants  and C1  C2     are arbitrary constants  complex in general   The differential equations are in their equivalent and alternative forms that lead to the solution through integration     In the integral solutions   and  are dummy variables of integration  the continuum analogues of indices in summation   and the notation xF   xa0d just means to integrate F   with respect to   then after the integration substitute  = x  without adding constants  explicitly stated                                                                                                P                                      1                                                                   x                                                   Q                                      1                                                                   y                                 +                                  P                                      2                                                                   x                                                   Q                                      2                                                                   y                                                                                                             d                      y                                                              d                      x                                                                                                                  =                0                                                                                      P                                      1                                                                   x                                                   Q                                      1                                                                   y                                                 d                x                +                                  P                                      2                                                                   x                                                   Q                                      2                                                                   y                                                 d                y                                                            =                0                                                             displaystyle    begin aligned P_ 1  x Q_ 1  y +P_ 2  x Q_ 2  y       frac  dy  dx  &=0    P_ 1  x Q_ 1  y    dx+P_ 2  x Q_ 2  y    dy&=0  end aligned      n                                                                                                                                        d                      y                                                              d                      x                                                                                                                  =                F                                 x                                                                                     d                y                                                            =                F                                 x                                                 d                x                                                             displaystyle    begin aligned    frac  dy  dx  &=F x     dy&=F x    dx  end aligned      n                                                                                                                                        d                      y                                                              d                      x                                                                                                                  =                F                                 y                                                                                     d                y                                                            =                F                                 y                                                 d                x                                                             displaystyle    begin aligned    frac  dy  dx  &=F y     dy&=F y    dx  end aligned      n                                                                            P                                 y                                                                                             d                      y                                                              d                      x                                                                      +                Q                                 x                                                                             =                0                                                                    P                                 y                                                 d                y                +                Q                                 x                                                 d                x                                                            =                0                                                             displaystyle    begin aligned P y    frac  dy  dx  +Q x &=0    P y    dy+Q x    dx&=0  end aligned      n                                                            d              y                                      d              x                                      =        F                                                       y              x                                                                          displaystyle    frac  dy  dx  =F  left    frac  y  x    right      !    n                                                                            y                M                                 x                y                                 +                x                N                                 x                y                                                                                                             d                      y                                                              d                      x                                                                                                                  =                0                                                                    y                M                                 x                y                                                 d                x                +                x                N                                 x                y                                                 d                y                                                            =                0                                                             displaystyle    begin aligned yM xy +xN xy       frac  dy  dx  &=0    yM xy    dx+xN xy    dy&=0  end aligned      n                        ln         u2061                 C        x                 =                                        x            y                                                              N                                                                        d                                                                                 N                                                                        M                                                                                                                          displaystyle   ln Cx =  int ^ xy    frac  N   lambda     d  lambda     lambda  N   lambda  -M   lambda          !    n    If N = M  the solution is xy = C                                                                             M                                 x                                 y                                                                                             d                      y                                                              d                      x                                                                      +                N                                 x                                 y                                                                             =                0                                                                    M                                 x                                 y                                                 d                y                +                N                                 x                                 y                                                 d                x                                                            =                0                                                             displaystyle    begin aligned M x y    frac  dy  dx  +N x y &=0    M x y    dy+N x y    dx&=0  end aligned      n    where                                                                       M                                                    x                                      =                                                          N                                                    y                                                               displaystyle    frac    partial M    partial x  =   frac    partial N    partial y       !    n    where Y y  and X x  are functions from the integrals rather than constant values  which are set to make the final function F x  y  satisfy the initial equation                                                                             M                                 x                                 y                                                                                             d                      y                                                              d                      x                                                                      +                N                                 x                                 y                                                                             =                0                                                                    M                                 x                                 y                                                 d                y                +                N                                 x                                 y                                                 d                x                                                            =                0                                                             displaystyle    begin aligned M x y    frac  dy  dx  +N x y &=0    M x y    dy+N x y    dx&=0  end aligned      n    where                                                                       M                                                    x                                                                                                N                                                    y                                                               displaystyle    frac    partial M    partial x    neq    frac    partial N    partial y       !    n                                                                                                       M                                                                   x                                      =                                                                                       N                                                                   y                                                               displaystyle    frac    partial    mu M     partial x  =   frac    partial    mu N     partial y       !    n                                                                            F                                 x                                 y                                 =                                                                                                                    y                                                                                   x                                                                  M                                 x                                                                                  d                                +                                                                        x                                                                                                                    y                                 N                                                                  y                                                 d                                                                                                                  +                Y                                 y                                 +                X                                 x                                 =                C                                                             displaystyle    begin aligned F x y =&  int ^ y   mu  x   lambda  M x   lambda     d  lambda +  int ^ x   mu    lambda  y N   lambda  y    d  lambda     &+Y y +X x =C  end aligned      n                                                                            d                                  2                                            y                                      d                              x                                  2                                                                    =        F                 y                                          displaystyle    frac  d^ 2 y  dx^ 2   =F y      !    n                                                            d              y                                      d              x                                      +        P                 x                 y        =        Q                 x                                          displaystyle    frac  dy  dx  +P x y=Q x      !    n                                                                            d                                  2                                            y                                      d                              x                                  2                                                                    +        2        p                 x                                                     d              y                                      d              x                                      +                                         p                         x                                                         2                                      +                          p                                                   x                                                  y        =        q                 x                          displaystyle    frac  d^ 2 y  dx^ 2   +2p x    frac  dy  dx  +  left p x ^ 2 +p   x   right y=q x     n                                                                            d                                  2                                            y                                      d                              x                                  2                                                                    +        b                                            d              y                                      d              x                                      +        c        y        =        r                 x                                          displaystyle    frac  d^ 2 y  dx^ 2   +b   frac  dy  dx  +cy=r x      !    n    Particular integral yp  in general the method of variation of parameters  though for very simple r x  inspection may work  24     If b2 > 4c  then    If b2 = 4c  then    If b2 < 4c  then                                                        j            =            0                                n                                    b                      j                                                                              d                                  j                                            y                                      d                              x                                  j                                                                    =        r                 x                                          displaystyle   sum _ j=0 ^ n b_ j    frac  d^ j y  dx^ j   =r x      !    n    Particular integral yp  in general the method of variation of parameters  though for very simple r x  inspection may work  24     Since j are the solutions of the polynomial of degree                                                      j            =            1                                n                                                                                   j                                   =        0                                 displaystyle   prod _ j=1 ^ n    alpha -  alpha _ j  =0     !     then     for j all different     for each root j repeated kj times     for some j complex  then setting  = j + ij  and using Euler  s formula  allows some terms in the previous results to be written in the form    where j is an arbitrary constant  phase shift      When all other methods for solving an ODE fail  or in the cases where we have some intuition about what the solution to a DE might look like  it is sometimes possible to solve a DE simply by guessing the solution and validating it is correct   To use this method  we simply guess a solution to the differential equation  and then plug the solution into the differential equation to validate if it satisfies the equation   If it does then we have a particular solution to the DE  otherwise we start over again and try another guess   For instance we could guess that the solution to a DE has the form                      y        =        A                  e                                  t                                   displaystyle y=Ae^   alpha t     since this is a very common solution that physically behaves in a sinusoidal way     In the case of a first order ODE that is non-homogeneous we need to first find a DE solution to the homogeneous portion of the DE  otherwise known as the characteristic equation  and then find a solution to the entire non-homogeneous equation by guessing  Finally  we add both of these solutions together to obtain the total solution to the ODE  that is                                   total solution                =                  homogeneous solution                +                  particular solution                         displaystyle    text total solution  =   text homogeneous solution  +   text particular solution      n    Chemical process modeling is a computer modeling technique used in chemical engineering process design  It typically involves using purpose-built software to define a system of interconnected components  1  which are then solved so that the steady-state or dynamic behavior of the system can be predicted  The system components and connections are represented as a process flow diagram  1   Simulations can be as simple as the mixing of two substances in a tank  or as complex as an entire alumina refinery  2     Chemical process modeling requires a knowledge of the properties of the chemicals involved in the simulation  1  as well as the physical properties and characteristics of the components of the system  such as tanks  pumps  pipes  pressure vessels  and so on     This industry-related article is a stub  You can help Wikipedia by expanding it     In computing  an emulator is hardware or software that enables one computer system  called the host  to behave like another computer system  called the guest   An emulator typically enables the host system to run software or use peripheral devices designed for the guest system  nEmulation refers to the ability of a computer program in an electronic device to emulate  or imitate  another program or device     Many printers  for example  are designed to emulate HP LaserJet printers because so much software is written for HP printers  If a non-HP printer emulates an HP printer  any software written for a real HP printer will also run in the non-HP printer emulation and produce equivalent printing  Since at least the 1990s  many video game enthusiasts and hobbyists have used emulators to play classic  and/or forgotten  arcade games from the 1980s using the games   original 1980s machine code and data  which is interpreted by a current-era system and to emulate old video game consoles     A hardware emulator is an emulator which takes the form of a hardware device  Examples include the DOS-compatible card installed in some 1990s-era Macintosh computers  such as the Centris 610 or Performa 630  that allowed them to run personal computer  PC  software programs and FPGA-based hardware emulators  The Church-Turing thesis implies that  theoretically  any operating environment can be emulated within any other environment  assuming memory limitations are ignored   However  in practice  it can be quite difficult  particularly when the exact behavior of the system to be emulated is not documented and has to be  sometimes tediously  deduced through reverse engineering  It also says nothing about timing constraints  if the emulator does not perform as quickly as it did using the original hardware  the software inside the emulation may run much more slowly  possibly triggering timer interrupts that alter behavior      \"Can a Commodore 64 emulate MS-DOS?\" nYes  it  s possible for a  Commodore  64 to emulate an IBM PC  which uses MS-DOS   in the same sense that it  s possible to bail out Lake Michigan with a teaspoon     Emulation is one strategy in pursuit of digital preservation and combating obsolescence  Emulation focuses on recreating an original computer environment  which can be time-consuming and difficult to achieve  but valuable because of its ability to maintain a closer connection to the authenticity of the digital object  operating system  or even gaming platform  2  Emulation addresses the original hardware and software environment of the digital object  and recreates it on a current machine  3   The emulator allows the user to have access to any kind of application or operating system on a current platform  while the software runs as it did in its original environment  4   Jeffery Rothenberg  an early proponent of emulation as a digital preservation strategy states  \"the ideal approach would provide a single extensible  long-term solution that can be designed once and for all and applied uniformly  automatically  and in organized synchrony  for example  at every refresh cycle  to all types of documents and media\"  5   He further states that this should not only apply to out of date systems  but also be upwardly mobile to future unknown systems  6  Practically speaking  when a certain application is released in a new version  rather than address compatibility issues and migration for every digital object created in the previous version of that application  one could create an emulator for the application  allowing access to all of said digital objects     Because of its primary use of digital formats  new media art relies heavily on emulation as a preservation strategy  Artists such as Cory Arcangel specialize in resurrecting obsolete technologies in their artwork and recognize the importance of a decentralized and deinstitutionalized process for the preservation of digital culture  In many cases  the goal of emulation in new media art is to preserve a digital medium so that it can be saved indefinitely and reproduced without error  so that there is no reliance on hardware that ages and becomes obsolete  The paradox is that the emulation and the emulator have to be made to work on future computers  14     Emulation techniques are commonly used during the design and development of new systems  It eases the development process by providing the ability to detect  recreate and repair flaws in the design even before the system is actually built  15  It is particularly useful in the design of multi-core systems  where concurrency errors can be very difficult to detect and correct without the controlled environment provided by virtual hardware  16  This also allows the software development to take place before the hardware is ready  17  thus helping to validate design decisions and give a little more control     Most emulators just emulate a hardware architectureif operating system firmware or software is required for the desired software  it must be provided as well  and may itself be emulated   Both the OS and the software will then be interpreted by the emulator  rather than being run by native hardware  Apart from this interpreter for the emulated binary machine  s language  some other hardware  such as input or output devices  must be provided in virtual form as well  for example  if writing to a specific memory location should influence what is displayed on the screen  then this would need to be emulated  While emulation could  if taken to the extreme  go down to the atomic level  basing its output on a simulation of the actual circuitry from a virtual power source  this would be a highly unusual solution  Emulators typically stop at a simulation of the documented hardware specifications and digital logic  Sufficient emulation of some hardware platforms requires extreme accuracy  down to the level of individual clock cycles  undocumented features  unpredictable analog elements  and implementation bugs  This is particularly the case with classic home computers such as the Commodore 64  whose software often depends on highly sophisticated low-level programming tricks invented by game programmers and the \"demoscene\"     In contrast  some other platforms have had very little use of direct hardware addressing  such as an emulator for the PlayStation 4  18  In these cases  a simple compatibility layer may suffice  This translates system calls for the foreign system into system calls for the host system e g   the Linux compatibility layer used on *BSD to run closed source Linux native software on FreeBSD  NetBSD and OpenBSD  For example  while the Nintendo 64 graphic processor was fully programmable  most games used one of a few pre-made programs  which were mostly self-contained and communicated with the game via FIFO  therefore  many emulators do not emulate the graphic processor at all  but simply interpret the commands received from the CPU as the original program would  Developers of software for embedded systems or video game consoles often design their software on especially accurate emulators called simulators before trying it on the real hardware  This is so that software can be produced and tested before the final hardware exists in large quantities  so that it can be tested without taking the time to copy the program to be debugged at a low level and without introducing the side effects of a debugger  In many cases  the simulator is actually produced by the company providing the hardware  which theoretically increases its accuracy  Math co-processor emulators allow programs compiled with math instructions to run on machines that don  t have the co-processor installed  but the extra work done by the CPU may slow the system down   If a math coprocessor isn  t installed or present on the CPU  when the CPU executes any co-processor instruction it will make a determined interrupt  coprocessor not available   calling the math emulator routines  When the instruction is successfully emulated  the program continues executing     Typically  an emulator is divided into modules that correspond roughly to the emulated computer  s subsystems  nMost often  an emulator will be composed of the following modules     Buses are often not emulated  either for reasons of performance or simplicity  and virtual peripherals communicate directly with the CPU or the memory subsystem     It is possible for the memory subsystem emulation to be reduced to simply an array of elements each sized like an emulated word  however  this model fails very quickly as soon as any location in the computer  s logical memory does not match physical memory  This clearly is the case whenever the emulated hardware allows for advanced memory management  in which case  the MMU logic can be embedded in the memory emulator  made a module of its own  or sometimes integrated into the CPU simulator   Even if the emulated computer does not feature an MMU  though  there are usually other factors that break the equivalence between logical and physical memory  many  if not most  architectures offer memory-mapped I/O  even those that do not often have a block of logical memory mapped to ROM  which means that the memory-array module must be discarded if the read-only nature of ROM is to be emulated  Features such as bank switching or segmentation may also complicate memory emulation  As a result  most emulators implement at least two procedures for writing to and reading from logical memory  and it is these procedures   duty to map every access to the correct location of the correct object     On a base-limit addressing system where memory from address 0 to address ROMSIZE-1 is read-only memory  while the rest is RAM  something along the line of the following procedures would be typical     The CPU simulator is often the most complicated part of an emulator  Many emulators are written using \"pre-packaged\" CPU simulators  in order to concentrate on good and efficient emulation of a specific machine  The simplest form of a CPU simulator is an interpreter  which is a computer program that follows the execution flow of the emulated program code and  for every machine code instruction encountered  executes operations on the host processor that are semantically equivalent to the original instructions  This is made possible by assigning a variable to each register and flag of the simulated CPU  The logic of the simulated CPU can then more or less be directly translated into software algorithms  creating a software re-implementation that basically mirrors the original hardware implementation     The following example illustrates how CPU simulation can be accomplished by an interpreter  In this case  interrupts are checked-for before every instruction executed  though this behavior is rare in real emulators for performance reasons  it is generally faster to use a subroutine to do the work of an interrupt      Interpreters are very popular as computer simulators  as they are much simpler to implement than more time-efficient alternative solutions  and their speed is more than adequate for emulating computers of more than roughly a decade ago on modern machines  However  the speed penalty inherent in interpretation can be a problem when emulating computers whose processor speed is on the same order of magnitude as the host machine dubious   discuss   Until not many years ago  emulation in such situations was considered completely impractical by many dubious   discuss      What allowed breaking through this restriction were the advances in dynamic recompilation techniques dubious   discuss   Simple a priori translation of emulated program code into code runnable on the host architecture is usually impossible because of several reasons     Various forms of dynamic recompilation  including the popular Just In Time compiler  JIT  technique  try to circumvent these problems by waiting until the processor control flow jumps into a location containing untranslated code  and only then  \"just in time\"  translates a block of the code into host code that can be executed  nThe translated code is kept in a code cache dubious   discuss   and the original code is not lost or affected  this way  even data segments can be  meaninglessly  translated by the recompiler  resulting in no more than a waste of translation time  Speed may not be desirable as some older games were not designed with the speed of faster computers in mind  A game designed for a 30 xa0MHz PC with a level timer of 300 game seconds might only give the player 30 seconds on a 300 xa0MHz PC  Other programs  such as some DOS programs  may not even run on faster computers  Particularly when emulating computers which were \"closed-box\"  in which changes to the core of the system were not typical  software may use techniques that depend on specific characteristics of the computer it ran on  e g  its CPU  s speed  and thus precise control of the speed of emulation is important for such applications to be properly emulated     Most emulators do not  as mentioned earlier  emulate the main system bus  each I/O device is thus often treated as a special case  and no consistent interface for virtual peripherals is provided  This can result in a performance advantage  since each I/O module can be tailored to the characteristics of the emulated device  designs based on a standard  unified I/O API can  however  rival such simpler models  if well thought-out  and they have the additional advantage of \"automatically\" providing a plug-in service through which third-party virtual devices can be used within the emulator  A unified I/O API may not necessarily mirror the structure of the real hardware bus  bus design is limited by several electric constraints and a need for hardware concurrency  management that can mostly be ignored in a software implementation     Even in emulators that treat each device as a special case  there is usually a common basic infrastructure for     The word \"emulator\" was coined in 1963 at IBM 19  during development of the NPL  IBM System/360  product line  using a \"new combination of software  microcode  and hardware\"  20  nThey discovered that simulation using additional instructions implemented in microcode and hardware  instead of software simulation using only standard instructions  to execute programs written for earlier IBM computers dramatically increased simulation speed  Earlier  IBM provided simulators for  e g   the 650 on the 705  21  In addition to simulators  IBM had compatibility features on the 709 and 7090  22  for which it nprovided the IBM 709 computer with a program to run legacy programs written for the IBM 704 on the 709 and later on the IBM 7090  This program used the instructions added by the compatibility feature 23  to trap instructions requiring special handling  all other 704 instructions ran the same on a 7090  The compatibility feature on the 1410 24  only required setting a console toggle switch  not a support program     In 1963  when microcode was first used to speed up this simulation process  IBM engineers coined the term \"emulator\" to describe the concept  In the 2000s  it has become common to use the word \"emulate\" in the context of software  However  before 1980  \"emulation\" referred only to emulation with a hardware or microcode assist  while \"simulation\" referred to pure software emulation  25  For example  a computer specially built for running programs designed for another architecture is an emulator  In contrast  a simulator could be a program which runs on a PC  so that old Atari games can be simulated on it  Purists continue to insist on this distinction  but currently the term \"emulation\" often means the complete imitation of a machine executing binary code while \"simulation\" often refers to computer simulation  where a computer program is used to simulate an abstract model  Computer simulation is used in virtually every scientific and engineering domain and Computer Science is no exception  with several projects simulating abstract models of computer systems  such as network simulation  which both practically and semantically differs from network emulation  26     Logic simulation is the use of a computer program to simulate the operation of a digital circuit such as a processor   This is done after a digital circuit has been designed in logic equations  but before the circuit is fabricated in hardware     Functional simulation is the use of a computer program to simulate the execution of a second computer program written in symbolic assembly language or compiler language  rather than in binary machine code  By using a functional simulator  programmers can execute and trace selected sections of source code to search for programming errors  bugs   without generating binary code  This is distinct from simulating execution of binary code  which is software emulation  The first functional simulator was written by Autonetics about 1960 for testing assembly language programs for later execution in military computer D-17B  This made it possible for flight programs to be written  executed  and tested before D-17B computer hardware had been built   Autonetics also programmed a functional simulator for testing flight programs for later execution in the military computer D-37C     Video game console emulators are programs that allow a personal computer or video game console to emulate another video game console  They are most often used to play older 1980s to 2000s-era video games on modern personal computers and more contemporary video game consoles  They are also used to translate games into other languages  to modify existing games  and in the development process of \"home brew\" DIY demos and in the creation of new games for older systems  The Internet has helped in the spread of console emulators  as most - if not all - would be unavailable for sale in retail outlets   Examples of console emulators that have been released in the last few decades are  RPCS3  Dolphin  Cemu  PCSX2  PPSSPP  ZSNES  Citra  ePSXe  Project64  Visual Boy Advance  Nestopia  and Yuzu     Terminal emulators are software programs that provide modern computers and devices interactive access to applications running on mainframe computer operating systems or other host systems such as HP-UX or OpenVMS   Terminals such as the IBM 3270 or VT100 and many others are no longer produced as physical devices  Instead  software running on modern operating systems simulates a \"dumb\" terminal and is able to render the graphical and text elements of the host application  send keystrokes and process commands using the appropriate terminal protocol  Some terminal emulation applications include Attachmate Reflection  IBM Personal Communications  and Micro Focus Rumba     Due to their popularity  emulators have been impersonated by malware  Most of these emulators are for video game consoles like the Xbox 360  Xbox One  Nintendo 3DS  etc  Generally such emulators make currently impossible  claims such as being able to run Xbox One and Xbox 360 games in a single program  27     As computers and global computer networks continued to advance and emulator developers grew more skilled in their work  the length of time between the commercial release of a console and its successful emulation began to shrink  Fifth generation consoles such as Nintendo 64  PlayStation and sixth generation handhelds  such as the Game Boy Advance  saw significant progress toward emulation during their production  This led to an effort by console manufacturers to stop unofficial emulation  but consistent failures such as Sega v  Accolade 977 F 2d 1510  9th Cir  1992   Sony Computer Entertainment  Inc  v  Connectix Corporation 203 F 3d 596  2000   and Sony Computer Entertainment America v  Bleem 214 F 3d 1022  2000   28  have had the opposite effect  According to all legal precedents  emulation is legal within the United States  However  unauthorized distribution of copyrighted code remains illegal  according to both country-specific copyright and international copyright law under the Berne Convention  29  better xa0source xa0needed  Under United States law  obtaining a dumped copy of the original machine  s BIOS is legal under the ruling Lewis Galoob Toys  Inc  v  Nintendo of America  Inc   964 F 2d 965  9th Cir  1992  as fair use as long as the user obtained a legally purchased copy of the machine  To mitigate this however  several emulators for platforms such as Game Boy Advance are capable of running without a BIOS file  using high-level emulation to simulate BIOS subroutines at a slight cost in emulation accuracy  citation needed     Construction and management simulation  CMS   1  sometimes also called management sim or building sim  is a subgenre of simulation game in which players build  expand or manage fictional communities or projects with limited resources  2  Strategy video games sometimes incorporate CMS aspects into their game economy  as players must manage resources while expanding their project  Pure CMS games differ from strategy games  however  in that \"the player  s goal is not to defeat an enemy  but to build something within the context of an ongoing process \" 1  Games in this category are sometimes also called \"management games\"  3  4  5     SimCity  1989  represents an early example of success in the genre  Other games in the genre range from city-building games like Caesar  since 1992   The Settlers  since 1993   the Anno series  since 1998   mixed business/politics/building games like Tropico  since 2001   pure business simulation games like Capitalism  and niche simulations like Theme Park     CMSs are often called \"simulation games\" for short  Although games can simulate many activities from vehicles to sports  players usually deduce the kind of simulation from the title of the game     Economics play a primary role in construction and management simulations  because they allow players to build things while operating within economic constraints  6  Some games may challenge the player to explore or recognize patterns  but the majority of the game challenges are economic in that they focus upon growth  6  These games are based in a setting where an economy can be built and managed  usually some kind of community  institution  or empire  2  The player  s role seldom corresponds to a real life activity  since the player is usually more involved in detailed decisions than a real manager or administrator  6  Players usually have two types of tools at their disposal  tools for building and tools for managing  1     Construction mechanisms in CMSs tend to be one of two types  plan-and-build where the construction is completed gradually  or purchase and place where the construction appears immediately  6  Random disasters can also create new construction challenges  1  and some games impose constraints on how things must be constructed  6   But usually the act of construction is quite simple  6  and the main challenge of a CMS is obtaining the resources required to complete construction  1  Players must manage resources within a growing economy  where resources are produced  consumed  and exchanged  1  Resources are drawn from a source  such as money from a bank  or gold from a mine  Some CMSs allow players to convert resources from one type to another  such as fermenting sugar into rum  1  Common resources include money  people  and building materials  6  Resources are utilized in one of two ways  either construction  where players build or buy things to serve some purpose  or maintenance  where players must make ongoing payments to prevent loss or decay  1  Sometimes demolishing a structure will cost resources  but this is often done at no cost  6     CMSs are usually single player games  as competition would force players to eschew creativity in favor of efficiency  and a race to accumulate resources  6  They typically have a free-form construction mode where players can build up as they see fit  which appeals to a player  s sense of creativity and desire for control  6  As such  many CMSs have no victory condition  although players can always lose by bankrupting themselves of resources  1  These games emphasize growth  and the player must successfully manage their economy in order to construct larger creations and gain further creative power  6     Unlike other genres  construction and management simulations seldom offer a progression in storyline  and the level design is a simple space where the player can build  Some games offer pre-built scenarios  which include victory conditions such as reaching a level of wealth  or surviving worsening conditions  But success in one scenario seldom affects another scenario  and players can usually try them in any order  6     Because the player must manage a complex internal economy  construction and management simulations frequently make use of a windowed interface  6  In contrast to genres such as action games  CMS players are given computer-like controls such as pull-down menus and buttons  Players may also understand the game economy through graphs and other analytic tools  This often includes advisers that warn players of problems and describe current needs  6  As such  CMS games have some of the most complex interfaces of any game type  1  These games can be quite popular even without the latest graphics  1     The player in a CMS is usually omnipresent  and does not have an avatar  As such  the player is usually given an isometric perspective of the world  or a free-roaming camera from an aerial viewpoint for modern 3D games  6  The game world often contains units and people who respond to the players   actions  but are seldom given direct orders  6     The Sumerian Game  1964   a text-based early mainframe game designed by Mabel Addis  was an economic simulation game based on the ancient Sumerian city-state of Lagash  7  It was adapted into The Sumer Game  a later version of which was called Hamurabi  a relatively simple text-only game originally written for the DEC PDP-8 in which the player controlled the economy of a city-state     Utopia was released in 1982 for the Intellivision  and is credited as the game that spawned the construction and management simulation genre  8  Utopia put players in charge of an island  allowing them to control its entire military and economy  The population had to be kept happy  and the military had to be strong enough to thwart attacks from rebels and pirates  This game required complex thought in an era where most games were about reflexes  The game sold fairly well  and it had an influence on games of all genres  9     In 1983  Koei released the historical simulation game Nobunaga  s Ambition  where the player takes the role of the historical figure Oda Nobunaga and must conquer  unify and manage the nation of Japan  It combines number crunching  Japanese history  and grand strategy simulation  including elements such as raising taxes and giving rice to prefectures  10  Nobunaga  s Ambition went on to define and set the standard for most console simulation games  11  and has had many sequels  while Koei continued to create other simulation games since  including the Romance of the Three Kingdoms series from 1986 and Bandit Kings of Ancient China in 1989  That same year  Capcom released a simulation game of their own  Destiny of an Emperor  also based on Chinese history  10     Utopia had a notable influence on SimCity in 1989  9  which is considered the first highly successful construction and management simulation  6  8  The game allows players to build a city from the ground up and then manage it  with challenges such as balancing a budget and maintaining popular opinion  and was considered a sophisticated simulation of city planning when it was released  12  It appealed to a wide audience in part because it was not a typical high-speed  violent game  6  and was notable for shunning a traditional win-or-lose game paradigm  12  SimCity has spawned numerous successful sequels and spinoffs  13   is credited with inventing the city-building subgenre  14  SimCity also led to several other successful games in the same mold such as SimTower and SimFarm  6  and launched its designer Will Wright into a position as one of the most influential people in the game industry  15  These games influenced the eventual release of the Tycoon series of games  which are also an important part of the genre  6     Several more specific genres have developed over time     City-building games are a subgenre of CMS games where the player acts as a city-planner or leader  Players normally look at the city from a point of view high in the sky  to grow and manage a simulated city  Players are only allowed to control building placement and city management features such as salaries and work priorities  while actual building is done by game citizens who are non-playable characters  The SimCity series  the Caesar series  and Cities  Skylines are examples of city-building games     Colony management games are similar to city-building games  but based on the player starting a colony in an isolated location with limited resources  and thus are required to collect and combine resources from the local area to build out their colony  in contrast to city-building games where resources are only limited by available city finances  These games utilize construction and management extensively  with incredible detail in more aspects of the game than other simulation genres  They also may feature combat against hostile entities of the remote environment  typically not a feature of other subgenres  and often require the player to consider fortifications  so this genre may be named \"base building games\" The colony management genre has fewer titles compared to the other subgenres due to the niche market  16  Notable titles include Dwarf Fortress  Rimworld  which takes inspiration from Dwarf Fortress   Oxygen Not Included  Frostpunk  and Surviving Mars  17  16     Business simulation games  also known as tycoon games  are a subset of CMSs that simulate a business or some other economy  with the goal for the player to help make the business financially successful      Some business games typically involve more management than construction  allowing the player to invest in virtual stock markets or similar trade systems  Rather than investing in physical buildings  construction can be abstract  such as purchasing stocks  The closest example of a   pure   economic simulation may be Capitalism  the goal of which is to build an industrial and financial empire      At a smaller and more concrete scale  a business simulation may put the player in charge of a business or commercial facility  designing its layouts  hiring staff  and implementing policies to manage the flow of customers and orders as the business grows  Such business simulations include Theme Hospital  Sim Tower  and Game Dev Story  One popular area for these simulations has included theme park management  including the early titles Theme Park and RollerCoaster Tycoon  which not only give the player the ability to manage the park but plan out and test the rides  18  19  20  Other niche business simulations include PC Building Simulator  which lets the player manage a small business dedicated towards building custom PCs and troubleshooting computer errors      These games also may deal at a somewhat larger scale involving managing business elements across a region or nation  which often will include managing the transport of goods between various destinations in addition to other business decisions  Examples include Transport Tycoon  Railroad Tycoon  and the A-Train series     Business and tycoon games need not present realistic business settings  as long as the player is still tasked to managing a facility of sorts with some type of economic factors  Dungeon Keeper and Evil Genius have the player as an evil overlord managing and expanding their base of henchmen from forces of good with limited resources and economies  for example     Active development of Internet technologies and the growth of the Internet audience in recent years gave a powerful impetus to the development of the industry of online games  and in particular  online business simulations  There are many varieties of online business simulationsbrowser-based and downloadable  single-player and multiplayer  real-time and turn-based  Among the most notable online business simulations such as Virtonomics or IndustryPlayer     A subset of life simulation games incorporate elements of business simulation games  but which the player-character has an active role in the game  s world and often tasked with activities similar to real-life functions in a business as to keep the business going  These life simulation games deemphasis the business and management elements though the player often still needs to make decisions on purchases and how they manage their time in game to be successful  Examples of such games include The Sims series  the Story of Seasons series  the Animal Crossing series  and Stardew Valley     Factory simulation or optimization games typically involve the efficient conversion of resources into products through a combination of manned workforce and automated systems  21  22  Some of these are closer to business simulations  where the player is challenged to make products in a cost-effective manner by establishing production lines to compete with other virtual competitors  such as in Big Pharma and Good Company  23  Other factory simulators are based on open-world survival games  with the goal to build enough parts from raw materials found in the world while fending off hostiles to be able to escape or achieve a very large-scale production target  such as in Factorio  Satisfactory  and Dyson Sphere Program  24  Other examples of this genre are Mindustry that combines factory simulation with the Tower defense genre and Shapez io  which has adopted the minimalistic art style of the  io Browser games     A government simulation or political simulation is a game that attempts to simulate the government and politics of all or part of a nation  These games may include geopolitical situations  involving the formation and execution of foreign policy   the creation of domestic political policies  or the simulation of political campaigns  Early examples from the Cold War era include Balance of Power  Crisis in the Kremlin  Conflict  Middle East Political Simulator  An early example of online play was Diplomacy  originally published as a board game in 1959  which was one of the first games to be played via e-mail     Sports management games have the player as the owner or team manager of a sports team  and guides decisions related to training  player selection  and other facets of the team as they progress through a season  ideally guiding the team towards a championship title  In some games  the management facets are layered atop the actual sports simulation  as in the case of Electronic Arts   FIFA or Madden NFL series  so that players can also play within the games as one of the athletes on the field  as well as manage the team through a season  Other sports management games  such as the Football Manager series  do not give player direct control on the actual sports matches  but may allow the player  as the team manager  to influence how they are played out  or otherwise simply simulate the games   results based on the team  s composition set by the player     An electrical network is an interconnection of electrical components  e g   batteries  resistors  inductors  capacitors  switches  transistors  or a model of such an interconnection  consisting of electrical elements  e g   voltage sources  current sources  resistances  inductances  capacitances     An electrical circuit is a network consisting of a closed loop  giving a return path for the current  Linear electrical networks  a special type consisting only of sources  voltage or current   linear lumped elements  resistors  capacitors  inductors   and linear distributed elements  transmission lines   have the property that signals are linearly superimposable   They are thus more easily analyzed  using powerful frequency domain methods such as Laplace transforms  to determine DC response  AC response  and transient response     A resistive circuit is a circuit containing only resistors and ideal current and voltage sources   Analysis of resistive circuits is less complicated than analysis of circuits containing capacitors and inductors   If the sources are constant  DC  sources  the result is a DC circuit  The effective resistance and current distribution properties of arbitrary resistor networks can be modeled in terms of their graph measures and geometrical properties  1     A network that contains active electronic components is known as an electronic circuit  Such networks are generally nonlinear and require more complex design and analysis tools     An active network contains at least one voltage source or current source that can supply energy to the network indefinitely  A passive network  does not contain an active source     An active network contains one or more sources of electromotive force  Practical examples of such sources include a battery or a generator  Active elements can inject power to the circuit  provide power gain  and control the current flow within the circuit     Passive networks do not contain any sources of electromotive force  They consist of passive elements like resistors and capacitors     A network is linear if its signals obey the principle of superposition  otherwise it is non-linear   Passive networks are generally taken to be linear  but there are exceptions   For instance  an inductor with an iron core can be driven into saturation if driven with a large enough current   In this region  the behaviour of the inductor is very non-linear     Discrete passive components  resistors  capacitors and inductors  are called lumped elements because all of their  respectively  resistance  capacitance and inductance is assumed to be located  \"lumped\"  at one place   This design philosophy is called the lumped-element model and networks so designed are called lumped-element circuits   This is the conventional approach to circuit design   At high enough frequencies  or for long enough circuits  such as power transmission lines   the lumped assumption no longer holds because there is a significant fraction of a wavelength across the component dimensions   A new design model is needed for such cases called the distributed-element model   Networks designed to this model are called distributed-element circuits     A distributed-element circuit that includes some lumped components is called a semi-lumped design   An example of a semi-lumped circuit is the combline filter     Sources can be classified as independent sources and dependent sources     An ideal independent source maintains the same voltage or current regardless of the other elements present in the circuit  Its value is either constant  DC  or sinusoidal  AC   The strength of voltage or current is not changed by any variation in the connected network     Dependent sources depend upon a particular element of the circuit for delivering the power or voltage or current depending upon the type of source it is     Electrical laws    A number of electrical laws apply to all electrical networks  These include    \"Kirchhoff s current law  The sum of all currents entering a node is equal to the sum of all currents leaving the node  n\"  \"Kirchhoff s voltage law  The directed sum of the electrical potential differences around a loop must be zero  n\"  \"Ohm s law  The voltage across a resistor is equal to the product of the resistance and the current flowing through it  n\"  \"Norton s theorem  Any network of voltage or current sources and resistors is electrically equivalent to an ideal current source in parallel with a single resistor  n\"  \"Thvenin s theorem  Any network of voltage or current sources and resistors is electrically equivalent to a single voltage source in series with a single resistor  n\"   Superposition theorem  In a linear network with several independent sources  the response in a particular branch when all the sources are acting simultaneously is equal to the linear sum of individual responses calculated by taking one independent source at a time     Other more complex laws may be needed if the network contains nonlinear or reactive components  Non-linear self-regenerative heterodyning systems can be approximated  Applying these laws results in a set of simultaneous equations that can be solved either algebraically or numerically      n     n n         n n             n        To design any electrical circuit  either analog or digital  electrical engineers need to be able to predict the voltages and currents at all places within the circuit   Simple linear circuits can be analyzed by hand using complex number theory  In more complex cases the circuit may be analyzed with specialized computer programs or estimation techniques such as the piecewise-linear model     Circuit simulation software  such as HSPICE  an analog circuit simulator   2  and languages such as VHDL-AMS and verilog-AMS allow engineers to design circuits without the time  cost and risk of error involved in building circuit prototypes     More complex circuits can be analyzed numerically with software such as SPICE or GNUCAP  or symbolically using software such as SapWin     When faced with a new circuit  the software first tries to find a steady state solution  that is  one where all nodes conform to Kirchhoff  s current law and the voltages across and through each element of the circuit conform to the voltage/current equations governing that element     Once the steady state solution is found  the operating points of each element in the circuit are known   For a small signal analysis  every non-linear element can be linearized around its operation point to obtain the small-signal estimate of the voltages and currents   This is an application of Ohm  s Law   The resulting linear circuit matrix can be solved with Gaussian elimination     Software such as the PLECS interface to Simulink uses piecewise-linear approximation of the equations governing the elements of a circuit   The circuit is treated as a completely linear network of ideal diodes   Every time a diode switches from on to off or vice versa  the configuration of the linear network changes   Adding more detail to the approximation of equations increases the accuracy of the simulation  but also increases its running time     In mathematics  a partial differential equation  PDE  is an equation which imposes relations between the various partial derivatives of a multivariable function     The function is often thought of as an \"unknown\" to be solved for  similarly to how x is thought of as an unknown number  to be solved for  in an algebraic equation like x2  3x + 2 = 0  However  it is usually impossible to write down explicit formulas for solutions of partial differential equations  There is  correspondingly  a vast amount of modern mathematical and scientific research on methods to numerically approximate solutions of certain partial differential equations using computers  Partial differential equations also occupy a large sector of pure mathematical research  in which the usual questions are  broadly speaking  on the identification of general qualitative features of solutions of various partial differential equations  citation needed  Among the many open questions are the existence and smoothness of solutions to the NavierStokes equations  named as one of the Millennium Prize Problems in 2000     Partial differential equations are ubiquitous in mathematically-oriented scientific fields  such as physics and engineering  For instance  they are foundational in the modern scientific understanding of sound  heat  diffusion  electrostatics  electrodynamics  fluid dynamics  elasticity  general relativity  and quantum mechanics  citation needed  They also arise from many purely mathematical considerations  such as differential geometry and the calculus of variations  among other notable applications  they are the fundamental tool in the proof of the Poincar conjecture from geometric topology     Partly due to this variety of sources  there is a wide spectrum of different types of partial differential equations  and methods have been developed for dealing with many of the individual equations which arise  As such  it is usually acknowledged that there is no \"general theory\" of partial differential equations  with specialist knowledge being somewhat divided between several essentially distinct subfields  1     Ordinary differential equations form a subclass of partial differential equations  corresponding to functions of a single variable  Stochastic partial differential equations and nonlocal equations are  as of 2020  particularly widely studied extensions of the \"PDE\" notion  More classical topics  on which there is still much active research  include elliptic and parabolic partial differential equations  fluid mechanics  Boltzmann equations  and dispersive partial differential equations     One says that a function u x  y  z  of three variables is \"harmonic\" or \"a solution of the Laplace equation\" if it satisfies the condition    The nature of this failure can be seen more concretely in the case of the following PDE  for a function v x  y  of two variables  consider the equation    The nature of this choice varies from PDE to PDE  To understand it for any given equation  existence and uniqueness theorems are usually important organizational principles  In many introductory textbooks  the role of existence and uniqueness theorems for ODE can be somewhat opaque  the existence half is usually unnecessary  since one can directly check any proposed solution formula  while the uniqueness half is often only present in the background in order to ensure that a proposed solution formula is as general as possible  By contrast  for PDE  existence and uniqueness theorems are often the only means by which one can navigate through the plethora of different solutions at hand  For this reason  they are also fundamental when carrying out a purely numerical simulation  as one must have an understanding of what data is to be prescribed by the user and what is to be left to the computer to calculate     To discuss such existence and uniqueness theorems  it is necessary to be precise about the domain of the \"unknown function \" Otherwise  speaking only in terms such as \"a function of two variables \" it is impossible to meaningfully formulate the results  That is  the domain of the unknown function must be regarded as part of the structure of the PDE itself     The following provides two classic examples of such existence and uniqueness theorems  Even though the two PDE in question are so similar  there is a striking difference in behavior  for the first PDE  one has the free prescription of a single function  while for the second PDE  one has the free prescription of two functions     Even more phenomena are possible  For instance  the following PDE  arising naturally in the field of differential geometry  illustrates an example where there is a simple and completely explicit solution formula  but with the free choice of only three numbers and not even one function     In contrast to the earlier examples  this PDE is nonlinear  owing to the square roots and the squares  A linear PDE is one such that  if it is homogeneous  the sum of any two solutions is also a solution  and all constant multiples of any solution is also a solution     Well-posedness refers to a common schematic package of information about a PDE  To say that a PDE is well-posed  one must have     This is  by the necessity of being applicable to several different PDE  somewhat vague  The requirement of \"continuity \" in particular  is ambiguous  since there are usually many inequivalent means by which it can be rigorously defined  It is  however  somewhat unusual to study a PDE without specifying a way in which it is well-posed     In a slightly weak form  the CauchyKowalevski theorem essentially states that if the terms in a partial differential equation are all made up of analytic functions  then on certain regions  there necessarily exist solutions of the PDE which are also analytic functions  Although this is a fundamental result  in many situations it is not useful since one cannot easily control the domain of the solutions produced  Furthermore  there are known examples of linear partial differential equations whose coefficients have derivatives of all orders  which are nevertheless not analytic  but which have no solutions at all  this surprising example was discovered by Hans Lewy in 1957  So the Cauchy-Kowalevski theorem is necessarily limited in its scope to analytic functions  This context precludes many phenomena of both physical and mathematical interest     When writing PDEs  it is common to denote partial derivatives using subscripts  For example     The Greek letter  denotes the Laplace operator  if u is a function of variables  then    A PDE is called linear if it is linear in the unknown and its derivatives  For example  for a function u of x and y  a second order linear PDE is of the form    Nearest to linear PDEs are semilinear PDEs  where the highest order derivatives appear only as linear terms  with coefficients that are functions of the independent variables only  The lower order derivatives and the unknown function may appear arbitrarily otherwise  For example  a general second order semilinear PDE in two variables is    In a quasilinear PDE the highest order derivatives likewise appear only as linear terms  but with coefficients possibly functions of the unknown and lower-order derivatives     A PDE without any linearity properties is called fully nonlinear  and possesses nonlinearities on one or more of the highest-order derivatives  An example is the MongeAmpre equation  which arises in differential geometry  2     Elliptic  parabolic  and hyperbolic partial differential equations of order two have been widely studied since the beginning of the twentieth century  However  there are many other important types of PDE  including the Kortewegde Vries equation  There are also hybrids such as the EulerTricomi equation  which vary from elliptic to hyperbolic for different regions of the domain  There are also important extensions of these basic types to higher-order PDE  but such knowledge is more specialized     The elliptic/parabolic/hyperbolic classification provides a guide to appropriate initial and boundary conditions and to the smoothness of the solutions  Assuming uxy = uyx  the general linear second-order PDE in two independent variables has the form    More precisely  replacing x by X  and likewise for other variables  formally this is done by a Fourier transform   converts a constant-coefficient PDE into a polynomial of the same degree  with the terms of the highest degree  a homogeneous polynomial  here a quadratic form  being most significant for the classification     Just as one classifies conic sections and quadratic forms into parabolic  hyperbolic  and elliptic based on the discriminant B2  4AC  the same can be done for a second-order PDE at a given point  However  the discriminant in a PDE is given by B2  AC due to the convention of the xy term being 2B rather than B  formally  the discriminant  of the associated quadratic form  is  2B 2  4AC = 4 B2  AC   with the factor of 4 dropped for simplicity     If there are independent variables x1  x2     xn  a general linear partial differential equation of second order has the form    The classification depends upon the signature of the eigenvalues of the coefficient matrix ai j     The classification of partial differential equations can be extended to systems of first-order equations  where the unknown u is now a vector with m components  and the coefficient matrices A are m by m matrices for  = 1  2     The partial differential equation takes the form    The geometric interpretation of this condition is as follows  if data for u are prescribed on the surface S  then it may be possible to determine the normal derivative of u on S from the differential equation  If the data on S and the differential equation determine the normal derivative of u on S  then S is non-characteristic  If the data on S and the differential equation do not determine the normal derivative of u on S  then the surface is characteristic  and the differential equation restricts the data on S  the differential equation is internal to S     Linear PDEs can be reduced to systems of ordinary differential equations by the important technique of separation of variables  This technique rests on a characteristic of solutions to differential equations  if one can find any solution that solves the equation and satisfies the boundary conditions  then it is the solution  this also applies to ODEs   We assume as an ansatz that the dependence of a solution on the parameters space and time can be written as a product of terms that each depend on a single parameter  and then see if this can be made to solve the problem  3     In the method of separation of variables  one reduces a PDE to a PDE in fewer variables  which is an ordinary differential equation if in one variable  these are in turn easier to solve     This is possible for simple PDEs  which are called separable partial differential equations  and the domain is generally a rectangle  a product of intervals   Separable PDEs correspond to diagonal matrices  thinking of \"the value for fixed x\" as a coordinate  each coordinate can be understood separately     This generalizes to the method of characteristics  and is also used in integral transforms     In special cases  one can find characteristic curves on which the equation reduces to an ODE  changing coordinates in the domain to straighten these curves allows separation of variables  and is called the method of characteristics     More generally  one may find characteristic surfaces     An integral transform may transform the PDE to a simpler one  in particular  a separable PDE  This corresponds to diagonalizing an operator     An important example of this is Fourier analysis  which diagonalizes the heat equation using the eigenbasis of sinusoidal waves     If the domain is finite or periodic  an infinite sum of solutions such as a Fourier series is appropriate  but an integral of solutions such as a Fourier integral is generally required for infinite domains  The solution for a point source for the heat equation given above is an example of the use of a Fourier integral     Often a PDE can be reduced to a simpler form with a known solution by a suitable change of variables  For example  the BlackScholes equation    Inhomogeneous equations clarification needed  can often be solved  for constant coefficient PDEs  always be solved  by finding the fundamental solution  the solution for a point source   then taking the convolution with the boundary conditions to get the solution     This is analogous in signal processing to understanding a filter by its impulse response     The superposition principle applies to any linear system  including linear systems of PDEs  A common visualization of this concept is the interaction of two waves in phase being combined to result in a greater amplitude  for example sin x + sin x = 2 sin x  The same principle can be observed in PDEs where the solutions may be real or complex and additive  If u1 and u2 are solutions of linear PDE in some function space R  then u = c1u1 + c2u2 with any constants c1 and c2 are also a solution of that PDE in the same function space     There are no generally applicable methods to solve nonlinear PDEs  Still  existence and uniqueness results  such as the CauchyKowalevski theorem  are often possible  as are proofs of important qualitative and quantitative properties of solutions  getting these results is a major part of analysis   Computational solution to the nonlinear PDEs  the split-step method  exist for specific equations like nonlinear Schrdinger equation     Nevertheless  some techniques can be used for several types of equations  The h-principle is the most powerful method to solve underdetermined equations  The RiquierJanet theory is an effective method for obtaining information about many analytic overdetermined systems     The method of characteristics can be used in some very special cases to solve nonlinear partial differential equations  5     In some cases  a PDE can be solved via perturbation analysis in which the solution is considered to be a correction to an equation with a known solution  Alternatives are numerical analysis techniques from simple finite difference schemes to the more mature multigrid and finite element methods  Many interesting problems in science and engineering are solved in this way using computers  sometimes high performance supercomputers     From 1870 Sophus Lie  s work put the theory of differential equations on a more satisfactory foundation  He showed that the integration theories of the older mathematicians can  by the introduction of what are now called Lie groups  be referred  to a common source  and that ordinary differential equations which admit the same infinitesimal transformations present comparable difficulties of integration  He also emphasized the subject of transformations of contact     A general approach to solving PDEs uses the symmetry property of differential equations  the continuous infinitesimal transformations of solutions to solutions  Lie theory   Continuous group theory  Lie algebras and differential geometry are used to understand the structure of linear and nonlinear partial differential equations for generating integrable equations  to find its Lax pairs  recursion operators  Bcklund transform and finally finding exact analytic solutions to the PDE     Symmetry methods have been recognized to study differential equations arising in mathematics  physics  engineering  and many other disciplines     The Adomian decomposition method  6  the Lyapunov artificial small parameter method  and his homotopy perturbation method are all special cases of the more general homotopy analysis method  7  These are series expansion methods  and except for the Lyapunov method  are independent of small physical parameters as compared to the well known perturbation theory  thus giving these methods greater flexibility and solution generality     The three most widely used numerical methods to solve PDEs are the finite element method  FEM   finite volume methods  FVM  and finite difference methods  FDM   as well other kind of methods called Meshfree methods  which were made to solve problems where the aforementioned methods are limited  The FEM has a prominent position among these methods and especially its exceptionally efficient higher-order version hp-FEM  Other hybrid versions of FEM and Meshfree methods include the generalized finite element method  GFEM   extended finite element method  XFEM   spectral finite element method  SFEM   meshfree finite element method  discontinuous Galerkin finite element method  DGFEM   Element-Free Galerkin Method  EFGM   Interpolating Element-Free Galerkin Method  IEFGM   etc     The finite element method  FEM   its practical application often known as finite element analysis  FEA   is a numerical technique for finding approximate solutions of partial differential equations  PDE  as well as of integral equations  8  9  The solution approach is based either on eliminating the differential equation completely  steady state problems   or rendering the PDE into an approximating system of ordinary differential equations  which are then numerically integrated using standard techniques such as Euler  s method  RungeKutta  etc     Finite-difference methods are numerical methods for approximating the solutions to differential equations using finite difference equations to approximate derivatives     Similar to the finite difference method or finite element method  values are calculated at discrete places on a meshed geometry  \"Finite volume\" refers to the small volume surrounding each node point on a mesh  In the finite volume method  surface integrals in a partial differential equation that contain a divergence term are converted to volume integrals  using the divergence theorem  These terms are then evaluated as fluxes at the surfaces of each finite volume  Because the flux entering a given volume is identical to that leaving the adjacent volume  these methods conserve mass by design     The energy method is a mathematical procedure that can be used to verify well-posedness of initial-boundary-value-problems  10  In the following example the energy method is used to decide where and which boundary conditions should be imposed such that the resulting IBVP is well-posed  Consider the one-dimensional hyperbolic PDE given by    where                                     0                 displaystyle   alpha   neq 0    is a constant and                     u                 x                 t                          displaystyle u x t     is an unknown function with initial condition                     u                 x                 0                 =        f                 x                          displaystyle u x 0 =f x      Multiplying with                     u                 displaystyle u    and integrating over the domain gives    Using that    Here                                                      displaystyle   |  cdot   |    denotes the standard L2-norm  nFor well-posedness we require that the energy of the solution is non-increasing  i e  that                                                                                   t                                              u                                        2                                  0                 textstyle    frac    partial     partial t    |u  |^ 2   leq 0     which is achieved by specifying                     u                 displaystyle u    at                     x        =        a                 displaystyle x=a    if                             >        0                 displaystyle   alpha >0    and at                     x        =        b                 displaystyle x=b    if                             <        0                 displaystyle   alpha <0     This corresponds to only imposing boundary conditions at the inflow  Note that well-posedness allows for growth in terms of data  initial and boundary  and thus it is sufficient to show that                                                                                   t                                              u                                        2                                  0                 textstyle    frac    partial     partial t    |u  |^ 2   leq 0    holds when all data is set to zero     Some common PDEs    Types of boundary conditions    Various topics    Iterative Stencil Loops  ISLs  are a class of numerical data processing solution 1  nwhich update array elements according to some fixed pattern  called a stencil  2   nThey are most commonly found in  computer simulations  e g  for computational fluid dynamics in the context of scientific and engineering applications   nOther notable examples include solving partial differential equations  1  the Jacobi kernel  the GaussSeidel method  2  image processing 1  and cellular automata  3   nThe regular structure of the arrays sets stencil techniques apart from other modeling methods such as the Finite element method  Most finite difference codes which operate on regular grids can be formulated as ISLs     ISLs perform a sequence of sweeps  called timesteps  through a given array  2  Generally this is a 2- or 3-dimensional regular grid  3  The elements of the arrays are often referred to as cells  In each timestep  all array elements are updated  2  Using neighboring array elements in a fixed pattern  the stencil   each cell  s new value is computed  In most cases boundary values are left unchanged  but in some cases  e g  LBM codes  those need to be adjusted during the computation as well  Since the stencil is the same for each element  the pattern of data accesses is repeated  4     More formally  we may define ISLs as a 5-tuple                              I                 S                           S                      0                                   s                 T                          displaystyle  I S S_ 0  s T     with the following meaning  3     Since I is a k-dimensional integer interval  the array will always have the topology of a finite regular grid  The array is also called simulation space and individual cells are identified by their index                     c                I                 displaystyle c  in I     The stencil is an ordered set of                     l                 displaystyle l    relative coordinates  We can now obtain for each cell                     c                 displaystyle c    the tuple of its neighbors indices                               I                      c                                   displaystyle I_ c     n    Their states are given by mapping the tuple                               I                      c                                   displaystyle I_ c     to the corresponding tuple of states                               N                      i                                   c                          displaystyle N_ i  c      where                               N                      i                                   I                          S                      l                                   displaystyle N_ i   colon I  to S^ l     is defined as follows     This is all we need to define the system  s state for the following time steps                               S                      i            +            1                                                         Z                                k                                  S                 displaystyle S_ i+1   colon   mathbb  Z  ^ k   to S    with                     i                          N                         displaystyle i  in   mathbb  N          Note that                               S                      i                                   displaystyle S_ i     is defined on                                           Z                                k                                   displaystyle   mathbb  Z  ^ k     and not just on                     I                 displaystyle I    since the boundary conditions need to be set  too  Sometimes the elements of                               I                      c                                   displaystyle I_ c     may be defined by a vector addition modulo the simulation space  s dimension to realize toroidal topologies     This may be useful for implementing periodic boundary conditions  which simplifies certain physical models     To illustrate the formal definition  we  ll have a look at how a two dimensional Jacobi iteration can be defined  The update function computes the arithmetic mean of a cell  s four neighbors  In this case we set off with an initial solution of 0  The left and right boundary are fixed at 1  while the upper and lower boundaries are set to 0  After a sufficient number of iterations  the system converges against a saddle-shape     The shape of the neighborhood used during the updates depends on the application itself  The most common stencils are the 2D or 3D versions of the von Neumann neighborhood and Moore neighborhood  The example above uses a 2D von Neumann stencil while LBM codes generally use its 3D variant  Conway  s Game of Life uses the 2D Moore neighborhood  That said  other stencils such as a 25-point stencil for seismic wave propagation 5  can be found  too     Many simulation codes may be formulated naturally as ISLs  Since computing time and memory consumption grow linearly with the number of array elements  parallel implementations of ISLs are of paramount importance to research  6   nThis is challenging since the computations are tightly coupled  because of the cell updates depending on neighboring cells  and most ISLs are memory bound  i e  the ratio of memory accesses to calculations is high   7   nVirtually all current parallel architectures have been explored for executing ISLs efficiently  8   nat the moment GPGPUs have proven to be most efficient  9     Due to both the importance of ISLs to computer simulations and their high computational requirements  there are a number of efforts which aim at creating reusable libraries to support scientists in performing stencil-based computations  The libraries are mostly concerned with the parallelization  but may also tackle other challenges  such as IO  steering and checkpointing  They may be classified by their API     This is a traditional design  The library manages a set of n-dimensional scalar arrays  which the user program may access to perform updates  The library handles the synchronization of the boundaries  dubbed ghost zone or halo   The advantage of this interface is that the user program may loop over the arrays  which makes it easy to integrate legacy code  10   The disadvantage is that the library can not handle cache blocking  as this has to be done within the loops 11   nor wrapping of the API-calls for accelerators  e g  via CUDA or OpenCL   Implementations include Cactus  a physics problem solving environment  and waLBerla     These libraries move the interface to updating single simulation cells  only the current cell and its neighbors are exposed  e g  via getter/setter methods  The advantage of this approach is that the library can control tightly which cells are updated in which order  which is useful not only to implement cache blocking  9   nbut also to run the same code on multi-cores and GPUs  12  This approach requires the user to recompile the source code together with the library  Otherwise a function call for every cell update would be required  which would seriously impair performance  This is only feasible with techniques such as class templates or metaprogramming  which is also the reason why this design is only found in newer libraries  Examples are Physis and LibGeoDecomp     In mathematics  a differential equation is an equation that relates one or more functions and their derivatives  1  In applications  the functions generally represent physical quantities  the derivatives represent their rates of change  and the differential equation defines a relationship between the two  Such relations are common  therefore  differential equations play a prominent role in many disciplines including engineering  physics  economics  and biology     Mainly the study of differential equations consists of the study of their solutions  the set of functions that satisfy each equation   and of the properties of their solutions  Only the simplest differential equations are solvable by explicit formulas  however  many properties of solutions of a given differential equation may be determined without computing them exactly     Often when a closed-form expression for the solutions is not available  solutions may be approximated numerically using computers  The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations  while many numerical methods have been developed to determine solutions with a given degree of accuracy     Differential equations first came into existence with the invention of calculus by Newton and Leibniz  In Chapter 2 of his 1671 work Methodus fluxionum et Serierum Infinitarum  2  Isaac Newton listed three kinds of differential equations     In all these cases  y is an unknown function of x  or of x1 and x2   and f is a given function     He solves these examples and others using infinite series and discusses the non-uniqueness of solutions     Jacob Bernoulli proposed the Bernoulli differential equation in 1695  3  This is an ordinary differential equation of the form    for which the following year Leibniz obtained solutions by simplifying it  4     Historically  the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d  Alembert  Leonhard Euler  Daniel Bernoulli  and Joseph-Louis Lagrange  5  6  7  8  In 1746  dAlembert discovered the one-dimensional wave equation  and within ten years Euler discovered the three-dimensional wave equation  9     The EulerLagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem  This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time  independent of the starting point  Lagrange solved this problem in 1755 and sent the solution to Euler  Both further developed Lagrange  s method and applied it to mechanics  which led to the formulation of Lagrangian mechanics     In 1822  Fourier published his work on heat flow in Thorie analytique de la chaleur  The Analytic Theory of Heat   10  in which he based his reasoning on Newton  s law of cooling  namely  that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures  Contained in this book was Fourier  s proposal of his heat equation for conductive diffusion of heat  This partial differential equation is now taught to every student of mathematical physics     In classical mechanics  the motion of a body is described by its position and velocity as the time value varies  Newton  s laws allow these variables to be expressed dynamically  given the position  velocity  acceleration and various forces acting on the body  as a differential equation for the unknown position of the body as a function of time     In some cases  this differential equation  called an equation of motion  may be solved explicitly    \"An example of modeling a real-world problem using differential equations is the determination of the velocity of a ball falling through the air  considering only gravity and air resistance  The ball s acceleration towards the ground is the acceleration due to gravity minus the deceleration due to air resistance  Gravity is considered constant  and air resistance may be modeled as proportional to the ball s velocity  This means that the ball s acceleration  which is a derivative of its velocity  depends on the velocity  and the velocity depends on time   Finding the velocity as a function of time involves solving a differential equation and verifying its validity  n\"   Differential equations can be divided into several types  Apart from describing the properties of the equation itself  these classes of differential equations can help inform the choice of approach to a solution  Commonly used distinctions include whether the equation is ordinary or partial  linear or non-linear  and homogeneous or heterogeneous  This list is far from exhaustive  there are many other properties and subclasses of differential equations which can be very useful in specific contexts     An ordinary differential equation  ODE  is an equation containing an unknown function of one real or complex variable x  its derivatives  and some given functions of x  The unknown function is generally represented by a variable  often denoted y   which  therefore  depends on x  Thus x is often called the independent variable of the equation  The term \"ordinary\" is used in contrast with the term partial differential equation  which may be with respect to more than one independent variable     Linear differential equations are the differential equations that are linear in the unknown function and its derivatives  Their theory is well developed  and in many cases one may express their solutions in terms of integrals     Most ODEs that are encountered in physics are linear  Therefore  most special functions may be defined as solutions of linear differential equations  see Holonomic function      As  in general  the solutions of a differential equation cannot be expressed by a closed-form expression  numerical methods are commonly used for solving differential equations on a computer     A partial differential equation  PDE  is a differential equation that contains unknown multivariable functions and their partial derivatives   This is in contrast to ordinary differential equations  which deal with functions of a single variable and their derivatives   PDEs are used to formulate problems involving functions of several variables  and are either solved in closed form  or used to create a relevant computer model     PDEs can be used to describe a wide variety of phenomena in nature such as sound  heat  electrostatics  electrodynamics  fluid flow  elasticity  or quantum mechanics  These seemingly distinct physical phenomena can be formalized similarly in terms of PDEs  Just as ordinary differential equations often model one-dimensional dynamical systems  partial differential equations often model multidimensional systems  Stochastic partial differential equations generalize partial differential equations for modeling randomness     A non-linear differential equation is a differential equation that is not a linear equation in the unknown function and its derivatives  the linearity or non-linearity in the arguments of the function are not considered here   There are very few methods of solving nonlinear differential equations exactly  those that are known typically depend on the equation having particular symmetries  Nonlinear differential equations can exhibit very complicated behaviour over extended time intervals  characteristic of chaos  Even the fundamental questions of existence  uniqueness  and extendability of solutions for nonlinear differential equations  and well-posedness of initial and boundary value problems for nonlinear PDEs are hard problems and their resolution in special cases is considered to be a significant advance in the mathematical theory  cf  NavierStokes existence and smoothness   However  if the differential equation is a correctly formulated representation of a meaningful physical process  then one expects it to have a solution  11     Linear differential equations frequently appear as approximations to nonlinear equations  These approximations are only valid under restricted conditions  For example  the harmonic oscillator equation is an approximation to the nonlinear pendulum equation that is valid for small amplitude oscillations  see below      Differential equations are described by their order  determined by the term with the highest derivatives  An equation containing only first derivatives is a first-order differential equation  an equation containing the second derivative is a second-order differential equation  and so on  12  13  Differential equations that describe natural phenomena almost always have only first and second order derivatives in them  but there are some exceptions  such as the thin film equation  which is a fourth order partial differential equation     In the first group of examples u is an unknown function of x  and c and  are constants that are supposed to be known  Two broad classifications of both ordinary and partial differential equations consist of distinguishing between linear and nonlinear differential equations  and between homogeneous differential equations and heterogeneous ones     In the next group of examples  the unknown function u depends on two variables x and t or x and y     Solving differential equations is not like solving algebraic equations  Not only are their solutions often unclear  but whether solutions are unique or exist at all are also notable subjects of interest     For first order initial value problems  the Peano existence theorem gives one set of circumstances in which a solution exists  Given any point                              a                 b                          displaystyle  a b     in the xy-plane  define some rectangular region                     Z                 displaystyle Z     such that                     Z        =                 l                 m                                  n                 p                          displaystyle Z= l m   times  p     and                              a                 b                          displaystyle  a b     is in the interior of                     Z                 displaystyle Z     If we are given a differential equation                                                         d              y                                      d              x                                      =        g                 x                 y                          displaystyle    frac  dy  dx  =g x y     and the condition that                     y        =        b                 displaystyle y=b    when                     x        =        a                 displaystyle x=a     then there is locally a solution to this problem if                     g                 x                 y                          displaystyle g x y     and                                                                       g                                                    x                                               displaystyle    frac    partial g    partial x      are both continuous on                     Z                 displaystyle Z     This solution exists on some interval with its center at                     a                 displaystyle a     The solution may not be unique   See Ordinary differential equation for other results      However  this only helps us with first order initial value problems  Suppose we had a linear initial value problem of the nth order     such that    For any nonzero                               f                      n                                   x                          displaystyle f_ n  x      if                                        f                      0                                             f                      1                                                             displaystyle    f_ 0  f_ 1    ldots        and                     g                 displaystyle g    are continuous on some interval containing                               x                      0                                   displaystyle x_ 0                          y                 displaystyle y    is unique and exists  14     The theory of differential equations is closely related to the theory of difference equations  in which the coordinates assume only discrete values  and the relationship involves values of the unknown function or functions and values at nearby coordinates  Many methods to compute numerical solutions of differential equations or study the properties of differential equations involve the approximation of the solution of a differential equation by the solution of a corresponding difference equation     The study of differential equations is a wide field in pure and applied mathematics  physics  and engineering  All of these disciplines are concerned with the properties of differential equations of various types  Pure mathematics focuses on the existence and uniqueness of solutions  while applied mathematics emphasizes the rigorous justification of the methods for approximating solutions  Differential equations play an important role in modeling virtually every physical  technical  or biological process  from celestial motion  to bridge design  to interactions between neurons  Differential equations such as those used to solve real-life problems may not necessarily be directly solvable  i e  do not have closed form solutions  Instead  solutions can be approximated using numerical methods     Many fundamental laws of physics and chemistry can be formulated as differential equations  In biology and economics  differential equations are used to model the behavior of complex systems  The mathematical theory of differential equations first developed together with the sciences where the equations had originated and where the results found application  However  diverse problems  sometimes originating in quite distinct scientific fields  may give rise to identical differential equations  Whenever this happens  mathematical theory behind the equations can be viewed as a unifying principle behind diverse phenomena  As an example  consider the propagation of light and sound in the atmosphere  and of waves on the surface of a pond  All of them may be described by the same second-order partial differential equation  the wave equation  which allows us to think of light and sound as forms of waves  much like familiar waves in the water  Conduction of heat  the theory of which was developed by Joseph Fourier  is governed by another second-order partial differential equation  the heat equation  It turns out that many diffusion processes  while seemingly different  are described by the same equation  the BlackScholes equation in finance is  for instance  related to the heat equation     The number of differential equations that have received a name  in various scientific areas is a witness of the importance of the topic  See List of named differential equations     Some CAS softwares can solve differential equations  These CAS softwares and their commands are worth mentioning     An operational amplifier  often op amp or opamp  is a DC-coupled high-gain electronic voltage amplifier with a differential input and  usually  a single-ended output  1  In this configuration  an op amp produces an output potential  relative to circuit ground  that is typically 100 000 times larger than the potential difference between its input terminals  Operational amplifiers had their origins in analog computers  where they were used to perform mathematical operations in linear  non-linear  and frequency-dependent circuits      The popularity of the op amp as a building block in analog circuits is due to its versatility  By using negative feedback  the characteristics of an op-amp circuit  its gain  input and output impedance  bandwidth etc  are determined by external components and have little dependence on temperature coefficients or engineering tolerance in the op amp itself     Op amps are used widely in electronic devices today  including a vast array of consumer  industrial  and scientific devices  Many standard IC op amps cost only a few cents  however  some integrated or hybrid operational amplifiers with special performance specifications may cost over US$100 in small quantities  2   Op amps may be packaged as components or used as elements of more complex integrated circuits     The op amp is one type of differential amplifier  Other types of differential amplifier include the fully differential amplifier  similar to the op amp  but with two outputs   the instrumentation amplifier  usually built from three op amps   the isolation amplifier  similar to the instrumentation amplifier  but with tolerance to common-mode voltages that would destroy an ordinary op amp   and negative-feedback amplifier  usually built from one or more op amps and a resistive feedback network     \"The amplifier s differential inputs consist of a non-inverting input  +  with voltage V+ and an inverting input    with voltage V  ideally the op amp amplifies only the difference in voltage between the two  which is called the differential input voltage  The output voltage of the op amp Vout is given by the equation n\"   where AOL is the open-loop gain of the amplifier  the term \"open-loop\" refers to the absence of an external feedback loop from the output to the input      The magnitude of AOL is typically very  large  100 000 or more for integrated circuit op amps   and therefore even a quite small difference between V+ and V drives the amplifier into clipping or saturation  The magnitude of AOL is not well controlled by the manufacturing process  and so it is impractical to use an open-loop amplifier as a stand-alone differential amplifier     Without negative feedback  and optionally positive feedback for regeneration  an op amp acts as a comparator  If the inverting input is held at ground  0 xa0V   and the input voltage Vin applied to the non-inverting input is positive  the output will be maximum positive  if Vin is negative  the output will be maximum negative  Since there is no feedback from the output to either input  this is an open-loop circuit acting as a comparator     If predictable operation is desired  negative feedback is used  by applying a portion of the output voltage to the inverting input  The closed-loop feedback greatly reduces the gain of the circuit   When negative feedback is used  the circuit  s overall gain and response is determined primarily by the feedback network  rather than by the op-amp characteristics  If the feedback network is made of components with values small relative to the op amp  s input impedance  the value of the op amp  s open-loop response AOL does not seriously affect the circuit  s performance  In this context  high input impedance at the input terminals and low output impedance at the output terminal s  are particularly useful features of an op amp      The response of the op-amp circuit with its input  output  and feedback circuits to an input is characterized mathematically by a transfer function  designing an op-amp circuit to have a desired transfer function is in the realm of electrical engineering   The transfer functions are important in most applications of op amps  such as in analog computers     In the non-inverting amplifier on the right  the presence of negative feedback via the voltage divider Rf  Rg determines the closed-loop gain ACL xa0= Vout / Vin  Equilibrium will be established when Vout is just sufficient to pull the inverting input to the same voltage as Vin  The voltage gain of the entire circuit is thus 1 + Rf / Rg  As a simple example  if Vin xa0= 1 xa0V and Rf xa0= Rg  Vout will be 2 xa0V  exactly the amount required to keep V at 1 xa0V  Because of the feedback provided by the Rf  Rg network  this is a closed-loop circuit     Another way to analyze this circuit proceeds by making the following  usually valid  assumptions  3     The input signal Vin appears at both  +  and    pins  resulting in a current i through Rg equal to Vin / Rg    \"Since Kirchhoff s current law states that the same current must leave a node as enter it  and since the impedance into the    pin is near infinity  we can assume practically all of the same current i flows through Rf  creating an output voltage n\"   By combining terms  we determine the closed-loop gain ACL     An ideal op amp is usually considered to have the following characteristics  4  5     These ideals can be summarized by the two \"golden rules\"     The first rule only applies in the usual case where the op amp is used in a closed-loop design  negative feedback  where there is a signal path of some sort feeding back from the output to the inverting input    These rules are commonly used as a good first approximation for analyzing or designing op-amp circuits  6  177    None of these ideals can be perfectly realized  A real op amp may be modeled with non-infinite or non-zero parameters using equivalent resistors and capacitors in the op-amp model  The designer can then include these effects into the overall performance of the final circuit  Some parameters may turn out to have negligible effect on the final design while others represent actual limitations of the final performance that must be evaluated     Real op amps differ from the ideal model in various aspects     Real operational amplifiers suffer from several non-ideal effects     The op-amp gain calculated at DC does not apply at higher frequencies  Thus  for high-speed operation  more sophisticated considerations must be used in an op-amp circuit design     Modern integrated FET or MOSFET op amps approximate more closely the ideal op amp than bipolar ICs when it comes to input impedance and input bias currents  Bipolars are generally better when it comes to input voltage offset  and often have lower noise  Generally  at room temperature  with a fairly large signal  and limited bandwidth  FET and MOSFET op amps now offer better performance     Sourced by many manufacturers  and in multiple similar products  an example of a bipolar transistor operational amplifier is the 741 integrated circuit designed in 1968 by David Fullagar at Fairchild Semiconductor after Bob Widlar  s LM301 integrated circuit design  11   nIn this discussion  we use the parameters of the hybrid-pi model to characterize the small-signal  grounded emitter characteristics of a transistor  In this model  the current gain of a transistor is denoted hfe  more commonly called the   12     A small-scale integrated circuit  the 741 op amp shares with most op amps an internal structure consisting of three gain stages  13     Additionally  it contains current mirror  outlined red  bias circuitry and compensation capacitor  30 xa0pF      The input stage consists of a cascaded differential amplifier  outlined in blue  followed by a current-mirror active load   This constitutes a transconductance amplifier  turning a differential voltage signal at the bases of Q1  Q2 into a current signal into the base of Q15     It entails two cascaded transistor pairs  satisfying conflicting requirements   nThe first stage consists of the matched NPN emitter follower pair Q1  Q2 that provide high input impedance  The second is the matched PNP common-base pair Q3  Q4 that eliminates the undesirable Miller effect  it drives an active load Q7 plus matched pair Q5  Q6     That active load is implemented as a modified Wilson current mirror  its role is to convert the  differential  input current signal to a single-ended signal without the attendant 50% losses  increasing the op amp  s open-loop gain by 3 xa0dB   nb 5   nThus  a small-signal differential current in Q3 versus Q4 appears summed  doubled  at the base of Q15  the input of the voltage gain stage      The  class-A  voltage gain stage  outlined in magenta  consists of the two NPN transistors Q15/Q19 connected in a Darlington configuration and uses the output side of current mirror Q12/Q13 as its collector  dynamic  load to achieve its high voltage gain  The output sink transistor Q20 receives its base drive from the common collectors of Q15 and Q19  the level-shifter Q16 provides base drive for the output source transistor Q14      The transistor Q22 prevents this stage from delivering excessive current to Q20 and thus limits the output sink current     The output stage  Q14  Q20  outlined in cyan  is a Class AB complementary-symmetry amplifier  It provides an output drive with impedance of ~50 xa0  in essence  current gain   nTransistor Q16  outlined in green  provides the quiescent current for the output transistors  and Q17 provides output current limiting      Provide appropriate quiescent current for each stage of the op amp     The resistor  39 xa0k  connecting the  diode-connected  Q11 and Q12  and the given supply voltage  VS+ xa0 xa0VS   determine the current in the current mirrors   matched pairs  Q10/Q11 and Q12/Q13  The collector current of Q11  i11  39 xa0k = VS+  VS  2 u202fVBE  For the typical VS = 20 xa0V  the standing current in Q11/Q12  as well as in Q13  would be ~1 xa0mA  A supply current for a typical 741 of about 2 xa0mA agrees with the notion that these two bias currents dominate the quiescent supply current     Transistors Q11 and Q10 form a Widlar current mirror  with quiescent current in Q10 i10 such that ln i11 / i10  = i10  5 xa0k / 28 xa0mV  where 5 xa0k represents the emitter resistor of Q10  and 28 xa0mV is VT  the thermal voltage at room temperature  In this case i10  20 xa0A     The biasing circuit of this stage is set by a feedback loop that forces the collector currents of Q10 and Q9 to  nearly  match  The small difference in these currents provides the drive for the common base of Q3/Q4  note that the base drive for input transistors Q1/Q2 is the input bias current and must be sourced externally   The summed quiescent currents of Q1/Q3 plus Q2/Q4 is mirrored from Q8 into Q9  where it is summed with the collector current in Q10  the result being applied to the bases of Q3/Q4     The quiescent currents of  Q1/Q3  resp   Q2/Q4  i1 will thus be half of i10  of order ~10 xa0A  Input bias current for the base of Q1  resp  Q2  will amount to i1 /   typically ~50 xa0nA  implying a current gain hfe  200 for Q1 Q2      This feedback circuit tends to draw the common base node of Q3/Q4 to a voltage Vcom  2 u202fVBE  where Vcom is the input common-mode voltage  At the same time  the magnitude of the quiescent current is relatively insensitive to the characteristics of the components Q1Q4  such as hfe  that would otherwise cause temperature dependence or part-to-part variations     Transistor Q7 drives Q5 and Q6 into conduction until their  equal  collector currents match that of Q1/Q3 and Q2/Q4  The quiescent current in Q7 is VBE / 50 xa0k  about 35 xa0A  as is the quiescent current in Q15  with its matching operating point  Thus  the quiescent currents are pairwise matched in Q1/Q2  Q3/Q4  Q5/Q6  and Q7/Q15      Quiescent currents in Q16 and Q19 are set by the current mirror Q12/Q13  which is running at ~1 xa0mA  Through some vague  mechanism  the collector current in Q19 tracks that standing current     In the circuit involving Q16  variously named rubber diode or VBE multiplier   the 4 5 xa0k resistor must be conducting about 100 xa0A  with the Q16 VBE roughly 700 xa0mV  Then the VCB must be about 0 45 xa0V and VCE at about 1 0 xa0V  Because the Q16 collector is driven by a current source and the Q16 emitter drives into the Q19 collector current sink  the Q16 transistor establishes a voltage difference between Q14 base and Q20 base of ~1 xa0V  regardless of the common-mode voltage of Q14/Q20 base  The standing current in Q14/Q20 will be a factor exp 100 xa0mV / VT   36 smaller than the 1 xa0mA quiescent current in the class A portion of the op amp  This  small  standing current in the output transistors establishes the output stage in class AB operation and reduces the crossover distortion of this stage      A small differential input voltage signal gives rise  through multiple stages of current amplification  to a much larger voltage signal on output     The input stage with Q1 and Q3 is similar to an emitter-coupled pair  long-tailed pair   with Q2 and Q4 adding some degenerating impedance  The input impedance is relatively high because of the small current through Q1-Q4   A typical 741 op amp has a differential input impedance of about 2 M   The common mode input impedance is even higher  as the input stage works at an essentially constant current     A differential voltage Vin at the op amp inputs  pins 3 and 2  respectively  gives rise to a small differential current in the bases of Q1 and Q2 iin  Vin /  2hiehfe    This differential base current causes a change in the differential collector current in each leg by iinhfe  Introducing the transconductance of Q1  gm = hfe / hie  the  small-signal  current at the base of Q15  the input of the voltage gain stage  is Vingm / 2     This portion of the op amp cleverly changes a differential signal at the op amp inputs to a single-ended signal at the base of Q15  and in a way that avoids wastefully discarding the signal in either leg  To see how  notice that a small negative change in voltage at the inverting input  Q2 base  drives it out of conduction  and this incremental decrease in current passes directly from Q4 collector to its emitter  resulting in a decrease in base drive for Q15  On the other hand  a small positive change in voltage at the non-inverting input  Q1 base  drives this transistor into conduction  reflected in an increase in current at the collector of Q3  This current drives Q7 further into conduction  which turns on current mirror Q5/Q6  Thus  the increase in Q3 emitter current is mirrored in an increase in Q6 collector current  the increased collector currents shunts more from the collector node and results in a decrease in base drive current for Q15  Besides avoiding wasting 3 xa0dB of gain here  this technique decreases common-mode gain and feedthrough of power supply noise      A current signal i at Q15  s base gives rise to a current in Q19 of order i2  the product of the hfe of each of Q15 and Q19  which are connected in a Darlington pair   This current signal develops a voltage at the bases of output transistors Q14/Q20 proportional to the hie of the respective transistor     Output transistors Q14 and Q20 are each configured as an emitter follower  so no voltage gain occurs there  instead  this stage provides current gain  equal to the hfe of Q14  resp  Q20      The output impedance is not zero  as it would be in an ideal op amp  but with negative feedback it approaches zero at low frequencies     The net open-loop small-signal voltage gain of the op amp involves the product of the current gain hfe of some 4 transistors  In practice  the voltage gain for a typical 741-style op amp is of order 200 000  and the current gain  the ratio of input impedance  ~26 M  to output impedance  ~50 xa0  provides yet more  power  gain     The ideal op amp has infinite common-mode rejection ratio  or zero common-mode gain     In the present circuit  if the input voltages change in the same direction  the negative feedback makes Q3/Q4 base voltage follow  with 2 VBE below  the input voltage variations  Now the output part  Q10  of Q10-Q11 current mirror keeps up the common current through Q9/Q8 constant in spite of varying voltage  Q3/Q4 collector currents  and accordingly the output current at the base of Q15  remain unchanged     In the typical 741 op amp  the common-mode rejection ratio is 90 xa0dB  implying an open-loop common-mode voltage gain of about 6     The innovation of the Fairchild A741 was the introduction of frequency compensation via an on-chip  monolithic  capacitor  simplifying application of the op amp by eliminating the need for external components for this function   nThe 30 pF capacitor stabilizes the amplifier via Miller compensation and functions in a manner similar to an op-amp integrator circuit  Also known as   dominant pole compensation   because it introduces a pole that masks  dominates  the effects of other poles into the open loop frequency response  in a 741 op amp this pole can be as low as 10 xa0Hz  where it causes a 3 xa0dB loss of open loop voltage gain      This internal compensation is provided to achieve unconditional stability of the amplifier in negative feedback configurations where the feedback network is non-reactive and the closed loop gain is unity or higher   nBy contrast  amplifiers requiring external compensation  such as the A748  may require external compensation or closed-loop gains significantly higher than unity     The \"offset null\" pins may be used to place external resistors  typically in the form of the two ends of a potentiometer  with the slider connected to VS  in parallel with the emitter resistors of Q5 and Q6  to adjust the balance of the Q5/Q6 current mirror  The potentiometer is adjusted such that the output is null  midrange  when the inputs are shorted together     The transistors Q3  Q4 help to increase the reverse VBE rating  the base-emitter junctions of the NPN transistors Q1 and Q2 break down at around 7 xa0V  but the PNP transistors Q3 and Q4 have VBE breakdown voltages around 50 xa0V  14     Variations in the quiescent current with temperature  or between parts with the same type number  are common  so crossover distortion and quiescent current may be subject to significant variation     The output range of the amplifier is about one volt less than the supply voltage  owing in part to VBE of the output transistors Q14 and Q20     The 25  resistor at the Q14 emitter  along with Q17  acts to limit Q14 current to about 25 mA  otherwise  Q17 conducts no current     Current limiting for Q20 is performed in the voltage gain stage  Q22 senses the voltage across Q19  s emitter resistor  50 xa0   as it turns on  it diminishes the drive current to Q15 base     Later versions of this amplifier schematic may show a somewhat different method of output current limiting     While the 741 was historically used in audio and other sensitive equipment  such use is now rare because of the improved noise performance of more modern op amps  Apart from generating noticeable hiss  741s and other older op amps may have poor common-mode rejection ratios and so will often introduce cable-borne mains hum and other common-mode interference  such as switch   clicks    into sensitive equipment     The \"741\" has come to often mean a generic op-amp IC  such as A741  LM301  558  LM324  TBA221  or a more modern replacement such as the TL071    The description of the 741 output stage is qualitatively similar for many other designs  that may have quite different input stages   except     Op amps may be classified by their construction     IC op amps may be classified in many ways  including     The use of op amps as circuit blocks is much easier and clearer than specifying all their individual circuit elements  transistors  resistors  etc    whether the amplifiers used are integrated or discrete circuits  In the first approximation op amps can be used as if they were ideal differential gain blocks  at a later stage limits can be placed on the acceptable range of parameters for each op amp     Circuit design follows the same lines for all electronic circuits  A specification is drawn up governing what the circuit is required to do  with allowable limits  For example  the gain may be required to be 100 times  with a tolerance of 5% but drift of less than 1% in a specified temperature range  the input impedance not less than one megohm  etc     A basic circuit is designed  often with the help of circuit modeling  on a computer   Specific commercially available op amps and other components are then chosen that meet the design criteria within the specified tolerances at acceptable cost  If not all criteria can be met  the specification may need to be modified     A prototype is then built and tested  changes to meet or improve the specification  alter functionality  or reduce the cost  may be made     That is  the op amp is being used as a voltage comparator  Note that a device designed primarily as a comparator may be better if  for instance  speed is important or a wide range of input voltages may be found  since such devices can quickly recover from full on or full off  \"saturated\"  states    \"A voltage level detector can be obtained if a reference voltage Vref is applied to one of the op amp s inputs  This means that the op amp is set up as a comparator to detect a positive voltage  If the voltage to be sensed  Ei  is applied to op amp s  +  input  the result is a noninverting positive-level detector  when Ei is above Vref  VO equals +Vsat  when Ei is below Vref  VO equals Vsat  If Ei is applied to the inverting input  the circuit is an inverting positive-level detector  When Ei is above Vref  VO equals Vsat  n\"   A zero voltage level detector  Ei = 0  can convert  for example  the output of a sine-wave from a function generator into a variable-frequency square wave  If Ei is a sine wave  triangular wave  or wave of any other shape that is symmetrical around zero  the zero-crossing detector  s output will be square   Zero-crossing detection may also be useful in triggering TRIACs at the best time to reduce mains interference and current spikes     Another typical configuration of op-amps is with positive feedback  which takes a fraction of the output signal back to the non-inverting input  An important application of it is the comparator with hysteresis  the Schmitt trigger   Some circuits may use positive feedback and negative feedback around the same amplifier  for example triangle-wave oscillators and active filters     Because of the wide slew range and lack of positive feedback  the response of all the open-loop level detectors described above will be relatively slow  External overall positive feedback may be applied  but  unlike internal positive feedback that may be applied within the latter stages of a purpose-designed comparator  this markedly affects the accuracy of the zero-crossing detection point  Using a general-purpose op amp  for example  the frequency of Ei for the sine to square wave converter should probably be below 100 xa0Hz  citation needed     In a non-inverting amplifier  the output voltage changes in the same direction as the input voltage     The gain equation for the op amp is    However  in this circuit V is a function of Vout because of the negative feedback through the R1 R2 network   R1 and R2 form a voltage divider  and as V is a high-impedance input  it does not load it appreciably   Consequently    where    Substituting this into the gain equation  we obtain    Solving for                               V                      out                                   displaystyle V_   text out          If                               A                      OL                                   displaystyle A_   text OL      is very large  this simplifies to    The non-inverting input of the operational amplifier needs a path for DC to ground  if the signal source does not supply a DC path  or if that source requires a given load impedance  then the circuit will require another resistor from the non-inverting input to ground  When the operational amplifier  s input bias currents are significant  then the DC source resistances driving the inputs should be balanced  15   The ideal value for the feedback resistors  to give minimal offset voltage  will be such that the two resistances in parallel roughly equal the resistance to ground at the non-inverting input pin   That ideal value assumes the bias currents are well matched  which may not be true for all op amps  16     In an inverting amplifier  the output voltage changes in an opposite direction to the input voltage     As with the non-inverting amplifier  we start with the gain equation of the op amp     This time  V is a function of both Vout and Vin due to the voltage divider formed by Rf and Rin   Again  the op-amp input does not apply an appreciable load  so    Substituting this into the gain equation and solving for                               V                      out                                   displaystyle V_   text out          If                               A                      OL                                   displaystyle A_   text OL      is very large  this simplifies to    A resistor is often inserted between the non-inverting input and ground  so both inputs \"see\" similar resistances   reducing the input offset voltage due to different voltage drops due to bias current  and may reduce distortion in some op amps     A DC-blocking capacitor may be inserted in series with the input resistor when a frequency response down to DC is not needed and any DC voltage on the input is unwanted  That is  the capacitive component of the input impedance inserts a DC zero and a low-frequency pole that gives the circuit a bandpass or high-pass characteristic     The potentials at the operational amplifier inputs remain virtually constant  near ground  in the inverting configuration   The constant operating potential typically results in  distortion levels that are lower than those attainable with the non-inverting topology     Most single  dual and quad op amps available have a standardized pin-out which permits one type to be substituted for another without wiring changes  A specific op amp may be chosen for its open loop gain  bandwidth  noise performance  input impedance  power consumption  or a compromise between any of these factors     1941  A vacuum tube op amp  An op amp  defined as a general-purpose  DC-coupled  high gain  inverting feedback amplifier  is first found in U S  Patent 2 401 779 \"Summing Amplifier\" filed by Karl D  Swartzel Jr  of Bell Labs in 1941  This design used three vacuum tubes to achieve a gain of 90 dB and operated on voltage rails of 350 V  It had a single inverting input rather than differential inverting and non-inverting inputs  as are common in today  s op amps  Throughout World War II  Swartzel  s design proved its value by being liberally used in the M9 artillery director designed at Bell Labs  This artillery director worked with the SCR584 radar system to achieve extraordinary hit rates  near 90%  that would not have been possible otherwise  17     1947  An op amp with an explicit non-inverting input  In 1947  the operational amplifier was first formally defined and named in a paper 18  by John R  Ragazzini of Columbia University  In this same paper a footnote mentioned an op-amp design by a student that would turn out to be quite significant  This op amp  designed by Loebe Julie  was superior in a variety of ways  It had two major innovations  Its input stage used a long-tailed triode pair with loads matched to reduce drift in the output and  far more importantly  it was the first op-amp design to have two inputs  one inverting  the other non-inverting   The differential input made a whole range of new functionality possible  but it would not be used for a long time due to the rise of the chopper-stabilized amplifier  17     1949  A chopper-stabilized op amp  In 1949  Edwin A  Goldberg designed a chopper-stabilized op amp  19  This set-up uses a normal op amp with an additional AC amplifier that goes alongside the op amp  The chopper gets an AC signal from DC by switching between the DC voltage and ground at a fast rate  60 xa0Hz or 400 xa0Hz   This signal is then amplified  rectified  filtered and fed into the op amp  s non-inverting input  This vastly improved the gain of the op amp while significantly reducing the output drift and DC offset  Unfortunately  any design that used a chopper couldn  t use their non-inverting input for any other purpose  Nevertheless  the much improved characteristics of the chopper-stabilized op amp made it the dominant way to use op amps  Techniques that used the non-inverting input regularly would not be very popular until the 1960s when op-amp ICs started to show up in the field     1953  A commercially available op amp  In 1953  vacuum tube op amps became commercially available with the release of the model K2-W from George A  Philbrick Researches  Incorporated   The designation on the devices shown  GAP/R  is an acronym for the complete company name  Two nine-pin 12AX7 vacuum tubes were mounted in an octal package and had a model K2-P chopper add-on available that would effectively \"use up\" the non-inverting input  This op amp was based on a descendant of Loebe Julie  s 1947 design and  along with its successors  would start the widespread use of op amps in industry     1961  A discrete IC op amp  With the birth of the transistor in 1947  and the silicon transistor in 1954  the concept of ICs became a reality  The introduction of the planar process in 1959 made transistors and ICs stable enough to be commercially useful  By 1961  solid-state  discrete op amps were being produced  These op amps were effectively small circuit boards with packages such as edge connectors  They usually had hand-selected resistors in order to improve things such as voltage offset and drift  The P45  1961  had a gain of 94 xa0dB and ran on 15 xa0V rails  It was intended to deal with signals in the range of 10 V     1961  A varactor bridge op amp  There have been many different directions taken in op-amp design  Varactor bridge op amps started to be produced in the early 1960s  20  21  They were designed to have extremely small input current and are still amongst the best op amps available in terms of common-mode rejection with the ability to correctly deal with hundreds of volts at their inputs     1962  An op amp in a potted module  By 1962  several companies were producing modular potted packages that could be plugged into printed circuit boards  citation needed  These packages were crucially important as they made the operational amplifier into a single black box which could be easily treated as a component in a larger circuit     1963  A monolithic IC op amp  In 1963  the first monolithic IC op amp  the A702 designed by Bob Widlar at Fairchild Semiconductor  was released  Monolithic ICs consist of a single chip as opposed to a chip and discrete parts  a discrete IC  or multiple chips bonded and connected on a circuit board  a hybrid IC   Almost all modern op amps are monolithic ICs  however  this first IC did not meet with much success  Issues such as an uneven supply voltage  low gain and a small dynamic range held off the dominance of monolithic op amps until 1965 when the A709 22   also designed by Bob Widlar  was released    \"1968  Release of the A741  The popularity of monolithic op amps was further improved upon the release of the LM101 in 1967  which solved a variety of issues  and the subsequent release of the A741 in 1968  The A741 was extremely similar to the LM101 except that Fairchild s facilities allowed them to include a 30 xa0pF compensation capacitor inside the chip instead of requiring external compensation  This simple difference has made the 741 the canonical op amp and many modern amps base their pinout on the 741s  The A741 is still in production  and has become ubiquitous in electronicsmany manufacturers produce a version of this classic chip  recognizable by part numbers containing 741  The same part is manufactured by several companies  n\"   1970  First high-speed  low-input current FET design  nIn the 1970s high speed  low-input current designs started to be made by using FETs  These would be largely replaced by op amps made with MOSFETs in the 1980s      1972  Single sided supply op amps being produced  A single sided supply op amp is one where the input and output voltages can be as low as the negative power supply voltage instead of needing to be at least two volts above it   The result is that it can operate in many applications with the negative supply pin on the op amp being connected to the signal ground  thus eliminating the need for a separate negative power supply     The LM324  released in 1972  was one such op amp that came in a quad package  four separate op amps in one package  and became an industry standard  In addition to packaging multiple op amps in a single package  the 1970s also saw the birth of op amps in hybrid packages  These op amps were generally improved versions of existing monolithic op amps  As the properties of monolithic op amps improved  the more complex hybrid ICs were quickly relegated to systems that are required to have extremely long service lives or other specialty systems     Recent trends  Recently supply voltages in analog circuits have decreased  as they have in digital logic  and low-voltage op amps have been introduced reflecting this   Supplies of 5 xa0V and increasingly 3 3 xa0V  sometimes as low as 1 8 xa0V  are common   To maximize the signal range modern op amps commonly have rail-to-rail output  the output signal can range from the lowest supply voltage to the highest  and sometimes rail-to-rail inputs  8     In mathematics  a differential-algebraic system of equations  DAEs  is a system of equations that either contains differential equations and algebraic equations  or is equivalent to such a system  Such systems occur as the general form of  systems of  differential equations for vectorvalued functions x in one independent variable t     where                     x                          a                 b                                               R                                n                                   displaystyle x  a b   to   mathbb  R  ^ n     is a vector of dependent variables                     x                 t                 =                           x                      1                                   t                                                     x                      n                                   t                                   displaystyle x t = x_ 1  t    dots  x_ n  t      and the system has as many equations                      F        =                           F                      1                                                              F                      n                                                                  R                                2            n            +            1                                                        R                                n                                   displaystyle F= F_ 1    dots  F_ n     mathbb  R  ^ 2n+1   to   mathbb  R  ^ n      nThey are distinct from ordinary differential equation  ODE  in that a DAE is not completely solvable for the derivatives of all components of the function x because these may not all appear  i e  some equations are algebraic   technically the distinction between an implicit ODE system  that may be rendered explicit  and a DAE system is that the Jacobian matrix                                                                       F                             u                             v                             t                                                                   u                                               displaystyle    frac    partial F u v t     partial u      is a singular matrix for a DAE system  1  This distinction between ODEs and DAEs is made because DAEs have different characteristics and are generally more difficult to solve  2     In practical terms  the distinction between DAEs and ODEs is often that the solution of a DAE system depends on the derivatives of the input signal and not just the signal itself as in the case of ODEs  3  this issue is commonly encountered in systems with hysteresis  4  such as the Schmitt trigger  5     This difference is more clearly visible if the system may be rewritten so that instead of x we consider a pair                              x                 y                          displaystyle  x y     of vectors of dependent variables and the DAE has the form    A DAE system of this form is called semi-explicit  1  Every solution of the second half g of the equation defines a unique direction for x via the first half f of the equations  while the direction for y is arbitrary  But not every point  x y t  is a solution of g  The variables in x and the first half f of the equations get the attribute differential  The components of y and the second half g of the equations are called the algebraic variables or equations of the system   The term algebraic in the context of DAEs only means free of derivatives and is not related to  abstract  algebra      The solution of a DAE consists of two parts  first the search for consistent initial values and second the computation of a trajectory  To find consistent initial values it is often necessary to consider the derivatives of some of the component functions of the DAE  The highest order of a derivative that is necessary for this process is called the differentiation index  The equations derived in computing the index and consistent initial values may also be of use in the computation of the trajectory  A semi-explicit DAE system can be converted to an implicit one by decreasing the differentiation index by one  and vice versa  6     The distinction of DAEs to ODEs becomes apparent if some of the dependent variables occur without their derivatives  The vector of dependent variables may then be written as pair                              x                 y                          displaystyle  x y     and the system of differential equations of the DAE appears in the form    where    As a whole  the set of DAEs is a function    Initial conditions must be a solution of the system of equations of the form    The behaviour of a pendulum of length L with center in  0 0  in Cartesian coordinates  x y  is described by the EulerLagrange equations    where                                      displaystyle   lambda     is a Lagrange multiplier  The momentum variables u and v should be constrained by the law of conservation of energy and their direction should point along the circle  Neither condition is explicit in those equations  Differentiation of the last equation leads to    restricting the direction of motion to the tangent of the circle  The next derivative of this equation implies    and the derivative of that last identity simplifies to                               L                      2                                                                                                                          3        g        v        =        0                 displaystyle L^ 2    dot    lambda   -3gv=0    which implicitly implies the conservation of energy since after integration the constant                     E        =                                            3              2                                      g        y                                                    1              2                                                L                      2                                  =                              1            2                                             u                      2                          +                  v                      2                                   +        g        y                 displaystyle E=   tfrac  3  2  gy-   tfrac  1  2  L^ 2   lambda =   frac  1  2   u^ 2 +v^ 2  +gy    is the sum of kinetic and potential energy     To obtain unique derivative values for all dependent variables the last equation was three times differentiated  This gives a differentiation index of 3  which is typical for constrained mechanical systems     If initial values                                        x                      0                                             u                      0                                            displaystyle  x_ 0  u_ 0      and a sign for y are given  the other variables are determined via                     y        =                                                    L                              2                                                                x                              2                                                             displaystyle y=  pm    sqrt  L^ 2 -x^ 2        and if                     y                0                 displaystyle y  neq 0    then                     v        =                u        x                  /                y                 displaystyle v=-ux/y    and                             =                 g        y                          u                      2                                            v                      2                                             /                          L                      2                                   displaystyle   lambda = gy-u^ 2 -v^ 2  /L^ 2      To proceed to the next point it is sufficient to get the derivatives of x and u  that is  the system to solve is now    This is a semi-explicit DAE of index 1  Another set of similar equations may be obtained starting from                                        y                      0                                             v                      0                                            displaystyle  y_ 0  v_ 0      and a sign for x     DAEs also naturally occur in the modelling of circuits with non-linear devices  Modified nodal analysis employing DAEs is used for example in the ubiquitous SPICE family of numeric circuit simulators  7  Similarly  Fraunhofer  s Analog Insydes Mathematica package can be used to derive DAEs from a netlist and then simplify or even solve the equations symbolically in some cases  8  9  It is worth noting that the index of a DAE  of a circuit  can be made arbitrarily high by cascading/coupling via capacitors operational amplifiers with positive feedback  4     DAE of the form    are called semi-explicit  The index-1 property requires that g is solvable for y  In other words  the differentiation index is 1 if by differentiation of the algebraic equations for t an implicit ODE system results     which is solvable for                                                                  x                                                                                                         y                                                                      displaystyle     dot  x         dot  y       if                     det                                                                                     y                                      g                         x                         y                         t                                                          0                  displaystyle   det   left   partial _ y g x y t   right   neq 0     n    Every sufficiently smooth DAE is almost everywhere reducible to this semi-explicit index-1 form     Two major problems in solving DAEs are index reduction and consistent initial conditions  Most numerical solvers require ordinary differential equations and algebraic equations of the form    It is a non-trivial task to convert arbitrary DAE systems into ODEs for solution by pure ODE solvers  Techniques which can be employed include Pantelides algorithm and dummy derivative index reduction method  Alternatively  a direct solution of high-index DAEs with inconsistent initial conditions is also possible  This solution approach involves a transformation of the derivative elements through orthogonal collocation on finite elements or direct transcription into algebraic expressions  This allows DAEs of any index to be solved without rearrangement in the open equation form    Once the model has been converted to algebraic equation form  it is solvable by large-scale nonlinear programming solvers  see APMonitor      Several measures of DAEs tractability in terms of numerical methods have developed  such as differentiation index  perturbation index  tractability index  geometric index  and the Kronecker index  10  11     We use the                                      displaystyle   Sigma     n-method to analyze a DAE  We construct for the DAE a signature matrix                             =                                                 i                         j                                            displaystyle   Sigma =   sigma _ i j       where each row corresponds to each equation                               f                      i                                   displaystyle f_ i     and each column corresponds to each variable                               x                      j                                   displaystyle x_ j      The entry in position                              i                 j                          displaystyle  i j     is                                                     i                         j                                   displaystyle   sigma _ i j      which denotes the highest order of derivative to which                               x                      j                                   displaystyle x_ j     occurs in                               f                      i                                   displaystyle f_ i      or                                              displaystyle -  infty     if                               x                      j                                   displaystyle x_ j     does not occur in                               f                      i                                   displaystyle f_ i         For the pendulum DAE above  the variables are                                        x                      1                                             x                      2                                             x                      3                                             x                      4                                             x                      5                                   =                 x                 y                 u                 v                                           displaystyle  x_ 1  x_ 2  x_ 3  x_ 4  x_ 5  = x y u v   lambda       The corresponding signature matrix is        In the field of numerical analysis  meshfree methods are those that do not require connection between nodes of the simulation domain  i e  a mesh  but are rather based on interaction of each node with all its neighbors  As a consequence  original extensive properties such as mass or kinetic energy are no longer assigned to mesh elements but rather to the single nodes  Meshfree methods enable the simulation of some otherwise difficult types of problems  at the cost of extra computing time and programming effort  The absence of a mesh allows Lagrangian simulations  in which the nodes can move according to the velocity field     Numerical methods such as the finite difference method  finite-volume method  and finite element method were originally defined on meshes of data points  In such a mesh  each point has a fixed number of predefined neighbors  and this connectivity between neighbors can be used to define mathematical operators like the derivative  These operators are then used to construct the equations to simulatesuch as the Euler equations or the NavierStokes equations     But in simulations where the material being simulated can move around  as in computational fluid dynamics  or where large deformations of the material can occur  as in simulations of plastic materials   the connectivity of the mesh can be difficult to maintain without introducing error into the simulation  If the mesh becomes tangled or degenerate during simulation  the operators defined on it may no longer give correct values  The mesh may be recreated during simulation  a process called remeshing   but this can also introduce error  since all the existing data points must be mapped onto a new and different set of data points  Meshfree methods are intended to remedy these problems  Meshfree methods are also useful for     In a traditional finite difference simulation  the domain of a one-dimensional simulation would be some function                     u                 x                 t                          displaystyle u x t      represented as a mesh of data values                               u                      i                                n                                   displaystyle u_ i ^ n     at points                               x                      i                                   displaystyle x_ i      where    We can define the derivatives that occur in the equation being simulated using some finite difference formulae on this domain  for example    and    Then we can use these definitions of                     u                 x                 t                          displaystyle u x t     and its spatial and temporal derivatives to write the equation being simulated in finite difference form  then simulate the equation with one of many finite difference methods     In this simple example  the steps  here the spatial step                     h                 displaystyle h    and timestep                     k                 displaystyle k     are constant along all the mesh  and the left and right mesh neighbors of the data value at                               x                      i                                   displaystyle x_ i     are the values at                               x                      i                        1                                   displaystyle x_ i-1     and                               x                      i            +            1                                   displaystyle x_ i+1      respectively  Generally in finite differences one can allow very simply for steps variable along the mesh  but all the original nodes should be preserved and they can move independently only by deforming the original elements  If even only two of all the nodes change their order  or even only one node is added to or removed from the simulation  that creates a defect in the original mesh and the simple finite difference approximation can no longer hold     Smoothed-particle hydrodynamics  SPH   one of the oldest meshfree methods  solves this problem by treating data points as physical particles with mass and density that can move around over time  and carry some value                               u                      i                                   displaystyle u_ i     with them  SPH then defines the value of                     u                 x                 t                          displaystyle u x t     between the particles by    where                               m                      i                                   displaystyle m_ i     is the mass of particle                     i                 displaystyle i                                                         i                                   displaystyle   rho _ i     is the density of particle                     i                 displaystyle i     and                     W                 displaystyle W    is a kernel function that operates on nearby data points and is chosen for smoothness and other useful qualities  By linearity  we can write the spatial derivative as    Then we can use these definitions of                     u                 x                 t                          displaystyle u x t     and its spatial derivatives to write the equation being simulated as an ordinary differential equation  and simulate the equation with one of many numerical methods  In physical terms  this means calculating the forces between the particles  then integrating these forces over time to determine their motion     The advantage of SPH in this situation is that the formulae for                     u                 x                 t                          displaystyle u x t     and its derivatives do not depend on any adjacency information about the particles  they can use the particles in any order  so it doesn  t matter if the particles move around or even exchange places     One disadvantage of SPH is that it requires extra programming to determine the nearest neighbors of a particle  Since the kernel function                     W                 displaystyle W    only returns nonzero results for nearby particles within twice the \"smoothing length\"  because we typically choose kernel functions with compact support   it would be a waste of effort to calculate the summations above over every particle in a large simulation  So typically SPH simulators require some extra code to speed up this nearest neighbor calculation     One of the earliest meshfree methods is smoothed particle hydrodynamics  presented in 1977  1  Libersky et al  2  were the first to apply SPH in solid mechanics  The main drawbacks of SPH are inaccurate results near boundaries and tension instability that was first investigated by Swegle  3     In the 1990s a new class of meshfree methods emerged based on the Galerkin method  This first method called the diffuse element method 4   DEM   pioneered by Nayroles et al   utilized the MLS approximation in the Galerkin solution of partial differential equations  with approximate derivatives of the MLS function  Thereafter Belytschko pioneered the Element Free Galerkin  EFG  method  5  which employed MLS with Lagrange multipliers to enforce boundary conditions  higher order numerical quadrature in the weak form  and full derivatives of the MLS approximation which gave better accuracy  Around the same time  the reproducing kernel particle method 6   RKPM  emerged  the approximation motivated in part to correct the kernel estimate in SPH  to give accuracy near boundaries  in non-uniform discretizations  and higher-order accuracy in general  Notably  in a parallel development  the Material point methods were developed around the same time 7  which offer similar capabilities  Material point methods are widely used in the movie industry to simulate large deformation solid mechanics  such as snow in the movie Frozen  8  RKPM and other meshfree methods were extensively developed by Chen  Liu  and Li in the late 1990s for a variety of applications and various classes of problems  9  During the 1990s and thereafter several other varieties were developed including those listed below     The following numerical methods are generally considered to fall within the general class of \"meshfree\" methods   Acronyms are provided in parentheses     Related methods     The primary areas of advancement in meshfree methods are to address issues with essential boundary enforcement  numerical quadrature  and contact and large deformations  22  The common weak form requires strong enforcement of the essential boundary conditions  yet meshfree methods in general lack the Kronecker delta property  This make essential boundary condition enforcement non-trivial  at least more difficult than the Finite element method  where they can be imposed directly  Techniques have been developed to overcome this difficulty and impose conditions strongly  Several methods have been developed to impose the essential boundary conditions weakly  including Lagrange multipliers  Nitche  s method  and the penalty method      As for quadrature  nodal integration is generally preferred which offers simplicity  efficiency  and keeps the meshfree method free of any mesh  as opposed to using Gauss quadrature  which necessitates a mesh to generate quadrature points and weights   Nodal integration however  suffers from numerical instability due to underestimation of strain energy associated with short-wavelength modes  23  and also yields inaccurate and non-convergent results due to under-integration of the weak form  24  One major advance in numerical integration has been the development of a stabilized conforming nodal integration  SCNI  which provides a nodal integration method which does not suffer from either of these problems  24  The method is based on strain-smoothing which satisfies the first order patch test  However  it was later realized that low-energy modes were still present in SCNI  and additional stabilization methods have been developed  This method has been applied to a variety of problems including thin and thick plates  poromechanics  convection-dominated problems  among others  22  More recently  a framework has been developed to pass arbitrary-order patch tests  based on a PetrovGalerkin method  25     One recent advance in meshfree methods aims at the development of computational tools for automation in modeling and simulations   This is enabled by the so-called weakened weak  W2  formulation based on the G space theory  26  27  The W2 formulation offers possibilities to formulate various  uniformly  \"soft\" models that work well with triangular meshes  Because a triangular mesh can be generated automatically  it becomes much easier in re-meshing and hence enables automation in modeling and simulation   In addition  W2 models can be made soft enough  in uniform fashion  to produce upper bound solutions  for force-driving problems   Together with stiff models  such as the fully compatible FEM models   one can conveniently bound the solution from both sides   This allows easy error estimation for generally complicated problems  as long as a triangular mesh can be generated  Typical W2 models are the Smoothed Point Interpolation Methods  or S-PIM   14  The S-PIM can be node-based  known as NS-PIM or LC-PIM   28  edge-based  ES-PIM   29  and cell-based  CS-PIM   30  The NS-PIM was developed using the so-called SCNI technique  24  It was then discovered that NS-PIM is capable of producing upper bound solution and volumetric locking free  31  The ES-PIM is found superior in accuracy  and CS-PIM behaves in between the NS-PIM and ES-PIM  Moreover  W2 formulations allow the use of polynomial and radial basis functions in the creation of shape functions  it accommodates the discontinuous displacement functions  as long as it is in G1 space   which opens further rooms for future developments  The W2 formulation has also led to the development of combination of meshfree techniques with the well-developed FEM techniques  and one can now use triangular mesh with excellent accuracy and desired softness  A typical such a formulation is the so-called smoothed finite element method  or S-FEM   32  The S-FEM is the linear version of S-PIM  but with most of the properties of the S-PIM and much simpler     It is a general perception that meshfree methods are much more expensive than the FEM counterparts  The recent study has found however  some meshfree methods such as the S-PIM and S-FEM can be much faster than the FEM counterparts  14  32     The S-PIM and S-FEM works well for solid mechanics problems   For CFD problems  the formulation can be simpler  via strong formulation  A Gradient Smoothing Methods  GSM  has also been developed recently for CFD problems  implementing the gradient smoothing idea in strong form  33  34  The GSM is similar to  FVM   but uses gradient smoothing operations exclusively in nested fashions  and is a general numerical method for PDEs     Nodal integration has been proposed as a technique to use finite elements to emulate a meshfree behaviour  citation needed   However  the obstacle that must be overcome in using nodally integrated elements is that the quantities at nodal points are not continuous  and the nodes are shared among multiple elements      n    An analog computer or analogue computer is a type of computer that uses the continuously variable aspects of physical phenomena such as electrical  mechanical  or hydraulic quantities to model the problem being solved  In contrast  digital computers represent varying quantities symbolically and by discrete values of both time and amplitude     Analog computers can have a very wide range of complexity  Slide rules and nomograms are the simplest  while naval gunfire control computers and large hybrid digital/analog computers were among the most complicated  1  Systems for process control and protective relays used analog computation to perform control and protective functions     Analog computers were widely used in scientific and industrial applications even after the advent of digital computers  because at the time they were typically much faster  but they started to become obsolete as early as the 1950s and 1960s  although they remained in use in some specific applications  such as aircraft flight simulators  the flight computer in aircraft  and for teaching control systems in universities  More complex applications  such as aircraft flight simulators and synthetic-aperture radar  remained the domain of analog computing  and hybrid computing  well into the 1980s  since digital computers were insufficient for the task  2    \"This is a list of examples of early computation devices considered precursors of the modern computers  Some of them may even have been dubbed  computers  by the press  though they may fail to fit modern definitions  n\"   The Antikythera mechanism was an orrery and is considered an early mechanical analog computer  according to Derek J  de Solla Price  3  It was designed to calculate astronomical positions  It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera  between Kythera and Crete  and has been dated to c  u2009100 BC during the Hellenistic period of Greece  Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later     Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use  nThe planisphere was first described by Ptolemy in the 2nd century AD  The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus  A combination of the planisphere and dioptra  the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy  An astrolabe incorporating a mechanical calendar computer 4  5  and gear-wheels was invented by Abi Bakr of Isfahan  Persia in 1235  6  Ab Rayhn al-Brn invented the first mechanical geared lunisolar calendar astrolabe  7  an early fixed-wired knowledge processing machine 8  with a gear train and gear-wheels  9  c  u2009AD 1000  The castle clock  a hydropowered mechanical astronomical clock invented by Al-Jazari in 1206  was the first programmable analog computer  10  11  12     The sector  a calculating instrument used for solving problems in proportion  trigonometry  multiplication and division  and for various functions  such as squares and cube roots  was developed in the late 16th century and found application in gunnery  surveying and navigation     The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage     The slide rule was invented around 16201630  shortly after the publication of the concept of the logarithm  It is a hand-operated analog computer for doing multiplication and division  As slide rule development progressed  added scales provided reciprocals  squares and square roots  cubes and cube roots  as well as transcendental functions such as logarithms and exponentials  circular and hyperbolic trigonometry and other functions  Aviation is one of the few fields where slide rules are still in widespread use  particularly for solving timedistance problems in light aircraft     Charles Babbage is considered by some to be \"father of the computer\"  In 1822  Babbage began design on the first mechanical computer  the Difference Engine  that eventually led to more complex electronic designs  though all the essential ideas of modern computers are to be found in Babbage  s Analytical Engine     In 18311835  mathematician and engineer Giovanni Plana devised a perpetual-calendar machine  which  through a system of pulleys and cylinders could predict the perpetual calendar for every year from AD xa00  that is  1 xa0BC  to AD xa04000  keeping track of leap years and varying day length  13     The tide-predicting machine invented by Sir William Thomson in 1872 was of great utility to navigation in shallow waters  It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location     The differential analyser  a mechanical analog computer designed to solve differential equations by integration  used wheel-and-disc mechanisms to perform the integration  In 1876 James Thomson had already discussed the possible construction of such calculators  but he had been stymied by the limited output torque of the ball-and-disk integrators  14  In a differential analyzer  the output of one integrator drove the input of the next integrator  or a graphing output  The torque amplifier was the advance that allowed these machines to work  Starting in the 1920s  Vannevar Bush and others developed mechanical differential analyzers     The Dumaresq was a mechanical calculating device invented around 1902 by Lieutenant John Dumaresq of the Royal Navy  It was an analog computer that related vital variables of the fire control problem to the movement of one  s own ship and that of a target ship  It was often used with other devices  such as a Vickers range clock to generate range and deflection data so the gun sights of the ship could be continuously set  A number of versions of the Dumaresq were produced of increasing complexity as development proceeded     By 1912 Arthur Pollen had developed an electrically driven mechanical analog computer for fire-control systems  based on the differential analyser  It was used by the Imperial Russian Navy in World War I  citation needed     Starting in 1929  AC network analyzers were constructed to solve calculation problems related to electrical power systems that were too large to solve with numerical methods at the time  15  These were essentially scale models of the electrical properties of the full-size system  Since network analyzers could handle problems too large for analytic methods or hand computation  they were also used to solve problems in nuclear physics and in the design of structures  More than 50 large network analyzers were built by the end of the 1950s     World War II era gun directors  gun data computers  and bomb sights used mechanical analog computers  In 1942 Helmut Hlzer built a fully electronic analog computer at Peenemnde Army Research Center 16  17  18  as an embedded control system  mixing device  to calculate V-2 rocket trajectories from the accelerations and orientations  measured by gyroscopes  and to stabilize and guide the missile  19  20  Mechanical analog computers were very important in gun fire control in World War II  The Korean War and well past the Vietnam War  they were made in significant numbers     In the period 19301945 in the Netherlands Johan van Veen developed an analogue computer to calculate and predict tidal currents when the geometry of the channels are changed  Around 1950 this idea was developed into the Deltar  an analogue computer supporting the closure of estuaries in the southwest of the Netherlands  the Delta Works      The FERMIAC was an analog computer invented by physicist Enrico Fermi in 1947 to aid in his studies of neutron transport  21  Project Cyclone was an analog computer developed by Reeves in 1950 for the analysis and design of dynamic systems  22  Project Typhoon was an analog computer developed by RCA in 1952  It consisted of over 4000 electron tubes and used 100 dials and 6000 plug-in connectors to program  23  The MONIAC Computer was a hydraulic model of a national economy first unveiled in 1949  24     Computer Engineering Associates was spun out of Caltech in 1950 to provide commercial services using the \"Direct Analogy Electric Analog Computer\"  \"the largest and most impressive general-purpose analyzer facility for the solution of field problems\"  developed there by Gilbert D  McCann  Charles H  Wilts  and Bart Locanthi  25  26     Educational analog computers illustrated the principles of analog calculation  The Heathkit EC-1  a $199 educational analog computer  was made by the Heath Company  US c  u20091960  27  It was programmed using patch cords that connected nine operational amplifiers and other components  28  General Electric also marketed an \"educational\" analog computer kit of a simple design in the early 1960s consisting of a two transistor tone generators and three potentiometers wired such that the frequency of the oscillator was nulled when the potentiometer dials were positioned by hand to satisfy an equation  The relative resistance of the potentiometer was then equivalent to the formula of the equation being solved  Multiplication or division could be performed  depending on which dials were inputs and which was the output  Accuracy and resolution was limited and a simple slide rule was more accuratehowever  the unit did demonstrate the basic principle     Analog computer designs were published in electronics magazines  One example is the PE Analogue Computer  published in Practical Electronics in the September 1978 edition  Another more modern hybrid computer design was published in Everyday Practical Electronics in 2002  29  An example described in the EPE Hybrid Computer was the flight of a VTOL aircraft like the Harrier jump jet  29  The altitude and speed of the aircraft were calculated by the analog part of the computer and sent to a PC via a digital microprocessor and displayed on the PC screen     In industrial process control  analog loop controllers were used to automatically regulate temperature  flow  pressure  or other process conditions  The technology of these controllers ranged from purely mechanical integrators  through vacuum-tube and solid-state devices  to emulation of analog controllers by microprocessors     The similarity between linear mechanical components  such as springs and dashpots  viscous-fluid dampers   and electrical components  such as capacitors  inductors  and resistors is striking in terms of mathematics  They can be modeled using equations of the same form     However  the difference between these systems is what makes analog computing useful  If one considers a simple massspring system  constructing the physical system would require making or modifying the springs and masses  This would be followed by attaching them to each other and an appropriate anchor  collecting test equipment with the appropriate input range  and finally  taking measurements  In more complicated cases  such as suspensions for racing cars  experimental construction  modification  and testing is both complicated and expensive     The electrical equivalent can be constructed with a few operational amplifiers  op amps  and some passive linear components  all measurements can be taken directly with an oscilloscope  In the circuit  the  simulated  stiffness of the spring  for instance  can be changed by adjusting the parameters of an integrator  The electrical system is an analogy to the physical system  hence the name  but it is less expensive to construct  generally safer  and typically much easier to modify     As well  an electronic circuit can typically operate at higher frequencies than the system being simulated  This allows the simulation to run faster than real time  which could  in some instances  be hours  weeks  or longer   Experienced users of electronic analog computers said that they offered a comparatively intimate control and understanding of the problem  relative to digital simulations     The drawback of the mechanical-electrical analogy is that electronics are limited by the range over which the variables may vary due to the fixed supply voltage  Therefore  each problem must be scaled to its parameters and dimensionse g   the expected magnitudes of the velocity and the position of a spring pendulum  Improperly scaled problems might suffer from higher noise levels  Floating-point digital calculations have a huge dynamic range but might also suffer from imprecision if tiny differences of huge values lead to numerical instability     These electric circuits can also easily perform a wide variety of simulations  For example  voltage can simulate water pressure and electric current can simulate rate of flow in terms of cubic metres per second  An integrator can provide the total accumulated volume of liquid  using an input current proportional to the  possibly varying  flow rate     Analog computers are especially well-suited to representing situations described by differential equations  Occasionally  they were used when a system of differential equations proved very difficult to solve by traditional means  As a simple example  the dynamics of a spring-mass system can be described by the equation                     m                                            y                                                    +        d                                            y                                                    +        c        y        =        m        g                 displaystyle m   ddot  y  +d   dot  y  +cy=mg     citation needed  with                     y                 displaystyle y    as the vertical position of a mass                     m                 displaystyle m                         d                 displaystyle d    the damping coefficient                      c                 displaystyle c    the spring constant and                     g                 displaystyle g    the gravity of Earth  For analog computing  the equation is programmed as                                                         y                                                    =                                                    d              m                                                                          y                                                                                                c              m                                      y                g                 displaystyle    ddot  y  =-   tfrac  d  m     dot  y  -   tfrac  c  m  y-g     The equivalent analog circuit consists of two integrators for the state variables                                                                 y                                                             displaystyle -   dot  y       speed  and                     y                 displaystyle y     position   one inverter  and three potentiometers  The circuit has to consider that both integration and addition units invert the signal polarity     The accuracy of an analog computer is limited by its computing elements as well as quality of the internal power and electrical interconnections  The precision of the analog computer readout was limited chiefly by the precision of the readout equipment used  generally three or four significant figures  The precision of a digital computer is limited by the word size  arbitrary-precision arithmetic  while relatively slow  provides any practical degree of precision that might be needed  However  in most cases the precision of an analog computer is absolutely sufficient given the uncertainty of the model characteristics and its technical parameters     Many small computers dedicated to specific computations are still part of industrial regulation equipment  but from the 1950s to the 1970s  general-purpose analog computers were the only systems fast enough for real time simulation of dynamic systems  especially in the aircraft  military and aerospace field     In the 1960s  the major manufacturer was Electronic Associates of Princeton  New Jersey  with its 231R Analog Computer  vacuum tubes  20 integrators  and subsequently its EAI 8800 Analog Computer  solid state operational amplifiers  64 integrators   30  Its challenger was Applied Dynamics of Ann Arbor  Michigan     Although the basic technology for analog computers is usually operational amplifiers  also called \"continuous current amplifiers\" because they have no low frequency limitation   in the 1960s an attempt was made in the French ANALAC computer to use an alternative technology  medium frequency carrier and non dissipative reversible circuits     In the 1970s every big company and administration concerned with problems in dynamics had a big analog computing center  for example     Analog computing devices are fast  digital computing devices are more versatile and accurate  so the idea is to combine the two processes for the best efficiency  An example of such hybrid elementary device is the hybrid multiplier where one input is an analog signal  the other input is a digital signal and the output is analog  It acts as an analog potentiometer upgradable digitally  This kind of hybrid technique is mainly used for fast dedicated real time computation when computing time is very critical as signal processing for radars and generally for controllers in embedded systems     In the early 1970s analog computer manufacturers tried to tie together their analog computer with a digital computer to get the advantages of the two techniques  In such systems  the digital computer controlled the analog computer  providing initial set-up  initiating multiple analog runs  and automatically feeding and collecting data  The digital computer may also participate to the calculation itself using analog-to-digital and digital-to-analog converters     The largest manufacturer of hybrid computers was Electronics Associates  Their hybrid computer model 8900 was made of a digital computer and one or more analog consoles  These systems were mainly dedicated to large projects such as the Apollo program and Space Shuttle at NASA  or Ariane in Europe  especially during the integration step where at the beginning everything is simulated  and progressively real components replace their simulated part  31     Only one company was known as offering general commercial computing services on its hybrid computers  CISI of France  in the 1970s     The best reference in this field is the 100 000 simulation runs for each certification of the automatic landing systems of Airbus and Concorde aircraft  32     After 1980  purely digital computers progressed more and more rapidly and were fast enough to compete with analog computers  nOne key to the speed of analog computers was their fully parallel computation  but this was also a limitation  The more equations required for a problem  the more analog components were needed  even when the problem wasn  t time critical  \"Programming\" a problem meant interconnecting the analog operators  even with a removable wiring panel this was not very versatile  Today there are no more big hybrid computers  but only hybrid components  citation needed     While a wide variety of mechanisms have been developed throughout history  some stand out because of their theoretical importance  or because they were manufactured in significant quantities     Most practical mechanical analog computers of any significant complexity used rotating shafts to carry variables from one mechanism to another  Cables and pulleys were used in a Fourier synthesizer  a tide-predicting machine  which summed the individual harmonic components  Another category  not nearly as well known  used rotating shafts only for input and output  with precision racks and pinions  The racks were connected to linkages that performed the computation  At least one U S  Naval sonar fire control computer of the later 1950s  made by Librascope  was of this type  as was the principal computer in the Mk  xa056 Gun Fire Control System     Online  there is a remarkably clear illustrated reference  OP xa01140  33  that describes the fire control computer mechanisms  33  nFor adding and subtracting  precision miter-gear differentials were in common use in some computers  the Ford Instrument Mark I Fire Control Computer contained about 160 of them    \"Integration with respect to another variable was done by a rotating disc driven by one variable  Output came from a pick-off device  such as a wheel  positioned at a radius on the disc proportional to the second variable   A carrier with a pair of steel balls supported by small rollers worked especially well  A roller  its axis parallel to the disc s surface  provided the output  It was held against the pair of balls by a spring   n\"   Arbitrary functions of one variable were provided by cams  with gearing to convert follower movement to shaft rotation    \"Functions of two variables were provided by three-dimensional cams  In one good design  one of the variables rotated the cam  A hemispherical follower moved its carrier on a pivot axis parallel to that of the cam s rotating axis  Pivoting motion was the output  The second variable moved the follower along the axis of the cam  One practical application was ballistics in gunnery  n\"   Coordinate conversion from polar to rectangular was done by a mechanical resolver  called a \"component solver\" in US Navy fire control computers   Two discs on a common axis positioned a sliding block with pin  stubby shaft  on it  One disc was a face cam  and a follower on the block in the face cam  s groove set the radius  The other disc  closer to the pin  contained a straight slot in which the block moved  The input angle rotated the latter disc  the face cam disc  for an unchanging radius  rotated with the other  angle  disc  a differential and a few gears did this correction     \"Referring to the mechanism s frame  the location of the pin corresponded to the tip of the vector represented by the angle and magnitude inputs  Mounted on that pin was a square block  n\"   Rectilinear-coordinate outputs  both sine and cosine  typically  came from two slotted plates  each slot fitting on the block just mentioned  The plates moved in straight lines  the movement of one plate at right angles to that of the other  The slots were at right angles to the direction of movement  Each plate  by itself  was like a Scotch yoke  known to steam engine enthusiasts     During World War II  a similar mechanism converted rectilinear to polar coordinates  but it was not particularly successful and was eliminated in a significant redesign  USN  Mk  xa01 to Mk  xa01A      Multiplication was done by mechanisms based on the geometry of similar right triangles  Using the trigonometric terms for a right triangle  specifically opposite  adjacent  and hypotenuse  the adjacent side was fixed by construction  One variable changed the magnitude of the opposite side  In many cases  this variable changed sign  the hypotenuse could coincide with the adjacent side  a zero input   or move beyond the adjacent side  representing a sign change    \"Typically  a pinion-operated rack moving parallel to the  trig -defined  opposite side would position a slide with a slot coincident with the hypotenuse  A pivot on the rack let the slide s angle change freely  At the other end of the slide  the angle  in trig  terms   a block on a pin fixed to the frame defined the vertex between the hypotenuse and the adjacent side  n\"   At any distance along the adjacent side  a line perpendicular to it intersects the hypotenuse at a particular point  The distance between that point and the adjacent side is some fraction that is the product of 1 the distance from the vertex  and 2 the magnitude of the opposite side    \"The second input variable in this type of multiplier positions a slotted plate perpendicular to the adjacent side  That slot contains a block  and that block s position in its slot is determined by another block right next to it  The latter slides along the hypotenuse  so the two blocks are positioned at a distance from the  trig   adjacent side by an amount proportional to the product  n\"   To provide the product as an output  a third element  another slotted plate  also moves parallel to the  trig   opposite side of the theoretical triangle  As usual  the slot is perpendicular to the direction of movement  A block in its slot  pivoted to the hypotenuse block positions it    \"A special type of integrator  used at a point where only moderate accuracy was needed  was based on a steel ball  instead of a disc  It had two inputs  one to rotate the ball  and the other to define the angle of the ball s rotating axis  That axis was always in a plane that contained the axes of two movement pick-off rollers  quite similar to the mechanism of a rolling-ball computer mouse  in that mechanism  the pick-off rollers were roughly the same diameter as the ball   The pick-off roller axes were at right angles  n\"   A pair of rollers \"above\" and \"below\" the pick-off plane were mounted in rotating holders that were geared together  That gearing was driven by the angle input  and established the rotating axis of the ball  The other input rotated the \"bottom\" roller to make the ball rotate     Essentially  the whole mechanism  called a component integrator  was a variable-speed drive with one motion input and two outputs  as well as an angle input  The angle input varied the ratio  and direction  of coupling between the \"motion\" input and the outputs according to the sine and cosine of the input angle     Although they did not accomplish any computation  electromechanical position servos were essential in mechanical analog computers of the \"rotating-shaft\" type for providing operating torque to the inputs of subsequent computing mechanisms  as well as driving output data-transmission devices such as large torque-transmitter synchros in naval computers     Other readout mechanisms  not directly part of the computation  included internal odometer-like counters with interpolating drum dials for indicating internal variables  and mechanical multi-turn limit stops    \"Considering that accurately controlled rotational speed in analog fire-control computers was a basic element of their accuracy  there was a motor with its average speed controlled by a balance wheel  hairspring  jeweled-bearing differential  a twin-lobe cam  and spring-loaded contacts  ship s AC power frequency was not necessarily accurate  nor dependable enough  when these computers were designed   n\"   Electronic analog computers typically have front panels with numerous jacks  single-contact sockets  that permit patch cords  flexible wires with plugs at both ends  to create the interconnections that define the problem setup  In addition  there are precision high-resolution potentiometers  variable resistors  for setting up  and  when needed  varying  scale factors  In addition  there is usually a zero-center analog pointer-type meter for modest-accuracy voltage measurement  Stable  accurate voltage sources provide known magnitudes     Typical electronic analog computers contain anywhere from a few to a hundred or more operational amplifiers  \"op amps\"   named because they perform mathematical operations  Op amps are a particular type of feedback amplifier with very high gain and stable input  low and stable offset   They are always used with precision feedback components that  in operation  all but cancel out the currents arriving from input components  The majority of op amps in a representative setup are summing amplifiers  which add and subtract analog voltages  providing the result at their output jacks  As well  op amps with capacitor feedback are usually included in a setup  they integrate the sum of their inputs with respect to time     Integrating with respect to another variable is the nearly exclusive province of mechanical analog integrators  it is almost never done in electronic analog computers  However  given that a problem solution does not change with time  time can serve as one of the variables     Other computing elements include analog multipliers  nonlinear function generators  and analog comparators     Electrical elements such as inductors and capacitors used in electrical analog computers had to be carefully manufactured to reduce non-ideal effects  For example  in the construction of AC power network analyzers  one motive for using higher frequencies for the calculator  instead of the actual power frequency  was that higher-quality inductors could be more easily made  Many general-purpose analog computers avoided the use of inductors entirely  re-casting the problem in a form that could be solved using only resistive and capacitive elements  since high-quality capacitors are relatively easy to make     The use of electrical properties in analog computers means that calculations are normally performed in real time  or faster   at a speed determined mostly by the frequency response of the operational amplifiers and other computing elements  In the history of electronic analog computers  there were some special high-speed types     Nonlinear functions and calculations can be constructed to a limited precision  three or four digits  by designing function generatorsspecial circuits of various combinations of resistors and diodes to provide the nonlinearity  Typically  as the input voltage increases  progressively more diodes conduct    \"When compensated for temperature  the forward voltage drop of a transistor s base-emitter junction can provide a usably accurate logarithmic or exponential function  Op amps scale the output voltage so that it is usable with the rest of the computer  n\"   Any physical process that models some computation can be interpreted as an analog computer  Some examples  invented for the purpose of illustrating the concept of analog computation  include using a bundle of spaghetti as a model of sorting numbers  a board  a set of nails  and a rubber band as a model of finding the convex hull of a set of points  and strings tied together as a model of finding the shortest path in a network  These are all described in Dewdney  1984     \"Analog computers often have a complicated framework  but they have  at their core  a set of key components that perform the calculations  The operator manipulates these through the computer s framework  n\"   Key hydraulic components might include pipes  valves and containers     Key mechanical components might include rotating shafts for carrying data within the computer  miter gear differentials  disc/ball/roller integrators  cams  2-D and 3-D   mechanical resolvers and multipliers  and torque servos     Key electrical/electronic components might include     The core mathematical operations used in an electric analog computer are     In some analog computer designs  multiplication is much preferred to division  Division is carried out with a multiplier in the feedback path of an Operational Amplifier     Differentiation with respect to time is not frequently used  and in practice is avoided by redefining the problem when possible  It corresponds in the frequency domain to a high-pass filter  which means that high-frequency noise is amplified  differentiation also risks instability     In general  analog computers are limited by non-ideal effects  An analog signal is composed of four basic components  DC and AC magnitudes  frequency  and phase  The real limits of range on these characteristics limit analog computers  Some of these limits include the operational amplifier offset  finite gain  and frequency response  noise floor  non-linearities  temperature coefficient  and parasitic effects within semiconductor devices  For commercially available electronic components  ranges of these aspects of input and output signals are always figures of merit     In the 1950s to 1970s  digital computers based on first vacuum tubes  transistors  integrated circuits and then micro-processors became more economical and precise  This led digital computers to largely replace analog computers  Even so  some research in analog computation is still being done  A few universities still use analog computers to teach control system theory  The American company Comdyna manufactured small analog computers  34  At Indiana University Bloomington  Jonathan Mills has developed the Extended Analog Computer based on sampling voltages in a foam sheet  35  At the Harvard Robotics Laboratory  36  analog computation is a research topic  Lyric Semiconductor  s error correction circuits use analog probabilistic signals  Slide rules are still popular among aircraft personnel  citation needed     With the development of very-large-scale integration  VLSI  technology  Yannis Tsividis   group at Columbia University has been revisiting analog/hybrid computers design in standard CMOS process  Two VLSI chips have been developed  an 80th-order analog computer  250 xa0nm  by Glenn Cowan 37  in 2005 38  and a 4th-order hybrid computer  65 xa0nm  developed by Ning Guo 39  in 2015  40  both targeting at energy-efficient ODE/PDE applications  Glenn  s chip contains 16 macros  in which there are 25 analog computing blocks  namely integrators  multipliers  fanouts  few nonlinear blocks  Ning  s chip contains one macro block  in which there are 26 computing blocks including integrators  multipliers  fanouts  ADCs  SRAMs and DACs  Arbitrary nonlinear function generation is made possible by the ADC+SRAM+DAC chain  where the SRAM block stores the nonlinear function data  The experiments from the related publications revealed that VLSI analog/hybrid computers demonstrated about 12 orders magnitude of advantage in both solution time and energy while achieving accuracy within 5%  which points to the promise of using analog/hybrid computing techniques in the area of energy-efficient approximate computing  citation needed  In 2016  a team of researchers developed a compiler to solve differential equations using analog circuits  41     These are examples of analog computers that have been constructed or practically used     Analog  audio  synthesizers can also be viewed as a form of analog computer  and their technology was originally based in part on electronic analog computer technology  The ARP 2600  s Ring Modulator was actually a moderate-accuracy analog multiplier     The Simulation Council  or Simulations Council  was an association of analog computer users in US  It is now known as The Society for Modeling and Simulation International  The Simulation Council newsletters from 1952 to 1963 are available online and show the concerns and technologies at the time  and the common use of analog computers for missilry  42     Finite element xa0 Boundary element  nLattice Boltzmann xa0 Riemann solver nDissipative particle dynamics nSmoothed particle hydrodynamics    Computational fluid dynamics  CFD  is a branch of fluid mechanics that uses numerical analysis and data structures to analyze and solve problems that involve fluid flows   Computers are used to perform the calculations required to simulate the free-stream flow of the fluid  and the interaction of the fluid  liquids and gases  with surfaces defined by boundary conditions  With high-speed supercomputers  better solutions can be achieved  and are often required to solve the largest and most complex problems  Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as transonic or turbulent flows  Initial validation of such software is typically performed using experimental apparatus such as wind tunnels   In addition  previously performed analytical or empirical analysis of a particular problem can be used for comparison   A final validation is often performed using full-scale testing  such as flight tests     CFD is applied to a wide range of research and engineering problems in many fields of study and industries  including aerodynamics and aerospace analysis  weather simulation  natural science and environmental engineering  industrial system design and analysis  biological engineering  fluid flows and heat transfer  and engine and combustion analysis     The fundamental basis of almost all CFD problems is the NavierStokes equations  which define many single-phase  gas or liquid  but not both  fluid flows   These equations can be simplified by removing terms describing viscous actions to yield the Euler equations   Further simplification  by removing terms describing vorticity yields the full potential equations  Finally  for small perturbations in subsonic and supersonic flows  not transonic or hypersonic  these equations can be linearized to yield the linearized potential equations     Historically  methods were first developed to solve the linearized potential equations  Two-dimensional  2D  methods  using conformal transformations of the flow about a cylinder to the flow about an airfoil were developed in the 1930s  1     One of the earliest type of calculations resembling modern CFD are those by Lewis Fry Richardson  in the sense that these calculations used finite differences and divided the physical space in cells  Although they failed dramatically  these calculations  together with Richardson  s book Weather Prediction by Numerical Process  2  set the basis for modern CFD and numerical meteorology  In fact  early CFD calculations during the 1940s using ENIAC used methods close to those in Richardson  s 1922 book  3     The computer power available paced development of three-dimensional methods   Probably the first work using computers to model fluid flow  as governed by the NavierStokes equations  was performed at Los Alamos National Lab  in the T3 group  4  5  This group was led by Francis H  Harlow  who is widely considered as one of the pioneers of CFD  From 1957 to late 1960s  this group developed a variety of numerical methods to simulate transient two-dimensional fluid flows  such as particle-in-cell method  6  fluid-in-cell method  7  vorticity stream function method  8  and nmarker-and-cell method  9  Fromm  s vorticity-stream-function method for 2D  transient  incompressible flow was the first treatment of strongly contorting incompressible flows in the world     The first paper with three-dimensional model was published by John Hess and A M O  Smith of Douglas Aircraft in 1967  10  This method discretized the surface of the geometry with panels  giving rise to this class of programs being called Panel Methods   Their method itself was simplified  in that it did not include lifting flows and hence was mainly applied to ship hulls and aircraft fuselages   The first lifting Panel Code  A230  was described in a paper written by Paul Rubbert and Gary Saaris of Boeing Aircraft in 1968  11   In time  more advanced three-dimensional Panel Codes were developed at Boeing  PANAIR  A502   12  Lockheed  Quadpan   13  Douglas  HESS   14  McDonnell Aircraft  MACAERO   15  NASA  PMARC  16  and Analytical Methods  WBAERO  17  USAERO 18  and VSAERO 19  20     Some  PANAIR  HESS and MACAERO  were higher order codes  using higher order distributions of surface singularities  while others  Quadpan  PMARC  USAERO and VSAERO  used single singularities on each surface panel   The advantage of the lower order codes was that they ran much faster on the computers of the time   Today  VSAERO has grown to be a multi-order code and is the most widely used program of this class  It has been used in the development of many submarines  surface ships  automobiles  helicopters  aircraft  and more recently wind turbines   Its sister code  USAERO is an unsteady panel method that has also been used for modeling such things as high speed trains and racing yachts   The NASA PMARC code from an early version of VSAERO and a derivative of PMARC  named CMARC  21  is also commercially available     In the two-dimensional realm  a number of Panel Codes have been developed for airfoil analysis and design   The codes typically have a boundary layer analysis included  so that viscous effects can be modeled  Richard Eppler xa0 de  developed the PROFILE code  partly with NASA funding  which became available in the early 1980s  22   This was soon followed by Mark Drela  s XFOIL code  23  Both PROFILE and XFOIL incorporate two-dimensional panel codes  with coupled boundary layer codes for airfoil analysis work   PROFILE uses a conformal transformation method for inverse airfoil design  while XFOIL has both a conformal transformation and an inverse panel method for airfoil design     An intermediate step between Panel Codes and Full Potential codes were codes that used the Transonic Small Disturbance equations   In particular  the three-dimensional WIBCO code  24  developed by Charlie Boppe of Grumman Aircraft in the early 1980s has seen heavy use     Developers turned to Full Potential codes  as panel methods could not calculate the non-linear flow present at transonic speeds   The first description of a means of using the Full Potential equations was published by Earll Murman and Julian Cole of Boeing in 1970  25  Frances Bauer  Paul Garabedian and David Korn of the Courant Institute at New York University  NYU  wrote a series of two-dimensional Full Potential airfoil codes that were widely used  the most important being named Program H  26   A further growth of Program H was developed by Bob Melnik and his group at Grumman Aerospace as Grumfoil  27  Antony Jameson  originally at Grumman Aircraft and the Courant Institute of NYU  worked with David Caughey to develop the important three-dimensional Full Potential code FLO22 28  in 1975   Many Full Potential codes emerged after this  culminating in Boeing  s Tranair  A633  code  29  which still sees heavy use     The next step was the Euler equations  which promised to provide more accurate solutions of transonic flows   The methodology used by Jameson in his three-dimensional FLO57 code 30   1981  was used by others to produce such programs as Lockheed  s TEAM program 31  and IAI/Analytical Methods   MGAERO program  32   MGAERO is unique in being a structured cartesian mesh code  while most other such codes use structured body-fitted grids  with the exception of NASA  s highly successful CART3D code  33  Lockheed  s SPLITFLOW code 34  and Georgia Tech  s NASCART-GT   35  Antony Jameson also developed the three-dimensional AIRPLANE code 36  which made use of unstructured tetrahedral grids     In the two-dimensional realm  Mark Drela and Michael Giles  then graduate students at MIT  developed the ISES Euler program 37   actually a suite of programs  for airfoil design and analysis   This code first became available in 1986 and has been further developed to design  analyze and optimize single or multi-element airfoils  as the MSES program  38   MSES sees wide use throughout the world   A derivative of MSES  for the design and analysis of airfoils in a cascade  is MISES  39  developed by Harold Youngren while he was a graduate student at MIT     The NavierStokes equations were the ultimate target of development   Two-dimensional codes  such as NASA Ames   ARC2D code first emerged   A number of three-dimensional codes were developed  ARC3D  OVERFLOW  CFL3D are three successful NASA contributions   leading to numerous commercial packages     CFD can be seen as a group of computational methodologies  discussed below  used to solve equations governing fluid flow   In the application of CFD  a critical step is to decide which set of physical assumptions and related equations need to be used for the problem at hand  40  To illustrate this step  the following summarizes the physical assumptions/simplifications taken in equations of a flow that is single-phase  see multiphase flow and two-phase flow   single-species  i e   it consists of one chemical species   non-reacting  and  unless said otherwise  compressible   Thermal radiation is neglected  and body forces due to gravity are considered  unless said otherwise    In addition  for this type of flow  the next discussion highlights the hierarchy of flow equations solved with CFD   Note that some of the following equations could be derived in more than one way     In all of these approaches the same basic procedure is followed     The stability of the selected discretisation is generally established numerically rather than analytically as with simple linear problems   Special care must also be taken to ensure that the discretisation handles discontinuous solutions gracefully   The Euler equations and NavierStokes equations both admit shocks  and contact surfaces     Some of the discretization methods being used are     The finite volume method  FVM  is a common approach used in CFD codes  as it has an advantage in memory usage and solution speed  especially for large problems  high Reynolds number turbulent flows  and source term dominated flows  like combustion   52     In the finite volume method  the governing partial differential equations  typically the Navier-Stokes equations  the mass and energy conservation equations  and the turbulence equations  are recast in a conservative form  and then solved over discrete control volumes  This discretization guarantees the conservation of fluxes through a particular control volume  The finite volume equation yields governing equations in the form     where                     Q                 displaystyle Q    is the vector of conserved variables                      F                 displaystyle F    is the vector of fluxes  see Euler equations or NavierStokes equations                       V                 displaystyle V    is the volume of the control volume element  and                               A                         displaystyle   mathbf  A      is the surface area of the control volume element     The finite element method  FEM  is used in structural analysis of solids  but is also applicable to fluids   However  the FEM formulation requires special care to ensure a conservative solution  The FEM formulation has been adapted for use with fluid dynamics governing equations  citation needed  Although FEM must be carefully formulated to be conservative  it is much more stable than the finite volume approach  53   However  FEM can require more memory and has slower solution times than the FVM  54     In this method  a weighted residual equation is formed     where                               R                      i                                   displaystyle R_ i     is the equation residual at an element vertex                     i                 displaystyle i                         Q                 displaystyle Q    is the conservation equation expressed on an element basis                                W                      i                                   displaystyle W_ i     is the weight factor  and                               V                      e                                   displaystyle V^ e     is the volume of the element     The finite difference method  FDM  has historical importance citation needed  and is simple to program   It is currently only used in few specialized codes  which handle complex geometry with high accuracy and efficiency by using embedded boundaries or overlapping grids  with the solution interpolated across each grid   citation needed      where                     Q                 displaystyle Q    is the vector of conserved variables  and                     F                 displaystyle F                         G                 displaystyle G     and                     H                 displaystyle H    are the fluxes in the                     x                 displaystyle x                         y                 displaystyle y     and                     z                 displaystyle z    directions respectively     Spectral element method is a finite element type method  It requires the mathematical problem  the partial differential equation  to be cast in a weak formulation  This is typically done by multiplying the differential equation by an arbitrary test function and integrating over the whole domain  Purely mathematically  the test functions are completely arbitrary - they belong to an infinite-dimensional function space  Clearly an infinite-dimensional function space cannot be represented on a discrete spectral element mesh  this is where the spectral element discretization begins  The most crucial thing is the choice of interpolating and testing functions  In a standard  low order FEM in 2D  for quadrilateral elements the most typical choice is the bilinear test or interpolating function of the form                     v                 x                 y                 =        a        x        +        b        y        +        c        x        y        +        d                 displaystyle v x y =ax+by+cxy+d     In a spectral element method however  the interpolating and test functions are chosen to be polynomials of a very high order  typically e g  of the 10th order in CFD applications   This guarantees the rapid convergence of the method  Furthermore  very efficient integration procedures must be used  since the number of integrations to be performed in numerical codes is big  Thus  high order Gauss integration quadratures are employed  since they achieve the highest accuracy with the smallest number of computations to be carried out  nAt the time there are some academic CFD codes based on the spectral element method and some more are currently under development  since the new time-stepping schemes arise in the scientific world     The lattice Boltzmann method  LBM  with its simplified kinetic picture on a lattice provides a computationally efficient description of hydrodynamics  nUnlike the traditional CFD methods  which solve the conservation equations of macroscopic properties  i e   mass  momentum  and energy  numerically  LBM models the fluid consisting of fictive particles  and such particles perform consecutive propagation and collision processes over a discrete lattice mesh  In this method  one works with the discrete in space and time version of the kinetic evolution equation in the Boltzmann Bhatnagar-Gross-Krook  BGK  form     In the boundary element method  the boundary occupied by the fluid is divided into a surface mesh     High-resolution schemes are used where shocks or discontinuities are present  Capturing sharp changes in the solution requires the use of second or higher-order numerical schemes that do not introduce spurious oscillations  This usually necessitates the application of flux limiters to ensure that the solution is total variation diminishing  citation needed     In computational modeling of turbulent flows  one common objective is to obtain a model that can predict quantities of interest  such as fluid velocity  for use in engineering designs of the system being modeled   For turbulent flows  the range of length scales and complexity of phenomena involved in turbulence make most modeling approaches prohibitively expensive  the resolution required to resolve all scales involved in turbulence is beyond what is computationally possible   The primary approach in such cases is to create numerical models to approximate unresolved phenomena   This section lists some commonly used computational models for turbulent flows     Turbulence models can be classified based on computational expense  which corresponds to the range of scales that are modeled versus resolved  the more turbulent scales that are resolved  the finer the resolution of the simulation  and therefore the higher the computational cost   If a majority or all of the turbulent scales are not modeled  the computational cost is very low  but the tradeoff comes in the form of decreased accuracy     In addition to the wide range of length and time scales and the associated computational cost  the governing equations of fluid dynamics contain a non-linear convection term and a non-linear and non-local pressure gradient term   These nonlinear equations must be solved numerically with the appropriate boundary and initial conditions     Reynolds-averaged NavierStokes  RANS  equations are the oldest approach to turbulence modeling  An ensemble version of the governing equations is solved  which introduces new apparent stresses known as Reynolds stresses  This adds a second order tensor of unknowns for which various models can provide different levels of closure  It is a common misconception that the RANS equations do not apply to flows with a time-varying mean flow because these equations are   time-averaged    In fact  statistically unsteady  or non-stationary  flows can equally be treated  This is sometimes referred to as URANS  There is nothing inherent in Reynolds averaging to preclude this  but the turbulence models used to close the equations are valid only as long as the time over which these changes in the mean occur is large compared to the time scales of the turbulent motion containing most of the energy     RANS models can be divided into two broad approaches     Large eddy simulation  LES  is a technique in which the smallest scales of the flow are removed through a filtering operation  and their effect modeled using subgrid scale models   This allows the largest and most important scales of the turbulence to be resolved  while greatly reducing the computational cost incurred by the smallest scales  This method requires greater computational resources than RANS methods  but is far cheaper than DNS     Detached eddy simulations  DES  is a modification of a RANS model in which the model switches to a subgrid scale formulation in regions fine enough for LES calculations  Regions near solid boundaries and where the turbulent length scale is less than the maximum grid dimension are assigned the RANS mode of solution  As the turbulent length scale exceeds the grid dimension  the regions are solved using the LES mode  Therefore  the grid resolution for DES is not as demanding as pure LES  thereby considerably cutting down the cost of the computation  Though DES was initially formulated for the Spalart-Allmaras model  Spalart et al   1997   it can be implemented with other RANS models  Strelets  2001   by appropriately modifying the length scale which is explicitly or implicitly involved in the RANS model  So while SpalartAllmaras model based DES acts as LES with a wall model  DES based on other models  like two equation models  behave as a hybrid RANS-LES model  Grid generation is more complicated than for a simple RANS or LES case due to the RANS-LES switch  DES is a non-zonal approach and provides a single smooth velocity field across the RANS and the LES regions of the solutions     Direct numerical simulation  DNS  resolves the entire range of turbulent length scales   This marginalizes the effect of models  but is extremely expensive   The computational cost is proportional to                     R                  e                      3                                   displaystyle Re^ 3      57  DNS is intractable for flows with complex geometries or flow configurations     The coherent vortex simulation approach decomposes the turbulent flow field into a coherent part  consisting of organized vortical motion  and the incoherent part  which is the random background flow  58   This decomposition is done using wavelet filtering   The approach has much in common with LES  since it uses decomposition and resolves only the filtered portion  but different in that it does not use a linear  low-pass filter   Instead  the filtering operation is based on wavelets  and the filter can be adapted as the flow field evolves  Farge and Schneider tested the CVS method with two flow configurations and showed that the coherent portion of the flow exhibited the                                                   40            39                                   displaystyle -   frac  40  39      energy spectrum exhibited by the total flow  and corresponded to coherent structures  vortex tubes   while the incoherent parts of the flow composed homogeneous background noise  which exhibited no organized structures   Goldstein and Vasilyev 59  applied the FDV model to large eddy simulation  but did not assume that the wavelet filter completely eliminated all coherent motions from the subfilter scales   By employing both LES and CVS filtering  they showed that the SFS dissipation was dominated by the SFS flow field  s coherent portion     Probability density function  PDF  methods for turbulence  first introduced by Lundgren  60  are based on tracking the one-point PDF of the velocity                                f                      V                                             v                                   x                         t                 d                  v                         displaystyle f_ V     boldsymbol  v      boldsymbol  x   t d   boldsymbol  v       which gives the probability of the velocity at point                               x                         displaystyle    boldsymbol  x      being between                               v                         displaystyle    boldsymbol  v      and                               v                +        d                  v                         displaystyle    boldsymbol  v  +d   boldsymbol  v        This approach is analogous to the kinetic theory of gases  in which the macroscopic properties of a gas are described by a large number of particles   PDF methods are unique in that they can be applied in the framework of a number of different turbulence models  the main differences occur in the form of the PDF transport equation   For example  in the context of large eddy simulation  the PDF becomes the filtered PDF  61   PDF methods can also be used to describe chemical reactions  62  63  and are particularly useful for simulating chemically reacting flows because the chemical source term is closed and does not require a model   The PDF is commonly tracked by using Lagrangian particle methods  when combined with large eddy simulation  this leads to a Langevin equation for subfilter particle evolution     The vortex method is a grid-free technique for the simulation of turbulent flows  It uses vortices as the computational elements  mimicking the physical structures in turbulence  Vortex methods were developed as a grid-free methodology that would not be limited by the fundamental smoothing effects associated with grid-based methods  To be practical  however  vortex methods require means for rapidly computing velocities from the vortex elements  in other words they require the solution to a particular form of the N-body problem  in which the motion of N objects is tied to their mutual influences   A breakthrough came in the late 1980s with the development of the fast multipole method  FMM   an algorithm by V  Rokhlin  Yale  and L  Greengard  Courant Institute   This breakthrough paved the way to practical computation of the velocities from the vortex elements and is the basis of successful algorithms     Software based on the vortex method offer a new means for solving tough fluid dynamics problems with minimal user intervention  citation needed   All that is required is specification of problem geometry and setting of boundary and initial conditions  Among the significant advantages of this modern technology     The vorticity confinement  VC  method is an Eulerian technique used in the simulation of turbulent wakes  It uses a solitary-wave like approach to produce a stable solution with no numerical spreading  VC can capture the small-scale features to within as few as 2 grid cells  Within these features  a nonlinear difference equation is solved as opposed to the finite difference equation  VC is similar to shock capturing methods  where conservation laws are satisfied  so that the essential integral quantities are accurately computed     The Linear eddy model is a technique used to simulate the convective mixing that takes place in turbulent flow  64  Specifically  it provides a mathematical way to describe the interactions of a scalar variable within the vector flow field  It is primarily used in one-dimensional representations of turbulent flow  since it can be applied across a wide range of length scales and Reynolds numbers  This model is generally used as a building block for more complicated flow representations  as it provides high resolution predictions that hold across a large range of flow conditions     The modeling of two-phase flow is still under development  Different methods have been proposed  including the Volume of fluid method  the level-set method and front tracking  65  66   These methods often involve a tradeoff between maintaining a sharp interface or conserving mass according to whom?    This is crucial since the evaluation of the density  viscosity and surface tension is based on the values averaged over the interface  citation needed  Lagrangian multiphase models  which are used for dispersed media  are based on solving the Lagrangian equation of motion for the dispersed phase  citation needed     Discretization in the space produces a system of ordinary differential equations for unsteady problems and algebraic equations for steady problems  Implicit or semi-implicit methods are generally used to integrate the ordinary differential equations  producing a system of  usually  nonlinear algebraic equations   Applying a Newton or Picard iteration produces a system of linear equations which is nonsymmetric in the presence of advection and indefinite in the presence of incompressibility  Such systems  particularly in 3D  are frequently too large for direct solvers  so iterative methods are used  either stationary methods such as successive overrelaxation or Krylov subspace methods  Krylov methods such as GMRES  typically used with preconditioning  operate by minimizing the residual over successive subspaces generated by the preconditioned operator     Multigrid has the advantage of asymptotically optimal performance on many problems  Traditional according to whom?  solvers and preconditioners are effective at reducing high-frequency components of the residual  but low-frequency components typically require many iterations to reduce  By operating on multiple scales  multigrid reduces all components of the residual by similar factors  leading to a mesh-independent number of iterations  citation needed     For indefinite systems  preconditioners such as incomplete LU factorization  additive Schwarz  and multigrid perform poorly or fail entirely  so the problem structure must be used for effective preconditioning  67  Methods commonly used in CFD are the SIMPLE and Uzawa algorithms which exhibit mesh-dependent convergence rates  but recent advances based on block LU factorization combined with multigrid for the resulting definite systems have led to preconditioners that deliver mesh-independent convergence rates  68     CFD made a major break through in late 70s with the introduction of LTRAN2  a 2-D code to model oscillating airfoils based on transonic small perturbation theory by Ballhaus and associates  69  It uses a Murman-Cole switch algorithm for modeling the moving shock-waves  70  Later it was extended to 3-D with use of a rotated difference scheme by AFWAL/Boeing that resulted in LTRAN3  71  72     CFD investigations are used to clarify the characteristics of aortic flow in details that are beyond the capabilities of experimental measurements  To analyze these conditions  CAD models of the human vascular system are extracted employing modern imaging techniques such as MRI or Computed Tomography  A 3D model is reconstructed from this data and the fluid flow can be computed  Blood properties such as density and viscosity  and realistic boundary conditions  e g   systemic pressure  have to be taken into consideration  Therefore  making it possible to analyze and optimize the flow in the cardiovascular system for different applications  73     Traditionally  CFD simulations are performed on CPUs  citation needed  In a more recent trend  simulations are also performed on GPUs  These typically contain slower but more processors  For CFD algorithms that feature good parallelism performance  i e  good speed-up by adding more cores  this can greatly reduce simulation times  Fluid-implicit particle 74  and lattice-Boltzmann methods 75  are typical examples of codes that scale well on GPUs         A computer is a machine that can be programmed to carry out sequences of arithmetic or logical operations automatically  Modern computers can perform generic sets of operations known as programs  These programs enable computers to perform a wide range of tasks  A computer system is a \"complete\" computer that includes the hardware  operating system  main software   and peripheral equipment needed and used for \"full\" operation  This term may also refer to a group of computers that are linked and function together  such as a computer network or computer cluster     A broad range of industrial and consumer products use computers as control systems  Simple special-purpose devices like microwave ovens and remote controls are included  as are factory devices like industrial robots and computer-aided design  as well as general-purpose devices like personal computers and mobile devices like smartphones  Computers power the Internet  which links hundreds of millions of other computers and users     Early computers were meant to be used only for calculations  Simple manual instruments like the abacus have aided people in doing calculations since ancient times  Early in the Industrial Revolution  some mechanical devices were built to automate long tedious tasks  such as guiding patterns for looms  More sophisticated electrical machines did specialized analog calculations in the early 20th century  The first digital electronic calculating machines were developed during World War II  The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET  MOS transistor  and monolithic integrated circuit  IC  chip technologies in the late 1950s  leading to the microprocessor and the microcomputer revolution in the 1970s  The speed  power and versatility of computers have been increasing dramatically ever since then  with transistor counts increasing at a rapid pace  as predicted by Moore  s law   leading to the Digital Revolution during the late 20th to early 21st centuries     Conventionally  a modern computer consists of at least one processing element  typically a central processing unit  CPU  in the form of a microprocessor  along with some type of computer memory  typically semiconductor memory chips  The processing element carries out arithmetic and logical operations  and a sequencing and control unit can change the order of operations in response to stored information  Peripheral devices include input devices  keyboards  mice  joystick  etc    output devices  monitor screens  printers  etc    and input/output devices that perform both functions  e g   the 2000s-era touchscreen   Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved     According to the Oxford English Dictionary  the first known use of computer was in a 1613 book called The Yong Mans Gleanings by the English writer Richard Braithwait  \"I haue   sic  read the truest computer of Times  and the best Arithmetician that euer  sic  breathed  and he reduceth thy dayes into a short number \" This usage of the term referred to a human computer  a person who carried out calculations or computations  The word continued with the same meaning until the middle of the 20th century  During the latter part of this period women were often hired as computers because they could be paid less than their male counterparts  1  By 1943  most human computers were women  2     The Online Etymology Dictionary gives the first attested use of computer in the 1640s  meaning   one who calculates    this is an \"agent noun from compute  v  \"  The Online Etymology Dictionary states that the use of the term to mean \"  calculating machine    of any type  is from 1897 \"  The Online Etymology Dictionary indicates that the \"modern use\" of the term  to mean   programmable digital electronic computer   dates from \"1945 under this name   in a  theoretical  sense  from 1937  as Turing machine\"  3     Devices have been used to aid computation for thousands of years  mostly using one-to-one correspondence with fingers  The earliest counting device was probably a form of tally stick  Later record keeping aids throughout the Fertile Crescent included calculi  clay spheres  cones  etc   which represented counts of items  probably livestock or grains  sealed in hollow unbaked clay containers  4  5  The use of counting rods is one example     The abacus was initially used for arithmetic tasks  The Roman abacus was developed from devices used in Babylonia as early as 2400 BC  Since then  many other forms of reckoning boards or tables have been invented  In a medieval European counting house  a checkered cloth would be placed on a table  and markers moved around on it according to certain rules  as an aid to calculating sums of money  6     The Antikythera mechanism is believed to be the earliest mechanical analog computer  according to Derek J  de Solla Price  7  It was designed to calculate astronomical positions  It was discovered in 1901 in the Antikythera wreck off the Greek island of Antikythera  between Kythera and Crete  and has been dated to c  u2009100 BC  Devices of a level of complexity comparable to that of the Antikythera mechanism would not reappear until a thousand years later     Many mechanical aids to calculation and measurement were constructed for astronomical and navigation use  The planisphere was a star chart invented by Ab Rayhn al-Brn in the early 11th century  8  The astrolabe was invented in the Hellenistic world in either the 1st or 2nd centuries BC and is often attributed to Hipparchus  A combination of the planisphere and dioptra  the astrolabe was effectively an analog computer capable of working out several different kinds of problems in spherical astronomy  An astrolabe incorporating a mechanical calendar computer 9  10  and gear-wheels was invented by Abi Bakr of Isfahan  Persia in 1235  11  Ab Rayhn al-Brn invented the first mechanical geared lunisolar calendar astrolabe  12  an early fixed-wired knowledge processing machine 13  with a gear train and gear-wheels  14  c  u20091000 AD     The sector  a calculating instrument used for solving problems in proportion  trigonometry  multiplication and division  and for various functions  such as squares and cube roots  was developed in the late 16th century and found application in gunnery  surveying and navigation     The planimeter was a manual instrument to calculate the area of a closed figure by tracing over it with a mechanical linkage     The slide rule was invented around 16201630 by the English clergyman William Oughtred  shortly after the publication of the concept of the logarithm  It is a hand-operated analog computer for doing multiplication and division  As slide rule development progressed  added scales provided reciprocals  squares and square roots  cubes and cube roots  as well as transcendental functions such as logarithms and exponentials  circular and hyperbolic trigonometry and other functions  Slide rules with special scales are still used for quick performance of routine calculations  such as the E6B circular slide rule used for time and distance calculations on light aircraft     In the 1770s  Pierre Jaquet-Droz  a Swiss watchmaker  built a mechanical doll  automaton  that could write holding a quill pen  By switching the number and order of its internal wheels different letters  and hence different messages  could be produced  In effect  it could be mechanically \"programmed\" to read instructions  Along with two other complex machines  the doll is at the Muse d  Art et d  Histoire of Neuchtel  Switzerland  and still operates  15     In 18311835  mathematician and engineer Giovanni Plana devised a Perpetual Calendar machine  which  though a system of pulleys and cylinders and over  could predict the perpetual calendar for every year from AD xa00  that is  1 xa0BC  to AD xa04000  keeping track of leap years and varying day length  The tide-predicting machine invented by the Scottish scientist Sir William Thomson in 1872 was of great utility to navigation in shallow waters  It used a system of pulleys and wires to automatically calculate predicted tide levels for a set period at a particular location     The differential analyser  a mechanical analog computer designed to solve differential equations by integration  used wheel-and-disc mechanisms to perform the integration  In 1876  Sir William Thomson had already discussed the possible construction of such calculators  but he had been stymied by the limited output torque of the ball-and-disk integrators  16  In a differential analyzer  the output of one integrator drove the input of the next integrator  or a graphing output  The torque amplifier was the advance that allowed these machines to work  Starting in the 1920s  Vannevar Bush and others developed mechanical differential analyzers     Charles Babbage  an English mechanical engineer and polymath  originated the concept of a programmable computer  Considered the \"father of the computer\"  17  he conceptualized and invented the first mechanical computer in the early 19th century  After working on his revolutionary difference engine  designed to aid in navigational calculations  in 1833 he realized that a much more general design  an Analytical Engine  was possible  The input of programs and data was to be provided to the machine via punched cards  a method being used at the time to direct mechanical looms such as the Jacquard loom  For output  the machine would have a printer  a curve plotter and a bell  The machine would also be able to punch numbers onto cards to be read in later  The Engine incorporated an arithmetic logic unit  control flow in the form of conditional branching and loops  and integrated memory  making it the first design for a general-purpose computer that could be described in modern terms as Turing-complete  18  19     The machine was about a century ahead of its time  All the parts for his machine had to be made by hand xa0 this was a major problem for a device with thousands of parts  Eventually  the project was dissolved with the decision of the British Government to cease funding  Babbage  s failure to complete the analytical engine can be chiefly attributed to political and financial difficulties as well as his desire to develop an increasingly sophisticated computer and to move ahead faster than anyone else could follow  Nevertheless  his son  Henry Babbage  completed a simplified version of the analytical engine  s computing unit  the mill  in 1888  He gave a successful demonstration of its use in computing tables in 1906     During the first half of the 20th century  many scientific computing needs were met by increasingly sophisticated analog computers  which used a direct mechanical or electrical model of the problem as a basis for computation  However  these were not programmable and generally lacked the versatility and accuracy of modern digital computers  20  The first modern analog computer was a tide-predicting machine  invented by Sir William Thomson  later to become Lord Kelvin  in 1872  The differential analyser  a mechanical analog computer designed to solve differential equations by integration using wheel-and-disc mechanisms  was conceptualized in 1876 by James Thomson  the elder brother of the more famous Sir William Thomson  16     The art of mechanical analog computing reached its zenith with the differential analyzer  built by H  L  Hazen and Vannevar Bush at MIT starting in 1927  This built on the mechanical integrators of James Thomson and the torque amplifiers invented by H  W  Nieman  A dozen of these devices were built before their obsolescence became obvious  By the 1950s  the success of digital electronic computers had spelled the end for most analog computing machines  but analog computers remained in use during the 1950s in some specialized applications such as education  slide rule  and aircraft  control systems      By 1938  the United States Navy had developed an electromechanical analog computer small enough to use aboard a submarine  This was the Torpedo Data Computer  which used trigonometry to solve the problem of firing a torpedo at a moving target  During World War II similar devices were developed in other countries as well     Early digital computers were electromechanical  electric switches drove mechanical relays to perform the calculation  These devices had a low operating speed and were eventually superseded by much faster all-electric computers  originally using vacuum tubes  The Z2  created by German engineer Konrad Zuse in 1939  was one of the earliest examples of an electromechanical relay computer  21     In 1941  Zuse followed his earlier machine up with the Z3  the world  s first working electromechanical programmable  fully automatic digital computer  22  23  The Z3 was built with 2000 relays  implementing a 22 xa0bit word length that operated at a clock frequency of about 510 xa0Hz  24  Program code was supplied on punched film while data could be stored in 64 words of memory or supplied from the keyboard  It was quite similar to modern machines in some respects  pioneering numerous advances such as floating point numbers  Rather than the harder-to-implement decimal system  used in Charles Babbage  s earlier design   using a binary system meant that Zuse  s machines were easier to build and potentially more reliable  given the technologies available at that time  25  The Z3 was not itself a universal computer but could be extended to be Turing complete  26  27      nPurely electronic circuit elements soon replaced their mechanical and electromechanical equivalents  at the same time that digital calculation replaced analog  The engineer Tommy Flowers  working at the Post Office Research Station in London in the 1930s  began to explore the possible use of electronics for the telephone exchange  Experimental equipment that he built in 1934 went into operation five years later  converting a portion of the telephone exchange network into an electronic data processing system  using thousands of vacuum tubes  20  In the US  John Vincent Atanasoff and Clifford E  Berry of Iowa State University developed and tested the AtanasoffBerry Computer  ABC  in 1942  28  the first \"automatic electronic digital computer\"  29  This design was also all-electronic and used about 300 vacuum tubes  with capacitors fixed in a mechanically rotating drum for memory  30     During World War II  the British code-breakers at Bletchley Park achieved a number of successes at breaking encrypted German military communications  The German encryption machine  Enigma  was first attacked with the help of the electro-mechanical bombes which were often run by women  31  32  To crack the more sophisticated German Lorenz SZ 40/42 machine  used for high-level Army communications  Max Newman and his colleagues commissioned Flowers to build the Colossus  30  He spent eleven months from early February 1943 designing and building the first Colossus  33  After a functional test in December 1943  Colossus was shipped to Bletchley Park  where it was delivered on 18 January 1944 34  and attacked its first message on 5 February  30     Colossus was the world  s first electronic digital programmable computer  20  It used a large number of valves  vacuum tubes   It had paper-tape input and was capable of being configured to perform a variety of boolean logical operations on its data  but it was not Turing-complete  Nine Mk II Colossi were built  The Mk I was converted to a Mk II making ten machines in total   Colossus Mark I contained 1 500 thermionic valves  tubes   but Mark II with 2 400 valves  was both 5 times faster and simpler to operate than Mark I  greatly speeding the decoding process  35  36     The ENIAC 37   Electronic Numerical Integrator and Computer  was the first electronic programmable computer built in the U S  Although the ENIAC was similar to the Colossus  it was much faster  more flexible  and it was Turing-complete  Like the Colossus  a \"program\" on the ENIAC was defined by the states of its patch cables and switches  a far cry from the stored program electronic machines that came later  Once a program was written  it had to be mechanically set into the machine with manual resetting of plugs and switches  The programmers of the ENIAC were six women  often known collectively as the \"ENIAC girls\"  38  39     It combined the high speed of electronics with the ability to be programmed for many complex problems  It could add or subtract 5000 times a second  a thousand times faster than any other machine  It also had modules to multiply  divide  and square root  High speed memory was limited to 20 words  about 80 bytes   Built under the direction of John Mauchly and J  Presper Eckert at the University of Pennsylvania  ENIAC  s development and construction lasted from 1943 to full operation at the end of 1945  The machine was huge  weighing 30 tons  using 200 kilowatts of electric power and contained over 18 000 vacuum tubes  1 500 relays  and hundreds of thousands of resistors  capacitors  and inductors  40     The principle of the modern computer was proposed by Alan Turing in his seminal 1936 paper  41  On Computable Numbers  Turing proposed a simple device that he called \"Universal Computing machine\" and that is now known as a universal Turing machine  He proved that such a machine is capable of computing anything that is computable by executing instructions  program  stored on tape  allowing the machine to be programmable  The fundamental concept of Turing  s design is the stored program  where all the instructions for computing are stored in memory  Von Neumann acknowledged that the central concept of the modern computer was due to this paper  42  Turing machines are to this day a central object of study in theory of computation  Except for the limitations imposed by their finite memory stores  modern computers are said to be Turing-complete  which is to say  they have algorithm execution capability equivalent to a universal Turing machine     Early computing machines had fixed programs  Changing its function required the re-wiring and re-structuring of the machine  30  With the proposal of the stored-program computer this changed  A stored-program computer includes by design an instruction set and can store in memory a set of instructions  a program  that details the computation  The theoretical basis for the stored-program computer was laid by Alan Turing in his 1936 paper  In 1945  Turing joined the National Physical Laboratory and began work on developing an electronic stored-program digital computer  His 1945 report \"Proposed Electronic Calculator\" was the first specification for such a device  John von Neumann at the University of Pennsylvania also circulated his First Draft of a Report on the EDVAC in 1945  20     The Manchester Baby was the world  s first stored-program computer  It was built at the University of Manchester in England by Frederic C  Williams  Tom Kilburn and Geoff Tootill  and ran its first program on 21 June 1948  43  It was designed as a testbed for the Williams tube  the first random-access digital storage device  44  Although the computer was considered \"small and primitive\" by the standards of its time  it was the first working machine to contain all of the elements essential to a modern electronic computer  45  As soon as the Baby had demonstrated the feasibility of its design  a project was initiated at the university to develop it into a more usable computer  the Manchester Mark 1  Grace Hopper was the first person to develop a compiler for programming language  2     The Mark 1 in turn quickly became the prototype for the Ferranti Mark 1  the world  s first commercially available general-purpose computer  46  Built by Ferranti  it was delivered to the University of Manchester in February 1951  At least seven of these later machines were delivered between 1953 and 1957  one of them to Shell labs in Amsterdam  47  In October 1947  the directors of British catering company J  Lyons & Company decided to take an active role in promoting the commercial development of computers  The LEO I computer became operational in April 1951 48  and ran the world  s first regular routine office computer job     The concept of a field-effect transistor was proposed by Julius Edgar Lilienfeld in 1925  John Bardeen and Walter Brattain  while working under William Shockley at Bell Labs  built the first working transistor  the point-contact transistor  in 1947  which was followed by Shockley  s bipolar junction transistor in 1948  49  50  From 1955 onwards  transistors replaced vacuum tubes in computer designs  giving rise to the \"second generation\" of computers  Compared to vacuum tubes  transistors have many advantages  they are smaller  and require less power than vacuum tubes  so give off less heat  Junction transistors were much more reliable than vacuum tubes and had longer  indefinite  service life  Transistorized computers could contain tens of thousands of binary logic circuits in a relatively compact space  However  early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis  which limited them to a number of specialised applications  51     At the University of Manchester  a team under the leadership of Tom Kilburn designed and built a machine using the newly developed transistors instead of valves  52  Their first transistorised computer and the first in the world  was operational by 1953  and a second version was completed there in April 1955  However  the machine did make use of valves to generate its 125 xa0kHz clock waveforms and in the circuitry to read and write on its magnetic drum memory  so it was not the first completely transistorized computer  That distinction goes to the Harwell CADET of 1955  53  built by the electronics division of the Atomic Energy Research Establishment at Harwell  53  54     The metaloxidesilicon field-effect transistor  MOSFET   also known as the MOS transistor  was invented by Mohamed M  Atalla and Dawon Kahng at Bell Labs in 1959  55  It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses  51  With its high scalability  56  and much lower power consumption and higher density than bipolar junction transistors  57  the MOSFET made it possible to build high-density integrated circuits  58  59  In addition to data processing  it also enabled the practical use of MOS transistors as memory cell storage elements  leading to the development of MOS semiconductor memory  which replaced earlier magnetic-core memory in computers  The MOSFET led to the microcomputer revolution  60  and became the driving force behind the computer revolution  61  62  The MOSFET is the most widely used transistor in computers  63  64  and is the fundamental building block of digital electronics  65     The next great advance in computing power came with the advent of the integrated circuit  IC   nThe idea of the integrated circuit was first conceived by a radar scientist working for the Royal Radar Establishment of the Ministry of Defence  Geoffrey W A  Dummer  Dummer presented the first public description of an integrated circuit at the Symposium on Progress in Quality Electronic Components in Washington  xa0D C  on 7 May 1952  66     The first working ICs were invented by Jack Kilby at Texas Instruments and Robert Noyce at Fairchild Semiconductor  67  Kilby recorded his initial ideas concerning the integrated circuit in July 1958  successfully demonstrating the first working integrated example on 12 September 1958  68  In his patent application of 6 February 1959  Kilby described his new device as \"a body of semiconductor material xa0    wherein all the components of the electronic circuit are completely integrated\"  69  70  However  Kilby  s invention was a hybrid integrated circuit  hybrid IC   rather than a monolithic integrated circuit  IC  chip  71  Kilby  s IC had external wire connections  which made it difficult to mass-produce  72     Noyce also came up with his own idea of an integrated circuit half a year later than Kilby  73  Noyce  s invention was the first true monolithic IC chip  74  72  His chip solved many practical problems that Kilby  s had not  Produced at Fairchild Semiconductor  it was made of silicon  whereas Kilby  s chip was made of germanium  Noyce  s monolithic IC was fabricated using the planar process  developed by his colleague Jean Hoerni in early 1959  In turn  the planar process was based on Mohamed M  Atalla  s work on semiconductor surface passivation by silicon dioxide in the late 1950s  75  76  77     Modern monolithic ICs are predominantly MOS  metal-oxide-semiconductor  integrated circuits  built from MOSFETs  MOS transistors   78  The earliest experimental MOS IC to be fabricated was a 16-transistor chip built by Fred Heiman and Steven Hofstein at RCA in 1962  79  General Microelectronics later introduced the first commercial MOS IC in 1964  80  developed by Robert Norman  79  Following the development of the self-aligned gate  silicon-gate  MOS transistor by Robert Kerwin  Donald Klein and John Sarace at Bell Labs in 1967  the first silicon-gate MOS IC with self-aligned gates was developed by Federico Faggin at Fairchild Semiconductor in 1968  81  The MOSFET has since become the most critical device component in modern ICs  82     The development of the MOS integrated circuit led to the invention of the microprocessor  83  84  and heralded an explosion in the commercial and personal use of computers  While the subject of exactly which device was the first microprocessor is contentious  partly due to lack of agreement on the exact definition of the term \"microprocessor\"  it is largely undisputed that the first single-chip microprocessor was the Intel 4004  85  designed and realized by Federico Faggin with his silicon-gate MOS IC technology  83  along with Ted Hoff  Masatoshi Shima and Stanley Mazor at Intel  86  87  In the early 1970s  MOS IC technology enabled the integration of more than 10 000 transistors on a single chip  59     System on a Chip  SoCs  are complete computers on a microchip  or chip  the size of a coin  88  They may or may not have integrated RAM and flash memory  If not integrated  the RAM is usually placed directly above  known as Package on package  or below  on the opposite side of the circuit board  the SoC  and the flash memory is usually placed right next to the SoC  this all done to improve data transfer speeds  as the data signals don  t have to travel long distances  Since ENIAC in 1945  computers have advanced enormously  with modern SoCs  Such as the Snapdragon 865  being the size of a coin while also being hundreds of thousands of times more powerful than ENIAC  integrating billions of transistors  and consuming only a few watts of power     The first mobile computers were heavy and ran from mains power  The 50lb IBM 5100 was an early example  Later portables such as the Osborne 1 and Compaq Portable were considerably lighter but still needed to be plugged in  The first laptops  such as the Grid Compass  removed this requirement by incorporating batteries  and with the continued miniaturization of computing resources and advancements in portable battery life  portable computers grew in popularity in the 2000s  89  The same developments allowed manufacturers to integrate computing resources into cellular mobile phones by the early 2000s     These smartphones and tablets run on a variety of operating systems and recently became the dominant computing device on the market  90  These are powered by System on a Chip  SoCs   which are complete computers on a microchip the size of a coin  88     Computers can be classified in a number of different ways  including     The term hardware covers all of those parts of a computer that are tangible physical objects  Circuits  computer chips  graphic cards  sound cards  memory  RAM   motherboard  displays  power supplies  cables  keyboards  printers and \"mice\" input devices are all hardware     A general-purpose computer has four main components  the arithmetic logic unit  ALU   the control unit  the memory  and the input and output devices  collectively termed I/O   These parts are interconnected by buses  often made of groups of wires  Inside each of these parts are thousands to trillions of small electrical circuits which can be turned off or on by means of an electronic switch  Each circuit represents a bit  binary digit  of information so that when the circuit is on it represents a \"1\"  and when off it represents a \"0\"  in positive logic representation   The circuits are arranged in logic gates so that one or more of the circuits may control the state of one or more of the other circuits     When unprocessed data is sent to the computer with the help of input devices  the data is processed and sent to output devices  The input devices may be hand-operated or automated  The act of processing is mainly regulated by the CPU  Some examples of input devices are     The means through which computer gives output are known as output devices  Some examples of output devices are     The control unit  often called a control system or central controller  manages the computer  s various components  it reads and interprets  decodes  the program instructions  transforming them into control signals that activate other parts of the computer  92  Control systems in advanced computers may change the order of execution of some instructions to improve performance     A key component common to all CPUs is the program counter  a special memory cell  a register  that keeps track of which location in memory the next instruction is to be read from  93    \"The control system s function is as follows this is a simplified description  and some of these steps may be performed concurrently or in a different order depending on the type of CPU  n\"   Since the program counter is  conceptually  just another set of memory cells  it can be changed by calculations done in the ALU  Adding 100 to the program counter would cause the next instruction to be read from a place 100 locations further down the program  Instructions that modify the program counter are often known as \"jumps\" and allow for loops  instructions that are repeated by the computer  and often conditional instruction execution  both examples of control flow      The sequence of operations that the control unit goes through to process an instruction is in itself like a short computer program  and indeed  in some more complex CPU designs  there is another yet smaller computer called a microsequencer  which runs a microcode program that causes all of these events to happen     The control unit  ALU  and registers are collectively known as a central processing unit  CPU   Early CPUs were composed of many separate components  Since the 1970s  CPUs have typically been constructed on a single MOS integrated circuit chip called a microprocessor     The ALU is capable of performing two classes of operations  arithmetic and logic  94  The set of arithmetic operations that a particular ALU supports may be limited to addition and subtraction  or might include multiplication  division  trigonometry functions such as sine  cosine  etc   and square roots  Some can operate only on whole numbers  integers  while others use floating point to represent real numbers  albeit with limited precision  However  any computer that is capable of performing just the simplest operations can be programmed to break down the more complex operations into simple steps that it can perform  Therefore  any computer can be programmed to perform any arithmetic operationalthough it will take more time to do so if its ALU does not directly support the operation  An ALU may also compare numbers and return boolean truth values  true or false  depending on whether one is equal to  greater than or less than the other  \"is 64 greater than 65?\"   Logic operations involve Boolean logic  AND  OR  XOR  and NOT  These can be useful for creating complicated conditional statements and processing boolean logic     Superscalar computers may contain multiple ALUs  allowing them to process several instructions simultaneously  95  Graphics processors and computers with SIMD and MIMD features often contain ALUs that can perform arithmetic on vectors and matrices     A computer  s memory can be viewed as a list of cells into which numbers can be placed or read  Each cell has a numbered \"address\" and can store a single number  The computer can be instructed to \"put the number 123 into the cell numbered 1357\" or to \"add the number that is in cell 1357 to the number that is in cell 2468 and put the answer into cell 1595 \" The information stored in memory may represent practically anything  Letters  numbers  even computer instructions can be placed into memory with equal ease  Since the CPU does not differentiate between different types of information  it is the software  s responsibility to give significance to what the memory sees as nothing but a series of numbers     In almost all modern computers  each memory cell is set up to store binary numbers in groups of eight bits  called a byte   Each byte is able to represent 256 different numbers  28 = 256   either from 0 to 255 or 128 to +127  To store larger numbers  several consecutive bytes may be used  typically  two  four or eight   When negative numbers are required  they are usually stored in two  s complement notation  Other arrangements are possible  but are usually not seen outside of specialized applications or historical contexts  A computer can store any kind of information in memory if it can be represented numerically  Modern computers have billions or even trillions of bytes of memory     The CPU contains a special set of memory cells called registers that can be read and written to much more rapidly than the main memory area  There are typically between two and one hundred registers depending on the type of CPU  Registers are used for the most frequently needed data items to avoid having to access main memory every time data is needed  As data is constantly being worked on  reducing the need to access main memory  which is often slow compared to the ALU and control units  greatly increases the computer  s speed     Computer main memory comes in two principal varieties     RAM can be read and written to anytime the CPU commands it  but ROM is preloaded with data and software that never changes  therefore the CPU can only read from it  ROM is typically used to store the computer  s initial start-up instructions  In general  the contents of RAM are erased when the power to the computer is turned off  but ROM retains its data indefinitely  In a PC  the ROM contains a specialized program called the BIOS that orchestrates loading the computer  s operating system from the hard disk drive into RAM whenever the computer is turned on or reset  In embedded computers  which frequently do not have disk drives  all of the required software may be stored in ROM  Software stored in ROM is often called firmware  because it is notionally more like hardware than software  Flash memory blurs the distinction between ROM and RAM  as it retains its data when turned off but is also rewritable  It is typically much slower than conventional ROM and RAM however  so its use is restricted to applications where high speed is unnecessary  96     In more sophisticated computers there may be one or more RAM cache memories  which are slower than registers but faster than main memory  Generally computers with this sort of cache are designed to move frequently needed data into the cache automatically  often without the need for any intervention on the programmer  s part     I/O is the means by which a computer exchanges information with the outside world  97  Devices that provide input or output to the computer are called peripherals  98  On a typical personal computer  peripherals include input devices like the keyboard and mouse  and output devices such as the display and printer  Hard disk drives  floppy disk drives and optical disc drives serve as both input and output devices  Computer networking is another form of I/O  nI/O devices are often complex computers in their own right  with their own CPU and memory  A graphics processing unit might contain fifty or more tiny computers that perform the calculations necessary to display 3D graphics  citation needed  Modern desktop computers contain many smaller computers that assist the main CPU in performing I/O  A 2016-era flat screen display contains its own computer circuitry     While a computer may be viewed as running one gigantic program stored in its main memory  in some systems it is necessary to give the appearance of running several programs simultaneously  This is achieved by multitasking i e  having the computer switch rapidly between running each program in turn  99  One means by which this is done is with a special signal called an interrupt  which can periodically cause the computer to stop executing instructions where it was and do something else instead  By remembering where it was executing prior to the interrupt  the computer can return to that task later  If several programs are running \"at the same time\"  then the interrupt generator might be causing several hundred interrupts per second  causing a program switch each time  Since modern computers typically execute instructions several orders of magnitude faster than human perception  it may appear that many programs are running at the same time even though only one is ever executing in any given instant  This method of multitasking is sometimes termed \"time-sharing\" since each program is allocated a \"slice\" of time in turn  100     Before the era of inexpensive computers  the principal use for multitasking was to allow many people to share the same computer  Seemingly  multitasking would cause a computer that is switching between several programs to run more slowly  in direct proportion to the number of programs it is running  but most programs spend much of their time waiting for slow input/output devices to complete their tasks  If a program is waiting for the user to click on the mouse or press a key on the keyboard  then it will not take a \"time slice\" until the event it is waiting for has occurred  This frees up time for other programs to execute so that many programs may be run simultaneously without unacceptable speed loss     Some computers are designed to distribute their work across several CPUs in a multiprocessing configuration  a technique once employed in only large and powerful machines such as supercomputers  mainframe computers and servers  Multiprocessor and multi-core  multiple CPUs on a single integrated circuit  personal and laptop computers are now widely available  and are being increasingly used in lower-end markets as a result     Supercomputers in particular often have highly unique architectures that differ significantly from the basic stored-program architecture and from general-purpose computers  101  They often feature thousands of CPUs  customized high-speed interconnects  and specialized computing hardware  Such designs tend to be useful for only specialized tasks due to the large scale of program organization required to successfully utilize most of the available resources at once  Supercomputers usually see usage in large-scale simulation  graphics rendering  and cryptography applications  as well as with other so-called \"embarrassingly parallel\" tasks     Software refers to parts of the computer which do not have a material form  such as programs  data  protocols  etc  Software is that part of a computer system that consists of encoded information or computer instructions  in contrast to the physical hardware from which the system is built  Computer software includes computer programs  libraries and related non-executable data  such as online documentation or digital media  It is often divided into system software and application software Computer hardware and software require each other and neither can be realistically used on its own  When software is stored in hardware that cannot easily be modified  such as with BIOS ROM in an IBM PC compatible computer  it is sometimes called \"firmware\"     There are thousands of different programming languagessome intended for general purpose  others useful for only highly specialized applications     The defining feature of modern computers which distinguishes them from all other machines is that they can be programmed  That is to say that some type of instructions  the program  can be given to the computer  and it will process them  Modern computers based on the von Neumann architecture often have machine code in the form of an imperative programming language  In practical terms  a computer program may be just a few instructions or extend to many millions of instructions  as do the programs for word processors and web browsers for example  A typical modern computer can execute billions of instructions per second  gigaflops  and rarely makes a mistake over many years of operation  Large computer programs consisting of several million instructions may take teams of programmers years to write  and due to the complexity of the task almost certainly contain errors     This section applies to most common RAM machinebased computers     In most cases  computer instructions are simple  add one number to another  move some data from one location to another  send a message to some external device  etc  These instructions are read from the computer  s memory and are generally carried out  executed  in the order they were given  However  there are usually specialized instructions to tell the computer to jump ahead or backwards to some other place in the program and to carry on executing from there  These are called \"jump\" instructions  or branches   Furthermore  jump instructions may be made to happen conditionally so that different sequences of instructions may be used depending on the result of some previous calculation or some external event  Many computers directly support subroutines by providing a type of jump that \"remembers\" the location it jumped from and another instruction to return to the instruction following that jump instruction     Program execution might be likened to reading a book  While a person will normally read each word and line in sequence  they may at times jump back to an earlier place in the text or skip sections that are not of interest  Similarly  a computer may sometimes go back and repeat the instructions in some section of the program over and over again until some internal condition is met  This is called the flow of control within the program and it is what allows the computer to perform tasks repeatedly without human intervention     Comparatively  a person using a pocket calculator can perform a basic arithmetic operation such as adding two numbers with just a few button presses  But to add together all of the numbers from 1 to 1 000 would take thousands of button presses and a lot of time  with a near certainty of making a mistake  On the other hand  a computer may be programmed to do this with just a few simple instructions  The following example is written in the MIPS assembly language     Once told to run this program  the computer will perform the repetitive addition task without further human intervention  It will almost never make a mistake and a modern PC can complete the task in a fraction of a second     In most computers  individual instructions are stored as machine code with each instruction being given a unique number  its operation code or opcode for short   The command to add two numbers together would have one opcode  the command to multiply them would have a different opcode  and so on  The simplest computers are able to perform any of a handful of different instructions  the more complex computers have several hundred to choose from  each with a unique numerical code  Since the computer  s memory is able to store numbers  it can also store the instruction codes  This leads to the important fact that entire programs  which are just lists of these instructions  can be represented as lists of numbers and can themselves be manipulated inside the computer in the same way as numeric data  The fundamental concept of storing programs in the computer  s memory alongside the data they operate on is the crux of the von Neumann  or stored program citation needed   architecture  In some cases  a computer might store some or all of its program in memory that is kept separate from the data it operates on  This is called the Harvard architecture after the Harvard Mark I computer  Modern von Neumann computers display some traits of the Harvard architecture in their designs  such as in CPU caches     While it is possible to write computer programs as long lists of numbers  machine language  and while this technique was used with many early computers  102  it is extremely tedious and potentially error-prone to do so in practice  especially for complicated programs  Instead  each basic instruction can be given a short name that is indicative of its function and easy to remember xa0 a mnemonic such as ADD  SUB  MULT or JUMP  These mnemonics are collectively known as a computer  s assembly language  Converting programs written in assembly language into something the computer can actually understand  machine language  is usually done by a computer program called an assembler     Programming languages provide various ways of specifying programs for computers to run  Unlike natural languages  programming languages are designed to permit no ambiguity and to be concise  They are purely written languages and are often difficult to read aloud  They are generally either translated into machine code by a compiler or an assembler before being run  or translated directly at run time by an interpreter  Sometimes programs are executed by a hybrid method of the two techniques     Machine languages and the assembly languages that represent them  collectively termed low-level programming languages  are generally unique to the particular architecture of a computer  s central processing unit  CPU   For instance  an ARM architecture CPU  such as may be found in a smartphone or a hand-held videogame  cannot understand the machine language of an x86 CPU that might be in a PC  103  Historically a significant number of other cpu architectures were created and saw extensive use  notably including the MOS Technology 6502 and 6510 in addition to the Zilog Z80     Although considerably easier than in machine language  writing long programs in assembly language is often difficult and is also error prone  Therefore  most practical programs are written in more abstract high-level programming languages that are able to express the needs of the programmer more conveniently  and thereby help reduce programmer error   High level languages are usually \"compiled\" into machine language  or sometimes into assembly language and then into machine language  using another computer program called a compiler  104  High level languages are less related to the workings of the target computer than assembly language  and more related to the language and structure of the problem s  to be solved by the final program  It is therefore often possible to use different compilers to translate the same high level language program into the machine language of many different types of computer  This is part of the means by which software like video games may be made available for different computer architectures such as personal computers and various video game consoles     Program design of small programs is relatively simple and involves the analysis of the problem  collection of inputs  using the programming constructs within languages  devising or using established procedures and algorithms  providing data for output devices and solutions to the problem as applicable  As problems become larger and more complex  features such as subprograms  modules  formal documentation  and new paradigms such as object-oriented programming are encountered  Large programs involving thousands of line of code and more require formal software methodologies  nThe task of developing large software systems presents a significant intellectual challenge  Producing software with an acceptably high reliability within a predictable schedule and budget has historically been difficult  the academic and professional discipline of software engineering concentrates specifically on this challenge     Errors in computer programs are called \"bugs\"  They may be benign and not affect the usefulness of the program  or have only subtle effects  But in some cases  they may cause the program or the entire system to \"hang\"  becoming unresponsive to input such as mouse clicks or keystrokes  to completely fail  or to crash  Otherwise benign bugs may sometimes be harnessed for malicious intent by an unscrupulous user writing an exploit  code designed to take advantage of a bug and disrupt a computer  s proper execution  Bugs are usually not the fault of the computer  Since computers merely execute the instructions they are given  bugs are nearly always the result of programmer error or an oversight made in the program  s design  105  nAdmiral Grace Hopper  an American computer scientist and developer of the first compiler  is credited for having first used the term \"bugs\" in computing after a dead moth was found shorting a relay in the Harvard Mark II computer in September 1947  106     Computers have been used to coordinate information between multiple locations since the 1950s  The U S  military  s SAGE system was the first large-scale example of such a system  which led to a number of special-purpose commercial systems such as Sabre  107  In the 1970s  computer engineers at research institutions throughout the United States began to link their computers together using telecommunications technology  The effort was funded by ARPA  now DARPA   and the computer network that resulted was called the ARPANET  108  The technologies that made the Arpanet possible spread and evolved     In time  the network spread beyond academic and military institutions and became known as the Internet  The emergence of networking involved a redefinition of the nature and boundaries of the computer  Computer operating systems and applications were modified to include the ability to define and access the resources of other computers on the network  such as peripheral devices  stored information  and the like  as extensions of the resources of an individual computer  Initially these facilities were available primarily to people working in high-tech environments  but in the 1990s the spread of applications like e-mail and the World Wide Web  combined with the development of cheap  fast networking technologies like Ethernet and ADSL saw computer networking become almost ubiquitous  In fact  the number of computers that are networked is growing phenomenally  A very large proportion of personal computers regularly connect to the Internet to communicate and receive information  \"Wireless\" networking  often utilizing mobile phone networks  has meant networking is becoming increasingly ubiquitous even in mobile computing environments     A computer does not need to be electronic  nor even have a processor  nor RAM  nor even a hard disk  While popular usage of the word \"computer\" is synonymous with a personal electronic computer  the modern 109  definition of a computer is literally  \"A device that computes  especially a programmable  usually  electronic machine that performs high-speed mathematical or logical operations or that assembles  stores  correlates  or otherwise processes information \" 110  Any device which processes information qualifies as a computer  especially if the processing is purposeful  citation needed     There is active research to make computers out of many promising new types of technology  such as optical computers  DNA computers  neural computers  and quantum computers  Most computers are universal  and are able to calculate any computable function  and are limited only by their memory capacity and operating speed  However different designs of computers can give very different performance for particular problems  for example quantum computers can potentially break some modern encryption algorithms  by quantum factoring  very quickly     There are many types of computer architectures     Of all these abstract machines  a quantum computer holds the most promise for revolutionizing computing  111  Logic gates are a common abstraction which can apply to most of the above digital or analog paradigms  The ability to store and execute lists of instructions called programs makes computers extremely versatile  distinguishing them from calculators  The ChurchTuring thesis is a mathematical statement of this versatility  any computer with a minimum capability  being Turing-complete  is  in principle  capable of performing the same tasks that any other computer can perform  Therefore  any type of computer  netbook  supercomputer  cellular automaton  etc   is able to perform the same computational tasks  given enough time and storage capacity     A computer will solve problems in exactly the way it is programmed to  without regard to efficiency  alternative solutions  possible shortcuts  or possible errors in the code  Computer programs that learn and adapt are part of the emerging field of artificial intelligence and machine learning  Artificial intelligence based products generally fall into two major categories  rule based systems and pattern recognition systems  Rule based systems attempt to represent the rules used by human experts and tend to be expensive to develop  Pattern based systems use data about a problem to generate conclusions  Examples of pattern based systems include voice recognition  font recognition  translation and the emerging field of on-line marketing     As the use of computers has spread throughout society  there are an increasing number of careers involving computers     The need for computers to work well together and to be able to exchange information has spawned the need for many standards organizations  clubs and societies of both a formal and informal nature         Distributed computing is a field of computer science that studies distributed systems  A distributed system is a system whose components are located on different networked computers  which communicate and coordinate their actions by passing messages to one another from any system  1  The components interact with one another in order to achieve a common goal  Three significant characteristics of distributed systems are  concurrency of components  lack of a global clock  and independent failure of components  1  Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications     A computer program that runs within a distributed system is called  a distributed program  and distributed programming is the process of writing such programs   2  There are many different types of implementations for the message passing mechanism  including pure HTTP  RPC-like connectors and message queues  3     Distributed computing also refers to the use of distributed systems to solve computational problems  In distributed computing  a problem is divided into many tasks  each of which is solved by one or more computers  4  which communicate with each other via message passing  5     The word distributed in terms such as \"distributed system\"  \"distributed programming\"  and \"distributed algorithm\" originally referred to computer networks where individual computers were physically distributed within some geographical area  6  The terms are nowadays used in a much wider sense  even referring to autonomous processes that run on the same physical computer and interact with each other by message passing  5     While there is no single definition of a distributed system  7  the following defining properties are commonly used as     A distributed system may have a common goal  such as solving a large computational problem  10  the user then perceives the collection of autonomous processors as a unit  Alternatively  each computer may have its own user with individual needs  and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users  11     Other typical properties of distributed systems include the following     Distributed systems are groups of networked computers which share a common goal for their work  nThe terms \"concurrent computing\"  \"parallel computing\"  and \"distributed computing\" have much overlap  and no clear distinction exists between them  15  The same system may be characterized both as \"parallel\" and \"distributed\"  the processors in a typical distributed system run concurrently in parallel  16  Parallel computing may be seen as a particular tightly coupled form of distributed computing  17  and distributed computing may be seen as a loosely coupled form of parallel computing  7  Nevertheless  it is possible to roughly classify concurrent systems as \"parallel\" or \"distributed\" using the following criteria     The figure on the right illustrates the difference between distributed and parallel systems  Figure  a  is a schematic view of a typical distributed system  the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link  Figure  b  shows the same distributed system in more detail  each computer has its own local memory  and information can be exchanged only by passing messages from one node to another by using the available communication links  Figure  c  shows a parallel system in which each processor has a direct access to a shared memory     The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems  see below for more detailed discussion   Nevertheless  as a rule of thumb  high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms  20     The use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s  21  The first widespread distributed systems were local-area networks such as Ethernet  which was invented in the 1970s  22     ARPANET  one of the predecessors of the Internet  was introduced in the late 1960s  and ARPANET e-mail was invented in the early 1970s  E-mail became the most successful application of ARPANET  23  and it is probably the earliest example of a large-scale distributed application  In addition to ARPANET  and its successor  the global Internet   other early worldwide computer networks included Usenet and FidoNet from the 1980s  both of which were used to support distributed discussion systems  24     The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s  The first conference in the field  Symposium on Principles of Distributed Computing  PODC   dates back to 1982  and its counterpart International Symposium on Distributed Computing  DISC  was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs  25     Various hardware and software architectures are used for distributed computing  At a lower level  it is necessary to interconnect multiple CPUs with some sort of network  regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables  At a higher level  it is necessary to interconnect processes running on those CPUs with some sort of communication system  26     Distributed programming typically falls into one of several basic architectures  clientserver  three-tier  n-tier  or peer-to-peer  or categories  loose coupling  or tight coupling  27     Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes  Through various message passing protocols  processes may communicate directly with one another  typically in a master/slave relationship  Alternatively  a \"database-centric\" architecture can enable distributed computing to be done without any form of direct inter-process communication  by utilizing a shared database  30  Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay  This enables distributed computing functions both within and beyond the parameters of a networked database  31     Reasons for using distributed systems and distributed computing may include     Examples of distributed systems and applications of distributed computing include the following  33     Many tasks that we would like to automate by using a computer are of questionanswer type  we would like to ask a question and the computer should produce an answer  In theoretical computer science  such tasks are called computational problems  Formally  a computational problem consists of instances together with a solution for each instance  Instances are questions that we can ask  and solutions are desired answers to these questions     Theoretical computer science seeks to understand which computational problems can be solved by using a computer  computability theory  and how efficiently  computational complexity theory   Traditionally  it is said that a problem can be solved by using a computer if we can design an algorithm that produces a correct solution for any given instance  Such an algorithm can be implemented as a computer program that runs on a general-purpose computer  the program reads a problem instance from input  performs some computation  and produces the solution as output  Formalisms such as random-access machines or universal Turing machines can be used as abstract models of a sequential general-purpose computer executing such an algorithm  35  36     The field of concurrent and distributed computing studies similar questions in the case of either multiple computers  or a computer that executes a network of interacting processes  which computational problems can be solved in such a network and how efficiently? However  it is not at all obvious what is meant by \"solving a problem\" in the case of a concurrent or distributed system  for example  what is the task of the algorithm designer  and what is the concurrent or distributed equivalent of a sequential general-purpose computer? citation needed     The discussion below focuses on the case of multiple computers  although many of the issues are the same for concurrent processes running on a single computer     Three viewpoints are commonly used     In the case of distributed algorithms  computational problems are typically related to graphs  Often the graph that describes the structure of the computer network is the problem instance  This is illustrated in the following example  citation needed     Consider the computational problem of finding a coloring of a given graph G  Different fields might take the following approaches     While the field of parallel algorithms has a different focus than the field of distributed algorithms  there is much interaction between the two fields  For example  the ColeVishkin algorithm for graph coloring 41  was originally presented as a parallel algorithm  but the same technique can also be used directly as a distributed algorithm     Moreover  a parallel algorithm can be implemented either in a parallel system  using shared memory  or in a distributed system  using message passing   42  The traditional boundary between parallel and distributed algorithms  choose a suitable network vs  run in any given network  does not lie in the same place as the boundary between parallel and distributed systems  shared memory vs  message passing      In parallel algorithms  yet another resource in addition to time and space is the number of computers  Indeed  often there is a trade-off between the running time and the number of computers  the problem can be solved faster if there are more computers running in parallel  see speedup   If a decision problem can be solved in polylogarithmic time by using a polynomial number of processors  then the problem is said to be in the class NC  43  The class NC can be defined equally well by using the PRAM formalism or Boolean circuitsPRAM machines can simulate Boolean circuits efficiently and vice versa  44     In the analysis of distributed algorithms  more attention is usually paid on communication operations than computational steps  Perhaps the simplest model of distributed computing is a synchronous system where all nodes operate in a lockstep fashion  This model is commonly known as the LOCAL model  During each communication round  all nodes in parallel  1  xa0receive the latest messages from their neighbours   2  xa0perform arbitrary local computation  and  3  xa0send new messages to their neighbors  In such systems  a central complexity measure is the number of synchronous communication rounds required to complete the task  45     This complexity measure is closely related to the diameter of the network  Let D be the diameter of the network  On the one hand  any computable problem can be solved trivially in a synchronous distributed system in approximately 2D communication rounds  simply gather all information in one location  D rounds   solve the problem  and inform each node about the solution  D rounds      On the other hand  if the running time of the algorithm is much smaller than D communication rounds  then the nodes in the network must produce their output without having the possibility to obtain information about distant parts of the network  In other words  the nodes must make globally consistent decisions based on information that is available in their local D-neighbourhood  Many distributed algorithms are known with the running time much smaller than D rounds  and understanding which problems can be solved by such algorithms is one of the central research questions of the field  46  Typically an algorithm which solves a problem in polylogarithmic time in the network size is considered efficient in this model     Another commonly used measure is the total number of bits transmitted in the network  cf  communication complexity   47  The features of this concept are typically captured with the CONGEST B  model  which similarly defined as the LOCAL model but where single messages can only contain B bits     Traditional computational problems take the perspective that the user asks a question  a computer  or a distributed system  processes the question  then produces an answer and stops  However  there are also problems where the system is required not to stop  including the dining philosophers problem and other similar mutual exclusion problems  In these problems  the distributed system is supposed to continuously coordinate the use of shared resources so that no conflicts or deadlocks occur     There are also fundamental challenges that are unique to distributed computing  for example those related to fault-tolerance  Examples of related problems include consensus problems  48  Byzantine fault tolerance  49  and self-stabilisation  50     Much research is also focused on understanding the asynchronous nature of distributed systems     Coordinator election  or leader election  is the process of designating a single process as the organizer of some task distributed among several computers  nodes   Before the task is begun  all network nodes are either unaware which node will serve as the \"coordinator\"  or leader  of the task  or unable to communicate with the current coordinator  After a coordinator election algorithm has been run  however  each node throughout the network recognizes a particular  unique node as the task coordinator  54     The network nodes communicate among themselves in order to decide which of them will get into the \"coordinator\" state  For that  they need some method in order to break the symmetry among them  For example  if each node has unique and comparable identities  then the nodes can compare their identities  and decide that the node with the highest identity is the coordinator  54     The definition of this problem is often attributed to LeLann  who formalized it as a method to create a new token in a token ring network in which the token has been lost  55     Coordinator election algorithms are designed to be economical in terms of total bytes transmitted  and time  The algorithm suggested by Gallager  Humblet  and Spira  56  for general undirected graphs has had a strong impact on the design of distributed algorithms in general  and won the Dijkstra Prize for an influential paper in distributed computing     Many other algorithms were suggested for different kind of network graphs  such as undirected rings  unidirectional rings  complete graphs  grids  directed Euler graphs  and others  A general method that decouples the issue of the graph family from the design of the coordinator election algorithm was suggested by Korach  Kutten  and Moran  57     In order to perform coordination  distributed systems employ the concept of coordinators  The coordinator election problem is to choose a process from among a group of processes on different processors in a distributed system to act as the central coordinator  Several central coordinator election algorithms exist  58     So far the focus has been on designing a distributed system that solves a given problem  A complementary research problem is studying the properties of a given distributed system  59  60     The halting problem is an analogous example from the field of centralised computation  we are given a computer program and the task is to decide whether it halts or runs forever  The halting problem is undecidable in the general case  and naturally understanding the behaviour of a computer network is at least as hard as understanding the behaviour of one computer  61     However  there are many interesting special cases that are decidable  In particular  it is possible to reason about the behaviour of a network of finite-state machines  One example is telling whether a given network of interacting  asynchronous and non-deterministic  finite-state machines can reach a deadlock  This problem is PSPACE-complete  62  i e   it is decidable  but not likely that there is an efficient  centralised  parallel or distributed  algorithm that solves the problem in the case of large networks      n    Oxford University Press  OUP  is the university press of University of Oxford  It is the largest university press in the world  and the second oldest after Cambridge University Press  1  2  3  It is a department of the University of Oxford and is governed by a group of 15 academics appointed by the vice-chancellor known as the delegates of the press  They are headed by the secretary to the delegates  who serves as OUP  s chief executive and as its major representative on other university bodies  Oxford University Press has had a similar governance structure since the 17th century  4  The Press is located on Walton Street  Oxford  opposite Somerville College  in the inner suburb of Jericho     The university became involved in the print trade around 1480  and grew into a major printer of Bibles  prayer books  and scholarly works  5  OUP took on the project that became the Oxford English Dictionary in the late 19th century  and expanded to meet the ever-rising costs of the work  6  As a result  the last hundred years has seen Oxford publish further English and bilingual dictionaries  children  s books  school textbooks  music  journals  the World  s Classics series  and a range of English language teaching texts  Moves into international markets led to OUP opening its own offices outside the United Kingdom  beginning with New York City in 1896  7  With the advent of computer technology and increasingly harsh trading conditions  the Press  s printing house at Oxford was closed in 1989  and its former paper mill at Wolvercote was demolished in 2004  By contracting out its printing and binding operations  the modern OUP publishes some 6 000 new titles around the world each year  citation needed     The first printer associated with Oxford University was Theoderic Rood  A business associate of William Caxton  Rood seems to have brought his own wooden printing press to Oxford from Cologne as a speculative venture  and to have worked in the city between around 1480 and 1483  The first book printed in Oxford  in 1478  8  an edition of Rufinus  s Expositio in symbolum apostolorum  was printed by another  anonymous  printer  Famously  this was mis-dated in Roman numerals as \"1468\"  thus apparently pre-dating Caxton  Rood  s printing included John Ankywyll  s Compendium totius grammaticae  which set new standards for teaching of Latin grammar  9     After Rood  printing connected with the university remained sporadic for over half a century  Records of surviving work are few  and Oxford did not put its printing on a firm footing until the 1580s  this succeeded the efforts of Cambridge University  which had obtained a licence for its press in 1534  In response to constraints on printing outside London imposed by the Crown and the Stationers   Company  Oxford petitioned Elizabeth I of England for the formal right to operate a press at the university  The chancellor  Robert Dudley  1st Earl of Leicester  pleaded Oxford  s case  Some royal assent was obtained  since the printer Joseph Barnes began work  and a decree of Star Chamber noted the legal existence of a press at \"the universitie of Oxforde\" in 1586  10     Oxford  s chancellor  Archbishop William Laud  consolidated the legal status of the university  s printing in the 1630s  Laud envisaged a unified press of world repute  Oxford would establish it on university property  govern its operations  employ its staff  determine its printed work  and benefit from its proceeds  To that end  he petitioned Charles I for rights that would enable Oxford to compete with the Stationers   Company and the King  s Printer  and obtained a succession of royal grants to aid it  These were brought together in Oxford  s \"Great Charter\" in 1636  which gave the university the right to print \"all manner of books\"  11  Laud also obtained the \"privilege\" from the Crown of printing the King James or Authorized Version of Scripture at Oxford  12  This \"privilege\" created substantial returns in the next 250 years  although initially it was held in abeyance  The Stationers   Company was deeply alarmed by the threat to its trade and lost little time in establishing a \"Covenant of Forbearance\" with Oxford  Under this  the Stationers paid an annual rent for the university not to exercise its complete printing rights xa0 money Oxford used to purchase new printing equipment for smaller purposes  13     Laud also made progress with internal organization of the Press  Besides establishing the system of Delegates  he created the wide-ranging supervisory post of \"Architypographus\"  an academic who would have responsibility for every function of the business  from print shop management to proofreading  The post was more an ideal than a workable reality  but it survived  mostly as a sinecure  in the loosely structured Press until the 18th century  In practice  Oxford  s Warehouse-Keeper dealt with sales  accounting  and the hiring and firing of print shop staff  14     Laud  s plans  however  hit terrible obstacles  both personal and political  Falling foul of political intrigue  he was executed in 1645  by which time the English Civil War had broken out  Oxford became a Royalist stronghold during the conflict  and many printers in the city concentrated on producing political pamphlets or sermons  Some outstanding mathematical and Orientalist works emerged at this timenotably  texts edited by Edward Pococke  the Regius Professor of Hebrewbut no university press on Laud  s model was possible before the Restoration of the Monarchy in 1660  15     It was finally established by the vice-chancellor  John Fell  Dean of Christ Church  Bishop of Oxford  and Secretary to the Delegates  Fell regarded Laud as a martyr  and was determined to honour his vision of the Press  Using the provisions of the Great Charter  Fell persuaded Oxford to refuse any further payments from the Stationers and drew all printers working for the university onto one set of premises  This business was set up in the cellars of the new Sheldonian Theatre  where Fell installed printing presses in 1668  making it the university  s first central print shop  16  A type foundry was added when Fell acquired a large stock of typographical punches and matrices from the Dutch Republicthe so-called \"Fell Types\"  He also induced two Dutch typefounders  Harman Harmanz and Peter de Walpergen  to work in Oxford for the Press  17  Finally  defying the Stationers   demands  Fell personally leased the right to print from the university in 1672  in partnership with Thomas Yate  Principal of Brasenose  and Sir Leoline Jenkins  Principal of Jesus College  18     Fell  s scheme was ambitious  Besides plans for academic and religious works  in 1674 he began to print a broadsheet calendar  known as the Oxford Almanack  Early editions featured symbolic views of Oxford  but in 1766 these gave way to realistic studies of city or university  19  The Almanacks have been produced annually without interruption from Fell  s time to the present day  20     Following the start of this work  Fell drew up the first formal programme for the university  s printing  Dating from 1675  this document envisaged hundreds of works  including the Bible in Greek  editions of the Coptic Gospels and works of the Church Fathers  texts in Arabic and Syriac  comprehensive editions of classical philosophy  poetry  and mathematics  a wide range of medieval scholarship  and also \"a history of insects  more perfect than any yet Extant \" 21  Though few of these proposed titles appeared during Fell  s life  Bible printing remained at the forefront of his mind  A full variant Greek text of Scripture proved impossible  but in 1675 Oxford printed a quarto King James edition  carrying Fell  s own textual changes and spellings  This work only provoked further conflict with the Stationers   Company  In retaliation  Fell leased the university  s Bible printing to three rogue Stationers  Moses Pitt  Peter Parker  and Thomas Guy  whose sharp commercial instincts proved vital to fomenting Oxford  s Bible trade  22  Their involvement  however  led to a protracted legal battle between Oxford and the Stationers  and the litigation dragged on for the rest of Fell  s life  He died in 1686  23     Yate and Jenkins predeceased Fell  leaving him with no obvious heir to oversee the print shop  As a result  his will left the partners   stock and lease in trust to Oxford University  and charged them with keeping together \"my founding Materialls of the Press \" 24  Fell  s main trustee was the Delegate Henry Aldrich  Dean of Christ Church  who took a keen interest in the decorative work of Oxford  s books  He and his colleagues presided over the end of Parker and Guy  s lease  and a new arrangement in 1691 whereby the Stationers leased the whole of Oxford  s printing privilege  including its unsold scholarly stock  Despite violent opposition from some printers in the Sheldonian  this ended the friction between Oxford and the Stationers  and marked the effective start of a stable university printing business  25     In 1713  Aldrich also oversaw the Press moving to the Clarendon Building  This was named in honour of Oxford University  s Chancellor  Edward Hyde  1st Earl of Clarendon  Oxford lore maintained its construction was funded by proceeds from his book The History of the Rebellion and Civil Wars in England  170204   In fact  most of the money came from Oxford  s new Bible printer John Baskettand the Vice-Chancellor William Delaune defaulted with much of the proceeds from Clarendon  s work  In any event  the result was Nicholas Hawksmoor  s beautiful but impractical structure beside the Sheldonian in Broad Street  The Press worked here until 1830  with its operations split into the so-called Learned Side and Bible Side in different wings of the building  26     Generally speaking  the early 18th century marked a lull in the Press  s expansion  It suffered from the absence of any figure comparable to Fell  and its history was marked by ineffectual or fractious individuals such as the Architypographus and antiquary Thomas Hearne  and the flawed project of Baskett  s first Bible  a gorgeously designed volume strewn with misprints  and known as the Vinegar Bible after a glaring typographical error in St  Luke  Other printing during this period included Richard Allestree  s contemplative texts  and Thomas Hanmer  s six-volume edition of Shakespeare   174344   27  In retrospect  these proved relatively minor triumphs  They were products of a university press that had come to embody increasing muddle  decay  and corrupt practice  and relied increasingly on leasing of its Bible and prayer book work to survive  citation needed     The business was rescued by the intervention of a single Delegate  William Blackstone  Disgusted by the chaotic state of the Press  and antagonized by the Vice-Chancellor George Huddesford  Blackstone subjected the print shop to close scrutiny  but his findings on its confused organization and sly procedures met with only \"gloomy and contemptuous silence\" from his colleagues  or \"at best with a languid indifference \" In disgust  Blackstone forced the university to confront its responsibilities by publishing a lengthy letter he had written to Huddesford  s successor  Thomas Randolph in May 1757  Here  Blackstone characterized the Press as an inbred institution that had given up all pretence of serving scholarship  \"languishing in a lazy obscurity  a nest of imposing mechanics \" To cure this disgraceful state of affairs  Blackstone called for sweeping reforms that would firmly set out the Delegates   powers and obligations  officially record their deliberations and accounting  and put the print shop on an efficient footing  28  Nonetheless  Randolph ignored this document  and it was not until Blackstone threatened legal action that changes began  The university had moved to adopt all of Blackstone  s reforms by 1760  29     By the late 18th century  the Press had become more focused  Early copyright law had begun to undercut the Stationers  and the university took pains to lease out its Bible work to experienced printers  When the American War of Independence deprived Oxford of a valuable market for its Bibles  this lease became too risky a proposition  and the Delegates were forced to offer shares in the Press to those who could take \"the care and trouble of managing the trade for our mutual advantage \" Forty-eight shares were issued  with the university holding a controlling interest  30  At the same time  classical scholarship revived  with works by Jeremiah Markland and Peter Elmsley  as well as early 19th-century texts edited by a growing number of academics from mainland Europe xa0 perhaps the most prominent being August Immanuel Bekker and Karl Wilhelm Dindorf  Both prepared editions at the invitation of the Greek scholar Thomas Gaisford  who served as a Delegate for 50 years  During his time  the growing Press established distributors in London  and employed the bookseller Joseph Parker in Turl Street for the same purposes in Oxford  Parker also came to hold shares in the Press itself  31     This expansion pushed the Press out of the Clarendon building  In 1825 the Delegates bought land in Walton Street  Buildings were constructed from plans drawn up by Daniel Robertson and Edward Blore  and the Press moved into them in 1830  32  This site remains the main office of OUP in the 21st century  at the corner of Walton Street and Great Clarendon Street  northwest of Oxford city centre     The Press now entered an era of enormous change  In 1830  it was still a joint-stock printing business in an academic backwater  offering learned works to a relatively small readership of scholars and clerics  The Press was the product of \"a society of shy hypochondriacs \" as one historian put it  33  Its trade relied on mass sales of cheap Bibles  and its Delegates were typified by Gaisford or Martin Routh  They were long-serving classicists  presiding over a learned business that printed 5 or 10 titles each year  such as Liddell and Scott  s Greek-English Lexicon  1843   and they displayed little or no desire to expand its trade  34  Steam power for printing must have seemed an unsettling departure in the 1830s  35     At this time  Thomas Combe joined the Press and became the university  s Printer until his death in 1872  Combe was a better business man than most Delegates  but still no innovator  he failed to grasp the huge commercial potential of India paper  which grew into one of Oxford  s most profitable trade secrets in later years  36  Even so  Combe earned a fortune through his shares in the business and the acquisition and renovation of the bankrupt paper mill at Wolvercote  He funded schooling at the Press and the endowment of St  Barnabas Church in Oxford  37  Combe  s wealth also extended to becoming the first patron of the Pre-Raphaelite Brotherhood  and he and his wife Martha bought most of the group  s early work  including The Light of the World by William Holman Hunt  38  Combe showed little interest  however  in producing fine printed work at the Press  39  The most well-known text associated with his print shop was the flawed first edition of Alice  s Adventures in Wonderland  printed by Oxford at the expense of its author Lewis Carroll  Charles Lutwidge Dodgson  in 1865  40     It took the 1850 Royal Commission on the workings of the university and a new Secretary  Bartholomew Price  to shake up the Press  41  Appointed in 1868  Price had already recommended to the university that the Press needed an efficient executive officer to exercise \"vigilant superintendence\" of the business  including its dealings with Alexander Macmillan  who became the publisher for Oxford  s printing in 1863 and in 1866 helped Price to create the Clarendon Press series of cheap  elementary school books xa0 perhaps the first time that Oxford used the Clarendon imprint  42  Under Price  the Press began to take on its modern shape  By 1865 the Delegacy had ceased to be   perpetual    and evolved into five perpetual and five junior posts filled by appointment from the university  with the Vice Chancellor a Delegate ex officio  a hothouse for factionalism that Price deftly tended and controlled  43  The university bought back shares as their holders retired or died  44  Accounts   supervision passed to the newly created Finance Committee in 1867  45  Major new lines of work began  To give one example  in 1875  the Delegates approved the series Sacred Books of the East under the editorship of Friedrich Max Mller  bringing a vast range of religious thought to a wider readership  46     Equally  Price moved OUP towards publishing in its own right  The Press had ended its relationship with Parker  s in 1863 and in 1870 bought a small London bindery for some Bible work  47  Macmillan  s contract ended in 1880  and wasn  t renewed  By this time  Oxford also had a London warehouse for Bible stock in Paternoster Row  and in 1880 its manager Henry Frowde  18411927  was given the formal title of Publisher to the University  Frowde came from the book trade  not the university  and remained an enigma to many  One obituary in Oxford  s staff magazine The Clarendonian admitted  \"Very few of us here in Oxford had any personal knowledge of him \" 48  Despite that  Frowde became vital to OUP  s growth  adding new lines of books to the business  presiding over the massive publication of the Revised Version of the New Testament in 1881 49  and playing a key role in setting up the Press  s first office outside Britain  in New York City in 1896  50     Price transformed OUP  In 1884  the year he retired as Secretary  the Delegates bought back the last shares in the business  51  The Press was now owned wholly by the university  with its own paper mill  print shop  bindery  and warehouse  Its output had increased to include school books and modern scholarly texts such as James Clerk Maxwell  s A Treatise on Electricity & Magnetism  1873   which proved fundamental to Einstein  s thought  52  Simply put  without abandoning its traditions or quality of work  Price began to turn OUP into an alert  modern publisher  In 1879  he also took on the publication that led that process to its conclusion  the huge project that became the Oxford English Dictionary  OED   53     Offered to Oxford by James Murray and the Philological Society  the \"New English Dictionary\" was a grand academic and patriotic undertaking  Lengthy negotiations led to a formal contract  Murray was to edit a work estimated to take 10 years and to cost approximately 9 000  54  Both figures were wildly optimistic  The Dictionary began to appear in print in 1884  but the first edition was not completed until 1928  13 years after Murray  s death  at a cost of around 375 000  55  This vast financial burden and its implications landed on Price  s successors  citation needed     The next Secretary struggled to address this problem  Philip Lyttelton Gell was appointed by the Vice-Chancellor Benjamin Jowett in 1884  Despite his education at Balliol and a background in London publishing  Gell found the operations of the Press incomprehensible  The Delegates began to work around him  and the university finally dismissed Gell in 1897  56  The Assistant Secretary  Charles Cannan  took over with little fuss and even less affection for his predecessor  \"Gell was always here  but I cannot make out what he did \" 57     Cannan had little opportunity for public wit in his new role  An acutely gifted classicist  he came to the head of a business that was successful in traditional terms but now moved into uncharted terrain  58  By themselves  specialist academic works and the undependable Bible trade could not meet the rising costs of the Dictionary and Press contributions to the University Chest  To meet these demands  OUP needed much more revenue  Cannan set out to obtain it  Outflanking university politics and inertia  he made Frowde and the London office the financial engine for the whole business  Frowde steered Oxford rapidly into popular literature  acquiring the World  s Classics series in 1906  The same year saw him enter into a so-called \"joint venture\" with Hodder & Stoughton to help with the publication of children  s literature and medical books  59  Cannan insured continuity to these efforts by appointing his Oxford protg  the Assistant Secretary Humphrey S  Milford  to be Frowde  s assistant  Milford became Publisher when Frowde retired in 1913  and ruled over the lucrative London business and the branch offices that reported to it until his own retirement in 1945  60  Given the financial health of the Press  Cannan ceased to regard scholarly books or even the Dictionary as impossible liabilities  \"I do not think the University can produce enough books to ruin us \" he remarked  61     His efforts were helped by the efficiency of the print shop  Horace Hart was appointed as Controller of the Press at the same time as Gell  but proved far more effective than the Secretary  With extraordinary energy and professionalism  he improved and enlarged Oxford  s printing resources  and developed Hart  s Rules as the first style guide for Oxford  s proofreaders  Subsequently  these became standard in print shops worldwide  62  In addition  he suggested the idea for the Clarendon Press Institute  a social club for staff in Walton Street  When the Institute opened in 1891  the Press had 540 employees eligible to join it  including apprentices  63  Finally  Hart  s general interest in printing led to him cataloguing the \"Fell Types\"  then using them in a series of Tudor and Stuart facsimile volumes for the Press  before ill health led to his death in 1915  64  By then  OUP had moved from being a parochial printer into a wide-ranging  university-owned publishing house with a growing international presence  citation needed     Frowde regularly remitted money back to Oxford  but he privately felt that the business was undercapitalized and would pretty soon become a serious drain on the university  s resources unless put on a sound commercial footing  He himself was authorized to invest money up to a limit in the business but was prevented from doing so by family troubles  Hence his interest in overseas sales  for by the 1880s and 1890s there was money to be made in India  while the European book market was in the doldrums  But Frowde  s distance from the Press  s decision-making meant he was incapable of influencing policy unless a Delegate spoke for him  Most of the time Frowde did whatever he could within the mandate given him by the Delegates  In 1905  when applying for a pension  he wrote to J  R  Magrath  the then Vice Chancellor  that during the seven years when he had served as manager of the Bible Warehouse the sales of the London Business had averaged about 20 000 and the profits 1 887 per year  By 1905  under his management as Publisher  the sales had risen to upwards of 200 000 per year and the profits in that 29 years of service averaged 8 242 per year  citation needed      Price  trying in his own way to modernize the Press against the resistance of its own historical inertia  had become overworked and by 1883 was so exhausted as to want to retire  Benjamin Jowett had become vice chancellor of the university in 1882  Impatient of the endless committees that would no doubt attend the appointment of a successor to Price  Jowett extracted what could be interpreted as permission from the delegates and headhunted Philip Lyttelton Gell  a former student acolyte of his  to be the next secretary to the delegates  Gell was making a name for himself at the publishing firm of Cassell  Petter and Galpin  a firm regarded as scandalously commercial by the delegates  Gell himself was a patrician who was unhappy with his work  where he saw himself as catering to the taste of \"one class  the lower middle\"  citation needed  and he grasped at the chance of working with the kind of texts and readerships OUP attracted  citation needed      Jowett promised Gell golden opportunities  little of which he actually had the authority to deliver  He timed Gell  s appointment to coincide with both the Long Vacation  from June to September  and the death of Mark Pattison  so potential opposition was prevented from attending the crucial meetings  Jowett knew the primary reason why Gell would attract hostility was that he had never worked for the Press nor been a delegate  and he had sullied himself in the city with raw commerce  His fears were borne out  Gell immediately proposed a thorough modernising of the Press with a marked lack of tact  and earned himself enduring enemies  Nevertheless  he was able to do a lot in tandem with Frowde  and expanded the publishing programmes and the reach of OUP until about 1898  Then his health broke down under the impossible work conditions he was being forced to endure by the Delegates   non-cooperation  The delegates then served him with a notice of termination of service that violated his contract  However  he was persuaded not to file suit and to go quietly  65  full citation needed     The delegates were not opposed primarily to his initiatives  but to his manner of executing them and his lack of sympathy with the academic way of life  In their view the Press was  and always would be  an association of scholars  Gell  s idea of \"efficiency\" appeared to violate that culture  although subsequently a very similar programme of reform was put into practice from the inside  citation needed     Charles Cannan  who had been instrumental in Gell  s removal  succeeded Gell in 1898  and Humphrey S  Milford  his younger colleague  effectively succeeded Frowde in 1907  Both were Oxford men who knew the system inside out  and the close collaboration with which they worked was a function of their shared background and worldview  Cannan was known for terrifying silences  and Milford had an uncanny ability  testified to by Amen House employees  to   disappear   in a room rather like a Cheshire cat  from which obscurity he would suddenly address his subordinates and make them jump  Whatever their reasons for their style of working  both Cannan and Milford had a very hardnosed view of what needed to be done  and they proceeded to do it  Indeed  Frowde knew within a few weeks of Milford  s entering the London office in  1904  that he would be replaced  Milford  however  always treated Frowde with courtesy  and Frowde remained in an advisory capacity till 1913  Milford rapidly teamed up with J  E  Hodder Williams of Hodder and Stoughton  setting up what was known as the Joint Account for the issue of a wide range of books in education  science  medicine and also fiction  Milford began putting in practice a number of initiatives  including the foundations of most of the Press  s global branches  citation needed      Milford took responsibility for overseas trade almost at once  and by 1906 he was making plans to send a traveller to India and the Far East jointly with Hodder and Stoughton  N  Graydon  first name unknown  was the first such traveller in 1907  and again in 1908 when he represented OUP exclusively in India  the Straits and the Far East  A H  Cobb replaced him in 1909  and in 1910 Cobb functioned as a travelling manager semi-permanently stationed in India  In 1911  E  V  Rieu went out to East Asia via the Trans-Siberian Railway  had several adventures in China and Russia  then came south to India and spent most of the year meeting educationists and officials all over India  In 1912  he arrived again in Bombay  now known as Mumbai  There he rented an office in the dockside area and set up the first overseas Branch  citation needed      In 1914  Europe was plunged into turmoil  The first effects of the war were paper shortages and losses and disturbances in shipping  then quickly a dire lack of hands as the staff were called up and went to serve on the field  Many of the staff including two of the pioneers of the Indian branch were killed in action  Curiously  sales through the years 1914 to 1917 were good and it was only towards the end of the war that conditions really began pinching  citation needed      Rather than bringing relief from shortages  the 1920s saw skyrocketing prices of both materials and labour  Paper especially was hard to come by  and had to be imported from South America through trading companies  Economies and markets slowly recovered as the 1920s progressed  In 1928  the Press  s imprint read   London  Edinburgh  Glasgow  Leipzig  Toronto  Melbourne  Cape Town  Bombay  Calcutta  Madras and Shanghai    Not all of these were full-fledged branches  in Leipzig there was a depot run by H  Bohun Beet  and in Canada and Australia there were small  functional depots in the cities and an army of educational representatives penetrating the rural fastnesses to sell the Press  s stock as well as books published by firms whose agencies were held by the Press  very often including fiction and light reading  In India  the Branch depots in Bombay  Madras  and Calcutta were imposing establishments with sizable stock inventories  for the Presidencies themselves were large markets  and the educational representatives there dealt mostly with upcountry trade  The Depression of 1929 dried profits from the Americas to a trickle  and India became   the one bright spot   in an otherwise dismal picture  Bombay was the nodal point for distribution to the Africas and onward sale to Australasia  and people who trained at the three major depots moved later on to pioneer branches in Africa and South East Asia  66     The Press  s experience of World War II was similar to World War I except that Milford was now close to retirement and   hated to see the young men go    The London blitz this time was much more intense and the London Business was shifted temporarily to Oxford  Milford  now extremely unwell and reeling under a series of personal bereavements  was prevailed upon to stay till the end of the war and keep the business going  As before  everything was in short supply  but the U-boat threat made shipping doubly uncertain  and the letterbooks are full of doleful records of consignments lost at sea  Occasionally an author  too  would be reported missing or dead  as well as staff who were now scattered over the battlefields of the globe  DORA  the Defence of the Realm Act  required the surrender of all nonessential metal for the manufacture of armaments  and many valuable electrotype plates were melted down by government order  citation needed      With the end of the war Milford  s place was taken by Geoffrey Cumberlege  This period saw consolidation in the face of the breakup of the Empire and the post-war reorganization of the Commonwealth  In tandem with institutions like the British Council  OUP began to reposition itself in the education market  Ngg wa Thiong  o in his book Moving the Centre  The Struggle for Cultural Freedom records how the Oxford Readers for Africa with their heavily Anglo-centric worldview struck him as a child in Kenya  67  The Press has evolved since then to be one of the largest players in a globally expanding scholarly and reference book market  citation needed      The North American branch was established in 1896 at 91 Fifth Avenue in New York City primarily as a distribution branch to facilitate the sale of Oxford Bibles in the United States  Subsequently  it took over marketing of all books of its parent from Macmillan  Its very first original publication  The Life of Sir William Osler  won the Pulitzer Prize in 1926  Since that time  OUP USA published fourteen more Pulitzer Prizewinning books  citation needed     The North American branch grew in sales between 1928 and 1936  eventually becoming one of the leading university presses in the United States  It is focused on scholarly and reference books  Bibles  and college and medical textbooks  In the 1990s  this office moved from 200 Madison Avenue  a building it shared with Putnam Publishing  to 198 Madison Avenue  the former B  Altman and Company Building  68     In December 1909 Cobb returned and rendered his accounts for his Asia trip that year  Cobb then proposed to Milford that the Press join a combination of firms to send commercial travellers around South America  to which Milford in principle agreed  Cobb obtained the services of a man called Steer  first name unknown  to travel through Argentina  Brazil  Uruguay  Chile and possibly other countries as well  with Cobb to be responsible for Steer  Hodder & Stoughton opted out of this venture  but OUP went ahead and contributed to it  citation needed      When OUP arrived on Indian shores  it was preceded by the immense prestige of the Sacred Books of the East  edited by Friedrich Max Mller  which had at last reached completion in 50 ponderous volumes  While actual purchase of this series was beyond the means of most Indians  libraries usually had a set  generously provided by the government of India  available on open reference shelves  and the books had been widely discussed in the Indian press  Although there had been plenty of criticism of them  the general feeling was that Max Mller had done India a favour by popularising ancient Asian  Persian  Arabic  Indian and Sinic  philosophy in the West  69  full citation needed  This prior reputation was useful  but the Indian Branch was not primarily in Bombay to sell Indological books  which OUP knew already sold well only in America  It was there to serve the vast educational market created by the rapidly expanding school and college network in British India  In spite of disruptions caused by war  it won a crucial contract to print textbooks for the Central Provinces in 1915 and this helped to stabilize its fortunes in this difficult phase  E  V  Rieu could not longer delay his callup and was drafted in 1917  the management then being under his wife Nellie Rieu  a former editor for the Athenaeum   with the assistance of her two British babies    It was too late to have important electrotype and stereotype plates shipped to India from Oxford  and the Oxford printing house itself was overburdened with government printing orders as the empire  s propaganda machine got to work  At one point non-governmental composition at Oxford was reduced to 32 pages a week  citation needed      By 1919  Rieu was very ill and had to be brought home  He was replaced by Geoffrey Cumberlege and Noel Carrington  Noel was the brother of Dora Carrington  the artist  and even got her to illustrate his Stories Retold edition of Don Quixote for the Indian market  Their father Charles Carrington had been a railway engineer in India in the nineteenth century  Noel Carrington  s unpublished memoir of his six years in India is in the Oriental and India Office Collections of the British Library  By 1915 there were makeshift depots at Madras and Calcutta  In 1920  Noel Carrington went to Calcutta to set up a proper branch  There he became friendly with Edward Thompson who involved him in the abortive scheme to produce the   Oxford Book of Bengali Verse    70  full citation needed  In Madras  there was never a formal branch in the same sense as Bombay and Calcutta  as the management of the depot there seems to have rested in the hands of two local academics  citation needed      OUP  s interaction with this area was part of their mission to India  since many of their travellers took in East and South East Asia on their way out to or back from India  Graydon on his first trip in 1907 had travelled the   Straits Settlements    largely the Federated Malay States and Singapore   China  and Japan  but was not able to do much  In 1909  A  H  Cobb visited teachers and booksellers in Shanghai  and found that the main competition there was cheap books from America  often straight reprints of British books  71  The copyright situation at the time  subsequent to the Chace Act of 1891  was such that American publishers could publish such books with impunity although they were considered contraband in all British territories  To secure copyright in both territories publishers had to arrange for simultaneous publication  an endless logistical headache in this age of steamships  Prior publication in any one territory forfeited copyright protection in the other  72     The Press had problems with Henzell  who were irregular with correspondence  They also traded with Edward Evans  another Shanghai bookseller  Milford observed    we ought to do much more in China than we are doing   and authorized Cobb in 1910 to find a replacement for Henzell as their representative to the educational authorities  citation needed  That replacement was to be Miss M  Verne McNeely  a redoubtable lady who was a member of the Society for the Propagation of Christian Knowledge  and also ran a bookshop  She looked after the affairs of the Press very capably and occasionally sent Milford boxes of complimentary cigars  Her association with OUP seems to date from 1910  although she did not have exclusive agency for OUP  s books  Bibles were the major item of trade in China  unlike India where educational books topped the lists  even if Oxford  s lavishly produced and expensive Bible editions were not very competitive beside cheap American ones  citation needed     Japan was a much less well-known market to OUP  and a small volume of trade was carried out largely through intermediaries  The Maruzen company was by far the largest customer  and had a special arrangement regarding terms  Other business was routed through H  L  Griffiths  a professional publishers   representative based in Sannomiya  Kobe  Griffiths travelled for the Press to major Japanese schools and bookshops and took a 10 percent commission  citation needed  Edmund Blunden had been briefly at the University of Tokyo and put the Press in touch with the university booksellers  Fukumoto Stroin  One important acquisition did come from Japan  however  A  S  Hornby  s Advanced Learner  s Dictionary  It also publishes textbooks for the primary and secondary education curriculum in Hong Kong  The Chinese-language teaching titles are published with the brand Keys Press     citation needed     Some trade with East Africa passed through Bombay  73  Following a period of acting mostly as a distribution agent for OUP titles published in the UK  in the 1960s OUP Southern Africa started publishing local authors  for the general reader  but also for schools and universities  under its Three Crowns Books imprint  Its territory includes Botswana  Lesotho  Swaziland and Namibia  as well as South Africa  the biggest market of the five  citation needed      OUP Southern Africa is now one of the three biggest educational publishers in South Africa  and focuses its attention on publishing textbooks  dictionaries  atlases and supplementary material for schools  and textbooks for universities  Its author base is overwhelmingly local  and in 2008 it entered into a partnership with the university to support scholarships for South Africans studying postgraduate degrees  citation needed      Prior to the twentieth century  the Press at Oxford had occasionally printed a piece of music or a book relating to musicology  It had also published the Yattendon Hymnal in 1899 and  more significantly  the first edition of The English Hymnal in 1906  under the editorship of Percy Dearmer and the then largely unknown Ralph Vaughan Williams  Sir William Henry Hadow  s multi-volume Oxford History of Music had appeared between 1901 and 1905  Such musical publishing enterprises  however  were rare  \"In nineteenth-century Oxford the idea that music might in any sense be educational would not have been entertained\"  74  and few of the Delegates or former Publishers were themselves musical or had extensive music backgrounds  citation needed      In the London office  however  Milford had musical taste  and had connections particularly with the world of church and cathedral musicians  In 1921  Milford hired Hubert J  Foss  originally as an assistant to Educational Manager V  H  Collins  In that work  Foss showed energy and imagination  However  as Sutcliffe says  Foss  a modest composer and gifted pianist  \"was not particularly interested in education  he was passionately interested in music \" 74  When shortly thereafter Foss brought to Milford a scheme for publishing a group of essays by well-known musicians on composers whose works were frequently played on the radio  Milford may have thought of it as less music-related than education-related  There is no clear record of the thought process whereby the Press would enter into the publishing of music for performance  Foss  s presence  and his knowledge  ability  enthusiasm  and imagination may well have been the catalyst bringing hitherto unconnected activities together in Milford  s mind  as another new venture similar to the establishment of the overseas branches  75     Milford may not have fully understood what he was undertaking  A fiftieth anniversary pamphlet published by the Music Department in 1973 says that OUP had \"no knowledge of the music trade  no representative to sell to music shops  andit seemsno awareness that sheet music was in any way a different commodity from books \" 76  However intentionally or intuitively  Milford took three steps that launched OUP on a major operation  He bought the Anglo-French Music Company and all its facilities  connections  and resources  He hired Norman Peterkin  a moderately well-known musician  as full-time sales manager for music  And in 1923  he established as a separate division the Music Department  with its own offices in Amen House and with Foss as first Musical Editor  Then  other than general support  Milford left Foss largely to his own devices  77     Foss responded with incredible energy  He worked to establish \"the largest possible list in the shortest possible time\"  78  adding titles at the rate of over 200 a year  eight years later there were 1 750 titles in the catalogue  In the year of the department  s establishment  Foss began a series of inexpensive but well edited and printed choral pieces under the series title \"Oxford Choral Songs\"  This series  under the general editorship of W  G  Whittaker  was OUP  s first commitment to the publishing of music for performance  rather than in book form or for study  The series plan was expanded by adding the similarly inexpensive but high-quality \"Oxford Church Music\" and \"Tudor Church Music\"  taken over from the Carnegie UK Trust   all these series continue today  The scheme of contributed essays Foss had originally brought to Milford appeared in 1927 as the Heritage of Music  two more volumes would appear over the next thirty years   Percy Scholes  s Listener  s Guide to Music  originally published in 1919  was similarly brought into the new department as the first of a series of books on music appreciation for the listening public  75  Scholes  s continuing work for OUP  designed to match the growth of broadcast and recorded music  plus his other work in journalistic music criticism  would be later comprehensively organized and summarized in the Oxford Companion to Music  citation needed      Perhaps most importantly  Foss seemed to have a knack for finding new composers of what he regarded as distinctively English music  which had broad appeal to the public  This concentration provided OUP two mutually reinforcing benefits  a niche in music publishing unoccupied by potential competitors  and a branch of music performance and composition that the English themselves had largely neglected  Hinnells proposes that the early Music Department  s \"mixture of scholarship and cultural nationalism\" in an area of music with largely unknown commercial prospects was driven by its sense of cultural philanthropy  given the Press  s academic background  and a desire to promote \"national music outside the German mainstream \" 79     In consequence  Foss actively promoted the performance and sought publication of music by Ralph Vaughan Williams  William Walton  Constant Lambert  Alan Rawsthorne  Peter Warlock  Philip Heseltine   Edmund Rubbra and other English composers  In what the Press called \"the most durable gentleman  s agreement in the history of modern music \" 78  Foss guaranteed the publication of any music that Vaughan Williams would care to offer them  In addition  Foss worked to secure OUP  s rights not only to music publication and live performance  but the \"mechanical\" rights to recording and broadcast  It was not at all clear at the time how significant these would become  Indeed  Foss  OUP  and a number of composers at first declined to join or support the Performing Right Society  fearing that its fees would discourage performance in the new media  Later years would show that  to the contrary  these forms of music would prove more lucrative than the traditional venues of music publishing  80     Whatever the Music Department  s growth in quantity  breadth of musical offering  and reputation amongst both musicians and the general public  the whole question of financial return came to a head in the 1930s  Milford as London publisher had fully supported the Music Department during its years of formation and growth  However  he came under increasing pressure from the Delegates in Oxford concerning the continued flow of expenditures from what seemed to them an unprofitable venture  In their mind  the operations at Amen House were supposed to be both academically respectable and financially remunerative  The London office \"existed to make money for the Clarendon Press to spend on the promotion of learning \" 81  Further  OUP treated its book publications as short-term projects  any books that did not sell within a few years of publication were written off  to show as unplanned or hidden income if in fact they sold thereafter   In contrast  the Music Department  s emphasis on music for performance was comparatively long-term and continuing  particularly as income from recurring broadcasts or recordings came in  and as it continued to build its relationships with new and upcoming musicians  The Delegates were not comfortable with Foss  s viewpoint  \"I still think this word   loss   is a misnomer  is it not really capital invested?\" wrote Foss to Milford in 1934  82     Thus it was not until 1939 that the Music Department showed its first profitable year  83  By then  the economic pressures of the Depression as well as the in-house pressure to reduce expenditures  and possibly the academic background of the parent body in Oxford  combined to make OUP  s primary musical business that of publishing works intended for formal musical education and for music appreciationagain the influence of broadcast and recording  83  This matched well with an increased demand for materials to support music education in British schools  a result of governmental reforms of education during the 1930s  note 1  The Press did not cease to search out and publish new musicians and their music  but the tenor of the business had changed  Foss  suffering personal health problems  chafing under economic constraints plus  as the war years drew on  shortages in paper  and disliking intensely the move of all the London operations to Oxford to avoid The Blitz  resigned his position in 1941  to be succeeded by Peterkin  84     On 27 August 2021  OUP will close Oxuniprint  its printing division  It will result in the loss of 20 jobs and follows a \"continued decline in sales\" aggravated by the COVID-19 pandemic  The closure will mark the \"final chapter\" of OUP  s centuries-long history of printing  85     The Oxford University Press Museum is located on Great Clarendon Street  Oxford  Visits must be booked in advance and are led by a member of the archive staff  Displays include a 19th-century printing press  the OUP buildings  and the printing and history of the Oxford Almanack  Alice in Wonderland and the Oxford English Dictionary  citation needed      OUP came to be known as \" The  Clarendon Press\" when printing moved from the Sheldonian Theatre to the Clarendon Building in Broad Street in 1713  The name continued to be used when OUP moved to its present site in Oxford in 1830  The label \"Clarendon Press\" took on a new meaning when OUP began publishing books through its London office in the early 20th century  To distinguish the two offices  London books were labelled \"Oxford University Press\" publications  while those from Oxford were labelled \"Clarendon Press\" books  This labelling ceased in the 1970s  when the London office of OUP closed  Today  OUP reserves \"Clarendon Press\" as an imprint for Oxford publications of particular academic importance  86     OUP as Oxford Journals has also been a major publisher of academic journals  both in the sciences and the humanities  as of 2016 update  it publishes over 200 journals on behalf of learned societies around the world  88  It has been noted as one of the first university presses to publish an open access journal  Nucleic Acids Research   and probably the first to introduce Hybrid open access journals  offering \"optional open access\" to authors to allow all readers online access to their paper without charge  89  The \"Oxford Open\" model applies to the majority of their journals  90  The OUP is a member of the Open Access Scholarly Publishers Association  citation needed      Since 2001  Oxford University Press has financially supported the Clarendon bursary  a University of Oxford graduate scholarship scheme  91     The University of Chicago Press is the largest and one of the oldest university presses in the United States  4  It is operated by the University of Chicago and publishes a wide variety of academic titles  including The Chicago Manual of Style  numerous academic journals  and advanced monographs in the academic fields     One of its quasi-independent projects is the BiblioVault  a digital repository for scholarly books     The Press building is located just south of the Midway Plaisance on the University of Chicago campus     The University of Chicago Press was founded in 1890  making it one of the oldest continuously operating university presses in the United States  5  6  Its first published book was Robert F  Harper  s Assyrian and Babylonian Letters Belonging to the Kouyunjik Collections of the British Museum  The book sold five copies during its first two years  but by 1900 the University of Chicago Press had published 127 books and pamphlets and 11 scholarly journals  including the current Journal of Political Economy  Journal of Near Eastern Studies  and  American Journal of Sociology     For its first three years  the Press was an entity discrete from the university  it was operated by the Boston publishing house D  C  Heath in conjunction with the Chicago printer R  R  Donnelley  This arrangement proved unworkable  however  and in 1894 the university officially assumed responsibility for the Press     In 1902  as part of the university  the Press started working on the Decennial Publications  Composed of articles and monographs by scholars and administrators on the state of the university and its faculty  s research  the Decennial Publications was a radical reorganization of the Press  This allowed the Press  by 1905  to begin publishing books by scholars not of the University of Chicago  A manuscript editing and proofreading department was added to the existing staff of printers and typesetters  leading  in 1906  to the first edition of The Chicago Manual of Style     By 1931  the Press was an established  leading academic publisher  Leading books of that era include Dr  Edgar J  Goodspeed  s The New Testament  An American Translation  the Press  s first nationally successful title  and its successor  Goodspeed and J  M  Povis Smith  s The Complete Bible  An American Translation  Sir William Alexander Craigie  s A Dictionary of American English on Historical Principles  published in four volumes in 1943  John Manly and Edith Rickert  s The Canterbury Tales  published in 1940  and  Kate Turabian  s A Manual for Writers of Term Papers  Theses  and Dissertations     In 1956  the Press first published paperback-bound books  including the Phoenix Books series  7  under its imprint  Of the Press  s best-known  books  most date from the 1950s  including translations of the Complete Greek Tragedies and Richmond Lattimore  s The Iliad of Homer  That decade also saw the first edition of A Greek-English Lexicon of the New Testament and Other Early Christian Literature  which has since been used by students of Biblical Greek worldwide     In 1966  Morris Philipson began his 34-year tenure as director of the University of Chicago Press  He committed time and resources to lengthening the backlist  becoming known for assuming ambitious scholarly projects  among the largest of which was The Lisle Letters  a vast collection of 16th-century correspondence by Arthur Plantagenet  1st Viscount Lisle  a wealth of information about every aspect of 16th-century life     As the Press  s scholarly volume expanded  the Press also advanced as a trade publisher  In 1992  Norman Maclean  s books A River Runs Through It and Young Men and Fire were national best sellers  and A River Runs Through It was made into a film directed by and starring Robert Redford     In 1982  Philipson was the first director of an academic press to win the Publisher Citation  one of PEN  s most prestigious awards  Shortly before he retired in June 2000  Philipson received the Association of American Publishers   Curtis Benjamin Award for Creative Publishing  awarded to the person whose \"creativity and leadership have left a lasting mark on American publishing \"    Paula Barker Duffy served as director of the Press from 2000 to 2007  Under her administration  the Press expanded its distribution operations and created the Chicago Digital Distribution Center and BiblioVault  Editorial depth in reference and regional books increased with titles such as The Encyclopedia of Chicago  Timothy J  Gilfoyle  s Millennium Park  and new editions of The Chicago Manual of Style  the Turabian Manual  and The University of Chicago Spanish Dictionary  The Press also launched an electronic reference work  The Chicago Manual of Style Online     In 2014  the Press received The International Academic and Professional Publisher Award for excellence at the London Book Fair  8     Garrett P  Kiely became the 15th director of the University of Chicago Press on September 1  2007  He heads one of academic publishing  s largest operations  employing more than 300 people across three divisionsbooks  journals  and distributionand publishing 81 journal titles and approximately 280 new books and 70 paperback reprints each year     The Press publishes over 50 new trade titles per year  across many subject areas  It also publishes regional titles  such as The Encyclopedia of Chicago  2004   edited by James R  Grossman  Ann Durkin Keating  and Janice Reiff  9  The Chicagoan  A Lost Magazine of the Jazz Age  2008  by Neil Harris  One More Time  The Best of Mike Royko  1999   a collection of columns by Pulitzer Prize-winning newspaperman Mike Royko of the Chicago Sun-Times and the Chicago Tribune  and many other books about the art  architecture  and nature of Chicago and the Midwest     The Press has recently expanded its digital offerings to include most newly published books as well as key backlist titles  In 2013  Chicago Journals began offering e-book editions of each new issue of each journal  for use on e-reader devices such as smartphones  iPad  and Amazon Kindle  The contents of The Chicago Manual of Style are available online to paid subscribers  The Chicago Distribution Center is recognized as a leading distributor of scholarly works  with over 100 client presses  10     The Books Division of the University of Chicago Press has been publishing books for scholars  students  and general readers since 1892 and has published over 11 000 books since its founding  The Books Division presently citation needed  has more than 6 000 books in print  including such well-known works as The Chicago Manual of Style  1906   The Structure of Scientific Revolutions  1962   by Thomas Kuhn  A River Runs Through It  1976   by Norman Maclean  and The Road to Serfdom  1944   by F  A  Hayek  In July 2009  the Press announced the Chicago Digital Editions program  which made many of the Press  s titles available in e-book form for sale to individuals  11  As of August 2016  more than 3 500 titles are available in this format  In August 2010  the Press published the 16th Edition of The Chicago Manual of Style simultaneously in print and online editions  The Books Division offers a Free E-book Of The Month program  through which site visitors may provide their e-mail address and receive a link to that month  s free  downloadable e-book selection     The Journals Division of the University of Chicago Press publishes and distributes influential scholarly publications on behalf of learned and professional societies and associations  foundations  museums  and other not-for-profit organizations  As of 2016 it publishes 81 titles in a wide range of academic disciplines including the biological and medical sciences  education  the humanities  the physical sciences  and the social sciences  12  All are peer-reviewed journals of original scholarship  with readerships that include scholars  scientists  and medical practitioners as well as interested  educated laypeople  Since 1974 the Press has published the prestigious humanities journal Critical Inquiry  The Journals Division has been a pioneer in making scholarly and scientific journals available in electronic form in conjunction with their print editions  Electronic publishing efforts were launched in 1995  by 2004 all the journals published by the University of Chicago Press were available online  In 2013  all new journal issues were also made available to subscribers in e-book format     The Distribution Services Division provides the University of Chicago Press  s customer service  warehousing  and related services  The Chicago Distribution Center  CDC  began providing distribution services in 1991  when the University of Tennessee Press became its first client  Currently when?  the CDC serves nearly 100 publishers including Northwestern University Press  Stanford University Press  Temple University Press  University of Iowa Press  University of Minnesota Press  and many others  Since 2001  with development funding from the Mellon Foundation  the Chicago Digital Distribution Center  CDDC  has been offering digital printing services and the BiblioVault digital repository services to book publishers  In 2009  the CDC enabled the sales of electronic books directly to individuals and provided digital delivery services for the University of Michigan Press among others  The Chicago Distribution Center has also partnered with an additional 15 presses  including the University of Missouri Press  West Virginia University Press  and publications of the Getty Foundation     Wolters Kluwer N V   Euronext  xa0WKL  is an  American Dutch  information services company  3  4  The company is headquartered in Alphen aan den Rijn  Netherlands  Global  and Philadelphia  United States  corporate   5  6  Wolters Kluwer in its current form was founded in 1987 with a merger between Kluwer Publishers and Wolters Samsom  7  8  The company serves legal  business  tax  accounting  finance  audit  risk  compliance  and healthcare markets  3  It operates in over 150 countries  3     Jan-Berend Wolters founded the Schoolbook publishing house in Groningen  Netherlands  in 1836  7  In 1858  the Noordhoff publishing house was founded alongside the Schoolbook publishing house  7  The two publishing houses merged in 1968  Wolters-Noordhoff merged with Information and Communications Union  ICU  in 1972 and took the name ICU  ICU changed its name to Wolters-Samsom in 1983  The company began serving foreign law firms and multinational companies in China in 1985  9  In 1987  Elsevier  the largest publishing house in the Netherlands  announced its intentions to buy up Kluwer  s stock  7  Kluwer merged with Wolters-Samsom to fend off Elsevier  s take-over bid and formed Wolters Kluwer  10   The merger made Wolters Kluwer the second largest publishing house in the Netherlands  7  10     After the merger  Wolters Kluwer began expanding internationally with the purchase of IPSOA Editore  Kieser Verlag  Technipublicaciones and Tele Consulte in 1989  8  By the end of the year  Wolters Kluwer expanded its presence to Spain  West Germany and France  7  The company also launched LEX  its legal information system  in Poland  11  In 1989  44% of the company  s revenue was earned in foreign markets  7     The following year  Wolters Kluwer purchased J  B  Lippincott & Co  from HarperCollins  8  The company acquired Liber  a Swedish publishing company  in 1993  The following year it established its first Eastern European subsidiary  IURA Edition  in Bratislava  Slovakia  The company acquired Jugend & Volk  Dalian  Fateco Frlag and Juristfrlaget  Deutscher Kommunal-Verlag Dr  Naujoks & Behrendt and Colex Data in 1995  Wolters Kluwer was operating in 16 countries and had approximately 8000 employees by the end of that year  7  nIn 1992 Spain was added to the countries where the company has a presence  Wolters Kluwer Espaa directed by Miguel Guibelalde  institutional director of the company      In 1994  Wolters Kluwer expanded its US legal business by acquiring Prentice Hall Law & Business from Simon & Schuster  12  13  In 1995  Wolters Kluwer acquired CT Corporation  The following year  it purchased CCH Inc   a tax and business materials publisher  for $1 9 billion  The purchase assisted in expanding the company  s business in Asia because of CCH Inc s involvement in Australia  New Zealand  Japan  Singapore  and Hong Kong  7  It also purchased Little  Brown and Companys medical and legal division that year  8  John Wiley & Son  s legal division was purchased in 1997  14  Waverly  Inc   Ovid Technologies  Inc  and Plenum Publishing Corporation were acquired in 1998 with the intention of developing Wolters Kluwers medical and scientific publishing industry  7     In 2002  Wolters Kluwer sold Kluwer Academic Publishers to the private equity firms Cinven and Candover Investments  15   It is now part of Springer   16  The company established its first three-year strategy to deliver sustained value to customers and shareholders in 2003  11  The New Delhi Wolters Kluwer Health office opened in 2006  In 2017  Wolters Kluwer Education was sold to Bridgepoint Capital  17  In September 2008  Wolters Kluwer acquired UpToDate  an evidence-based electronic clinical information resource  18  The following month  the company received a multi-year contract to provide prescription and patient-level data to the United States Food and Drug Administration  19  In 2009  Wolters Kluwer was named the Best Place to Work  in Spain by the Great Place to Work Institute  20     Wolters Kluwer acquired FRSGlobal  financial regulatory reporting and risk management firm in September 2010  21  The acquisition enabled Wolters Kluwer to provide financial organizations comprehensive compliance and risk solutions  buzzword  The company acquired SASGAS  a financial reporting software solutions buzzword  provider  to the foreign and domestic bank market in China in October 2011  22  That December  Wolters Kluwer acquired Medknow  an open access publisher  23  Also in 2011  Wolters Kluwer sold its pharmaceutical industry-related Marketing and Publishing Service division to Springer Science+Business Media  which led to a workforce reduction at its facility in Ambler  Pennsylvania  eventually leading to the site  s closure in 2013  24     In 2012  Wolters Kluwer acquired Acclipse  an accounting software provider  and Finarch  an integrated finance and risk solutions  buzzword  3  The company  s health division tested technology to identify and treat sepsis that December  4  Wolters Kluwer acquired Health Language  a medical terminology management provider  in January 2013  3  In May 2013  it acquired Prosoft Tecnologia  a Brazilian provider of tax and accounting software  25  The company acquired CitizenHawk  an American online brand protection and global domain recovery specialist  in September 2013  26  That month  Wolters Kluwer acquired Svenson  an Austrian regulatory reporting solutions buzzword  provider  27  The acquisition enabled both companies to assist Austrian banks and insurance companies in meeting national and international regulatory requirements  27     The company became the fifth participant in the AAISalliance  an arrangement of information providers that make their services available for member insurance companies of the American Association of Insurance Services  AAIS  in April 2014  28  In May 2014  Wolters Kluwer launched UpToDate  a clinical decision support resource  in the United Kingdom  29  UpToDate was launched throughout western Europe a month later  30  Wolters Kluwer acquired Datacert  a Houston  Texas-based enterprise legal management software and services provider in April 2014  31     The company partnered with Anhembi Morumbi University  a private university in So Paulo  Brazil  to provide information and resources to healthcare students and professionals in June 2014  32  That month  the company  s CCH eSign solution buzzword  won the CPA Practice Advisor Magazine  s 2014 Tax & Accounting Technology Innovation Award  33  The solution buzzword  won the Software and Information Industry Association  s Best Enterprise Mobile Application award that year  34  The company partnered with Broadridge Tax Services in August 2014 to facilitate tax reporting and reconciliation  35  In September  the company  s UpToDate resource was released in Latin America  36  That month the company extended its partnership with the American Internal Revenue Service  37  2014 marked the 15th year of their collaboration  37  In May 2016  the company acquired Enablon  a global provider of Environmental  Health  Safety & Sustainability and Operational Risk Management software and SaaS solutions buzzword   38  In 2017 Wolters Kluwer partnered with Skopos Labs to develop the Federal Developments Knowledge Center to help legal professionals stay up-to-date on actions by the President and Congress  39     Wolters Kluwer operated under four divisions as of 2013  Legal & Regulatory Solutions  sold to Peninsula Business Services in 2017 40    Tax & Accounting  Health and the Governance  Risk & Compliance Division  3  The company is active in over 150 countries  Approximately 74% of the company  s revenue came from online  software and services in 2013  3  The United States medical publishing business is run through Wolters Kluwer Health  24     Wolters Kluwer is listed on the Dow Jones Sustainability Index  41  The company received the Bronze Class Sustainability Award 2014 from RobecoSAM  42  Wolters Kluwer is recognized as one of the Global 100 Most Sustainable Corporations in the World by Corporate Knights  43     Modelling frameworks are used in modelling and simulation and can consist of a software infrastructure to develop and run mathematical models  They have provided a substantial step forward in the area of biophysical modelling with respect to monolithic implementations  1  2  3  4  The separation of algorithms from data  the reusability of I/O procedures and integration services  and the isolation of modelling solutions in discrete units has brought a solid advantage in the development of simulation systems  Modelling frameworks for agriculture have evolved over time  with different approaches and targets 5     BioMA is a software framework developed focusing on platform-independent  re-usable components  including multi-model implementations at fine granularity     BioMA  Biophysical Model Applications  is a public domain software framework designed and implemented for developing  parameterizing and running modelling solutions based on biophysical models in the domains of agriculture and environment  6  It is based on discrete conceptual units codified in freely extensible software components   7     The goal of this framework is to rapidly bridge from prototypes to operational applications  enabling running and comparing different modelling solutions  A key aspect of the framework is the transparency which allows for quality evaluation of outputs in the various steps of the modelling workflow  The framework is based on framework-independent components  both for the modelling solutions and the graphical user  s interfaces  The goal is not only to provide a framework for model development and operational use but also  and of no lesser importance  to provide a loose collection of objects re-usable either standalone or in different frameworks  The software is developed using Microsoft C# language in the  NET framework     The framework is a development of the work carried out under the APES 8  task of the 6th EU Framework Program SEAMLESS project     Deployments of the platform and its tools and components have been used     BioMA applications and modelling solutions are the simulation tools used by the MARS unit of the European Commission to simulate agricultural production under scenarios of climate change   BioMA is also used in the EU FP7 project MODEXTREME     The simulation system is discretized in layers  each with its own features and requirements  Such layers are the Model Layer  ModL   where fine granularity models are implemented as discrete units  54  the Composition Layer  CompL   where basic models are linked into more complex  aggregated models  and the Configuration Layer  ConfL   which allows providing context specific parameterization  in the software sense  for operational use  Applications can span from simple console applications to user-interacting applications based on the model-view-controller pattern  in the simplest cases linking either directly to either the ModL or the CompL  or accessing model ConfL  In all cases  the component oriented architecture allows implementing a set of functionalities which impact on the richness of functionality of the system and on its transparency  Layers implement no top-down dependency among them  hence facilitating the independent reuse of tools  utilities  and model components in different applications and frameworks     Advanced applications can be grouped under two categories     Applications can be built based on the libraries as in the following figure  The libraries can be extended implementing new models  as shown in the software development kits  and new libraries can be added    \"Model components and tools can be autonomously downloaded with the SDK at the components  portal   Same for modelling solutions  starting from 2016   n\"   Applications must be requested by email  and  similarly to components  applications will be made available for free autonomous download during 2016     Code of core components is available under the MIT license  however  the reuse of binaries falls under the Creative Commons license as below  implying the no-commercial  share-alike clauses  55  circular reference     Application and tools are available under the Creative Commons license as binaries  however code can be shared under specific agreements between parties  Model component developers may make code available  however  they must make binaries available for reuse  56     Theoretical production ecology tries to quantitatively study the growth of crops   nThe plant is treated as a kind of biological factory  which processes light  carbon dioxide  water  and nutrients into harvestable parts  nMain parameters kept into consideration are temperature  sunlight  standing crop biomass  plant production distribution  nutrient and water supply     Modelling is essential in theoretical production ecology   nUnit of modelling usually is the crop  the assembly of plants per standard surface unit  Analysis results for an individual plant are generalised to the standard surface  e g  the leaf area index is the projected surface area of all crop leaves above a unit area of ground     The usual system of describing plant production divides the plant production process into at least five separate processes  which are influenced by several external parameters     Two cycles of biochemical reactions constitute the basis of plant production  the light reaction and the dark reaction  1     Important parameters in theoretical production models thus are     Theoretical production ecology assumes that the growth of common agricultural crops  such as cereals and tubers  usually consists of four  or five  phases      Plant production models exist in varying levels of scope  cell  physiological  individual plant  crop  geographical region  global  and of generality  the model can be crop-specific or be more generally applicable  In this section the emphasis will be on crop-level based models as the crop is the main area of interest from an agronomical point of view     As of 2005  several crop production models are in use  The crop growth model SUCROS has been developed during more than 20 years and is based on earlier models  Its latest revision known dates from 1997  The IRRI and Wageningen University more recently developed the rice growth model ORYZA2000  This model is used for modeling rice growth  Both crop growth models are open source  Other more crop-specific plant growth models exist as well     SUCROS is programmed in the Fortran computer programming language  The model can and has been applied to a variety of weather regimes and crops  Because the source code of Sucros is open source  the model is open to modifications of users with FORTRAN programming experience  nThe official maintained version of SUCROS comes into two flavours  SUCROS I  which has non-inhibited unlimited crop growth  which means that only solar radiation and temperature determine growth  and SUCROS II  in which crop growth is limited only by water shortage     The ORYZA2000 rice growth model has been developed at the IRRI in cooperation with Wageningen University  This model  too  is programmed in FORTRAN  The scope of this model is limited to rice  which is the main food crop for Asia     The United States Department of Agriculture has sponsored a number of applicable crop growth models for various major US crops  such as cotton  soy bean  wheat and rice  3  nOther widely used models are the precursor of SUCROS  SWATR   CERES  several incarnations of PLANTGRO  SUBSTOR  the FAO-sponsored CROPWAT   AGWATER  the erosion-specific model EPIC  4  and the cropping system CropSyst  5     A less mechanistic growth and competition model  called the conductance model  has been developed  mainly at Warwick-HRI  Wellesbourne  UK   This model simulates light interception and growth of individual plants based on the lateral expansion of their crown zone areas   Competition between plants is simulated by a set algorithms related to competition for space and resultant light intercept as the canopy closes   Some versions of the model assume overtopping of some species by others   Although the model cannot take account of water or mineral nutrients  it can simulate individual plant growth  variability in growth within plant communities and inter-species competition   This model was written in Matlab   See Benjamin and Park  2007  Weed Research 47  284298 for a recent review     Traffic simulation or the simulation of transportation systems is the mathematical modeling of transportation systems  e g   freeway junctions  arterial routes  roundabouts  downtown grid systems  etc   through the application of computer software to better help plan  design  and operate transportation systems  1  Simulation of transportation systems started over forty years ago  when?   2  and is an important area of discipline in traffic engineering and transportation planning today  Various national and local transportation agencies  academic institutions and consulting firms use simulation to aid in their management of transportation networks     Simulation in transportation is important because it can study models too complicated for analytical or numerical treatment  can be used for experimental studies  can study detailed relations that might be lost in analytical or numerical treatment and can produce attractive visual demonstrations of present and future scenarios     To understand simulation  it is important to understand the concept of system state  which is a set of variables that contains enough information to describe the evolution of the system over time  3  System state can be either discrete or continuous  Traffic simulation models are classified according to discrete and continuous time  state  and space  4     Simulation methods in transportation can employ a selection of theories  including probability and statistics  differential equations and numerical methods     One of the earliest discrete event simulation models is the Monte Carlo simulation  where a series of random numbers are used to synthesise traffic conditions  5     This was followed by the cellular automata model that generates randomness from deterministic rules     More recent methods use either discrete event simulation or continuous-time simulation  Discrete event simulation models are both stochastic  with random components  and dynamic  time is a variable   Single server queues for instance can be modeled very well using discrete event simulation  as servers are usually at a single location and so are discrete  e g  traffic lights   Continuous time simulation  on the other hand  can solve the shortcoming of discrete event simulation where the model is required to have input  state and output trajectories within a time interval  The method requires the use of differential equations  specifically numerical integration methods  6  These equations can range from simple methods  such as Euler  s method  to higher order Taylor  s series methods  such as Heun  s method and Runge-Kutta  7     A class of microscopic continuous-time models  known as car-following models  are also based on differential equations  Significant models include the Pipes  intelligent driver model and Gipps   model  They model the behavior of each individual vehicle  \"microscopic\"  in order to see its implications on the whole traffic system  \"macroscopic\"   Employing a numerical method with a car-following model  such as Gipps   with Heun  s  can generate important information for traffic conditions  such as system delays and identification of bottlenecks     The methods noted above are generally used to model the behavior of an existing system  and are often focused around specific areas of interest under a range of conditions  such as a change in layout  lane closures  and different levels of traffic flow   Transport planning and forecasting can be used to develop a wider understanding of traffic demands over a broad geographic area  and predicting future traffic levels at different links  sections  in the network  incorporating different growth scenarios  with feedback loops to incorporate the effect of congestion on the distribution of trips     Traffic simulation models are useful from a microscopic  macroscopic and sometimes mesoscopic perspectives  Simulation can be applied to both transportation planning and to transportation design and operations  In transportation planning the simulation models evaluate the impacts of regional urban development patterns on the performance of the transportation infrastructure  Regional planning organizations use these models to evaluate what-if scenarios in the region  such as air quality to help develop land use policies that lead to more sustainable travel  On the other hand  modeling of transportation system operations and design focus on a smaller scale  such as a highway corridor and pinch-points  Lane types  signal timing and other traffic related questions are investigated to improve local system effectiveness and efficiency  8  While certain simulation models are specialized to model either operations or system planning  certain models have the capability to model both to some degree     Whether it is for planning or for systems operations  simulations can be used for a variety of transportation modes     Ground transportation for both passenger and goods movement is perhaps the area where simulation is most widely used  Simulation can be carried out at a corridor level  or at a more complex roadway grid network level to analyze planning  design and operations such as delay  pollution  and congestion  Ground transportation models can include all modes of roadway travel  including vehicles  trucks  buses  bicycles and pedestrians  In traditional road traffic models  aggregate representation of traffic is typically used where all vehicles of a particular group obey the same rules of behavior  in micro-simulation  driver behavior and network performance are included so that complete traffic problems  e g  Intelligent transportation system  shockwaves  can be examined  9     Rail is an important mode of travel for both freight and passengers  Modeling railways for freight movement is important to determine the operational efficiency and rationalize planning decisions  10  Freight simulation can include aspects such as dedicated truck lanes  commodity flow  corridor and system capacity  traffic assignment/network flow  and freight plans that involve travel demand forecasting  11     Maritime and air transportation presents two areas that are important for the economy  Maritime simulation primarily includes container terminal modeling  that deals with the logistics of container handling to improve system efficiency  Air transportation simulation primarily involves modeling of the airport terminal operations  baggage handling  security checkpoint   and runway operations     In addition to simulating individual modes  it is often more important to simulate a multi-modal network  since in reality modes are integrated and represent more complexities that each individual mode can overlook  Inter-modal network simulation can also better understand the impact of a certain network from a comprehensive perspective to more accurately represent its impact in order to realize important policy implications   An example of an inter-modal simulator is Commuter developed by Azalient which introduces both dynamic route and mode choice by agents during simulation - this type of modeling is referred to as nanosimulation as it considers demand and travel at a finer level of detail than traditional microsimulation     Simulation in transportation can also be integrated with urban environment simulation  where a large urban area is simulated which includes roadway networks  to better understand land use and other planning implications of the traffic network on the urban environment     Simulation software 12  is getting better in a variety of different ways  With new advancements in mathematics  engineering and computing  simulation software programs are increasingly becoming faster  more powerful  more detail oriented and more realistic  13     Transportation models generally can be classified into microscopic  mesoscopic  macroscopic  and metascopic models  Microscopic models study individual elements of transportation systems  such as individual vehicle dynamics and individual traveler behavior  Mesoscopic models analyze transportation elements in small groups  within which elements are considered homogeneous  A typical example is vehicle platoon dynamics and household-level travel behavior  Macroscopic models deal with aggregated characteristics of transportation elements  such as aggregated traffic flow dynamics and zonal-level travel demand analysis     Microsimulation models track individual vehicle movements on a second or subsecond basis  Microsimulation relies on random numbers to generate vehicles  select routing decisions  and determine behavior  Because of this variation  it is necessary to run the model several times with different random number seeds to obtain the desired accuracy  There will be a   warm-up   period before the system reaches a steady state  and this period should be excluded from the results     Microsimulation models usually produce two types of results  animated displays  and numerical output in text files  It is important to understand how the software has accumulated and summarized the numerical results to prevent incorrect interpretation  Animation can allow the analyst to quickly assess the performance  however it is limited to qualitative comparisons  The main indication of a problem that can be seen in an animation is the forming of persistent queues    \" Measures of Effectiveness   MOEs  may be calculated or defined in a manner which is unique to each simulation program  MOEs are the system performance statistics that categorize the degree to which a particular alternative meets the project objectives  The following MOEs are most common when analyzing simulation models  n\"   Other commonly reported metrics from traffic simulation tools include     \"The output of a microsimulation model is different from that of the US Federal Highway Capacity Manual  HCM   For example  most HCM procedures assume that the operation of one intersection will not be affected by the conditions of an adjacent roadway  with the exception of HCS 2000 Freeways    Rubbernecking  and long queues from one location interfering with another location would contradict this assumption  n\"   The HCM 2010 provides revised guidance on what types of output from traffic simulation software are most suitable for analysis in  and comparison to  the HCM for example vehicle trajectories and raw loop detector output    \"In the HCM delay is used to estimate the level of service  LOS  for intersections  However  there are distinct differences between the way microsimulation programs and the HCM define delay  The HCM bases its delay on adjusted flow using mean control delay for the highest 15 minute period within the hour  The distinction between total delay and control delay is important  Control delay is when a signal control causes a group to slow down or stop  It s important to look at the software s documentation to understand how it calculates delay  In order to use microsimulation outputs to find LOS  the delay must be accumulated over 15 minute intervals and averaged over several runs with different random seeds  Because the HCM uses adjusted flow  another way to compare delay is divide the simulation input s 15 minute peak volume by the peak hour factor  PHF  to increase the simulation s volume  n\"   HCM 2000 defines a queue as a line of vehicles  bicycles  or persons waiting to be served by the system in which the flow rate from the front of the queue determines the average speed within the queue  Slowly moving vehicles or people joining the rear of the queue are usually considered part of the queue  These definitions are somewhat relative and can be ambiguous  In most microsimulation programs the queue length cannot exceed the storage capacity for that turn-bay or lane  Overflows into the adjacent link or off the network are usually not accounted for  even though this may affect the results   If this is the case  a work-around can be to temporarily ignore those effects and extend the network or storage area for the link to include the maximum queue length   14     UrbanSim is an open source urban simulation system designed by Paul Waddell of the University of California  Berkeley and developed with numerous collaborators to support metropolitan land use  transportation  and environmental planning  It has been distributed on the web since 1998  with regular revisions and updates  from www urbansim org  Synthicity Inc coordinates the development of UrbanSim and provides professional services to support its application  The development of UrbanSim has been funded by several grants from the National Science Foundation  the U S  Environmental Protection Agency  the Federal Highway Administration  as well as support from states  metropolitan planning agencies and research councils in Europe and South Africa  Reviews of UrbanSim and comparison to other urban modeling platforms may be found in references  5  6  7     The first documented application of UrbanSim was a prototype application to the Eugene-Springfield  Oregon setting  8  9  Later applications of the system have been documented in several U S  cities  including Detroit  Michigan  10  Salt Lake City  Utah  11  12  San Francisco  California  13  and Seattle  Washington  14  In Europe  UrbanSim has been applied in Paris  France  15  16  17  Brussels  Belgium  and Zurich  Switzerland with various other applications not yet documented in published papers     The initial implementation of UrbanSim was implemented in Java  18  19  The software architecture was modularized and reimplemented in Python beginning in 2005  making extensive use of the Numpy numerical library  The software has been generalized and abstracted from the UrbanSim model system  and is now referred to as the Open Platform for Urban Simulation  OPUS   in order to facilitate a plug-in architecture for models such as activity-based travel  dynamic traffic assignment  emissions  and land cover change  20  OPUS includes a Graphical User Interface  and a concise expression language to facilitate access to complex internal operations by non-programmers  21  Beginning in 2012  UrbanSim was re-implemented using current Scientific Python libraries such as Pandas   UrbanSim Inc  has developed the UrbanSim Cloud Platform that deploys simulations on the cloud for scalability  enabling hundreds or even thousands of simulations to be run simultaneously  and a web browser based User Interface that features a 3D web map view of inputs and outputs from the simulation   UrbanSim models have been pre-built for 400 metropolitan areas within the United States at a census block level of detail  Users anywhere in the world can also build UrbanSim models using zone and parcel templates  by uploading local data and using the cloud resources to auto-specify and calibrate the models using local data   Details are available at www urbansim com     Earlier urban model systems were generally based on deterministic solution algorithms such as Spatial Interaction or Spatial Input-Output  that emphasize repeatability and uniqueness of convergence to an equilibrium  but rest on strong assumptions about behavior  such as agents having perfect information of all the alternative locations in the metropolitan area  transactions being costless  and markets being perfectly competitive  Housing booms and busts  and the financial crisis  are relatively clear examples of market imperfections that motivate the use of less restrictive assumptions in UrbanSim  Rather than calibrating the model to a cross-sectional equilibrium  or base-year set of conditions  statistical methods have been developed to calibrate uncertainty in UrbanSim arising from its use of Monte Carlo methods and from uncertainty in data and models  against observed data over a longitudinal period  using a method known as Bayesian Melding  22  In addition to its less strong assumptions about markets  UrbanSim departs from earlier model designs that used high levels of aggregation of geography into large zones  and agents such as households and jobs into large groups assumed to be homogeneous  Instead  UrbanSim adopts a microsimulation approach meaning that it represents individual agents within the simulation  This is an agent-level model system  but unlike most agent-based models  it does not focus exclusively on the interactions of adjacent agents  Households  businesses or jobs  buildings  and land areas represented alternatively by parcels  gridcells  or zones  are used to represent the agents and locations within a metropolitan area  The parcel level modeling applications allow for the first time the representation of accessibility at a walking scale  something that cannot be effectively done at high levels of spatial aggregation  23     One of the motivations for the UrbanSim project is to not only provide robust predictions of the potential outcomes of different transportation investments and land use policies  but also to facilitate more deliberative civic engagement in what are often contentious debates about transportation infrastructure  or land policies  with uneven distributions of benefits and costs  Initial work on this topic has adopted an approach called Value Sensitive Design  24  25  Recent work has also emerged to integrate new forms of visualization  including 3D simulated landscapes  26  27     A robotics suite is a visual environment for robot control and simulation  They are typically an end-to-end platform for robotics development and include tools for visual programming and creating and debugging robot applications  Developers can often interact with robots through web-based or visual interfaces     One objective of a robotics suite is to support a variety of different robot platforms through a common programming interface  The key point about a robotics suite is that the same code will run either with a simulated robot or the corresponding real robot without modification     Some robotic suites are based in free software  free hardware and both free software and hardware             This robotics-related article is a stub  You can help Wikipedia by expanding it     Reservoir simulation is an area of reservoir engineering in which computer models are used to predict the flow of fluids  typically  oil  water  and gas  through porous media     Under the model in the broad scientific sense of the word  they understand a real or mentally created structure that reproduces or reflects the object being studied  The name of the model comes from the Latin word modulus  which means measure  pattern  Modeling is one of the main methods of knowledge of nature and society  It is widely used in technology and is an important step in the implementation of scientific and technological progress     The creation of models of oil fields and the implementation of calculations of field development on their basis is one of the main areas of activity of engineers and oil researchers     On the basis of geological and physical information about the properties of an oil  gas or gas condensate field  consideration of the capabilities of the systems and technologies for its development create quantitative ideas about the development of the field as a whole  A system of interrelated quantitative ideas about the development of a field is a model of its development  which consists of a reservoir model and a model of a field development process     The investment project is a system of quantitative ideas about its geological and physical properties  used in the calculations of field development  The field of deposits and deposits is a system of quantitative ideas about the process of extracting oil and gas from the subsoil  Generally speaking  any combination of reservoir models and development process can be used in an oil field development model  as long as this combination most accurately reflects reservoir properties and processes  At the same time  the choice of a particular reservoir model may entail taking into account any additional features of the process model and vice versa     The reservoir model should  of course  be distinguished from its design scheme  which takes into account only the geometric shape of the reservoir  For example  a reservoir model may be a stratified heterogeneous reservoir  In the design scheme  the reservoir with the same model of it can be represented as a reservoir of a circular shape  a rectilinear reservoir  etc     Layer models and processes for extracting oil and gas from them are always clothed in a mathematical form  i e  characterized by certain mathematical relationships     The main task of the engineer engaged in the calculation of the development of an oil field is to draw up a calculation model based on individual concepts derived from a geological-geophysical study of the field  as well as hydrodynamic studies of wells     Modern computer and computational achievements make it possible to take into account the properties of the layers and the processes occurring in them when calculating the development of deposits with considerable detail     The possibilities of geological  geophysical and hydrodynamic cognition of development objects are continuously expanding  Yet these possibilities are far from endless  Therefore  there is always a need to build and use such a field development model in which the degree of knowledge of the object and the design requirements would be adequate    Traditional finite difference simulators dominate both theoretical and practical work in reservoir simulation   Conventional FD simulation is underpinned by three physical concepts  conservation of mass  isothermal fluid phase behavior  and the Darcy approximation of fluid flow through porous media   Thermal simulators  most commonly used for heavy crude oil applications  add conservation of energy to this list  allowing temperatures to change within the reservoir     Numerical techniques and approaches that are common in modern simulators     The simulation model computes the saturation change of three phases  oil  water and gas and  pressure of each phase in each cell at each time step   As a result of declining pressure as in a reservoir depletion study  gas will be liberated from the oil  If pressures increase as a result of water or gas injection  the  gas is re-dissolved into the oil phase     A simulation project of a developed field  usually requires  \"history matching\" where historical field production and pressures are compared to calculated values  nIt was realised at an early stage that this was essentially an optimisation process  corresponding to Maximum Likelihood  As such  it can be automated  and there are multiple commercial and software packages designed to accomplish just that   The model  s parameters are adjusted until a reasonable match is achieved on a field basis and usually for all wells   Commonly  producing water cuts or water-oil ratios and gas-oil ratios are matched     Without FD models  recovery estimates and oil rates can also be calculated using numerous analytical techniques which include material balance equations  including HavlenaOdeh and Tarner method   fractional flow curve methods  such as the BuckleyLeverett one-dimensional displacement method  the Deitz method for inclined structures  or coning models   and sweep efficiency estimation techniques for water floods and decline curve analysis   These methods were developed and used prior to traditional or \"conventional\" simulations tools as computationally inexpensive models based on simple homogeneous reservoir description  Analytical methods generally cannot capture all the details of the given reservoir or process  but are typically numerically fast and at times  sufficiently reliable  In modern reservoir engineering  they are generally used as screening or preliminary evaluation tools   Analytical methods are especially suitable for potential assets evaluation when the data are limited and the time is critical  or for broad studies as a pre-screening tool if a large number of processes and / or technologies are to be evaluated  The analytical methods are often developed and promoted in the academia or in-house  however commercial packages also exist     Many programs are available for reservoir simulation  The most well known  in alphabetical order  are     Open source     Commercial     Reservoir simulation is ultimately used for forecasting future oil production  decision making  and reservoir management   nThe state of the art framework for reservoir management is closed-loop field development  CLFD  optimization which utilizes reservoir simulation  together with geostatistics  data assimilation  and selection of representative models  for optimal reservoir operations     Other references    Organizational studies is \"the examination of how individuals construct organizational structures  processes  and practices and how these  in turn  shape social relations and create institutions that ultimately influence people\"  1     Organizational studies comprise different areas that deal with the different aspects of the organizations  many of the approaches are functionalist but critical research also provide alternative frame for understanding in the field  Fundamental to the study of management is organizational change  2     With the recent historical turn  there is growing interest in historical organization studies  promising a closer union between organizational and historical research whose validity derives from historical veracity and conceptual rigor  enhancing understanding of historical  contemporary and future-directed social realities  3         This sociology-related article is a stub  You can help Wikipedia by expanding it     Altreva Adaptive Modeler is a software application for creating agent-based financial market simulation models for the purpose of forecasting prices of real world market traded stocks or other securities  2  The technology it uses is based on the theory of agent-based computational economics  ACE   the computational study of economic processes modeled as dynamic systems of interacting heterogeneous agents     Altreva  s Adaptive Modeler and other agent-based models are used to simulate financial markets to capture the complex dynamics of a large diversity of investors and traders with different strategies  different trading time frames  and different investment goals  3  Agent-based models based on heterogeneous and boundedly rational  learning  agents have shown to be able to explain the empirical features of financial markets better than traditional financial models that are based on representative rational agents  4     The software creates an agent-based model for a particular stock  consisting of a population of trader agents and a virtual market  Each agent represents a virtual trader/investor and has its own trading rule and funds  The model is then evolved step by step in the following way  At every step a new  historical  real market price is imported  All agents evaluate their trading rule and place orders on the virtual market  The virtual market then determines the clearing price and executes all matching orders  The clearing price is taken as the forecast for the next step real market price   So the virtual market serves as a one-step-ahead prediction market for the real market   This process is repeated for every new received real market price  Meanwhile  the trading rules evolve through a special adaptive form of genetic programming  The forecasts are thus based on the behavior of the entire market instead of only the best performing trading rule  This intends to increase the robustness of the model and its ability to adapt to changing market circumstances  5     To avoid overfitting  or curve-fitting  to historical data - and unlike many other techniques used in trading software such as optimizing of trading rules by repeated backtesting  genetic algorithms and neural networks - Adaptive Modeler does not optimize trading rules on historical data  Instead its models evolve incrementally over the available price data so that agents experience every price change only once  as in the real world   Also there is no difference in the processing of historical and new price data  Therefore  there is no specific reason to expect that a model  s back-tested historical performance is better than its future performance  unlike when trading rules have been optimized on historical data   The historical results can therefore be considered more meaningful than results demonstrated by techniques based on optimization  6     In an example model for the S&P 500 index  7  Adaptive Modeler demonstrates significant risk-adjusted excess returns after transaction costs  On back-tested historical price data covering a period of 58 years  19502008  a compound average annual return of 20 6% was achieved  followed by a compound average annual return of 22 2% over the following 6 year out-of-sample period  2008-2014      Adaptive Modeler was used in a study to demonstrate increased complexity of trading rules in an evolutionary forecasting model during a critical period of a company  s history  8     In a study of profitability of technical trading in the foreign exchange markets  researchers using Adaptive Modeler found economically and statistically significant out-of-sample excess returns  after transaction costs  for the six most traded currency pairs  The returns were superior to those achieved by traditional econometric forecasting models  9     Adaptive Modeler was also used to study the impact of different levels of trader rationality on market properties and efficiency  10  It was found that artificial markets with more intelligent traders  compared to markets with less intelligent or zero-intelligence traders  showed improved forecasting performance  though also experienced higher volatility and lower trading volume  consistent with earlier findings   The markets with more intelligent traders also replicated the stylized facts of real financial markets the best     As an example of virtual intelligent life in a complex system  such as a stock market   Adaptive Modeler was used as an illustration of simple agents interacting in a complex  nonlinear  way to forecast stock prices  11     Electrical power system simulation involves power system modeling and network simulation in order to analyze electrical power systems using design/offline or real-time data  Power system simulation software  s are a class of computer simulation programs that focus on the operation of electrical power systems  These types of computer programs are used in a wide range of planning and operational situations for     Applications of power system simulation include  long-term generation and transmission expansion planning  short-term operational simulations  and market analysis  e g  price forecasting   nThese programs typically make use of mathematical optimization techniques such linear programming  quadratic programming  and mixed integer programming     Key elements of power systems that are modeled include     There are many power simulation software packages in commercial and non-commercial forms that range from utility-scale software to study tools     The load-flow calculation 1  is the most common network analysis tool for examining the undisturbed and disturbed network within the scope of operational and strategic planning       Using network topology  transmission line parameters  transformer parameters  generator location and limits  and load location and compensation  the load-flow calculation can provide voltage magnitudes and angles for all nodes and loading of network components  such as cables and transformers  With this information  compliance to operating limitations such as those stipulated by voltage ranges and maximum loads  can be examined  This is  for example  important for determining the transmission capacity of underground cables  where the influence of cable bundling on the load capability of each cable has to be taken also into account     Due to the ability to determine losses and reactive-power allocation  load-flow calculation also supports the planning engineer in the investigation of the most economical operation mode of the network     When changing over from single and/or multi-phase infeed low-voltage meshed networks to isolated networks  load-flow calculation is essential for operational and economical reasons  Load-flow calculation is also the basis of all further network studies  such as motor start-up or investigation of scheduled or unscheduled outages of equipment within the outage simulation     Especially when investigating motor start-up  2  the load-flow calculation results give helpful hints  for example  of whether the motor can be started in spite of the voltage drop caused by the start-up current     Short circuit analysis analyzes the power flow after a fault occurs in a power network  The faults may be three-phase short circuit  one-phase grounded  two-phase short circuit  two-phase grounded  one-phase break  two-phase break or complex faults  Results of such an analysis may help determine the following      The goal of transient stability simulation of power systems is to analyse the stability of a power system from sub-second to several tens of seconds  Stability in this aspect is the ability of the system to quickly return to a stable operating condition after being exposed to a disturbance such as for example a tree falling over an overhead line resulting in the automatic disconnection of that line by its protection systems  In engineering terms  a power system is deemed stable if the substation voltage levels and the rotational speeds of motors and generators return to their normal values in a quick and continuous manner     Models typically use the following inputs     The acceptable amount of time it takes grid voltages return to their intended levels is dependent on the magnitude of voltage disturbance  and the most common standard is specified by the CBEMA curve in Figure  1  This curve informs both electronic equipment design and grid stability data reporting  5     The problem of unit commitment involves finding the least-cost dispatch of available generation resources to meet the electrical load     Generating resources can include a wide range of types     The key decision variables that are decided by the computer program are     The latter decisions are binary  0 1   which means that the mathematical problem is not continuous     In addition  generating plants are subject to a number of complex technical constraints  including     These constraints have many different variants  all this gives rise to a large class of mathematical optimization problems     Electricity flows through an AC network according to Kirchhoff  s Laws   Transmission lines are subject to thermal limits  simple megawatt limits on flow   as well as voltage and electrical stability constraints     The simulator must calculate the flows in the AC network that result from any given combination of unit commitment and generator megawatt dispatch  and ensure that AC line flows are within both the thermal limits and the voltage and stability constraints   This may include contingencies such as the loss of any one transmission or generation element - a so-called security-constrained optimal power flow  SCOPF   and if the unit commitment is optimized inside this framework we have a security-constrained unit commitment  SCUC      In optimal power flow  OPF  the generalised scalar objective to be minimised is given by     where u is a set of the control variables  x is a set of independent variables  and the subscript 0 indicates that the variable refers to the pre-contingency power system     The SCOPF is bound by equality and inequality constraint limits  The equality constraint limits are given by the pre and post contingency power flow equations  where k refers to the kth contingency case     The equipment and operating limits are given by the following inequalities     The objective function in OPF can take on different forms relating to active or reactive power quantities that we wish to either minimise or maximise  For example we may wish to minimise transmission losses or minimise real power generation costs on a power network     Other power flow solution methods like stochastic optimization incorporate the uncertainty found in modeling power systems by using the probability distributions of certain variables whose exact values are not known  When uncertainties in the constraints are present  such as for dynamic line ratings  chance constrained optimization can be used where the probability of violating a constraint is limited to a certain value  Another technique to model variability is the Monte Carlo method  in which different combinations of inputs and resulting outputs are considered based on the probability of their occurrence in the real world  This method can be applied to simulations for system security and unit commitment risk  and it is increasingly being used to model probabilistic load flow with renewable and/or distributed generation  6     The cost of producing a megawatt of electrical energy is a function of     In addition to this  generating plant incur fixed costs including     Assuming perfect competition  the market-based price of electricity would be based purely on the cost of producing the next megawatt of power  the so-called short-run marginal cost  SRMC    This price however might not be sufficient to cover the fixed costs of generation  and thus power market prices rarely show purely SRMC pricing   In most established power markets  generators are free to offer their generation capacity at prices of their choosing   Competition and use of financial contracts keeps these prices close to SRMC  but inevitably offers price above SRMC do occur  for example during the California energy crisis of 2001      In the context of power system simulation  a number of techniques have been applied to simulate imperfect competition in electrical power markets     Various heuristics have also been applied to this problem   The aim is to provide realistic forecasts of power market prices  given the forecast supply-demand situation     Power system long-term optimization focuses on optimizing the multi-year expansion and retirement plan for generation  transmission  and distribution facilities  The optimization problem will typically consider the long term investment cash flow and a simplified version of OPF / UC  Unit commitment   to make sure the power system operates in a secure and economic way  This area can be categorized as     A well-defined power systems study requirement is critical to the success of any project as it will reduce the challenge of selecting the qualified service provider and the right analysis software  The system study specification describes the project scope  analysis types  and the required deliverable  The study specification 8  must be written to match the specific project and industry requirements and will vary based on the type of analysis     General Electric  s MAPS  Multi-Area Production Simulation  is a production simulation model used by various Regional Transmission Organizations and Independent System Operators in the United States to plan for the economic impact of proposed electric transmission and generation facilities in FERC-regulated electric wholesale markets  9  10  11  12  13   nPortions of the model may also be used for the commitment and dispatch phase  updated on 5 minute intervals  in operation of wholesale electric markets for RTO and ISO regions   nABB  s PROMOD is a similar software package  14  nThese ISO and RTO regions also utilize a GE software package called MARS  Multi-Area Reliability Simulation  to ensure the power system meets reliability criteria  a loss-of-load-expectation  LOLE  of no greater than 0 1 days per year   Further  a GE software package called PSLF  Positive Sequence Load Flow   a Siemens software package called PSSE  Power System Simulation for Engineering  and Electrical Transient Analyzer Program  ETAP  by Operation Technology Inc  15  analyzes load flow on the power system for short-circuits and stability during preliminary planning studies by RTOs and ISOs  16  17     Traffic engineering is a branch of civil engineering that uses engineering techniques to achieve the safe and efficient movement of people and goods on roadways  It focuses mainly on research for safe and efficient traffic flow  such as road geometry  sidewalks and crosswalks  cycling infrastructure  traffic signs  road surface markings and traffic lights  Traffic engineering deals with the functional part of transportation system  except the infrastructures provided     Traffic engineering is closely associated with other disciplines     Typical traffic engineering projects involve designing traffic control device installations and modifications  including traffic signals  signs  and pavement markings  Examples of Engineering Plans include pole engineering analysis and Storm Water Prevention Programs  SWPP   1  However  traffic engineers also consider traffic safety by investigating locations with high crash rates and developing countermeasures to reduce crashes   Traffic flow management can be short-term  preparing construction traffic control plans  including detour plans for pedestrian and vehicular traffic  or long-term  estimating the impacts of proposed commercial/residential developments on traffic patterns    Increasingly  traffic problems are being addressed by developing systems for intelligent transportation systems  often in conjunction with other engineering disciplines  such as computer engineering and electrical engineering     Traditionally  road improvements have consisted mainly of building additional infrastructure  However  dynamic elements are now being introduced into road traffic management  Dynamic elements have long been used in rail transport  These include sensors to measure traffic flows and automatic  interconnected  guidance systems to manage traffic  for example  traffic signs which open a lane in different directions depending on the time of day   Also  traffic flow and speed sensors are used to detect problems and alert operators  so that the cause of the congestion can be determined  and measures can be taken to minimize delays  These systems are collectively called intelligent transportation systems     The relationship between lane flow  Q  vehicles per hour   space mean speed  V  kilometers per hour  and density  K  vehicles per kilometer  is     n                    Q        =        K        V                 displaystyle Q=KV    n    Observation on limited access facilities suggests that up to a maximum flow  speed does not decline while density increases   However  above a critical threshold  increased density reduces speed   Additionally  beyond a further threshold  increased density reduces flow as well     Therefore  speeds and lane flows at bottlenecks can be kept high during peak periods by managing traffic density using devices that limit the rate at which vehicles can enter the highway  Ramp meters  signals on entrance ramps that control the rate at which vehicles are allowed to enter the mainline facility  provide this function  at the expense of increased delay for those waiting at the ramps      Highway safety engineering is a branch of traffic engineering that deals with reducing the frequency and severity of crashes  It uses physics and vehicle dynamics  as well as road user psychology and human factors engineering  to reduce the influence of factors that contribute to crashes  A well-drafted Traffic Control Plan  TCP  is critical to any job involving roadway work  A properly-prepared TCP will specify equipment  signage  placement  and personnel  2     A typical traffic safety investigation follows these steps  3     In the field of management  strategic management involves the formulation and implementation of the major goals and initiatives taken by an organization  s  managers on behalf of stakeholders  based on consideration of resources and an assessment of the internal and external environments in which the organization operates  1  2  3  4  Strategic management provides overall direction to an enterprise and involves specifying the organization  s objectives  developing policies and plans to achieve those objectives  and then allocating resources to implement the plans  5  Academics and practicing managers have developed numerous models and frameworks to assist in strategic decision-making in the context of complex environments and competitive dynamics  6  Strategic management is not static in nature  the models often quantify  include a feedback loop to monitor execution and to inform the next round of planning  7  8  9     Michael Porter identifies three principles underlying strategy  10     Corporate strategy involves answering a key question from a portfolio perspective  \"What business should we be in?\" Business strategy involves answering the question  \"How shall we compete in this business?\" 11  12     Management theory and practice often make a distinction between strategic management and operational management  with operational management concerned primarily with improving efficiency and controlling costs within the boundaries set by the organization  s strategy  citation needed     Strategy is defined as \"the determination of the basic long-term goals of an enterprise  and the adoption of courses of action and the allocation of resources necessary for carrying out these goals \" 13   Strategies are established to set direction  focus effort  define or clarify the organization  and provide consistency or guidance in response to the environment  14     Strategic management involves the related concepts of strategic planning and strategic thinking  Strategic planning is analytical in nature and refers to formalized procedures to produce the data and analyses used as inputs for strategic thinking  which synthesizes the data resulting in the strategy  Strategic planning may also refer to control mechanisms used to implement the strategy once it is determined  In other words  strategic planning happens around the strategic thinking or strategy making activity  15     Strategic management is often described as involving two major processes  formulation and implementation of strategy  While described sequentially below  in practice the two processes are iterative and each provides input for the other  15     Formulation of strategy involves analyzing the environment in which the organization operates  then making a series of strategic decisions about how the organization will compete  Formulation ends with a series of goals or objectives and measures for the organization to pursue    nEnvironmental analysis includes the     Strategic decisions are based on insight from the environmental assessment and are responses to strategic questions about how the organization will compete  such as     The answers to these and many other strategic questions result in the organization  s strategy and a series of specific short-term and long-term goals or objectives and related measures  15     The second major process of strategic management is implementation  which involves decisions regarding how the organization  s resources  i e   people  process and IT systems  will be aligned and mobilized towards the objectives   Implementation results in how the organization  s resources are structured  such as by product or service or geography   leadership arrangements  communication  incentives  and monitoring mechanisms to track progress towards objectives  among others  15     Running the day-to-day operations of the business is often referred to as \"operations management\" or specific terms for key departments or functions  such as \"logistics management\" or \"marketing management \" which take over once strategic management decisions are implemented  15     Bruce Henderson 17     In 1988  Henry Mintzberg described the many different definitions and perspectives on strategy reflected in both academic research and in practice  18  19  He examined the strategic process and concluded it was much more fluid and unpredictable than people had thought  Because of this  he could not point to one process that could be called strategic planning  Instead Mintzberg concludes that there are five types of strategies     In 1998  Mintzberg developed these five types of management strategy into 10 schools of thought and grouped them into three categories  The first group is normative  It consists of the schools of informal design and conception  the formal planning  and analytical positioning  The second group  consisting of six schools  is more concerned with how strategic management is actually done  rather than prescribing optimal plans or positions  The six schools are entrepreneurial  visionary  cognitive  learning/adaptive/emergent  negotiation  corporate culture and business environment  The third and final group consists of one school  the configuration or transformation school  a hybrid of the other schools organized into stages  organizational life cycles  or episodes  20     Michael Porter defined strategy in 1980 as the \"   broad formula for how a business is going to compete  what its goals should be  and what policies will be needed to carry out those goals\" and the \"   combination of the ends  goals  for which the firm is striving and the means  policies  by which it is seeking to get there \" He continued that  \"The essence of formulating competitive strategy is relating a company to its environment \" 21     Some complexity theorists define strategy as the unfolding of the internal and external aspects of the organization that results in actions in a socio-economic context  22  23  24     The strategic management discipline originated in the 1950s and 1960s  Among the numerous early contributors  the most influential were Peter Drucker  Philip Selznick  Alfred Chandler  Igor Ansoff  25  and Bruce Henderson  6  The discipline draws from earlier thinking and texts on   strategy   dating back thousands of years  Prior to 1960  the term \"strategy\" was primarily used regarding war and politics  not business  26  Many companies built strategic planning functions to develop and execute the formulation and implementation processes during the 1960s  27     Peter Drucker was a prolific management theorist and author of dozens of management books  with a career spanning five decades  He addressed fundamental strategic questions in a 1954 book The Practice of Management writing  \"    the first responsibility of top management is to ask the question   what is our business?   and to make sure it is carefully studied and correctly answered \" He wrote that the answer was determined by the customer  He recommended eight areas where objectives should be set  such as market standing  innovation  productivity  physical and financial resources  worker performance and attitude  profitability  manager performance and development  and public responsibility  28     In 1957  Philip Selznick initially used the term \"distinctive competence\" in referring to how the Navy was attempting to differentiate itself from the other services  6  He also formalized the idea of matching the organization  s internal factors with external environmental circumstances  29  This core idea was developed further by Kenneth R  Andrews in 1963 into what we now call SWOT analysis  in which the strengths and weaknesses of the firm are assessed in light of the opportunities and threats in the business environment  6     Alfred Chandler recognized the importance of coordinating management activity under an all-encompassing strategy  Interactions between functions were typically handled by managers who relayed information back and forth between departments  Chandler stressed the importance of taking a long-term perspective when looking to the future  In his 1962 ground breaking work Strategy and Structure  Chandler showed that a long-term coordinated strategy was necessary to give a company structure  direction and focus  He says it concisely  \"structure follows strategy \" Chandler wrote that      \"Strategy is the determination of the basic long-term goals of an enterprise  and the adoption of courses of action and the allocation of resources necessary for carrying out these goals \" 13     Igor Ansoff built on Chandler  s work by adding concepts and inventing a vocabulary  He developed a grid that compared strategies for market penetration  product development  market development and horizontal and vertical integration and diversification  He felt that management could use the grid to systematically prepare for the future  In his 1965 classic Corporate Strategy  he developed gap analysis to clarify the gap between the current reality and the goals and to develop what he called \"gap reducing actions\"  30  Ansoff wrote that strategic management had three parts  strategic planning  the skill of a firm in converting its plans into reality  and the skill of a firm in managing its own internal resistance to change  31     Bruce Henderson  founder of the Boston Consulting Group  wrote about the concept of the experience curve in 1968  following initial work begun in 1965  The experience curve refers to a hypothesis that unit production costs decline by 2030% every time cumulative production doubles  This supported the argument for achieving higher market share and economies of scale  32     Porter wrote in 1980 that companies have to make choices about their scope and the type of competitive advantage they seek to achieve  whether lower cost or differentiation  The idea of strategy targeting particular industries and customers  i e   competitive positions  with a differentiated offering was a departure from the experience-curve influenced strategy paradigm  which was focused on larger scale and lower cost  21  Porter revised the strategy paradigm again in 1985  writing that superior performance of the processes and activities performed by organizations as part of their value chain is the foundation of competitive advantage  thereby outlining a process view of strategy  33     The direction of strategic research also paralleled a major paradigm shift in how companies competed  specifically a shift from the production focus to market focus   The prevailing concept in strategy up to the 1950s was to create a product of high technical quality  If you created a product that worked well and was durable  it was assumed you would have no difficulty profiting  This was called the production orientation  Henry Ford famously said of the Model T car  \"Any customer can have a car painted any color that he wants  so long as it is black \" 34     Management theorist Peter F Drucker wrote in 1954 that it was the customer who defined what business the organization was in  16  In 1960 Theodore Levitt argued that instead of producing products then trying to sell them to the customer  businesses should start with the customer  find out what they wanted  and then produce it for them  The fallacy of the production orientation was also referred to as marketing myopia in an article of the same name by Levitt  35     Over time  the customer became the driving force behind all strategic business decisions  This marketing concept  in the decades since its introduction  has been reformulated and repackaged under names including market orientation  customer orientation  customer intimacy  customer focus  customer-driven and market focus     In 1985  Professor Ellen Earle-Chaffee summarized what she thought were the main elements of strategic management theory where consensus generally existed as of the 1970s  writing that strategic management  11     Chaffee further wrote that research up to that point covered three models of strategy  which were not mutually exclusive      The progress of strategy since 1960 can be charted by a variety of frameworks and concepts introduced by management consultants and academics  These reflect an increased focus on cost  competition and customers  These \"3 Cs\" were illuminated by much more robust empirical analysis at ever-more granular levels of detail  as industries and organizations were disaggregated into business units  activities  processes  and individuals in a search for sources of competitive advantage  26     By the 1960s  the capstone business policy course at the Harvard Business School included the concept of matching the distinctive competence of a company  its internal strengths and weaknesses  with its environment  external opportunities and threats  in the context of its objectives  This framework came to be known by the acronym SWOT and was \"a major step forward in bringing explicitly competitive thinking to bear on questions of strategy\"  Kenneth R  Andrews helped popularize the framework via a 1963 conference and it remains commonly used in practice  6     The experience curve was developed by the Boston Consulting Group in 1966  26  It is a hypothesis that total per unit costs decline systematically by as much as 1525% every time cumulative production  i e   \"experience\"  doubles  It has been empirically confirmed by some firms at various points in their history  36  Costs decline due to a variety of factors  such as the learning curve  substitution of labor for capital  automation   and technological sophistication  Author Walter Kiechel wrote that it reflected several insights  including     Kiechel wrote in 2010  \"The experience curve was  simply  the most important concept in launching the strategy revolution   with the experience curve  the strategy revolution began to insinuate an acute awareness of competition into the corporate consciousness \" Prior to the 1960s  the word competition rarely appeared in the most prominent management literature  U S  companies then faced considerably less competition and did not focus on performance relative to peers  Further  the experience curve provided a basis for the retail sale of business ideas  helping drive the management consulting  nindustry  26     The concept of the corporation as a portfolio of business units  with each plotted graphically based on its market share  a measure of its competitive position relative to its peers  and industry growth rate  a measure of industry attractiveness   was summarized in the growthshare matrix developed by the Boston Consulting Group around 1970  By 1979  one study estimated that 45% of the Fortune 500 companies were using some variation of the matrix in their strategic planning  This framework helped companies decide where to invest their resources  i e   in their high market share  high growth businesses  and which businesses to divest  i e   low market share  low growth businesses   26  The growth-share matrix was followed by G E  multi factoral model  developed by General Electric     Companies continued to diversify as conglomerates until the 1980s  when deregulation and a less restrictive antitrust environment led to the view that a portfolio of operating divisions in different industries was worth more as many independent companies  leading to the breakup of many conglomerates  26   While the popularity of portfolio theory has waxed and waned  the key dimensions considered  industry attractiveness and competitive position  remain central to strategy  6     In response to the evident problems of \"over diversification\"  C  K  Prahalad and Gary Hamel suggested that companies should build portfolios of businesses around shared technical or operating competencies  and should develop structures and processes to enhance their core competencies  37     Michael Porter also addressed the issue of the appropriate level of diversification  In 1987  he argued that corporate strategy involves two questions  1  What business should the corporation be in? and 2  How should the corporate office manage its business units? He mentioned four concepts of corporate strategy each of which suggest a certain type of portfolio and a certain role for the corporate office  the latter three can be used together  38     Building on Porter  s ideas  Michael Goold  Andrew Campbell and Marcus Alexander developed the concept of \"parenting advantage\" to be applied at the corporate level  as a parallel to the concept of \"competitive advantage\" applied at the business level  Parent companies  they argued  should aim to \"add more value\" to their portfolio of businesses than rivals   If they succeed  they have a parenting advantage   The right level of diversification depends  therefore  on the ability of the parent company to add value in comparison to others   Different parent companies with different skills should expect to have different portfolios   See Corporate Level Strategy 1995 and Strategy for the Corporate Level 2014    In 1980  Porter defined the two types of competitive advantage an organization can achieve relative to its rivals  lower cost or differentiation  This advantage derives from attribute s  that allow an organization to outperform its competition  such as superior market position  skills  or resources  In Porter  s view  strategic management should be concerned with building and sustaining competitive advantage  33     Porter developed a framework for analyzing the profitability of industries and how those profits are divided among the participants in 1980  In five forces analysis he identified the forces that shape the industry structure or environment  The framework involves the bargaining power of buyers and suppliers  the threat of new entrants  the availability of substitute products  and the competitive rivalry of firms in the industry  These forces affect the organization  s ability to raise its prices as well as the costs of inputs  such as raw materials  for its processes  21     The five forces framework helps describe how a firm can use these forces to obtain a sustainable competitive advantage  either lower cost or differentiation  Companies can maximize their profitability by competing in industries with favorable structure  Competitors can take steps to grow the overall profitability of the industry  or to take profit away from other parts of the industry structure  Porter modified Chandler  s dictum about structure following strategy by introducing a second level of structure  while organizational structure follows strategy  it in turn follows industry structure  21     Porter wrote in 1980 that strategy target either cost leadership  differentiation  or focus  21  These are known as Porter  s three generic strategies and can be applied to any size or form of business  Porter claimed that a company must only choose one of the three or risk that the business would waste precious resources  Porter  s generic strategies detail the interaction between cost minimization strategies  product differentiation strategies  and market focus strategies     Porter described an industry as having multiple segments that can be targeted by a firm  The breadth of its targeting refers to the competitive scope of the business   Porter defined two types of competitive advantage  lower cost or differentiation relative to its rivals  Achieving competitive advantage results from a firm  s ability to cope with the five forces better than its rivals  Porter wrote  \" A chieving competitive advantage requires a firm to make a choice   about the type of competitive advantage it seeks to attain and the scope within which it will attain it \" He also wrote  \"The two basic types of competitive advantage  differentiation and lower cost  combined with the scope of activities for which a firm seeks to achieve them lead to three generic strategies for achieving above average performance in an industry  cost leadership  differentiation and focus  The focus strategy has two variants  cost focus and differentiation focus \" 33     The concept of choice was a different perspective on strategy  as the 1970s paradigm was the pursuit of market share  size and scale  influenced by the experience curve  Companies that pursued the highest market share position to achieve cost advantages fit under Porter  s cost leadership generic strategy  but the concept of choice regarding differentiation and focus represented a new perspective  26     Porter  s 1985 description of the value chain refers to the chain of activities  processes or collections of processes  that an organization performs in order to deliver a valuable product or service for the market  These include functions such as inbound logistics  operations  outbound logistics  marketing and sales  and service  supported by systems and technology infrastructure  By aligning the various activities in its value chain with the organization  s strategy in a coherent way  a firm can achieve a competitive advantage  Porter also wrote that strategy is an internally consistent configuration of activities that differentiates a firm from its rivals  A robust competitive position cumulates from many activities which should fit coherently together  39     Porter wrote in 1985  \"Competitive advantage cannot be understood by looking at a firm as a whole  It stems from the many discrete activities a firm performs in designing  producing  marketing  delivering and supporting its product  Each of these activities can contribute to a firm  s relative cost position and create a basis for differentiation   the value chain disaggregates a firm into its strategically relevant activities in order to understand the behavior of costs and the existing and potential sources of differentiation \" 6     Interorganizational relationships allow independent organizations to get access to resources or to enter new markets  Interorganizational relationships represent a critical lever of competitive advantage  40     The field of strategic management has paid much attention to the different forms of relationships between organizations ranging from strategic alliances to buyer-supplier relationships  joint ventures  networks  R&D consortia  licensing  and franchising  41     On the one hand  scholars drawing on organizational economics  e g   transaction costs theory  have argued that firms use interorganizational relationships when they are the most efficient form comparatively to other forms of organization such as operating on its own or using the market  On the other hand  scholars drawing on organizational theory  e g   resource dependence theory  suggest that firms tend to partner with others when such relationships allow them to improve their status  power  reputation  or legitimacy     A key component to the strategic management of inter-organizational relationships relates to the choice of governance mechanisms  While early research focused on the choice between equity and non equity forms  42  recent scholarship studies the nature of the contractual and relational arrangements between organizations  43  44     Researchers have also noted  although to a lesser extent  45  the dark side of interorganizational relationships  such as conflict  disputes  opportunism and unethical behaviors  Relational or collaborative risk can be defined as the uncertainty about whether potentially significant and/or disappointing outcomes of collaborative activities will be realized  46  Companies can assess  monitor and manage collaborative risks  Empirical studies show that managers assess risks as lower when they external partners  higher if they are satisfied with their own performance  and lower when their business environment is turbulent  47     Gary Hamel and C  K  Prahalad described the idea of core competency in 1990  the idea that each organization has some capability in which it excels and that the business should focus on opportunities in that area  letting others go or outsourcing them  Further  core competency is difficult to duplicate  as it involves the skills and coordination of people across a variety of functional areas or processes used to deliver value to customers  By outsourcing  companies expanded the concept of the value chain  with some elements within the entity and others without  48  Core competency is part of a branch of strategy called the resource-based view of the firm  which postulates that if activities are strategic as indicated by the value chain  then the organization  s capabilities and ability to learn or adapt are also strategic  6     Peter Drucker wrote in 1994 about the \"Theory of the Business \" which represents the key assumptions underlying a firm  s strategy   These assumptions are in three categories  a  the external environment  including society  market  customer  and technology  b  the mission of the organization  and c  the core competencies needed to accomplish the mission  He continued that a valid theory of the business has four specifications  1  assumptions about the environment  mission  and core competencies must fit reality  2  the assumptions in all three areas have to fit one another  3  the theory of the business must be known and understood throughout the organization  and 4  the theory of the business has to be tested constantly     He wrote that organizations get into trouble when the assumptions representing the theory of the business no longer fit reality   He used an example of retail department stores  where their theory of the business assumed that people who could afford to shop in department stores would do so  However  many shoppers abandoned department stores in favor of specialty retailers  often located outside of malls  when time became the primary factor in the shopping destination rather than income     Drucker described the theory of the business as a \"hypothesis\" and a \"discipline \" He advocated building in systematic diagnostics  monitoring and testing of the assumptions comprising the theory of the business to maintain competitiveness  49     Strategic thinking involves the generation and application of unique business insights to opportunities intended to create competitive advantage for a firm or organization  It involves challenging the assumptions underlying the organization  s strategy and value proposition  Mintzberg wrote in 1994 that it is more about synthesis  i e   \"connecting the dots\"  than analysis  i e   \"finding the dots\"   It is about \"capturing what the manager learns from all sources  both the soft insights from his or her personal experiences and the experiences of others throughout the organization and the hard data from market research and the like  and then synthesizing that learning into a vision of the direction that the business should pursue \" Mintzberg argued that strategic thinking is the critical part of formulating strategy  more so than strategic planning exercises  27     General Andre Beaufre wrote in 1963 that strategic thinking \"is a mental process  at once abstract and rational  which must be capable of synthesizing both psychological and material data   The strategist must have a great capacity for both analysis and synthesis  analysis is necessary to assemble the data on which he makes his diagnosis  synthesis in order to produce from these data the diagnosis itself--and the diagnosis in fact amounts to a choice between alternative courses of action \" 50     Will Mulcaster 51  argued that while much research and creative thought has been devoted to generating alternative strategies  too little work has been done on what influences the quality of strategic decision making and the effectiveness with which strategies are implemented  For instance  in retrospect it can be seen that the financial crisis of 20089 could have been avoided if the banks had paid more attention to the risks associated with their investments  but how should banks change the way they make decisions to improve the quality of their decisions in the future? Mulcaster  s Managing Forces framework addresses this issue by identifying 11 forces that should be incorporated into the processes of decision making and strategic implementation  The 11 forces are  Time  Opposing forces  Politics  Perception  Holistic effects  Adding value  Incentives  Learning capabilities  Opportunity cost  Risk and Style     Strategic planning is a means of administering the formulation and implementation of strategy  Strategic planning is analytical in nature and refers to formalized procedures to produce the data and analyses used as inputs for strategic thinking  which synthesizes the data resulting in the strategy  Strategic planning may also refer to control mechanisms used to implement the strategy once it is determined  In other words  strategic planning happens around the strategy formation process  15     Porter wrote in 1980 that formulation of competitive strategy includes consideration of four key elements     The first two elements relate to factors internal to the company  i e   the internal environment   while the latter two relate to factors external to the company  i e   the external environment   21     There are many analytical frameworks which attempt to organize the strategic planning process  Examples of frameworks that address the four elements described above include      nA number of strategists use scenario planning techniques to deal with change  The way Peter Schwartz put it in 1991 is that strategic outcomes cannot be known in advance so the sources of competitive advantage cannot be predetermined  52  The fast changing business environment is too uncertain for us to find sustainable value in formulas of excellence or competitive advantage  Instead  scenario planning is a technique in which multiple outcomes can be developed  their implications assessed  and their likeliness of occurrence evaluated  According to Pierre Wack  scenario planning is about insight  complexity  and subtlety  not about formal analysis and numbers  53  The flowchart to the right provides a process for classifying a phenomenon as a scenario in the intuitive logics tradition  54     Some business planners are starting to use a complexity theory approach to strategy  Complexity can be thought of as chaos with a dash of order  55  Chaos theory deals with turbulent systems that rapidly become disordered  Complexity is not quite so unpredictable  It involves multiple agents interacting in such a way that a glimpse of structure may appear     Once the strategy is determined  various goals and measures may be established to chart a course for the organization  measure performance and control implementation of the strategy  Tools such as the balanced scorecard and strategy maps help crystallize the strategy  by relating key measures of success and performance to the strategy  These tools measure financial  marketing  production  organizational development  and innovation measures to achieve a   balanced   perspective   Advances in information technology and data availability enable the gathering of more information about performance  allowing managers to take a much more analytical view of their business than before     Strategy may also be organized as a series of \"initiatives\" or \"programs\"  each of which comprises one or more projects  Various monitoring and feedback mechanisms may also be established  such as regular meetings between divisional and corporate management to control implementation     A key component to strategic management which is often overlooked when planning is evaluation  There are many ways to evaluate whether or not strategic priorities and plans have been achieved  one such method is Robert Stake  s Responsive Evaluation  56  Responsive evaluation provides a naturalistic and humanistic approach to program evaluation  In expanding beyond the goal-oriented or pre-ordinate evaluation design  responsive evaluation takes into consideration the program  s background  history   conditions  and transactions among stakeholders  It is largely emergent  the design unfolds as contact is made with stakeholders     While strategies are established to set direction  focus effort  define or clarify the organization  and provide consistency or guidance in response to the environment  these very elements also mean that certain signals are excluded from consideration or de-emphasized  Mintzberg wrote in 1987  \"Strategy is a categorizing scheme by which incoming stimuli can be ordered and dispatched \" Since a strategy orients the organization in a particular manner or direction  that direction may not effectively match the environment  initially  if a bad strategy  or over time as circumstances change  As such  Mintzberg continued  \"Strategy  once established  is a force that resists change  not encourages it \" 14     Therefore  a critique of strategic management is that it can overly constrain managerial discretion in a dynamic environment  \"How can individuals  organizations and societies cope as well as possible with     issues too complex to be fully understood  given the fact that actions initiated on the basis of inadequate understanding may lead to significant regret?\" 57   Some theorists insist on an iterative approach  considering in turn objectives  implementation and resources  58  I e   a \"   repetitive learning cycle  rather than  a linear progression towards a clearly defined final destination \" 59  Strategies must be able to adjust during implementation because \"humans rarely can proceed satisfactorily except by learning from experience  and modest probes  serially modified on the basis of feedback  usually are the best method for such learning \" 60     In 2000  Gary Hamel coined the term strategic convergence to explain the limited scope of the strategies being used by rivals in greatly differing circumstances  He lamented that successful strategies are imitated by firms that do not understand that for a strategy to work  it must account for the specifics of each situation  61  nWoodhouse and Collingridge claim that the essence of being strategic lies in a capacity for \"intelligent trial-and error\" 60  rather than strict adherence to finely honed strategic plans  Strategy should be seen as laying out the general path rather than precise steps  62  Means are as likely to determine ends as ends are to determine means  63   The objectives that an organization might wish to pursue are limited by the range of feasible approaches to implementation   There will usually be only a small number of approaches that will not only be technically and administratively possible  but also satisfactory to the full range of organizational stakeholders   In turn  the range of feasible implementation approaches is determined by the availability of resources     Various strategic approaches used across industries  themes  have arisen over the years   These include the shift from product-driven demand to customer- or marketing-driven demand  described above   the increased use of self-service approaches to lower cost  changes in the value chain or corporate structure due to globalization  e g   off-shoring of production and assembly   and the internet     One theme in strategic competition has been the trend towards self-service  often enabled by technology  where the customer takes on a role previously performed by a worker to lower costs for the firm and perhaps prices  10  Examples include     One definition of globalization refers to the integration of economies due to technology and supply chain process innovation  Companies are no longer required to be vertically integrated  i e   designing  producing  assembling  and selling their products   In other words  the value chain for a company  s product may no longer be entirely within one firm  several entities comprising a virtual firm may exist to fulfill the customer requirement   For example  some companies have chosen to outsource production to third parties  retaining only design and sales functions inside their organization  10     The internet has dramatically empowered consumers and enabled buyers and sellers to come together with drastically reduced transaction and intermediary costs  creating much more robust marketplaces for the purchase and sale of goods and services  Examples include online auction sites  internet dating services  and internet book sellers  In many industries  the internet has dramatically altered the competitive landscape  Services that used to be provided within one entity  e g   a car dealership providing financing and pricing information  are now provided by third parties  65  Further  compared to traditional media like television  the internet has caused a major shift in viewing habits through on demand content which has led to an increasingly fragmented audience  citation needed     Author Phillip Evans said in 2013 that networks are challenging traditional hierarchies  Value chains may also be breaking up  \"deconstructing\"  where information aspects can be separated from functional activity  Data that is readily available for free or very low cost makes it harder for information-based  vertically integrated businesses to remain intact  Evans said  \"The basic story here is that what used to be vertically integrated  oligopolistic competition among essentially similar kinds of competitors is evolving  by one means or another  from a vertical structure to a horizontal one  Why is that happening? It  s happening because transaction costs are plummeting and because scale is polarizing  The plummeting of transaction costs weakens the glue that holds value chains together  and allows them to separate \"  He used Wikipedia as an example of a network that has challenged the traditional encyclopedia business model  66  Evans predicts the emergence of a new form of industrial organization called a \"stack\"  analogous to a technology stack  in which competitors rely on a common platform of inputs  services or information   essentially layering the remaining competing parts of their value chains on top of this common platform  67     In the recent decade  sustainabilityor ability to successfully sustain a company in a context of rapidly changing environmental  social  health  and economic circumstanceshas emerged as crucial aspect of any strategy development  Research focusing on corporations and leaders who have integrated sustainability into commercial strategy has led to emergence of the concept of \"embedded sustainability\"  defined by its authors Chris Laszlo and Nadya Zhexembayeva as \"incorporation of environmental  health  and social value into the core business with no trade-off in price or qualityin other words  with no social or green premium \" 68  Their research showed that embedded sustainability offers at least seven distinct opportunities for business value and competitive advantage creation  a  better risk-management  b  increased efficiency through reduced waste and resource use  c  better product differentiation  d  new market entrances  e  enhanced brand and reputation  f  greater opportunity to influence industry standards  and g  greater opportunity for radical innovation  69  Research further suggested that innovation driven by resource depletion can result in fundamental competitive advantages for a company  s products and services  as well as the company strategy as a whole  when right principles of innovation are applied  70  Asset managers who committed to integrating embedded sustainability factors in their capital allocation decisions created a stronger return on investment than managers that did not strategically integrate sustainability into their similar business model  71     In 1990  Peter Senge  who had collaborated with Arie de Geus at Dutch Shell  popularized de Geus   notion of the \"learning organization\"  72  The theory is that gathering and analyzing information is a necessary requirement for business success in the information age  To do this  Senge claimed that an organization would need to be structured such that  73     Senge identified five disciplines of a learning organization  They are     Geoffrey Moore  1991  and R  Frank and P  Cook 74  also detected a shift in the nature of competition  Markets driven by technical standards or by \"network effects\" can give the dominant firm a near-monopoly  75  The same is true of networked industries in which interoperability requires compatibility between users  Examples include Internet Explorer  s and Amazon  s early dominance of their respective industries  IE  s later decline shows that such dominance may be only temporary     Moore showed how firms could attain this enviable position by using E M  Rogers   five stage adoption process and focusing on one group of customers at a time  using each group as a base for reaching the next group  The most difficult step is making the transition between introduction and mass acceptance   See Crossing the Chasm   If successful a firm can create a bandwagon effect in which the momentum builds and its product becomes a de facto standard     In 1969  Peter Drucker coined the phrase Age of Discontinuity to describe the way change disrupts lives  76  In an age of continuity attempts to predict the future by extrapolating from the past can be accurate  But according to Drucker  we are now in an age of discontinuity and extrapolating is ineffective  He identifies four sources of discontinuity  new technologies  globalization  cultural pluralism and knowledge capital     In 1970  Alvin Toffler in Future Shock described a trend towards accelerating rates of change  77  He illustrated how social and technical phenomena had shorter lifespans with each generation  and he questioned society  s ability to cope with the resulting turmoil and accompanying anxiety  In past eras periods of change were always punctuated with times of stability  This allowed society to assimilate the change before the next change arrived  But these periods of stability had all but disappeared by the late 20th century  In 1980 in The Third Wave  Toffler characterized this shift to relentless change as the defining feature of the third phase of civilization  the first two phases being the agricultural and industrial waves   78     In 1978  Derek F  Abell  Abell  D  1978  described \"strategic windows\" and stressed the importance of the timing  both entrance and exit  of any given strategy  This led some strategic planners to build planned obsolescence into their strategies  79     In 1983  Noel Tichy wrote that because we are all beings of habit we tend to repeat what we are comfortable with  80  He wrote that this is a trap that constrains our creativity  prevents us from exploring new ideas  and hampers our dealing with the full complexity of new issues  He developed a systematic method of dealing with change that involved looking at any new issue from three angles  technical and production  political and resource allocation  and corporate culture     In 1989  Charles Handy identified two types of change  81  \"Strategic drift\" is a gradual change that occurs so subtly that it is not noticed until it is too late  By contrast  \"transformational change\" is sudden and radical  It is typically caused by discontinuities  or exogenous shocks  in the business environment  The point where a new trend is initiated is called a \"strategic inflection point\" by Andy Grove  Inflection points can be subtle or radical     In 1990  Richard Pascale wrote that relentless change requires that businesses continuously reinvent themselves  82  His famous maxim is Nothing fails like success by which he means that what was a strength yesterday becomes the root of weakness today  We tend to depend on what worked yesterday and refuse to let go of what worked so well for us in the past  Prevailing strategies become self-confirming  To avoid this trap  businesses must stimulate a spirit of inquiry and healthy debate  They must encourage a creative process of self-renewal based on constructive conflict     In 1996  Adrian Slywotzky showed how changes in the business environment are reflected in value migrations between industries  between companies  and within companies  83  He claimed that recognizing the patterns behind these value migrations is necessary if we wish to understand the world of chaotic change  In Profit Patterns  1999  he described businesses as being in a state of strategic anticipation as they try to spot emerging patterns  Slywotsky and his team identified 30 patterns that have transformed industry after industry  84     In 1997  Clayton Christensen  1997  took the position that great companies can fail precisely because they do everything right since the capabilities of the organization also define its disabilities  85  Christensen  s thesis is that outstanding companies lose their market leadership when confronted with disruptive technology  He called the approach to discovering the emerging markets for disruptive technologies agnostic marketing  i e   marketing under the implicit assumption that no one  not the company  not the customers  can know how or in what quantities a disruptive product can or will be used without the experience of using it     In 1999  Constantinos Markides reexamined the nature of strategic planning  86  He described strategy formation and implementation as an ongoing  never-ending  integrated process requiring continuous reassessment and reformation  Strategic management is planned and emergent  dynamic and interactive     J  Moncrieff  1999  stressed strategy dynamics  87  He claimed that strategy is partially deliberate and partially unplanned  The unplanned element comes from emergent strategies that result from the emergence of opportunities and threats in the environment and from \"strategies in action\"  ad hoc actions across the organization      David Teece pioneered research on resource-based strategic management and the dynamic capabilities perspective  defined as the ability to integrate  build  and reconfigure internal and external competencies to address rapidly changing environments\"  88  His 1997 paper  with Gary Pisano and Amy Shuen  \"Dynamic Capabilities and Strategic Management\" was the most cited paper in economics and business for the period from 1995 to 2005  89     In 2000  Gary Hamel discussed strategic decay  the notion that the value of every strategy  no matter how brilliant  decays over time  61     A large group of theorists felt the area where western business was most lacking was product quality  W  Edwards Deming  90  Joseph M  Juran  91  A  Kearney  92  Philip Crosby 93  and Armand Feignbaum 94  suggested quality improvement techniques such total quality management  TQM   continuous improvement  kaizen   lean manufacturing  Six Sigma  and return on quality  ROQ      Contrarily  James Heskett  1988   95  Earl Sasser  1995   William Davidow  96  Len Schlesinger  97  A  Paraurgman  1988   Len Berry  98  Jane Kingman-Brundage  99  Christopher Hart  and Christopher Lovelock  1994   felt that poor customer service was the problem  They gave us fishbone diagramming  service charting  Total Customer Service  TCS   the service profit chain  service gaps analysis  the service encounter  strategic service vision  service mapping  and service teams  Their underlying assumption was that there is no better source of competitive advantage than a continuous stream of delighted customers     Process management uses some of the techniques from product quality management and some of the techniques from customer service management  It looks at an activity as a sequential process  The objective is to find inefficiencies and make the process more effective  Although the procedures have a long history  dating back to Taylorism  the scope of their applicability has been greatly widened  leaving no aspect of the firm free from potential process improvements  Because of the broad applicability of process management techniques  they can be used as a basis for competitive advantage     Carl Sewell  100  Frederick F  Reichheld  101  C  Gronroos  102  and Earl Sasser 103  observed that businesses were spending more on customer acquisition than on retention  They showed how a competitive advantage could be found in ensuring that customers returned again and again  Reicheld broadened the concept to include loyalty from employees  suppliers  distributors and shareholders  They developed techniques for estimating customer lifetime value  CLV  for assessing long-term relationships  The concepts begat attempts to recast selling and marketing into a long term endeavor that created a sustained relationship  called relationship selling  relationship marketing  and customer relationship management   Customer relationship management  CRM  software became integral to many firms     Michael Hammer and James Champy felt that these resources needed to be restructured  104  In a process that they labeled reengineering  firm  s reorganized their assets around whole processes rather than tasks  In this way a team of people saw a project through  from inception to completion  This avoided functional silos where isolated departments seldom talked to each other  It also eliminated waste due to functional overlap and interdepartmental communications     In 1989 Richard Lester and the researchers at the MIT Industrial Performance Center identified seven best practices and concluded that firms must accelerate the shift away from the mass production of low cost standardized products  The seven areas of best practice were  105     The search for best practices is also called benchmarking  106  This involves determining where you need to improve  finding an organization that is exceptional in this area  then studying the company and applying its best practices in your firm     Professor Richard P  Rumelt described strategy as a type of problem solving in 2011  He wrote that good strategy has an underlying structure called a kernel   The kernel has three parts  1  A diagnosis that defines or explains the nature of the challenge  2  A guiding policy for dealing with the challenge  and 3  Coherent actions designed to carry out the guiding policy  107     President Kennedy outlined these three elements of strategy in his Cuban Missile Crisis Address to the Nation of 22 October 1962      Active strategic management required active information gathering and active problem solving  In the early days of Hewlett-Packard  HP   Dave Packard and Bill Hewlett devised an active management style that they called management by walking around  MBWA   Senior HP managers were seldom at their desks  They spent most of their days visiting employees  customers  and suppliers  This direct contact with key people provided them with a solid grounding from which viable strategies could be crafted  Management consultants Tom Peters and Robert H  Waterman had used the term in their 1982 book In Search of Excellence  Lessons From America  s Best-Run Companies  109  Some Japanese managers employ a similar system  which originated at Honda  and is sometimes called the 3 G  s  Genba  Genbutsu  and Genjitsu  which translate into \"actual place\"  \"actual thing\"  and \"actual situation\"      In 2010  IBM released a study summarizing three conclusions of 1500 CEOs around the world  1  complexity is escalating  2  enterprises are not equipped to cope with this complexity  and 3  creativity is now the single most important leadership competency  IBM said that it is needed in all aspects of leadership  including strategic thinking and planning  110     Similarly  McKeown argued that over-reliance on any particular approach to strategy is dangerous and that multiple methods can be used to combine the creativity and analytics to create an \"approach to shaping the future\"  that is difficult to copy  111     A 1938 treatise by Chester Barnard  based on his own experience as a business executive  described the process as informal  intuitive  non-routinized and involving primarily oral  2-way communications  Bernard says \"The process is the sensing of the organization as a whole and the total situation relevant to it  It transcends the capacity of merely intellectual methods  and the techniques of discriminating the factors of the situation  The terms pertinent to it are \"feeling\"  \"judgement\"  \"sense\"  \"proportion\"  \"balance\"  \"appropriateness\"  It is a matter of art rather than science \" 112     In 1973  Mintzberg found that senior managers typically deal with unpredictable situations so they strategize in ad hoc  flexible  dynamic  and implicit ways  He wrote  \"The job breeds adaptive information-manipulators who prefer the live concrete situation  The manager works in an environment of stimulus-response  and he develops in his work a clear preference for live action \" 113     In 1982  John Kotter studied the daily activities of 15 executives and concluded that they spent most of their time developing and working a network of relationships that provided general insights and specific details for strategic decisions  They tended to use \"mental road maps\" rather than systematic planning techniques  114     Daniel Isenberg  s 1984 study of senior managers found that their decisions were highly intuitive  Executives often sensed what they were going to do before they could explain why  115  He claimed in 1986 that one of the reasons for this is the complexity of strategic decisions and the resultant information uncertainty  116     Zuboff claimed that information technology was widening the divide between senior managers  who typically make strategic decisions  and operational level managers  who typically make routine decisions   She alleged that prior to the widespread use of computer systems  managers  even at the most senior level  engaged in both strategic decisions and routine administration  but as computers facilitated  She called it \"deskilled\"  routine processes  these activities were moved further down the hierarchy  leaving senior management free for strategic decision making     In 1977  Abraham Zaleznik distinguished leaders from managers  He described leaders as visionaries who inspire  while managers care about process  117  He claimed that the rise of managers was the main cause of the decline of American business in the 1970s and 1980s  Lack of leadership is most damaging at the level of strategic management where it can paralyze an entire organization  118     According to Corner  Kinichi  and Keats  119  strategic decision making in organizations occurs at two levels  individual and aggregate  They developed a model of parallel strategic decision making  The model identifies two parallel processes that involve getting attention  encoding information  storage and retrieval of information  strategic choice  strategic outcome and feedback  The individual and organizational processes interact at each stage  For instance  competition-oriented objectives are based on the knowledge of competing firms  such as their market share  120     The 1980s also saw the widespread acceptance of positioning theory  Although the theory originated with Jack Trout in 1969  it didn  t gain wide acceptance until Al Ries and Jack Trout wrote their classic book Positioning  The Battle For Your Mind  1979   The basic premise is that a strategy should not be judged by internal company factors but by the way customers see it relative to the competition  Crafting and implementing a strategy involves creating a position in the mind of the collective consumer  Several techniques enabled the practical use of positioning theory  Perceptual mapping for example  creates visual displays of the relationships between positions  Multidimensional scaling  discriminant analysis  factor analysis and conjoint analysis are mathematical techniques used to determine the most relevant characteristics  called dimensions or factors  upon which positions should be based  Preference regression can be used to determine vectors of ideal positions and cluster analysis can identify clusters of positions     In 1992 Jay Barney saw strategy as assembling the optimum mix of resources  including human  technology and suppliers  and then configuring them in unique and sustainable ways  121      James Gilmore and Joseph Pine found competitive advantage in mass customization  122  Flexible manufacturing techniques allowed businesses to individualize products for each customer without losing economies of scale  This effectively turned the product into a service  They also realized that if a service is mass-customized by creating a \"performance\" for each individual client  that service would be transformed into an \"experience\"  Their book  The Experience Economy  123  along with the work of Bernd Schmitt convinced many to see service provision as a form of theatre  This school of thought is sometimes referred to as customer experience management  CEM      Many industries with a high information component are being transformed  124  For example  Encarta demolished Encyclopdia Britannica  whose sales have plummeted 80% since their peak of $650 million in 1990  before it was in turn  eclipsed by collaborative encyclopedias like Wikipedia  The music industry was similarly disrupted  The technology sector has provided some strategies directly  For example  from the software development industry agile software development provides a model for shared development processes     Peter Drucker conceived of the \"knowledge worker\" in the 1950s  He described how fewer workers would do physical labor  and more would apply their minds  In 1984  John Naisbitt theorized that the future would be driven largely by information  companies that managed information well could obtain an advantage  however the profitability of what he called \"information float\"  information that the company had and others desired  would disappear as inexpensive computers made information more accessible     Daniel Bell  1985  examined the sociological consequences of information technology  while Gloria Schuck and Shoshana Zuboff looked at psychological factors  125  Zuboff distinguished between \"automating technologies\" and \"informating technologies\"  She studied the effect that both had on workers  managers and organizational structures  She largely confirmed Drucker  s predictions about the importance of flexible decentralized structure  work teams  knowledge sharing and the knowledge worker  s central role  Zuboff also detected a new basis for managerial authority  based on knowledge  also predicted by Drucker  which she called \"participative management\"  126     McKinsey & Company developed a capability maturity model in the 1970s to describe the sophistication of planning processes  with strategic management ranked the highest  The four stages include     The long-term PIMS study  started in the 1960s and lasting for 19 years  attempted to understand the Profit Impact of Marketing Strategies  PIMS   particularly the effect of market share  The initial conclusion of the study was unambiguous  the greater a company  s market share  the greater their rate of profit  Market share provides economies of scale  It also provides experience curve advantages  The combined effect is increased profits  127     The benefits of high market share naturally led to an interest in growth strategies  The relative advantages of horizontal integration  vertical integration  diversification  franchises  mergers and acquisitions  joint ventures and organic growth were discussed  Other research indicated that a low market share strategy could still be very profitable  Schumacher  1973   128  Woo and Cooper  1982   129  Levenson  1984   130  and later Traverso  2002  131  showed how smaller niche players obtained very high returns     In the 1980s business strategists realized that there was a vast knowledge base stretching back thousands of years that they had barely examined  They turned to military strategy for guidance  Military strategy books such as The Art of War by Sun Tzu  On War by von Clausewitz  and The Red Book by Mao Zedong became business classics  From Sun Tzu  they learned the tactical side of military strategy and specific tactical prescriptions  From von Clausewitz  they learned the dynamic and unpredictable nature of military action  From Mao  they learned the principles of guerrilla warfare  Important marketing warfare books include Business War Games by Barrie James  Marketing Warfare by Al Ries and Jack Trout and Leadership Secrets of Attila the Hun by Wess Roberts     The four types of business warfare theories are     The marketing warfare literature also examined leadership and motivation  intelligence gathering  types of marketing weapons  logistics and communications     By the twenty-first century marketing warfare strategies had gone out of favour in favor of non-confrontational approaches  In 1989  Dudley Lynch and Paul L  Kordis published Strategy of the Dolphin  Scoring a Win in a Chaotic World  \"The Strategy of the Dolphin\" was developed to give guidance as to when to use aggressive strategies and when to use passive strategies  A variety of aggressiveness strategies were developed     In 1993  J  Moore used a similar metaphor  132  Instead of using military terms  he created an ecological theory of predators and prey see ecological model of competition   a sort of Darwinian management strategy in which market interactions mimic long term ecological stability     Author Phillip Evans said in 2014 that \"Henderson  s central idea was what you might call the Napoleonic idea of concentrating mass against weakness  of overwhelming the enemy  What Henderson recognized was that  in the business world  there are many phenomena which are characterized by what economists would call increasing returnsscale  experience  The more you do of something  disproportionately the better you get  And therefore he found a logic for investing in such kinds of overwhelming mass in order to achieve competitive advantage  And that was the first introduction of essentially a military concept of strategy into the business world      It was on those two ideas  Henderson  s idea of increasing returns to scale and experience  and Porter  s idea of the value chain  encompassing heterogenous elements  that the whole edifice of business strategy was subsequently erected \" 133     Like Peters and Waterman a decade earlier  James Collins and Jerry Porras spent years conducting empirical research on what makes great companies  Six years of research uncovered a key underlying principle behind the 19 successful companies that they studied  They all encourage and preserve a core ideology that nurtures the company  Even though strategy and tactics change daily  the companies  nevertheless  were able to maintain a core set of values  These core values encourage employees to build an organization that lasts  In Built To Last  1994  they claim that short term profit goals  cost cutting  and restructuring will not stimulate dedicated employees to build a great company that will endure  134  In 2000 Collins coined the term \"built to flip\" to describe the prevailing business attitudes in Silicon Valley  It describes a business culture where technological change inhibits a long term focus  He also popularized the concept of the BHAG  Big Hairy Audacious Goal      Arie de Geus  1997  undertook a similar study and obtained similar results  135  He identified four key traits of companies that had prospered for 50 years or more  They are     A company with these key characteristics he called a living company because it is able to perpetuate itself  If a company emphasizes knowledge rather than finance  and sees itself as an ongoing community of human beings  it has the potential to become great and endure for decades  Such an organization is an organic entity capable of learning  he called it a \"learning organization\"  and capable of creating its own processes  goals  and persona  135     Will Mulcaster 136  suggests that firms engage in a dialogue that centres around these questions         Today  s Hours    The Computer Simulation Oral History Archive  2003-2018  includes video and audio interviews of computer simulation pioneers  The video oral histories of computer simulation pioneers were funded by the National Science Foundation  NSF  and were conducted from 2013-2016  The purpose of this grant initiative was to capture and preserve accounts of seminal projects  related pivotal events  and distinguished project contributors from the perspectives of  and the words of  individuals who witnessed the relevant history of computer simulation firsthand  The importance of collecting these accounts is also based on the remarkable degree to which computer simulation has heavily influenced the design of computing software     The Computer Simulation Oral History Archive is a part of the Computer Simulation Archive  which was established in 1998 with substantial initial donations of papers and research materials by three pioneers in the field of computer simulationRobert G  Sargent  Alan Pritsker  and Julian Reitman  The Computer Simulation Oral History Archive includes interviews conducted from 2003-2016  almost all of which were funded by a grant from the National Science Foundation       Computer simulation was established as a separate discipline of research and practice during the mid-1950s  with many seminal works in the field published from the mid-1950s to the early 1970s  Reflecting the diverse backgrounds of the fields pioneers  simulation encompasses theory  methodology  and practice arising at the interface of applied probability  computer science  electrical and computer engineering  industrial and systems engineering  management  manufacturing engineering  operations research  and statistics      The roots of the computer simulation field are revealed in the broad diversity of current application domains in which the development and use of large-scale computer simulation models are critical to the design  improvement  and operational control of computer and telecommunications networks  financial systems  healthcare delivery systems  transportation systems  and governmental and military systems  The field comprises discrete-event simulation  Monte Carlo methods  combined discrete-continuous simulation as well as hybrid analytic/simulation computer models  It is noteworthy that as the field has matured  it has contributed significantly to the evolution of allied disciplinesfor example  object-oriented programming in computer science and innovative resampling schemes in statistics     The Computer Simulation Oral History Archive is a part of the Computer Simulation Archive  which was established in 1998 with substantial initial donations of papers and research materials by three pioneers in the field of computer simulationRobert G  Sargent  Alan Pritsker  and Julian Reitman  The Computer Simulation Oral History Archive includes interviews conducted from 2003-2016  almost all of which were funded by a grant from the National Science Foundation       Computer simulation was established as a separate discipline of research and practice during the mid-1950s  with many seminal works in the field published from the mid-1950s to the early 1970s  Reflecting the diverse backgrounds of the fields pioneers  simulation encompasses theory  methodology  and practice arising at the interface of applied probability  computer science  electrical and computer engineering  industrial and systems engineering  management  manufacturing engineering  operations research  and statistics      The roots of the computer simulation field are revealed in the broad diversity of current application domains in which the development and use of large-scale computer simulation models are critical to the design  improvement  and operational control of computer and telecommunications networks  financial systems  healthcare delivery systems  transportation systems  and governmental and military systems  The field comprises discrete-event simulation  Monte Carlo methods  combined discrete-continuous simulation as well as hybrid analytic/simulation computer models  It is noteworthy that as the field has matured  it has contributed significantly to the evolution of allied disciplinesfor example  object-oriented programming in computer science and innovative resampling schemes in statistics     The Computer Simulation Oral History Archive  2003-2018  includes video and audio interviews of computer simulation pioneers  The video oral histories of computer simulation pioneers were funded by the National Science Foundation  NSF  and were conducted from 2013-2016  The purpose of this grant initiative was to capture and preserve accounts of seminal projects  related pivotal events  and distinguished project contributors from the perspectives of  and the words of  individuals who witnessed the relevant history of computer simulation firsthand  The importance of collecting these accounts is also based on the remarkable degree to which computer simulation has heavily influenced the design of computing software     This collection is arranged alphabetically     Collection is open for research  access requires at least 48 hours advance notice  Because of the nature of certain archival formats  including digital and audio-visual materials  access will require additional advanced notice  Copies of digital files will be provided for use in the SCRC Reading Room upon request      We perform virus scans upon ingest and upon request for access  However  not all virus software profiles will catch all threats  especially newer  not yet recognized ones  The researcher assumes all risk when opening files     While electronic files are made available to researchers  some files may not open with current software or at all  Researchers may be required to find and use legacy software packages to read files      Some or all electronic files may be unavailable or restricted due to privacy reasons  agreement with the donor  or because files cannot be retrieved from original media    \"The nature of the NC State University Libraries  Special Collections means that copyright or other information about restrictions may be difficult or even impossible to determine despite reasonable efforts  The NC State University Libraries claims only physical ownership of most Special Collections materials \"   The materials from our collections are made available for use in research  teaching  and private study  pursuant to U S  Copyright law  The user must assume full responsibility for any use of the materials  including but not limited to  infringement of copyright and publication rights of reproduced materials  Any materials used for academic research or otherwise should be fully credited with the source    \"This collection may contain materials with sensitive or confidential information that is protected under federal or state right to privacy laws and regulations  Researchers are advised that the disclosure of certain information pertaining to identifiable living individuals represented in this collection without the consent of those individuals may have legal ramifications  e g   a cause of action under common law for invasion of privacy may arise if facts concerning an individual s private life are published that would be deemed highly offensive to a reasonable person  for which North Carolina State University assumes no responsibility \"    Identification of item   Computer Simulation Oral History Archive  MC 00488  Special Collections Research Center  NC State University Libraries  Raleigh  NC         Accession numbers 2013 0172  2013 0177  2013 0178  2013 0265  2013 0288  2013 0300  2014 0092  2014 0124  2018 0169 each interview donated by the interviewee     Processed by  Gwynn Thayer  June 2014  machine-readable finding aid created by  Gwynn Thayer  June 2014  Digital materials processed by Jessica Rayman  2016 March and April  Updated by Gwynn Thayer  April 2017  Finding aid updated by Taylor de Klerk  2018 August     Digital copy exists  Pending staff review and approval  access will be provided for use in the SCRC Reading Room upon request  Access may be restricted     File count is approximate and may exclude system files  deleted files  and duplicates that may have been created during processing     Digital copy exists  Pending staff review and approval  access will be provided for use in the SCRC Reading Room upon request  Access may be restricted     File count is approximate and may exclude system files  deleted files  and duplicates that may have been created during processing     Collection is open for research  access requires at least 48 hours advance notice  Because of the nature of certain archival formats  including digital and audio-visual materials  access will require additional advanced notice  Copies of digital files will be provided for use in the SCRC Reading Room upon request      We perform virus scans upon ingest and upon request for access  However  not all virus software profiles will catch all threats  especially newer  not yet recognized ones  The researcher assumes all risk when opening files     While electronic files are made available to researchers  some files may not open with current software or at all  Researchers may be required to find and use legacy software packages to read files      Some or all electronic files may be unavailable or restricted due to privacy reasons  agreement with the donor  or because files cannot be retrieved from original media      Identification of item   Computer Simulation Oral History Archive  MC 00488      NC State University Libraries Special Collections Research Center   \"The nature of the NC State University Libraries  Special Collections means that copyright or other information about restrictions may be difficult or even impossible to determine despite reasonable efforts  The NC State University Libraries claims only physical ownership of most Special Collections materials \"   The materials from our collections are made available for use in research  teaching  and private study  pursuant to U S  Copyright law  The user must assume full responsibility for any use of the materials  including but not limited to  infringement of copyright and publication rights of reproduced materials  Any materials used for academic research or otherwise should be fully credited with the source    \"This collection may contain materials with sensitive or confidential information that is protected under federal or state right to privacy laws and regulations  Researchers are advised that the disclosure of certain information pertaining to identifiable living individuals represented in this collection without the consent of those individuals may have legal ramifications  e g   a cause of action under common law for invasion of privacy may arise if facts concerning an individual s private life are published that would be deemed highly offensive to a reasonable person  for which North Carolina State University assumes no responsibility \"   2 Broughton Drive  t t t t tCampus Box 7111  t t t t tRaleigh  NC 27695-7111  t t t t t 919  515-3364 t t t t    1070 Partners Way t t t t tCampus Box 7132 t t t t tRaleigh  NC 27606-7132 t t t t t 919  515-7110 t t t t        An atmospheric model is a mathematical model constructed around the full set of primitive dynamical equations which govern atmospheric motions  It can supplement these equations with parameterizations for turbulent diffusion  radiation  moist processes  clouds and precipitation   heat exchange  soil  vegetation  surface water  the kinematic effects of terrain  and convection  Most atmospheric models are numerical  i e  they discretize equations of motion  They can predict microscale phenomena such as tornadoes and boundary layer eddies  sub-microscale turbulent flow over buildings  as well as synoptic and global flows  The horizontal domain of a model is either global  covering the entire Earth  or regional  limited-area   covering only part of the Earth   The different types of models run are thermotropic  barotropic  hydrostatic  and nonhydrostatic   Some of the model types make assumptions about the atmosphere which lengthens the time steps used and increases computational speed     Forecasts are computed using mathematical equations for the physics and dynamics of the atmosphere   These equations are nonlinear and are impossible to solve exactly  Therefore  numerical methods obtain approximate solutions   Different models use different solution methods   Global models often use spectral methods for the horizontal dimensions and finite-difference methods for the vertical dimension  while regional models usually use finite-difference methods in all three dimensions   For specific locations  model output statistics use climate information  output from numerical weather prediction  and current surface weather observations to develop statistical relationships which account for model bias and resolution issues     The main assumption made by the thermotropic model is that while the magnitude of the thermal wind may change  its direction does not change with respect to height  and thus the baroclinicity in the atmosphere can be simulated using the 500 xa0mb  15 xa0inHg  and 1 000 xa0mb  30 xa0inHg  geopotential height surfaces and the average thermal wind between them  1  2     Barotropic models assume the atmosphere is nearly barotropic  which means that the direction and speed of the geostrophic wind are independent of height   In other words  no vertical wind shear of the geostrophic wind   It also implies that thickness contours  a proxy for temperature  are parallel to upper level height contours   In this type of atmosphere  high and low pressure areas are centers of warm and cold temperature anomalies   Warm-core highs  such as the subtropical ridge and Bermuda-Azores high  and cold-core lows have strengthening winds with height  with the reverse true for cold-core highs  shallow arctic highs  and warm-core lows  such as tropical cyclones   3  A barotropic model tries to solve a simplified form of atmospheric dynamics based on the assumption that the atmosphere is in geostrophic balance  that is  that the Rossby number of the air in the atmosphere is small  4  If the assumption is made that the atmosphere is divergence-free  the curl of the Euler equations reduces into the barotropic vorticity equation  This latter equation can be solved over a single layer of the atmosphere  Since the atmosphere at a height of approximately 5 5 kilometres  3 4 xa0mi  is mostly divergence-free  the barotropic model best approximates the state of the atmosphere at a geopotential height corresponding to that altitude  which corresponds to the atmosphere  s 500 xa0mb  15 xa0inHg  pressure surface  5     Hydrostatic models filter out vertically moving acoustic waves from the vertical momentum equation  which significantly increases the time step used within the model  s run   This is known as the hydrostatic approximation   Hydrostatic models use either pressure or sigma-pressure vertical coordinates   Pressure coordinates intersect topography while sigma coordinates follow the contour of the land   Its hydrostatic assumption is reasonable as long as horizontal grid resolution is not small  which is a scale where the hydrostatic assumption fails   Models which use the entire vertical momentum equation are known as nonhydrostatic   A nonhydrostatic model can be solved anelastically  meaning it solves the complete continuity equation for air assuming it is incompressible  or elastically  meaning it solves the complete continuity equation for air and is fully compressible   Nonhydrostatic models use altitude or sigma altitude for their vertical coordinates   Altitude coordinates can intersect land while sigma-altitude coordinates follow the contours of the land  6     The history of numerical weather prediction began in the 1920s through the efforts of Lewis Fry Richardson who utilized procedures developed by Vilhelm Bjerknes  7  8    It was not until the advent of the computer and computer simulation that computation time was reduced to less than the forecast period itself   ENIAC created the first computer forecasts in 1950  5  9  and more powerful computers later increased the size of initial datasets and included more complicated versions of the equations of motion  10   In 1966  West Germany and the United States began producing operational forecasts based on primitive-equation models  followed by the United Kingdom in 1972 and Australia in 1977  7  11   The development of global forecasting models led to the first climate models  12  13   The development of limited area  regional  models facilitated advances in forecasting the tracks of tropical cyclone as well as air quality in the 1970s and 1980s  14  15     Because the output of forecast models based on atmospheric dynamics requires corrections near ground level  model output statistics  MOS  were developed in the 1970s and 1980s for individual forecast points  locations   16  17   Even with the increasing power of supercomputers  the forecast skill of numerical weather models only extends to about two weeks into the future  since the density and quality of observationstogether with the chaotic nature of the partial differential equations used to calculate the forecastintroduce errors which double every five days  18  19   The use of model ensemble forecasts since the 1990s helps to define the forecast uncertainty and extend weather forecasting farther into the future than otherwise possible  20  21  22     The atmosphere is a fluid  As such  the idea of numerical weather prediction is to sample the state of the fluid at a given time and use the equations of fluid dynamics and thermodynamics to estimate the state of the fluid at some time in the future   On land  terrain maps  available at resolutions down to 1 kilometre  0 62 xa0mi  globally  are used to help model atmospheric circulations within regions of rugged topography  in order to better depict features such as downslope winds  mountain waves  and related cloudiness which affects incoming solar radiation  23   The main inputs from country-based weather services are surface observations from automated weather stations at ground level over land and from weather buoys at sea  The World Meteorological Organization acts to standardize the instrumentation  observing practices and timing of these observations worldwide  Stations either report hourly in METAR reports  24  or every six hours in SYNOP reports  25  Models are initialized using this observed data  The irregularly spaced observations are processed by data assimilation and objective analysis methods  which perform quality control and obtain values at locations usable by the model  s mathematical algorithms  The grid used for global models is geodesic or icosahedral  spaced by latitude  longitude  and elevation  26  The data are then used in the model as the starting point for a forecast  27     A variety of methods are used to gather observational data for use in numerical models  Sites launch radiosondes  which rise through the troposphere and well into the stratosphere  28  Information from weather satellites is used where traditional data sources are not available   Commerce provides pilot reports along aircraft routes 29  and ship reports along shipping routes  30   Research projects use reconnaissance aircraft to fly in and around weather systems of interest  such as tropical cyclones  31  32   Reconnaissance aircraft are also flown over the open oceans during the cold season into systems which cause significant uncertainty in forecast guidance  or are expected to be of high impact 37 xa0days into the future over the downstream continent  33   Sea ice began to be initialized in forecast models in 1971  34   Efforts to involve sea surface temperature in model initialization began in 1972 due to its role in modulating weather in higher latitudes of the Pacific  35     A model is a computer program that produces meteorological information for future times at given locations and altitudes    Within any model is a set of equations  known as the primitive equations  used to predict the future state of the atmosphere  36   These equations are initialized from the analysis data and rates of change are determined   These rates of change predict the state of the atmosphere a short time into the future  with each time increment known as a time step   The equations are then applied to this new atmospheric state to find new rates of change  and these new rates of change predict the atmosphere at a yet further time into the future   Time stepping is repeated until the solution reaches the desired forecast time   The length of the time step chosen within the model is related to the distance between the points on the computational grid  and is chosen to maintain numerical stability  37   Time steps for global models are on the order of tens of minutes  38  while time steps for regional models are between one and four minutes  39   The global models are run at varying times into the future   The UKMET Unified model is run six days into the future  40  the European Centre for Medium-Range Weather Forecasts model is run out to 10 xa0days into the future  41  while the Global Forecast System model run by the Environmental Modeling Center is run 16 xa0days into the future  42     The equations used are nonlinear partial differential equations which are impossible to solve exactly through analytical methods  43  with the exception of a few idealized cases  44  Therefore  numerical methods obtain approximate solutions   Different models use different solution methods  some global models use spectral methods for the horizontal dimensions and finite difference methods for the vertical dimension  while regional models and other global models usually use finite-difference methods in all three dimensions  43   The visual output produced by a model solution is known as a prognostic chart  or prog  45     Weather and climate model gridboxes have sides of between 5 kilometres  3 1 xa0mi  and 300 kilometres  190 xa0mi    A typical cumulus cloud has a scale of less than 1 kilometre  0 62 xa0mi   and would require a grid even finer than this to be represented physically by the equations of fluid motion   Therefore  the processes that such clouds represent are parameterized  by processes of various sophistication   In the earliest models  if a column of air in a model gridbox was unstable  i e   the bottom warmer than the top  then it would be overturned  and the air in that vertical column mixed   More sophisticated schemes add enhancements  recognizing that only some portions of the box might convect and that entrainment and other processes occur   Weather models that have gridboxes with sides between 5 kilometres  3 1 xa0mi  and 25 kilometres  16 xa0mi  can explicitly represent convective clouds  although they still need to parameterize cloud microphysics  46   The formation of large-scale  stratus-type  clouds is more physically based  they form when the relative humidity reaches some prescribed value   Still  sub grid scale processes need to be taken into account   Rather than assuming that clouds form at 100% relative humidity  the cloud fraction can be related to a critical relative humidity of 70% for stratus-type clouds  and at or above 80% for cumuliform clouds  47  reflecting the sub grid scale variation that would occur in the real world     The amount of solar radiation reaching ground level in rugged terrain  or due to variable cloudiness  is parameterized as this process occurs on the molecular scale  48   Also  the grid size of the models is large when compared to the actual size and roughness of clouds and topography   Sun angle as well as the impact of multiple cloud layers is taken into account  49   Soil type  vegetation type  and soil moisture all determine how much radiation goes into warming and how much moisture is drawn up into the adjacent atmosphere   Thus  they are important to parameterize  50     The horizontal domain of a model is either global  covering the entire Earth  or regional  covering only part of the Earth  Regional models also are known as limited-area models  or LAMs   Regional models use finer grid spacing to resolve explicitly smaller-scale meteorological phenomena  since their smaller domain decreases computational demands   Regional models use a compatible global model for initial conditions of the edge of their domain   Uncertainty and errors within LAMs are introduced by the global model used for the boundary conditions of the edge of the regional model  as well as within the creation of the boundary conditions for the LAMs itself  51     The vertical coordinate is handled in various ways  Some models  such as Richardson  s 1922 model  use geometric height                      z                 displaystyle z     as the vertical coordinate  Later models substituted the geometric                     z                 displaystyle z    coordinate with a pressure coordinate system  in which the geopotential heights of constant-pressure surfaces become dependent variables  greatly simplifying the primitive equations  52   This follows since pressure decreases with height through the Earth  s atmosphere  53  The first model used for operational forecasts  the single-layer barotropic model  used a single pressure coordinate at the 500-millibar  15 xa0inHg  level  5  and thus was essentially two-dimensional  High-resolution modelsalso called mesoscale modelssuch as the Weather Research and Forecasting model tend to use normalized pressure coordinates referred to as sigma coordinates  54     Some of the better known global numerical models are     Some of the better known regional numerical models are     Because forecast models based upon the equations for atmospheric dynamics do not perfectly determine weather conditions near the ground  statistical corrections were developed to attempt to resolve this problem  Statistical models were created based upon the three-dimensional fields produced by numerical weather models  surface observations  and the climatological conditions for specific locations   These statistical models are collectively referred to as model output statistics  MOS   58  and were developed by the National Weather Service for their suite of weather forecasting models  16   The United States Air Force developed its own set of MOS based upon their dynamical weather model by 1983  17     Model output statistics differ from the perfect prog technique  which assumes that the output of numerical weather prediction guidance is perfect  59   MOS can correct for local effects that cannot be resolved by the model due to insufficient grid resolution  as well as model biases   Forecast parameters within MOS include maximum and minimum temperatures  percentage chance of rain within a several hour period  precipitation amount expected  chance that the precipitation will be frozen in nature  chance for thunderstorms  cloudiness  and surface winds  60     In 1956  Norman Phillips developed a mathematical model that realistically depicted monthly and seasonal patterns in the troposphere  This was the first successful climate model  12  13  Several groups then began working to create general circulation models  61  The first general circulation climate model combined oceanic and atmospheric processes and was developed in the late 1960s at the Geophysical Fluid Dynamics Laboratory  a component of the U S  National Oceanic and Atmospheric Administration  62  By the early 1980s  the U S  National Center for Atmospheric Research had developed the Community Atmosphere Model  CAM   which can be run by itself or as the atmospheric component of the Community Climate System Model  The latest update  version 3 1  of the standalone CAM was issued on 1 February 2006  63  64  65  In 1986  efforts began to initialize and model soil and vegetation types  resulting in more realistic forecasts  66  Coupled ocean-atmosphere climate models  such as the Hadley Centre for Climate Prediction and Research  s HadCM3 model  are being used as inputs for climate change studies  61     Air pollution forecasts depend on atmospheric models to provide fluid flow information for tracking the movement of pollutants  67  In 1970  a private company in the U S  developed the regional Urban Airshed Model  UAM   which was used to forecast the effects of air pollution and acid rain  In the mid- to late-1970s  the United States Environmental Protection Agency took over the development of the UAM and then used the results from a regional air pollution study to improve it  Although the UAM was developed for California  it was during the 1980s used elsewhere in North America  Europe  and Asia  15     The Movable Fine-Mesh model  which began operating in 1978  was the first tropical cyclone forecast model to be based on atmospheric dynamics  14  Despite the constantly improving dynamical model guidance made possible by increasing computational power  it was not until the 1980s that numerical weather prediction  NWP  showed skill in forecasting the track of tropical cyclones  And it was not until the 1990s that NWP consistently outperformed statistical or simple dynamical models  68  Predicting the intensity of tropical cyclones using NWP has also been challenging  As of 2009  dynamical guidance remained less skillful than statistical methods  69      n    A flight simulator is a device that artificially re-creates aircraft flight and the environment in which it flies  for pilot training  design  or other purposes   It includes replicating the equations that govern how aircraft fly  how they react to applications of flight controls  the effects of other aircraft systems  and how the aircraft reacts to external factors such as air density  turbulence  wind shear  cloud  precipitation  etc   Flight simulation is used for a variety of reasons  including flight training  mainly of pilots   the design and development of the aircraft itself  and research into aircraft characteristics and control handling qualities  1     In 1910  on the initiative of the French commanders Clolus and Laffont and Lieutenant Clavenad  the first ground training aircraft for military aircraft were built  The \"Tonneau Antoinette\"  Antoinette barrel   created by the Antoinette company  seems to be the precursor of flight simulators     An area of training was for air gunnery handled by the pilot or a specialist air gunner   Firing at a moving target requires aiming ahead of the target  which involves the so-called lead angle  to allow for the time the bullets require to reach the vicinity of the target   This is sometimes also called \"deflection shooting\" and requires skill and practice   During World War I  some ground-based simulators were developed to teach this skill to new pilots  2     The best-known early flight simulation device was the Link Trainer  produced by Edwin Link in Binghamton  New York  USA  which he started building in 1927  He later patented his design  which was first available for sale in 1929   The Link Trainer was a basic metal frame flight simulator usually painted in its well-known blue color   Some of these early war era flight simulators still exist  but it is becoming increasingly difficult to find working examples  3     The Link family firm in Binghamton manufactured player pianos and organs  and Ed Link was therefore familiar with such components as leather bellows and reed switches   He was also a pilot  but dissatisfied with the amount of real flight training that was available  he decided to build a ground-based device to provide such training without the restrictions of weather and the availability of aircraft and flight instructors   His design had a pneumatic motion platform driven by inflatable bellows which provided pitch and roll cues  A vacuum motor similar to those used in player pianos rotated the platform  providing yaw cues   A generic replica cockpit with working instruments was mounted on the motion platform   When the cockpit was covered  pilots could practice flying by instruments in a safe environment   The motion platform gave the pilot cues as to real angular motion in pitch  nose up and down   roll  wing up or down  and yaw  nose left and right   4     Initially  aviation flight schools showed little interest in the \"Link Trainer\"  Link also demonstrated his trainer to the U S  Army Air Force  USAAF   but with no result  However  the situation changed in 1934 when the Army Air Force was given a government contract to fly the postal mail   This included having to fly in bad weather as well as good  for which the USAAF had not previously carried out much training  During the first weeks of the mail service  nearly a dozen Army pilots were killed  The Army Air Force hierarchy remembered Ed Link and his trainer   Link flew in to meet them at Newark Field in New Jersey  and they were impressed by his ability to arrive on a day with poor visibility  due to practice on his training device  The result was that the USAAF purchased six Link Trainers  and this can be said to mark the start of the world flight simulation industry  4     The principal pilot trainer used during World War II was the Link Trainer   Some 10 000 were produced to train 500 000 new pilots from allied nations  many in the US and Canada because many pilots were trained in those countries before returning to Europe or the Pacific to fly combat missions  4  Almost all US Army Air Force pilots were trained in a Link Trainer  5     A different type of World War II trainer was used for navigating at night by the stars   The Celestial Navigation Trainer of 1941 was 13 7 xa0m  45 xa0ft  high and capable of accommodating the navigation team of a bomber crew   It enabled sextants to be used for taking \"star shots\" from a projected display of the night sky  4     In 1954 United Airlines bought four flight simulators at a cost of $3 million from Curtiss-Wright that were similar to the earlier models  with the addition of visuals  sound and movement   This was the first of today  s modern flight simulators for commercial aircraft  6     The simulator manufacturers are consolidating and integrate vertically as training offers double-digit growth  CAE forecast 255 000 new airline pilots from 2017 to 2027  70 a day   and 180 000 first officers evolving to captains  nThe largest manufacturer is Canadian CAE Inc  with a 70% market share and $2 8 billion annual revenues  manufacturing training devices for 70 years but moved into training in 2000 with multiple acquisitions  Now CAE makes more from training than from producing the simulators  nCrawley-based L3 CTS entered the market in 2012 by acquiring Thales Training & Simulation  s manufacturing plant near Gatwick Airport where it assembles up to 30 devices a year  then UK CTC training school in 2015  Aerosim in Sanford  Florida in 2016  and Portuguese academy G Air in October 2017  7     With a 20% market share  equipment still accounts for more than half of L3 CTS turnover but that could soon be reversed as it educates 1 600 commercial pilots each year  7% of the 22 000 entering the profession annually  and aims for 10% in a fragmented market  nThe third largest is TRU Simulation + Training  created in 2014 when parent Textron Aviation merged its simulators with Mechtronix  OPINICUS and ProFlight  focusing on simulators and developing the first full-flight simulators for the 737 MAX and the 777X  nThe fourth is FlightSafety International  focused on general  business and regional aircraft  nAirbus and Boeing have invested in their own training centres  aiming for higher margins than aircraft manufacturing like MRO  competing with their suppliers CAE and L3  7     In June 2018  there were 1 270 commercial airline simulators in service  up by 50 over a year  85% FFSs and 15% FTDs  nCAE supplied 56% of this installed base  L3 CTS 20% and FlightSafety International 10%  while CAE  s training centres are the largest operator  with a 13% share  nNorth America has 38% of the world  s training devices  Asia-Pacific 25% and Europe 24%  nBoeing types represent 45% of all simulated aircraft  followed by Airbus with 35%  then Embraer at 7%  Bombardier at 6% and ATR at 3%  8     Several different devices are utilized in modern flight training   Cockpit Procedures Trainer  CPT  are used to practice basic cockpit procedures  such as processing emergency checklists  and for cockpit familiarization   Certain aircraft systems may or may not be simulated   The aerodynamic model is usually extremely generic if present at all  9     Statistically significant assessments of skill transfer based on training on a simulator and leading to handling an actual aircraft are difficult to make  particularly where motion cues are concerned   Large samples of pilot opinion are required and many subjective opinions tend to be aired  particularly by pilots not used to making objective assessments and responding to a structured test schedule   For many years  it was believed that 6 DOF motion-based simulation gave the pilot closer fidelity to flight control operations and aircraft responses to control inputs and external forces and gave a better training outcome for students than non-motion-based simulation   This is described as \"handling fidelity\"  which can be assessed by test flight standards such as the numerical Cooper-Harper rating scale for handling qualities  Recent scientific studies have shown that the use of technology such as vibration or dynamic seats within flight simulators can be equally effective in the delivery of training as large and expensive 6-DOF FFS devices  10  11     Before September 2018  12  when a manufacturer wished to have an ATD model approved  a document that contains the specifications for the model line and that proves compliance with the appropriate regulations is submitted to the FAA   Once this document  called a Qualification Approval Guide  QAG   has been approved  all future devices conforming to the QAG are automatically approved and individual evaluation is neither required nor available  13     The actual procedure accepted by all CAAs  Civil Aviation Authorities  around the world is to propose 30 days prior qualification date  40 days for CAAC  a MQTG document  Master Qualification Test Guide   which is proper to a unique simulator device and will live along the device itself  containing objective  and functional and subjective tests to demonstrate the representativeness of the simulator compare to the airplane  The results will be compared to Flight Test Data provided by aircraft OEMs or from test campaign ordered by simulator OEMs or also can be compared by POM  Proof Of Match  data provided by aircraft OEMs development simulators  Some of the QTGs will be rerun during the year to prove during continuous qualification that the simulator is still in the tolerances approved by the CAA  14  15  16     The following levels of qualification are currently being granted for both airplane and helicopter FSTD     The largest flight simulator in the world is the Vertical Motion Simulator  VMS  at NASA Ames Research Center  south of San Francisco  This has a very large-throw motion system with 60 feet  +/- 30 xa0ft  of vertical movement  heave    The heave system supports a horizontal beam on which are mounted 40 xa0ft rails  allowing lateral movement of a simulator cab of +/- 20 feet   A conventional 6-degree of freedom hexapod platform is mounted on the 40 xa0ft beam  and an interchangeable cabin is mounted on the platform   This design permits quick switching of different aircraft cabins  Simulations have ranged from blimps  commercial and military aircraft to the Space Shuttle   In the case of the Space Shuttle  the large Vertical Motion Simulator was used to investigate a longitudinal pilot-induced oscillation  PIO  that occurred on an early Shuttle flight just before landing   After identification of the problem on the VMS  it was used to try different longitudinal control algorithms and recommend the best for use in the Shuttle program  21     AMST Systemtechnik GmbH  AMST  of Austria and Environmental Tectonics Corporation  ETC  of Philadelphia  US  manufacture a range of simulators for disorientation training  that have full freedom in yaw   The most complex of these devices is the Desdemona simulator at the TNO Research Institute in The Netherlands  manufactured by AMST  This large simulator has a gimballed cockpit mounted on a framework which adds vertical motion  The framework is mounted on rails attached to a rotating platform  The rails allow the simulator cab to be positioned at different radii from the centre of rotation and this gives a sustained G capability up to about 3 5  22  23     In the fields of information technology and systems management  application performance management  APM  is the monitoring and management of performance and availability of software applications  APM strives to detect and diagnose complex application performance problems to maintain an expected level of service  APM is \"the translation of IT metrics into business meaning   i e   value  \" 1     Two sets of performance metrics are closely monitored  The first set of performance metrics defines the performance experienced by end users of the application  One example of performance is average response times under peak load  The components of the set include load and response times     The second set of performance metrics measures the computational resources used by the application for the load  indicating whether there is adequate capacity to support the load  as well as possible locations of a performance bottleneck  Measurement of these quantities establishes an empirical performance baseline for the application  The baseline can then be used to detect changes in performance  Changes in performance can be correlated with external events and subsequently used to predict future changes in application performance  3     The use of APM is common for Web applications  which lends itself best to the more detailed monitoring techniques  4  In addition to measuring response time for a user  response times for components of a Web application can also be monitored to help pinpoint causes of delay  There also exist HTTP appliances that can decode transaction-specific response times at the Web server layer of the application     In their APM Conceptual Framework  Gartner Research describes five dimensions of APM  5  6  7  8     In 2016  Gartner Research has updated its definition  into three main functional dimensions  9     Since the first half of 2013  APM has entered into a period of intense competition of technology and strategy with a multiplicity of vendors and viewpoints  10  This has caused an upheaval in the marketplace with vendors from unrelated backgrounds  including network monitoring  systems management  application instrumentation  and web performance monitoring  adopting messaging around APM which?   As a result  the term APM has become diluted and has evolved into a concept for managing application performance across many diverse computing platforms  rather than a single market  clarification needed  11  With so many vendors to choose from  selecting one can be a challenge  It is important to evaluate each carefully to ensure its capabilities meet your needs  12     Two challenges for implementing APM are  1  it can be difficult to instrument an application to monitor application performance  especially among components of an application  and  2  applications can be virtualized  which increases the variability of the measurements  13  14  To alleviate the first problem application service management  ASM  provides an application-centric approach  where business service performance visibility is a key objective  The second aspect present in distributed  virtual and cloud-based applications poses a unique challenge for application performance monitoring because most of the key system components are no longer hosted on a single machine  Each function is now likely to have been designed as an Internet service that runs on multiple virtualized systems  The applications themselves are very likely to be moving from one system to another to meet service-level objectives and deal with momentary outages  15     Applications themselves are becoming increasingly difficult to manage as they move toward highly distributed  multi-tier  multi-element constructs that in many cases rely on application development frameworks such as  NET or Java  16  The APM Conceptual Framework was designed to help prioritize an approach on what to focus on first for a quick implementation and overall understanding of the five-dimensional APM model  The framework slide outlines three areas of focus for each dimension and describes their potential benefits  These areas are referenced as \"Primary\" below  with the lower priority dimensions referenced as \"Secondary  \" 17      nMeasuring the transit of traffic from user request to data and back again is part of capturing the end-user-experience  EUE   18  The outcome of this measuring is referred to as Real-time Application monitoring  aka Top Down monitoring   which has two components  passive and active  Passive monitoring is usually an agentless appliance implemented using network port mirroring  A key feature to consider is the ability to support multi-component analytics  e g   database  client/browser   Active monitoring  on the other hand  consists of synthetic probes and web robots predefined to report system availability and business transactions  Active monitoring is a good complement to passive monitoring  together  these two components help provide visibility into application health during off peak hours when transaction volume is low      User experience management  UEM  is a subcategory that emerged from the EUE dimension to monitor the behavioral context of the user  UEM  as practiced today  goes beyond availability to capture latencies and inconsistencies as human beings interact with applications and other services  19  UEM is usually agent-based and may include JavaScript injection to monitor at the end user device  UEM is considered another facet of Real-time Application monitoring     Application Discovery and Dependency Mapping  ADDM  offerings exist to automate the process of mapping transactions and applications to underlying infrastructure components  20  When preparing to implement a runtime application architecture  it is necessary to ensure that up/down monitoring is in place for all nodes and servers within the environment  aka  bottom-up monitoring   This helps lay the foundation for event correlation and provides the basis for a general understanding on how network topologies interact with application architectures     Focus on user-defined transactions or the URL page definitions that have some meaning to the business community  For example  if there are 200 to 300 unique page definitions for a given application  group them together into 8-12 high-level categories  This allows for meaningful SLA reports  and provides trending information on application performance from a business perspective  start with broad categories and refine them over time  For a deeper understanding  see Business transaction management     Deep dive component monitoring  DDCM  requires an agent installation and is generally targeted at middleware  focusing on web  application  and messaging servers  It should provide a real-time view of the J2EE and  NET stacks  tying them back to the user-defined business transactions  A robust monitor shows a clear path from code execution  e g   spring and struts  to the URL rendered  and finally to the user request  Since DDCM is closely related to the second dimension in the APM model  most products in this field also provide application discovery dependency mapping  ADDM  as part of their offering     It is important to arrive at a common set of metrics to collect and report on for each application  then standardize on a common view on how to present the application performance data  Collecting raw data from the other tool sets across the APM model provides flexibility in application reporting  This allows for answering a wide variety of performance questions as they arise  despite the different platforms each application may be running on  Too much information is overwhelming  That is why it is important to keep reports simple or they won  t be used  21         An aircraft is a vehicle or machine that is able to fly by gaining support from the air  It counters the force of gravity by using either static lift or by using the dynamic lift of an airfoil  2  or in a few cases the downward thrust from jet engines  Common examples of aircraft include airplanes  helicopters  airships  including blimps   gliders  paramotors  and hot air balloons  3     The human activity that surrounds aircraft is called aviation  The science of aviation  including designing and building aircraft  is called aeronautics  Crewed aircraft are flown by an onboard pilot  but unmanned aerial vehicles may be remotely controlled or self-controlled by onboard computers  Aircraft may be classified by different criteria  such as lift type  aircraft propulsion  usage and others     Flying model craft and stories of manned flight go back many centuries  however  the first manned ascent xa0 and safe descent xa0 in modern times took place by larger hot-air balloons developed in the 18th century  Each of the two World Wars led to great technical advances  Consequently  the history of aircraft can be divided into five eras     Aerostats use buoyancy to float in the air in much the same way that ships float on the water  They are characterized by one or more large cells or canopies  filled with a relatively low-density gas such as helium  hydrogen  or hot air  which is less dense than the surrounding air  When the weight of this is added to the weight of the aircraft structure  it adds up to the same weight as the air that the craft displaces     Small hot-air balloons  called sky lanterns  were first invented in ancient China prior to the 3rd century BC and used primarily in cultural celebrations  and were only the second type of aircraft to fly  the first being kites  which were first invented in ancient China over two thousand years ago   See Han Dynasty     A balloon was originally any aerostat  while the term airship was used for large  powered aircraft designs xa0 usually fixed-wing  4  5  6  7  8  9  In 1919 Frederick Handley Page was reported as referring to \"ships of the air \" with smaller passenger types as \"Air yachts \" 10  In the 1930s  large intercontinental flying boats were also sometimes referred to as \"ships of the air\" or \"flying-ships\"  11  12  xa0 though none had yet been built  The advent of powered balloons  called dirigible balloons  and later of rigid hulls allowing a great increase in size  began to change the way these words were used  Huge powered aerostats  characterized by a rigid outer framework and separate aerodynamic skin surrounding the gas bags  were produced  the Zeppelins being the largest and most famous  There were still no fixed-wing aircraft or non-rigid balloons large enough to be called airships  so \"airship\" came to be synonymous with these aircraft  Then several accidents  such as the Hindenburg disaster in 1937  led to the demise of these airships  Nowadays a \"balloon\" is an unpowered aerostat and an \"airship\" is a powered one     A powered  steerable aerostat is called a dirigible  Sometimes this term is applied only to non-rigid balloons  and sometimes dirigible balloon is regarded as the definition of an airship  which may then be rigid or non-rigid   Non-rigid dirigibles are characterized by a moderately aerodynamic gasbag with stabilizing fins at the back  These soon became known as blimps  During World War II  this shape was widely adopted for tethered balloons  in windy weather  this both reduces the strain on the tether and stabilizes the balloon  The nickname blimp was adopted along with the shape  In modern times  any small dirigible or airship is called a blimp  though a blimp may be unpowered as well as powered     Heavier-than-air aircraft  such as airplanes  must find some way to push air or gas downwards so that a reaction occurs  by Newton  s laws of motion  to push the aircraft upwards  This dynamic movement through the air is the origin of the term  There are two ways to produce dynamic upthrust xa0 aerodynamic lift  and powered lift in the form of engine thrust     Aerodynamic lift involving wings is the most common  with fixed-wing aircraft being kept in the air by the forward movement of wings  and rotorcraft by spinning wing-shaped rotors sometimes called rotary wings  A wing is a flat  horizontal surface  usually shaped in cross-section as an aerofoil  To fly  air must flow over the wing and generate lift  A flexible wing is a wing made of fabric or thin sheet material  often stretched over a rigid frame  A kite is tethered to the ground and relies on the speed of the wind over its wings  which may be flexible or rigid  fixed  or rotary     With powered lift  the aircraft directs its engine thrust vertically downward  V/STOL aircraft  such as the Harrier Jump Jet and Lockheed Martin F-35B take off and land vertically using powered lift and transfer to aerodynamic lift in steady flight     A pure rocket is not usually regarded as an aerodyne because it does not depend on the air for its lift  and can even fly into space   however  many aerodynamic lift vehicles have been powered or assisted by rocket motors  Rocket-powered missiles that obtain aerodynamic lift at very high speed due to airflow over their bodies are a marginal case     The forerunner of the fixed-wing aircraft is the kite  Whereas a fixed-wing aircraft relies on its forward speed to create airflow over the wings  a kite is tethered to the ground and relies on the wind blowing over its wings to provide lift  Kites were the first kind of aircraft to fly and were invented in China around 500 BC  Much aerodynamic research was done with kites before test aircraft  wind tunnels  and computer modelling programs became available     The first heavier-than-air craft capable of controlled free-flight were gliders  A glider designed by George Cayley carried out the first true manned  controlled flight in 1853     The practical  powered  fixed-wing aircraft  the airplane or aeroplane  was invented by Wilbur and Orville Wright  Besides the method of propulsion  fixed-wing aircraft are in general characterized by their wing configuration  The most important wing characteristics are     A variable geometry aircraft can change its wing configuration during flight     A flying wing has no fuselage  though it may have small blisters or pods  The opposite of this is a lifting body  which has no wings  though it may have small stabilizing and control surfaces     Wing-in-ground-effect vehicles are generally not considered aircraft  13  They \"fly\" efficiently close to the surface of the ground or water  like conventional aircraft during takeoff  An example is the Russian ekranoplan nicknamed the \"Caspian Sea Monster\"  Man-powered aircraft also rely on ground effect to remain airborne with minimal pilot power  but this is only because they are so underpoweredin fact  the airframe is capable of flying higher     Rotorcraft  or rotary-wing aircraft  use a spinning rotor with aerofoil section blades  a rotary wing  to provide lift  Types include helicopters  autogyros  and various hybrids such as gyrodynes and compound rotorcraft     Helicopters have a rotor turned by an engine-driven shaft  The rotor pushes air downward to create lift  By tilting the rotor forward  the downward flow is tilted backward  producing thrust for forward flight  Some helicopters have more than one rotor and a few have rotors turned by gas jets at the tips     Autogyros have unpowered rotors  with a separate power plant to provide thrust  The rotor is tilted backward  As the autogyro moves forward  air blows upward across the rotor  making it spin  This spinning increases the speed of airflow over the rotor  to provide lift  Rotor kites are unpowered autogyros  which are towed to give them forward speed or tethered to a static anchor in high-wind for kited flight     Cyclogyros rotate their wings about a horizontal axis     Compound rotorcraft have wings that provide some or all of the lift in forward flight  They are nowadays classified as powered lift types and not as rotorcraft  Tiltrotor aircraft  such as the Bell Boeing V-22 Osprey   tiltwing  tail-sitter  and coleopter aircraft have their rotors/propellers horizontal for vertical flight and vertical for forward flight     The smallest aircraft are toys/recreational items  and nano aircraft     The largest aircraft by dimensions and volume  as of 2016  is the 302 xa0ft  92 xa0m  long British Airlander 10  a hybrid blimp  with helicopter and fixed-wing features  and reportedly capable of speeds up to 90 xa0mph  140 xa0km/h  78 xa0kn   and an airborne endurance of two weeks with a payload of up to 22 050 xa0lb  10 000 xa0kg   14  15  16     The largest aircraft by weight and largest regular fixed-wing aircraft ever built  as of 2016 update   is the Antonov An-225 Mriya  That Ukrainian-built six-engine Russian transport of the 1980s is 84 xa0m  276 xa0ft  long  with an 88 xa0m  289 xa0ft  wingspan  It holds the world payload record  after transporting 428 834 xa0lb  194 516 xa0kg  of goods  and has recently flown 100 xa0t  220 000 xa0lb  loads commercially  With a maximum loaded weight of 550700 xa0t  1 210 0001 540 000 xa0lb   it is also the heaviest aircraft built to date  It can cruise at 500 xa0mph  800 xa0km/h  430 xa0kn   17  18  19  20  21     The largest military airplanes are the Ukrainian/Russian Antonov An-124 Ruslan  world  s second-largest airplane  also used as a civilian transport   22  and American Lockheed C-5 Galaxy transport  weighing  loaded  over 380 xa0t  840 000 xa0lb   21  23  The 8-engine  piston/propeller Hughes H-4 Hercules \"Spruce Goose\" xa0 an American World War II wooden flying boat transport with a greater wingspan  94m/260ft  than any current aircraft and a tail height equal to the tallest  Airbus A380-800 at 24 1m/78ft  xa0 flew only one short hop in the late 1940s and never flew out of ground effect  21     The largest civilian airplanes  apart from the above-noted An-225 and An-124  are the Airbus Beluga cargo transport derivative of the Airbus A300 jet airliner  the Boeing Dreamlifter cargo transport derivative of the Boeing 747 jet airliner/transport  the 747-200B was  at its creation in the 1960s  the heaviest aircraft ever built  with a maximum weight of over 400 xa0t  880 000 xa0lb    23  and the double-decker Airbus A380 \"super-jumbo\" jet airliner  the world  s largest passenger airliner   21  24     The fastest recorded powered aircraft flight and fastest recorded aircraft flight of an air-breathing powered aircraft was of the NASA X-43A Pegasus  a scramjet-powered  hypersonic  lifting body experimental research aircraft  at Mach 9 6  exactly 3 292 8 xa0m/s  11 854 xa0km/h  6 400 7 xa0kn  7 366 xa0mph   The X-43A set that new mark  and broke its own world record of Mach 6 3  exactly 2 160 9 xa0m/s  7 779 xa0km/h  4 200 5 xa0kn  4 834 xa0mph   set in March 2004  on its third and final flight on 16 November 2004  25  26     Prior to the X-43A  the fastest recorded powered airplane flight  and still the record for the fastest manned  powered airplane / fastest manned  non-spacecraft aircraft  was of the North American X-15A-2  rocket-powered airplane at Mach 6 72  or 2 304 96 xa0m/s  8 297 9 xa0km/h  4 480 48 xa0kn  5 156 0 xa0mph   on 3 October 1967  On one flight it reached an altitude of 354 300 xa0ft  108 000 xa0m   27  28  29     The fastest known  production aircraft  other than rockets and missiles  currently or formerly operational  as of 2016  are     Gliders are heavier-than-air aircraft that do not employ propulsion once airborne  Take-off may be by launching forward and downward from a high location  or by pulling into the air on a tow-line  either by a ground-based winch or vehicle  or by a powered \"tug\" aircraft  For a glider to maintain its forward air speed and lift  it must descend in relation to the air  but not necessarily in relation to the ground   Many gliders can \"soar\"  i e   gain height from updrafts such as thermal currents  The first practical  controllable example was designed and built by the British scientist and pioneer George Cayley  whom many recognise as the first aeronautical engineer  Common examples of gliders are sailplanes  hang gliders and paragliders     Balloons drift with the wind  though normally the pilot can control the altitude  either by heating the air or by releasing ballast  giving some directional control  since the wind direction changes with altitude   A wing-shaped hybrid balloon can glide directionally when rising or falling  but a spherically shaped balloon does not have such directional control     Kites are aircraft 43  that are tethered to the ground or other object  fixed or mobile  that maintains tension in the tether or kite line  they rely on virtual or real wind blowing over and under them to generate lift and drag  Kytoons are balloon-kite hybrids that are shaped and tethered to obtain kiting deflections  and can be lighter-than-air  neutrally buoyant  or heavier-than-air     Powered aircraft have one or more onboard sources of mechanical power  typically aircraft engines although rubber and manpower have also been used  Most aircraft engines are either lightweight reciprocating engines or gas turbines  Engine fuel is stored in tanks  usually in the wings but larger aircraft also have additional fuel tanks in the fuselage     Propeller aircraft use one or more propellers  airscrews  to create thrust in a forward direction  The propeller is usually mounted in front of the power source in tractor configuration but can be mounted behind in pusher configuration  Variations of propeller layout include contra-rotating propellers and ducted fans     Many kinds of power plant have been used to drive propellers  Early airships used man power or steam engines  The more practical internal combustion piston engine was used for virtually all fixed-wing aircraft until World War II and is still used in many smaller aircraft  Some types use turbine engines to drive a propeller in the form of a turboprop or propfan  Human-powered flight has been achieved  but has not become a practical means of transport  Unmanned aircraft and models have also used power sources such as electric motors and rubber bands     Jet aircraft use airbreathing jet engines  which take in air  burn fuel with it in a combustion chamber  and accelerate the exhaust rearwards to provide thrust     Different jet engine configurations include the turbojet and turbofan  sometimes with the addition of an afterburner  Those with no rotating turbomachinery include the pulsejet and ramjet  These mechanically simple engines produce no thrust when stationary  so the aircraft must be launched to flying speed using a catapult  like the V-1 flying bomb  or a rocket  for example  Other engine types include the motorjet and the dual-cycle Pratt & Whitney J58     Compared to engines using propellers  jet engines can provide much higher thrust  higher speeds and  above about 40 000 xa0ft  12 000 xa0m   greater efficiency  44  They are also much more fuel-efficient than rockets  As a consequence nearly all large  high-speed or high-altitude aircraft use jet engines     Some rotorcraft  such as helicopters  have a powered rotary wing or rotor  where the rotor disc can be angled slightly forward so that a proportion of its lift is directed forwards  The rotor may  like a propeller  be powered by a variety of methods such as a piston engine or turbine  Experiments have also used jet nozzles at the rotor blade tips     Aircraft are designed according to many factors such as customer and manufacturer demand  safety protocols and physical and economic constraints  For many types of aircraft the design process is regulated by national airworthiness authorities     The key parts of an aircraft are generally divided into three categories     The approach to structural design varies widely between different types of aircraft  Some  such as paragliders  comprise only flexible materials that act in tension and rely on aerodynamic pressure to hold their shape  A balloon similarly relies on internal gas pressure  but may have a rigid basket or gondola slung below it to carry its payload  Early aircraft  including airships  often employed flexible doped aircraft fabric covering to give a reasonably smooth aeroshell stretched over a rigid frame  Later aircraft employed semi-monocoque techniques  where the skin of the aircraft is stiff enough to share much of the flight loads  In a true monocoque design there is no internal structure left  With the recent emphasis on sustainability hemp has picked up some attention  having a way smaller carbon foot print and 10 times stronger than steel  hemp could become the standard of manufacturing in the future   45     The key structural parts of an aircraft depend on what type it is     Lighter-than-air types are characterised by one or more gasbags  typically with a supporting structure of flexible cables or a rigid framework called its hull  Other elements such as engines or a gondola may also be attached to the supporting structure     Heavier-than-air types are characterised by one or more wings and a central fuselage  The fuselage typically also carries a tail or empennage for stability and control  and an undercarriage for takeoff and landing  Engines may be located on the fuselage or wings  On a fixed-wing aircraft the wings are rigidly attached to the fuselage  while on a rotorcraft the wings are attached to a rotating vertical shaft  Smaller designs sometimes use flexible materials for part or all of the structure  held in place either by a rigid frame or by air pressure  The fixed parts of the structure comprise the airframe     The avionics comprise the aircraft flight control systems and related equipment  including the cockpit instrumentation  navigation  radar  monitoring  and communications systems     The flight envelope of an aircraft refers to its approved design capabilities in terms of airspeed  load factor and altitude  46  47  The term can also refer to other assessments of aircraft performance such as maneuverability  When an aircraft is abused  for instance by diving it at too-high a speed  it is said to be flown outside the envelope  something considered foolhardy since it has been taken beyond the design limits which have been established by the manufacturer  Going beyond the envelope may have a known outcome such as flutter or entry to a non-recoverable spin  possible reasons for the boundary      The range is the distance an aircraft can fly between takeoff and landing  as limited by the time it can remain airborne     For a powered aircraft the time limit is determined by the fuel load and rate of consumption     For an unpowered aircraft  the maximum flight time is limited by factors such as weather conditions and pilot endurance  Many aircraft types are restricted to daylight hours  while balloons are limited by their supply of lifting gas  The range can be seen as the average ground speed multiplied by the maximum time in the air     The Airbus A350 is now the longest range airliner     Flight dynamics is the science of air vehicle orientation and control in three dimensions  The three critical flight dynamics parameters are the angles of rotation around three axes which pass through the vehicle  s center of gravity  known as pitch  roll  and yaw    \"Flight dynamics is concerned with the stability and control of an aircraft s rotation about each of these axes  n\"   An aircraft that is unstable tends to diverge from its intended flight path and so is difficult to fly  A very stable aircraft tends to stay on its flight path and is difficult to maneuver  Therefore  it is important for any design to achieve the desired degree of stability  Since the widespread use of digital computers  it is increasingly common for designs to be inherently unstable and rely on computerised control systems to provide artificial stability     A fixed wing is typically unstable in  pitch  roll  and yaw  Pitch and yaw stabilities of conventional fixed wing designs require horizontal and vertical stabilisers  48  49  which act similarly to the feathers on an arrow  50  These stabilizing surfaces allow equilibrium of aerodynamic forces and to stabilise the flight dynamics of pitch and yaw  48  49  They are usually mounted on the tail section  empennage   although in the canard layout  the main aft wing replaces the canard foreplane as pitch stabilizer  Tandem wing and tailless aircraft rely on the same general rule to achieve stability  the aft surface being the stabilising one     A rotary wing is typically unstable in yaw  requiring a vertical stabiliser     A balloon is typically very stable in pitch and roll due to the way the payload is slung underneath the center of lift     Flight control surfaces enable the pilot to control an aircraft  s flight attitude and are usually part of the wing or mounted on  or integral with  the associated stabilizing surface  Their development was a critical advance in the history of aircraft  which had until that point been uncontrollable in flight     Aerospace engineers develop control systems for a vehicle  s orientation  attitude  about its center of mass  The control systems include actuators  which exert forces in various directions  and generate rotational forces or moments about the aerodynamic center of the aircraft  and thus rotate the aircraft in pitch  roll  or yaw  For example  a pitching moment is a vertical force applied at a distance forward or aft from the aerodynamic center of the aircraft  causing the aircraft to pitch up or down  Control systems are also sometimes used to increase or decrease drag  for example to slow the aircraft to a safe speed for landing     The two main aerodynamic forces acting on any aircraft are lift supporting it in the air and drag opposing its motion  Control surfaces or other techniques may also be used to affect these forces directly  without inducing any rotation     Aircraft permit long distance  high speed travel and may be a more fuel efficient mode of transportation in some circumstances  Aircraft have environmental and climate impacts beyond fuel efficiency considerations  however  They are also relatively noisy compared to other forms of travel and high altitude aircraft generate contrails  which experimental evidence suggests may alter weather patterns     Aircraft are produced in several different types optimized for various uses  military aircraft  which includes not just combat types but many types of supporting aircraft  and civil aircraft  which include all non-military types  experimental and model     A military aircraft is any aircraft that is operated by a legal or insurrectionary armed service of any type  51  Military aircraft can be either combat or non-combat     Most military aircraft are powered heavier-than-air types  Other types  such as gliders and balloons  have also been used as military aircraft  for example  balloons were used for observation during the American Civil War and World War I  and military gliders were used during World War II to land troops     Civil aircraft divide into commercial and general types  however there are some overlaps     Commercial aircraft include types designed for scheduled and charter airline flights  carrying passengers  mail and other cargo  The larger passenger-carrying types are the airliners  the largest of which are wide-body aircraft  Some of the smaller types are also used in general aviation  and some of the larger types are used as VIP aircraft     General aviation is a catch-all covering other kinds of private  where the pilot is not paid for time or expenses  and commercial use  and involving a wide range of aircraft types such as business jets  bizjets   trainers  homebuilt  gliders  warbirds and hot air balloons to name a few  The vast majority of aircraft today are general aviation types     An experimental aircraft is one that has not been fully proven in flight  or that carries a Special Airworthiness Certificate  called an Experimental Certificate in United States parlance  This often implies that the aircraft is testing new aerospace technologies  though the term also refers to amateur-built and kit-built aircraft  many of which are based on proven designs     A model aircraft is a small unmanned type made to fly for fun  for static display  for aerodynamic research or for other purposes  A scale model is a replica of some larger design     Category  By country     Noise control or noise mitigation is a set of strategies to reduce noise pollution or to reduce the impact of that noise  whether outdoors or indoors     The main areas of noise mitigation or abatement are  transportation noise control  architectural design  urban planning through zoning codes  1  and occupational noise control  Roadway noise and aircraft noise are the most pervasive sources of environmental noise  Social activities may generate noise levels that consistently affect the health of populations residing in or occupying areas  both indoor and outdoor  near entertainment venues that feature amplified sounds and music that present significant challenges for effective noise mitigation strategies     Multiple techniques have been developed to address interior sound levels  many of which are encouraged by local building codes  In the best case of project designs  planners are encouraged to work with design engineers to examine trade-offs of roadway design and architectural design  These techniques include design of exterior walls  party walls  and floor and ceiling assemblies  moreover  there are a host of specialized means for damping reverberation from special-purpose rooms such as auditoria  concert halls  entertainment and social venues  dining areas  audio recording rooms  and meeting rooms     Many of these techniques rely upon material science applications of constructing sound baffles or using sound-absorbing liners for interior spaces  Industrial noise control is a subset of interior architectural control of noise  with emphasis on specific methods of sound isolation from industrial machinery and for protection of workers at their task stations     Sound masking is the active addition of noise to reduce the annoyance of certain sounds  the opposite of soundproofing     Organizations each have their own standards  recommendations/guidelines  and directives for what levels of noise workers are permitted to be around before noise controls must be put into place     OSHA  s requirements state that when workers are exposed to noise levels above 90 A-weighted decibels  dBA  in 8-hour time-weighted averages  TWA   administrative controls and/or new engineering controls must be implemented in the workplace  OSHA also requires that impulse noises and impact noises must be controlled to prevent these noises reaching past 140 xa0dB peak sound pressure levels  SPL   2  3     MSHA requires that administrative and/or engineering controls must be implemented in the workplace when miners are exposed to levels above 90 dBA TWA  If noise levels exceed 115 dBA  miners are required to wear hearing protection  MSHA  therefore  requires that noise levels be reduced below 115 xa0dB TWA  Measuring noise levels for noise control decision making must integrate all noises from 90dBA to 140 dBA  4  3     The FRA recommends that worker exposure to noise should be reduced when their noise exposure exceeds 90 dBA for an 8-hour TWA  Noise measurements must integrate all noises  including intermittent  continuous  impact  and impulse noises between 80 dBA to 140 dBA  5  3     The Department of Defense  DoD  suggests that noise levels be controlled primarily through engineering controls  The DoD requires that all steady-state noises be reduced to levels below 85 dBA and that impulse noises be reduced below 140 xa0dB peak SPL  TWA exposures are not considered for the DoD  s requirements  6  3     The European Parliament and Council directive require noise levels to be reduced or eliminated using administrative and engineering controls  This directive requires lower exposure action levels of 80 dBA for 8 hours with 135 xa0dB peak SPL  along with upper exposure action levels of 85 dBA for 8 hours with 137 peak dBSPL  Exposure limits are 87 dBA for 8 hours with peak levels of 140 peak dBSPL  7  3     An effective model for noise control is the source  path  and receiver model by Bolt and Ingard  8  Hazardous noise can be controlled by reducing the noise output at its source  minimizing the noise as it travels along a path to the listener  and providing equipment to the listener or receiver to attenuate the noise     A variety of measures aim to reduce hazardous noise at its source  Programs such as Buy Quiet and the National Institute for Occupational Safety and Health  NIOSH  Prevention through design promote research and design of quiet equipment and renovation and replacement of older hazardous equipment with modern technologies  9     The principle of noise reduction through pathway modifications applies to the alteration of direct and indirect pathways for noise  3  Noise that travels across reflective surfaces  such as smooth floors  can be hazardous  Pathway alterations include physical materials  such as foam  absorb sound and walls to provide a sound barrier that modifies existing systems that decrease hazardous noise   Sound dampening enclosures for loud equipment and isolation chambers from which workers can remotely control equipment can also be designed  These methods prevent sound from traveling along a path to the worker or other listeners     In the industrial or commercial setting  workers must comply with the appropriate Hearing conservation program  Administrative controls  such as the restriction of personnel in noisy areas  prevents unnecessary noise exposure  Personal protective equipment such as foam ear plugs or ear muffs to attenuate sound provide a last line of defense for the listener     Source control in roadway noise has provided little reduction in vehicle noise  except for the development of the hybrid vehicle  nevertheless  hybrid use will need to attain a market share of roughly fifty percent to have a major impact on noise source reduction of city streets  citation needed  Highway noise is today less affected by motor type  since the effects in higher speed are aerodynamic and tire noise related   Other contributions to the reduction of noise at the source are  improved tire tread designs for trucks in the 1970s  better shielding of diesel stacks in the 1980s  and local vehicle regulation of unmuffled vehicles  10     The most fertile areas for roadway noise mitigation are in urban planning decisions  roadway design  noise barrier design  11  speed control  surface pavement selection  and truck restrictions   Speed control is effective since the lowest sound emissions arise from vehicles moving smoothly at 30 to 60 kilometers per hour   Above that range  sound emissions double with every five miles per hour of speed  At the lowest speeds  braking and  engine  acceleration noise dominates     Selection of road surface pavement can make a difference of a factor of two in sound levels  for the speed regime above 30 kilometers per hour   Quieter pavements are porous with a negative surface texture and use small to medium-sized aggregates  the loudest pavements have transversely-grooved surfaces  positive surface textures  and larger aggregates   Surface friction and roadway safety are important considerations as well for pavement decisions     When designing new urban freeways or arterials  there are numerous design decisions regarding alignment and roadway geometrics  12  Use of a computer model to calculate sound levels has become standard practice since the early 1970s  In this way exposure of sensitive receptors to elevated sound levels can be minimized  An analogous process exists for urban mass transit systems and other rail transportation decisions  Early examples of urban rail systems designed using this technology were  Boston MBTA line expansions  1970s   San Francisco BART system expansion  1981   Houston METRORail system  1982   and the MAX Light Rail system in Portland  Oregon  1983      Noise barriers can be applied to existing or planned surface transportation projects  They are one of the most effective actions taken in retrofitting existing roadways and commonly can reduce adjacent land-use sound levels by up to ten decibels  A computer model is required to design the barrier since terrain  micrometeorology and other locale-specific factors make the endeavor a very complex undertaking  For example  a roadway in cut or strong prevailing winds can produce a setting where atmospheric sound propagation is unfavorable to any noise barrier     As in the case of roadway noise  little progress has been made in quelling aircraft noise at the source  other than elimination of loud engine designs from the 1960s and earlier   Because of its velocity and volume  jet turbine engine exhaust noise defies reduction by any simple means     The most promising forms of aircraft noise abatement are through land planning  flight operations restrictions and residential soundproofing  Flight restrictions can take the form of preferred runway use  departure flight path and slope  and time-of-day restrictions  These tactics are sometimes controversial since they can impact aircraft safety  flying convenience and airline economics     In 1979  the US Congress authorized 13  the FAA to devise technology and programs to attempt to insulate homes near airports  While this obviously does not aid the exterior environment  the program has been effective for residential and school interiors  Some of the airports at which the technology was applied early on were San Francisco International Airport  14  Seattle-Tacoma International Airport  John Wayne International Airport and San Jose International Airport 15  in California     The underlying technology is a computer model which simulates the propagation of aircraft noise and its penetration into buildings  Variations in aircraft types  flight patterns and local meteorology can be analyzed along with benefits of alternative building retrofit strategies such as roof upgrading  window glazing improvement  fireplace baffling  caulking construction seams and other measures  The computer model allows cost-effectiveness evaluations of a host of alternative strategies     In Canada  Transport Canada prepares noise exposure forecasts  NEF  for each airport  using a computer model similar to that used in the US  Residential land development is discouraged within high impact areas identified by the forecast  16     In 1998  the flight paths across all of Scandinavia were changed as the new Oslo-Gardermoen Airport was opened  These new paths were straighter  reducing fuel use and disturbing fewer people  however  vociferous protests came from people near the new paths who had not been disturbed before  and they took legal action  NIMBY effect   citation needed     Architectural acoustics noise control practices include interior sound reverberation reduction  inter-room noise transfer mitigation  and exterior building skin augmentation     In the case of construction of new  or remodeled  apartments  condominiums  hospitals  and hotels  many states and cities have stringent building codes with requirements of acoustical analysis  in order to protect building occupants   With regard to exterior noise  the codes usually require measurement of the exterior acoustic environment in order to determine the performance standard required for exterior building skin design   The architect can work with the acoustical scientist to arrive at the best cost-effective means of creating a quiet interior  normally 45 dBA    The most important elements of design of the building skin are usually  glazing  glass thickness  double pane design etc    perforated metal  used internally or externally   17   roof material  caulking standards  chimney baffles  exterior door design  mail slots  attic ventilation ports  and mounting of through-the-wall air conditioners     Regarding sound generated inside the building  there are two principal types of transmission   Firstly  airborne sound travels through walls or floor and ceiling assemblies and can emanate from either human activities in adjacent living spaces or from mechanical noise within the building systems   Human activities might include voice  noise from amplified sound systems  or animal noise   Mechanical systems are elevator systems  boilers  refrigeration or air conditioning systems  generators and trash compactors   Aerodynamic sources include fans  pneumatics  and combustion   Noise control for aerodynamic sources include quiet air nozzles  pneumatic silencers and quiet fan technology   Since many mechanical sounds are inherently loud  the principal design element is to require the wall or ceiling assembly to meet certain performance standards  18   typically Sound transmission class of 50   which allows considerable attenuation of the sound level reaching occupants     The second type of interior sound is called Impact Insulation Class  IIC  transmission   This effect arises not from airborne transmission  but rather from the transmission of sound through the building itself   The most common perception of IIC noise is from the footfall of occupants in living spaces above  Low-frequency noise is transferred easily through the ground and buildings  This type of noise is more difficult to abate  but consideration must be given to isolating the floor assembly above or hanging the lower ceiling on resilient channel     Both of the transmission effects noted above may emanate either from building occupants or from building mechanical systems such as elevators  plumbing systems or heating  ventilating and air conditioning units   In some cases  it is merely necessary to specify the best available quieting technology in selecting such building hardware   In other cases  shock mounting of systems to control vibration may be in order   In the case of plumbing systems  there are specific protocols developed  especially for water supply lines  to create isolation clamping of pipes within building walls   In the case of central air systems  it is important to baffle any ducts that could transmit sound between different building areas     Designing special-purpose rooms has more exotic challenges  since these rooms may have requirements for unusual features such as concert performance  sound studio recording  lecture halls   In these cases reverberation and reflection must be analyzed in order to not only quiet the rooms  but to prevent echo effects from occurring   In these situations special sound baffles and sound absorptive lining materials may be specified to dampen unwanted effects     Acoustical wall and ceiling panels are a common commercial and residential solution for noise control in already-constructed buildings  Acoustic panels may be constructed of a variety of materials  though commercial acoustic applications will frequently be composed of fiberglass or mineral wool-based acoustic substrates  For example   Mineral fiberboard is a commonly used acoustical substrate  and commercial thermal insulations  such as those used in the insulation of boiler tanks  are frequently repurposed for noise-controlling acoustic use based on their effectiveness at minimizing reverberations  The ideal acoustical panels are those without a face or finish material that could interfere with the performance of the acoustical infill  but aesthetic and safety concerns typically lead to fabric coverings or other finishing materials to minimize impedance  Panel finishings are occasionally made of a porous configuration of wood or metal     The effectiveness of post-construction acoustic treatment is limited by the amount of space able to be allocated to acoustic treatment  and so on-site acoustical wall panels are frequently made to conform to the shape of the preexisting space  This is done by \"framing\" the perimeter track into shape  infilling the acoustical substrate and then stretching and tucking the fabric into the perimeter frame system  On-site wall panels can be constructed to work around door frames  baseboard  or any other intrusion  Large panels  generally greater than 50 feet  can be created on walls and ceilings with this method     Double-glazed and thicker windows can also prevent sound transmission from the outdoors     Industrial noise is traditionally associated with manufacturing settings where industrial machinery produces intense sound levels  19  often upwards of 85 decibels   While this circumstance is the most dramatic  there are many other work environments where sound levels may lie in the range of 70 to 75 decibels  entirely composed of office equipment  music  public address systems  and even exterior noise intrusion  Either type of environment may result in noise health effects if the sound intensity and exposure time is too great     In the case of industrial equipment  the most common techniques for noise protection of workers consist of shock mounting source equipment  creation of acrylic glass or other solid barriers  and provision of ear protection equipment  In certain cases the machinery itself can be re-designed to operate in a manner less prone to produce grating  grinding  frictional  or other motions that induce sound emissions  In recent years  Buy Quiet programs and initiatives have arisen in an effort to combat occupational noise exposures  These programs promote the purchase of quieter tools and equipment and encourage manufacturers to design quieter equipment  20     In the case of more conventional office environments  the techniques in architectural acoustics discussed above may apply  Other solutions may involve researching the quietest models of office equipment  particularly printers and photocopy machines   Impact printers and other equipment were often fitted with \"acoustic hoods\"  enclosures to reduce emitted noise  One source of annoying  if not loud  sound level emissions are lighting fixtures  notably older fluorescent globes    These fixtures can be retrofitted or analyzed to see whether over-illumination is present  a common office environment issue  If over-illumination is occurring  de-lamping or reduced light bank usage may apply  Photographers can quieten noisy still cameras on a film set using sound blimps     Reductions in cost of technology have allowed noise control technology to be used not only in performance facilities and recording studios  but also in noise-sensitive small businesses such as restaurants  21   Acoustically absorbent materials such as fiberglass duct liner  wood fiber panels and recycled denim jeans serve as artwork-bearing canvasses in environments in which aesthetics are important  21     Using a combination of sound absorption materials  arrays of microphones and speakers  and a digital processor  a restaurant operator can use a tablet computer to selectively control noise levels at different places in the restaurant  the microphone arrays pick up sound and send it to the digital processor  which controls the speakers to output sound signals on command  21     Post-construction residential acoustic treatment throughout the 20th century was only commonly the practice of music-listening enthusiasts  However  developments in home recording technology and fidelity have led to a drastic increase in the spread and popularity of residential acoustic treatment in the pursuit of home recording fidelity and accuracy  A large secondary market of homemade and home use acoustic panels  bass trap  and similar constructed products has developed resulting from this demand  with many small companies and individuals wrapping industrial and commercial-grade insulations in fabric for use in home recording studios  theatre rooms  and music practice spaces     Communities may use zoning codes to isolate noisy urban activities from areas that should be protected from such unhealthy exposures and to establish noise standards in areas that may not be conducive to such isolation strategies  Because low-income neighborhoods are often at greater risk of noise pollution  the establishment of such zoning codes is often an environmental justice issue  22  Mixed-use areas present especially difficult conflicts that require special attention to the need to protect people from the harmful effects of noise pollution   Noise is generally one consideration in an environmental impact statement  if applicable  such as transportation system construction      General     A noise barrier  also called a soundwall  noise wall  sound berm  sound barrier  or acoustical barrier  is an exterior structure designed to protect inhabitants of sensitive land use areas from noise pollution  Noise barriers are the most effective method of mitigating roadway  railway  and industrial noise sources   nother than cessation of the source activity or use of source controls     In the case of surface transportation noise  other methods of reducing the source noise intensity include encouraging the use of hybrid and electric vehicles  improving automobile aerodynamics and tire design  and choosing low-noise paving material  Extensive use of noise barriers began in the United States after noise regulations were introduced in the early 1970s     Noise barriers have been built in the United States since the mid-twentieth century  when vehicular traffic burgeoned  I-680 in Milpitas  California was the first noise barrier  1  In the late 1960s  analytic acoustical technology emerged to mathematically evaluate the efficacy of a noise barrier design adjacent to a specific roadway  By the 1990s  noise barriers that included use of transparent materials were being designed in Denmark and other western European countries  2     The best of these early computer models considered the effects of roadway geometry  topography  vehicle volumes  vehicle speeds  truck mix  road surface type  and micro-meteorology  Several U S  research groups developed variations of the computer modeling techniques  Caltrans Headquarters in Sacramento  California  the ESL Inc  group in Sunnyvale  California  the Bolt  Beranek and Newman 3  group in Cambridge  Massachusetts  and a research team at the University of Florida  Possibly the earliest published work that scientifically designed a specific noise barrier was the study for the Foothill Expressway in Los Altos  California  4     Numerous case studies across the U S  soon addressed dozens of different existing and planned highways  Most were commissioned by state highway departments and conducted by one of the four research groups mentioned above  The U S  National Environmental Policy Act  enacted in 1970  effectively mandated the quantitative analysis of noise pollution from every Federal-Aid Highway Act Project in the country  propelling noise barrier model development and application  With passage of the Noise Control Act of 1972  5  demand for noise barrier design soared from a host of noise regulation spinoff     By the late 1970s  more than a dozen research groups in the U S  were applying similar computer modeling technology and addressing at least 200 different locations for noise barriers each year  As of 2006 update   this technology is considered a standard in the evaluation of noise pollution from highways  The nature and accuracy of the computer models used is nearly identical to the original 1970s versions of the technology     The acoustical science of noise barrier design is based upon treating an airway or railway as a line source  dubious   discuss  The theory is based upon blockage of sound ray travel toward a particular receptor  however  diffraction of sound must be addressed  Sound waves bend  downward  when they pass an edge  such as the apex of a noise barrier  Barriers that block line of sight of a highway or other source will therefore block more sound  6  Further complicating matters is the phenomenon of refraction  the bending of sound rays in the presence of an inhomogeneous atmosphere  Wind shear and thermocline produce such inhomogeneities  The sound sources modeled must include engine noise  tire noise  and aerodynamic noise  all of which vary by vehicle type and speed     The noise barrier may be constructed on private land  on a public right-of-way  or on other public land  Because sound levels are measured using a logarithmic scale  a reduction of nine decibels is equivalent to elimination of approximately 86 percent of the unwanted sound power     Several different materials may be used for sound barriers  These materials can include masonry  earthwork  such as earth berm   steel  concrete  wood  plastics  insulating wool  or composites  7  Walls that are made of absorptive material mitigate sound differently than hard surfaces  8  It is now when?  also possible to make noise barriers with active materials such as solar photovoltaic panels to generate electricity while also reducing traffic noise  9  10  11     A wall with porous surface material and sound-dampening content material can be absorptive where little or no noise is reflected back towards the source or elsewhere  Hard surfaces such as masonry or concrete are considered to be reflective where most of the noise is reflected back towards the noise source and beyond  12     Noise barriers can be effective tools for noise pollution abatement  but certain locations and topographies are not suitable for use of noise barriers  Cost and aesthetics also play a role in the choice of noise barriers  In some cases  a roadway is surrounded by a noise abatement structure or dug into a tunnel using the cut-and-cover method     Potential disadvantages of noise barriers include    \"Noise abatement walls often block rail passengers  or road users  view and attract graffiti  n\"  \"This noise abatement wall in the Netherlands has a transparent section at the driver s eye-level to reduce the visual impact for road users  n\"   Low walls close to the track avoid optical impact     Roadside noise barriers have been shown to reduce the near-road air pollution concentration levels  Within 1550 m from the roadside  air pollution concentration levels at the lee side of the noise barriers may be reduced by up to 50% compared to open road values  13     Noise barriers force the pollution plumes coming from the road to move up and over the barrier creating the effect of an elevated source and enhancing vertical dispersion of the plume  The deceleration and the deflection of the initial flow by the noise barrier force the plume to disperse horizontally  A highly turbulent shear zone characterized by slow velocities and a re-circulation cavity is created in the lee of the barrier which further enhances the dispersion  this mixes ambient air with the pollutants downwind behind the barrier  14         Logistics is generally the detailed organization and implementation of a complex operation  In a general business sense  logistics is the management of the flow of things between the point of origin and the point of consumption to meet the requirements of customers or corporations  The resources managed in logistics may include tangible goods such as materials  equipment  and supplies  as well as food and other consumable items      In military science  logistics is concerned with maintaining army supply lines while disrupting those of the enemy  since an armed force without resources and transportation is defenseless  Military logistics was already practiced in the ancient world and as the modern military has a significant need for logistics solutions  advanced implementations have been developed  In military logistics  logistics officers manage how and when to move resources to the places they are needed     Logistics management is the part of supply chain management and supply chain engineering that plans  implements  and controls the efficient  effective forward  and reverse flow and storage of goods  services  and related information between the point of origin and point of consumption to meet customer  s requirements  The complexity of logistics can be modeled  analyzed  visualized  and optimized by dedicated simulation software  The minimization of the use of resources is a common motivation in all logistics fields  A professional working in the field of logistics management is called a logistician      nThe term logistics is attested in English from 1846  and is from French  logistique  where it was either coined or popularized by military officer and writer Antoine-Henri Jomini  who defined it in his Summary of the Art of War  Prcis de l  Art de la Guerre   The term appears in the 1830 edition  then titled Analytic Table  Tableau Analytique   1  and Jomini explains that it is derived from French  logis  lit  u2009  lodgings    cognate to English lodge   in the terms French  marchal des logis  lit  u2009  marshall of lodgings   and French  major-gnral des logis  lit  u2009  major-general of lodging       Autrefois les officiers de ltat-major se nommaient  marchal des logis  major-gnral des logis  de l est venu le terme de logistique  quon emploie pour dsigner ce qui se rapporte aux marches dune arme          nFormerly the officers of the general staff were named  marshall of lodgings  major-general of lodgings  from there came the term of logistics  logistique   which we employ to designate those who are in charge of the functioning of an army     The term is credited to Jomini  and the term and its etymology criticized by Georges de Chambray in 1832  writing  2     Logistique  Ce mot me parat tre tout--fait nouveau  car je ne l  avais encore vu nulle part dans la littrature militaire   il parat le faire driver du mot logis  tymologie singulire         Logistic  This word appears to me to be completely new  as I have not yet seen it anywhere in military literature   he appears to derive it from the word lodgings  logis   a peculiar etymology     Chambray also notes that the term logistique was present in the Dictionnaire de l  Acadmie franaise as a synonym for algebra     The French word  logistique is a homonym of the existing mathematical term  from Ancient Greek    romanized  xa0logistiks  a traditional division of Greek mathematics  the mathematical term is presumably the origin of the term logistic in logistic growth and related terms  Some sources give this instead as the source of logistics  3  either ignorant of Jomini  s statement that it was derived from logis  or dubious and instead believing it was in fact of Greek origin  or influenced by the existing term of Greek origin     Jomini originally defined logistics as  1    \"    l art de bien ordonner les marches d une arme  de bien combiner l ordre des troupes dans les colonnes  les tems  temps  de leur dpart  leur itinraire  les moyens de communications ncessaires pour assurer leur arrive  point nomm     n\"       the art of well-ordering the functionings of an army  of well combining the order of troops in columns  the times of their departure  their itinerary  the means of communication necessary to assure their arrival at a named point           The Oxford English Dictionary defines logistics as \"the branch of military science relating to procuring  maintaining and transporting material  personnel and facilities\"  However  the New Oxford American Dictionary defines logistics as \"the detailed coordination of a complex operation involving many people  facilities  or supplies\"  and the Oxford Dictionary on-line defines it as \"the detailed organization and implementation of a complex operation\"  4  As such  logistics is commonly seen as a branch of engineering that creates \"people systems\" rather than \"machine systems\"     According to the Council of Supply Chain Management Professionals  previously the Council of Logistics Management   5  logistics is the process of planning  implementing and controlling procedures for the efficient and effective ntransportation and storage of goods including services and related information from the point of origin to the point of consumption for the purpose of conforming to customer requirements and includes inbound  outbound  internal and external movements  6     Academics and practitioners traditionally refer to the terms operations or production management when referring to physical transformations taking place in a single business location  factory  restaurant or even bank clerking  and reserve the term logistics for activities related to distribution  that is  moving products on the territory  Managing a distribution center is seen  therefore  as pertaining to the realm of logistics since  while in theory  the products made by a factory are ready for consumption they still need to be moved along the distribution network according to some logic  and the distribution center aggregates and processes orders coming from different areas of the territory  That being said  from a modeling perspective  there are similarities between operations management and logistics  and companies sometimes use hybrid professionals  with for example a \"Director of Operations\" or a \"Logistics Officer\" working on similar problems  Furthermore  the term supply chain management originally refers to  among other issues  having an integrated vision of both production and logistics from point of origin to point of production  7  All these terms may suffer from semantic change as a side effect of advertising     Inbound logistics is one of the primary processes of logistics concentrating on purchasing and arranging the inbound movement of materials  parts  or unfinished inventory from suppliers to manufacturing or assembly plants  warehouses  or retail stores     Outbound logistics is the process related to the storage and movement of the final product and the related information flows from the end of the production line to the end-user     Given the services performed by logisticians  the main fields of logistics can be broken down as follows     Procurement logistics consists of activities such as market research  requirements planning  make-or-buy decisions  supplier management  ordering  and order controlling  The targets in procurement logistics might be contradictory  maximizing efficiency by concentrating on core competences  outsourcing while maintaining the autonomy of the company  or minimizing procurement costs while maximizing security within the supply process     Advance Logistics consists of the activities required to set up or establish a plan for logistics activities to occur     Global Logistics 8  is technically the process of managing the   flow   of goods through what is called a supply chain  from its place of production  to other parts of the world  This often requires an intermodal transport system  transport via ocean  air  rail  and truck  The effectiveness of global logistics is measured in the Logistics Performance Index     Distribution logistics has  as main tasks  the delivery of the finished products to the customer  It consists of order processing  warehousing  and transportation  Distribution logistics is necessary because the time  place  and quantity of production differ with the time  place  and quantity of consumption     Disposal logistics has as its main function to reduce logistics cost s  and enhance service s  related to the disposal of waste produced during the operation of a business     Reverse logistics denotes all those operations related to the reuse of products and materials  The reverse logistics process includes the management and the sale of surpluses  as well as products being returned to vendors from buyers  Reverse logistics stands for all operations related to the reuse of products and materials  It is \"the process of planning  implementing  and controlling the efficient  cost-effective flow of raw materials  in-process inventory  finished goods and related information from the point of consumption to the point of origin for the purpose of recapturing value or proper disposal  More precisely  reverse logistics is the process of moving goods from their typical final destination for the purpose of capturing value  or proper disposal  The opposite of reverse logistics is forward logistics \"    Green Logistics describes all attempts to measure and minimize the ecological impact of logistics activities  This includes all activities of the forward and reverse flows  This can be achieved through intermodal freight transport  path optimization  vehicle saturation and city logistics     RAM Logistics  see also Logistic engineering  combines both business logistics and military logistics since it is concerned with highly complicated technological systems for which Reliability  Availability and Maintainability are essential  ex  weapon systems and military supercomputers     Asset Control Logistics  companies in the retail channels  both organized retailers and suppliers  often deploy assets required for the display  preservation  promotion of their products  Some examples are refrigerators  stands  display monitors  seasonal equipment  poster stands & frames     Emergency logistics  or Humanitarian Logistics  is a term used by the logistics  supply chain  and manufacturing industries to denote specific time-critical modes of transport used to move goods or objects rapidly in the event of an emergency  9  The reason for enlisting emergency logistics services could be a production delay or anticipated production delay  or an urgent need for specialized equipment to prevent events such as aircraft being grounded  also known as \"aircraft on ground\"AOG   ships being delayed  or telecommunications failure  Humanitarian logistics involves governments  the military  aid agencies  donors  non-governmental organizations and emergency logistics services are typically sourced from a specialist provider  9  10     The term production logistics describes logistic processes within a value-adding system  ex  factory or a mine   Production logistics aims to ensure that each machine and workstation receives the right product in the right quantity and quality at the right time  The concern is with production  testing  transportation  storage  and supply  Production logistics can operate in existing as well as new plants  since manufacturing in an existing plant is a constantly changing process  machines are exchanged and new ones added  which gives the opportunity to improve the production logistics system accordingly  11  Production logistics provides the means to achieve customer response and capital efficiency  Production logistics becomes more important with decreasing batch sizes  In many industries  e g  mobile phones   the short-term goal is a batch size of one  allowing even a single customer  s demand to be fulfilled efficiently  Track and tracing  which is an essential part of production logistics due to product safety and reliability issues  is also gaining importance  especially in the automotive and medical industries     Construction Logistics has been employed by civilizations for thousands of years  As the various human civilizations tried to build the best possible works of construction for living and protection  Now construction logistics has emerged as a vital part of construction  In the past few years  construction logistics has emerged as a different field of knowledge and study within the subject of supply chain management and logistics     In military science  maintaining one  s supply lines while disrupting those of the enemy is a crucialsome would say the most crucialelement of military strategy  since an armed force without resources and transportation is defenseless  The historical leaders Hannibal  Alexander the Great  and the Duke of Wellington are considered to have been logistical geniuses  Alexander  s expedition benefited considerably from his meticulous attention to the provisioning of his army  13  Hannibal is credited to have \"taught logistics\" to the Romans during the Punic Wars 14  and the success of the Anglo-Portuguese army in the Peninsula War was due to the effectiveness of Wellington  s supply system  despite the numerical disadvantage  15  The defeat of the British in the American War of Independence and the defeat of the Axis in the African theater of World War II are attributed by some scholars to logistical failures  16     Militaries have a significant need for logistics solutions and so have developed advanced implementations  Integrated Logistics Support  ILS  is a discipline used in military industries to ensure an easily supportable system with a robust customer service  logistic  concept at the lowest cost and in line with  often high  reliability  availability  maintainability  and other requirements  as defined for the project     In military logistics  Logistics Officers manage how and when to move resources to the places they are needed     Supply chain management in military logistics often deals with a number of variables in predicting cost  deterioration  consumption  and future demand  The United States Armed Forces   categorical supply classification was developed in such a way that categories of supply with similar consumption variables are grouped together for planning purposes  For instance  peacetime consumption of ammunition and fuel will be considerably lower than wartime consumption of these items  whereas other classes of supply such as subsistence and clothing have a relatively consistent consumption rate regardless of war or peace     Some classes of supply have a linear demand relationship  as more troops are added  more supply items are needed  or as more equipment is used  more fuel and ammunition are consumed  Other classes of supply must consider a third variable besides usage and quantity  time  As equipment ages  more and more repair parts are needed over time  even when usage and quantity stay consistent  By recording and analyzing these trends over time and applying them to future scenarios  the US Armed Forces can accurately supply troops with the items necessary at the precise moment they are needed  17  History has shown that good logistical planning creates a lean and efficient fighting force  The lack thereof can lead to a clunky  slow  and ill-equipped force with too much or too little supply     One definition of business logistics speaks of \"having the right item in the right quantity at the right time at the right place for the right price in the right condition to the right customer\"  18  Business logistics incorporates all industry sectors and aims to manage the fruition of project life cycles  supply chains  and resultant efficiencies     The term \"business logistics\" has evolved since the 1960s 19  due to the increasing complexity of supplying businesses with materials and shipping out products in an increasingly globalized supply chain  leading to a call for professionals called \"supply chain logisticians\"     In business  logistics may have either an internal focus  inbound logistics  or an external focus  outbound logistics   covering the flow and storage of materials from point of origin to point of consumption  see supply-chain management   The main functions of a qualified logistician include inventory management  purchasing  transportation  warehousing  consultation  and the organizing and planning of these activities  Logisticians combine professional knowledge of each of these functions to coordinate resources in an organization     There are two fundamentally different forms of logistics  one optimizes a steady flow of material through a network of transport links and storage nodes  while the other coordinates a sequence of resources to carry out some project  e g   restructuring a warehouse      The nodes of a distribution network include     There may be some intermediaries operating for representative matters between nodes such as sales agents or brokers     A logistic family is a set of products that share a common characteristic  weight and volumetric characteristics  physical storing needs  temperature  radiation       handling needs  order frequency  package size  etc  The following metrics may be used by the company to organize its products in different families  20     Other metrics may present themselves in both physical or monetary form  such as the standard Inventory turnover     Unit loads are combinations of individual items which are moved by handling systems  usually employing a pallet of normed dimensions  21     Handling systems include  trans-pallet handlers  counterweight handler  retractable mast handler  bilateral handlers  trilateral handlers  AGV and other handlers     Storage systems include  pile stocking  cell racks  either static or movable   cantilever racks and gravity racks  22     Order processing is a sequential process involving  processing withdrawal list  picking  selective removal of items from loading units   sorting  assembling items based on the destination   package formation  weighting  labeling  and packing   order consolidation  gathering packages into loading units for transportation  control and bill of lading   23     Picking can be both manual or automated  Manual picking can be both man to goods  i e  operator using a cart or conveyor belt  or goods to man  i e  the operator benefiting from the presence of a mini-load ASRS  vertical or horizontal carousel or from an Automatic Vertical Storage System  AVSS   Automatic picking is done either with dispensers or depalletizing robots     Sorting can be done manually through carts or conveyor belts  or automatically through sorters     Cargo  i e  merchandise being transported  can be moved through a variety of transportation means and is organized in different shipment categories  Unit loads are usually assembled into higher standardized units such as  ISO containers  swap bodies or semi-trailers  Especially for very long distances  product transportation will likely benefit from using different transportation means  multimodal transport  intermodal transport  no handling  and combined transport  minimal road transport   When moving cargo  typical constraints are maximum weight and volume     Operators involved in transportation include  all train  road vehicles  boats  airplanes companies  couriers  freight forwarders and multi-modal transport operators     Merchandise being transported internationally is usually subject to the Incoterms standards issued by the International Chamber of Commerce     Similarly to production systems  logistic systems need to be properly configured and managed  Actually a number of methodologies have been directly borrowed from operations management such as using Economic Order Quantity models for managing inventory in the nodes of the network  24  Distribution resource planning  DRP  is similar to MRP  except that it doesn  t concern activities inside the nodes of the network but planning distribution when moving goods through the links of the network     Traditionally in logistics configuration may be at the level of the warehouse  node  or at level of the distribution system  network      Regarding a single warehouse  besides the issue of designing and building the warehouse  configuration means solving a number of interrelated technical-economic problems  dimensioning rack cells  choosing a palletizing method  manual or through robots   rack dimensioning and design  number of racks  number and typology of retrieval systems  e g  stacker cranes   Some important constraints have to be satisfied  fork and load beams resistance to bending and proper placement of sprinklers  Although picking is more of a tactical planning decision than a configuration problem  it is important to take it into account when deciding the layout of the racks inside the warehouse and buying tools such as handlers and motorized carts since once those decisions are taken they will work as constraints when managing the warehouse  the same reasoning for sorting when designing the conveyor system or installing automatic dispensers     Configuration at the level of the distribution system concerns primarily the problem of location of the nodes in geographic space and distribution of capacity among the nodes  The first may be referred to as facility location  with the special case of site selection  while the latter to as capacity allocation  The problem of outsourcing typically arises at this level  the nodes of a supply chain are very rarely owned by a single enterprise  Distribution networks can be characterized by numbers of levels  namely the number of intermediary nodes between supplier and consumer     This distinction is more useful for modeling purposes  but it relates also to a tactical decision regarding safety stocks  considering a two-level network  if safety inventory is kept only in peripheral warehouses then it is called a dependent system  from suppliers   if safety inventory is distributed among central and peripheral warehouses it is called an independent system  from suppliers   20  Transportation from producer to the second level is called primary transportation  from the second level to a consumer is called secondary transportation     Although configuring a distribution network from zero is possible  logisticians usually have to deal with restructuring existing networks due to presence of an array of factors  changing demand  product or process innovation  opportunities for outsourcing  change of government policy toward trade barriers  innovation in transportation means  both vehicles or thoroughfares   the introduction of regulations  notably those regarding pollution  and availability of ICT supporting systems  e g  ERP or e-commerce      Once a logistic system is configured  management  meaning tactical decisions  takes place  once again  at the level of the warehouse and of the distribution network  Decisions have to be made under a set of constraints  internal  such as using the available infrastructure  or external  such as complying with the given product shelf lifes and expiration dates     At the warehouse level  the logistician must decide how to distribute merchandise over the racks  Three basic situations are traditionally considered  shared storage  dedicated storage  rack space reserved for specific merchandise  and class-based storage  class meaning merchandise organized in different areas according to their access index      Picking efficiency varies greatly depending on the situation  23  For a man to goods situation  a distinction is carried out between high-level picking  vertical component significant  and low-level picking  vertical component insignificant   A number of tactical decisions regarding picking must be made     At the level of the distribution network  tactical decisions involve mainly inventory control and delivery path optimization  Note that the logistician may be required to manage the reverse flow along with the forward flow     Although there is some overlap in functionality  warehouse management systems  WMS  can differ significantly from warehouse control systems  WCS   Simply put  a WMS plans a weekly activity forecast based on such factors as statistics and trends  whereas a WCS acts like a floor supervisor  working in real-time to get the job done by the most effective means  For instance  a WMS can tell the system that it is going to need five of stock-keeping unit  SKU  A and five of SKU B hours in advance  but by the time it acts  other considerations may have come into play or there could be a logjam on a conveyor  A WCS can prevent that problem by working in real-time and adapting to the situation by making a last-minute decision based on current activity and operational status  Working synergistically  WMS and WCS can resolve these issues and maximize efficiency for companies that rely on the effective operation of their warehouse or distribution center  25     Logistics outsourcing involves a relationship between a company and an LSP  logistic service provider   which  compared with basic logistics services  has more customized offerings  encompasses a broad number of service activities  is characterized by a long-term orientation  and thus has a strategic nature  26     Outsourcing does not have to be complete externalization to an LSP  but can also be partial     Third-party logistics  3PL  involves using external organizations to execute logistics activities that have traditionally been performed within an organization itself  27  According to this definition  third-party logistics includes any form of outsourcing of logistics activities previously performed in house  For example  if a company with its own warehousing facilities decides to employ external transportation  this would be an example of third-party logistics  Logistics is an emerging business area in many countries     The concept of a fourth-party logistics  4PL  provider was first defined by Andersen Consulting  now Accenture  as an integrator that assembles the resources  planning capabilities  and technology of its own organization and other organizations to design  build  and run comprehensive supply chain solutions  Whereas a third-party logistics  3PL  service provider targets a single function  a 4PL targets management of the entire process  Some have described a 4PL as a general contractor that manages other 3PLs  truckers  forwarders  custom house agents  and others  essentially taking responsibility of a complete process for the customer     Horizontal business alliances often occur between logistics service providers  i e   the cooperation between two or more logistics companies that are potentially competing  28  In a horizontal alliance  these partners can benefit twofold  On one hand  they can \" resources which are directly exploitable\"  In this example extending common transportation networks  their warehouse infrastructure and the ability to provide more complex service packages can be achieved by combining resources  On the other hand  partners can \"access intangible resources  which are not directly exploitable\"  This typically includes know-how and information and  in turn  innovation  28     Logistics automation is the application of computer software or automated machinery to improve the efficiency of logistics operations  Typically this refers to operations within a warehouse or distribution center with broader tasks undertaken by supply chain engineering systems and enterprise resource planning systems     Industrial machinery can typically identify products through either barcode or RFID technologies  Information in traditional bar codes is stored as a sequence of black and white bars varying in width  which when read by laser is translated into a digital sequence  which according to fixed rules can be converted into a decimal number or other data  Sometimes information in a bar code can be transmitted through radio frequency  more typically radio transmission is used in RFID tags  An RFID tag is a card containing a memory chip and an antenna that transmits signals to a reader  RFID may be found on merchandise  animals  vehicles  and people as well     A logistician is a professional logistics practitioner  Professional logisticians are often certified by professional associations  One can either work in a pure logistics company  such as a shipping line  airport  or freight forwarder  or within the logistics department of a company  However  as mentioned above  logistics is a broad field  encompassing procurement  production  distribution  and disposal activities  Hence  career perspectives are broad as well  nA new trend in the industry is the 4PL  or fourth-party logistics  firms  consulting companies offering logistics services     Some universities and academic institutions train students as logisticians  offering undergraduate and postgraduate programs  A university with a primary focus on logistics is Khne Logistics University in Hamburg  Germany  It is non-profit and supported by Khne-Foundation of the logistics entrepreneur Klaus Michael Khne     The Chartered Institute of Logistics and Transport  CILT   established in the United Kingdom in 1919  received a Royal Charter in 1926  The Chartered Institute is one of the professional bodies or institutions for the logistics and transport sectors that offer professional qualifications or degrees in logistics management  CILT programs can be studied at centers around the UK  some of which also offer distance learning options  29  The institute also have overseas branches namely The Chartered Institute of Logistics & Transport Australia  CILTA  30  in Australia and Chartered Institute of Logistics and Transport in Hong Kong  CILTHK  31  in Hong Kong  In the UK  Logistics Management programs are conducted by many universities and professional bodies such as CILT  These programs are generally offered at the postgraduate level     The Global Institute of Logistics 32  established in New York in 2003 is a Think tank for the profession and is primarily concerned with intercontinental maritime logistics  It is particularly concerned with container logistics and the role of the seaport authority in the maritime logistics chain     The International Association of Public Health Logisticians  IAPHL  33  is a professional network that promotes the professional development of supply chain managers and others working in the field of public health logistics and commodity security  with particular focus on developing countries  The association supports logisticians worldwide by providing a community of practice  where members can network  exchange ideas  and improve their professional skills     There are many museums in the world which cover various aspects of practical logistics  These include museums of transportation  customs  packing  and industry-based logistics  However  only the following museums are fully dedicated to logistics     General logistic    Military logistics    Risk management is the identification  evaluation  and prioritization of risks  defined in ISO 31000 as the effect of uncertainty on objectives  followed by coordinated and economical application of resources to minimize  monitor  and control the probability or impact of unfortunate events 1  or to maximize the realization of opportunities     Risks can come from various sources including uncertainty in international markets  threats from project failures  at any phase in design  development  production  or sustaining of life-cycles   legal liabilities  credit risk  accidents  natural causes and disasters  deliberate attack from an adversary  or events of uncertain or unpredictable root-cause  There are two types of events i e  negative events can be classified as risks while positive events are classified as opportunities   Risk management standards have been developed by various institutions  including the Project Management Institute  the National Institute of Standards and Technology  actuarial societies  and ISO standards  2  3  Methods  definitions and goals vary widely according to whether the risk management method is in the context of project management  security  engineering  industrial processes  financial portfolios  actuarial assessments  or public health and safety     Strategies to manage threats  uncertainties with negative consequences  typically include avoiding the threat  reducing the negative effect or probability of the threat  transferring all or part of the threat to another party  and even retaining some or all of the potential or actual consequences of a particular threat  The opposite of these strategies can be used to respond to opportunities  uncertain future states with benefits      Certain risk management standards have been criticized for having no measurable improvement on risk  whereas the confidence in estimates and decisions seems to increase  1     A widely used vocabulary for risk management is defined by ISO Guide 73 2009  \"Risk management  Vocabulary \" 2     In ideal risk management  a prioritization process is followed whereby the risks with the greatest loss  or impact  and the greatest probability of occurring are handled first  Risks with lower probability of occurrence and lower loss are handled in descending order  In practice the process of assessing overall risk can be difficult  and balancing resources used to mitigate between risks with a high probability of occurrence but lower loss  versus a risk with high loss but lower probability of occurrence can often be mishandled     Intangible risk management identifies a new type of a risk that has a 100% probability of occurring but is ignored by the organization due to a lack of identification ability  For example  when deficient knowledge is applied to a situation  a knowledge risk materializes  Relationship risk appears when ineffective collaboration occurs  Process-engagement risk may be an issue when ineffective operational procedures are applied  These risks directly reduce the productivity of knowledge workers  decrease cost-effectiveness  profitability  service  quality  reputation  brand value  and earnings quality  Intangible risk management allows risk management to create immediate value from the identification and reduction of risks that reduce productivity     Opportunity cost represents a unique challenge for risk managers  It can be difficult to determine when to put resources toward risk management and when to use those resources elsewhere  Again  ideal risk management minimizes spending  or manpower or other resources  and also minimizes the negative effects of risks     Risk is defined as the possibility that an event will occur that adversely affects the achievement of an objective  Uncertainty  therefore  is a key aspect of risk   Systems like the Committee of Sponsoring Organizations of the Treadway Commission Enterprise Risk Management  COSO ERM   can assist managers in mitigating risk factors   Each company may have different internal control components  which leads to different outcomes  For example  the framework for ERM components includes Internal Environment  Objective Setting  Event Identification  Risk Assessment  Risk Response  Control Activities  Information and Communication  and Monitoring     For the most part  these methods consist of the following elements  performed  more or less  in the following order     The International Organization for Standardization  ISO  identifies the following principles of risk management  4     Risk management should     Benoit Mandelbrot distinguished between \"mild\" and \"wild\" risk and argued that risk assessment and management must be fundamentally different for the two types of risk  5  Mild risk follows normal or near-normal probability distributions  is subject to regression to the mean and the law of large numbers  and is therefore relatively predictable  Wild risk follows fat-tailed distributions  e g   Pareto or power-law distributions  is subject to regression to the tail  infinite mean or variance  rendering the law of large numbers invalid or ineffective   and is therefore difficult or impossible to predict  A common error in risk assessment and management is to underestimate the wildness of risk  assuming risk to be mild when in fact it is wild  which must be avoided if risk assessment and management are to be valid and reliable  according to Mandelbrot     According to the standard ISO 31000 \"Risk management  Principles and guidelines on implementation \" 3  the process of risk management consists of several steps as follows     This involves    \"After establishing the context  the next step in the process of managing risk is to identify potential risks   Risks are about events that  when triggered  cause problems or benefits  Hence  risk identification can start with the source of problems and those of competitors  benefit   or with the problem s consequences  n\"   Some examples of risk sources are  stakeholders of a project  employees of a company or the weather over an airport     When either source or problem is known  the events that a source may trigger or the events that can lead to a problem can be investigated  For example  stakeholders withdrawing during a project may endanger funding of the project  confidential information may be stolen by employees even within a closed network  lightning striking an aircraft during takeoff may make all people on board immediate casualties     The chosen method of identifying risks may depend on culture  industry practice and compliance  The identification methods are formed by templates or the development of templates for identifying source  problem or event  Common risk identification methods are     Once risks have been identified  they must then be assessed as to their potential severity of impact  generally a negative impact  such as damage or loss  and to the probability of occurrence  These quantities can be either simple to measure  in the case of the value of a lost building  or impossible to know for sure in the case of an unlikely event  the probability of occurrence of which is unknown  Therefore  in the assessment process it is critical to make the best educated decisions in order to properly prioritize the implementation of the risk management plan     Even a short-term positive improvement can have long-term negative impacts   Take the \"turnpike\" example  A highway is widened to allow more traffic  More traffic capacity leads to greater development in the areas surrounding the improved traffic capacity  Over time  traffic thereby increases to fill available capacity  Turnpikes thereby need to be expanded in a seemingly endless cycles  There are many other engineering examples where expanded capacity  to do any function  is soon filled by increased demand  Since expansion comes at a cost  the resulting growth could become unsustainable without forecasting and management     The fundamental difficulty in risk assessment is determining the rate of occurrence since statistical information is not available on all kinds of past incidents and is particularly scanty in the case of catastrophic events  simply because of their infrequency  Furthermore  evaluating the severity of the consequences  impact  is often quite difficult for intangible assets  Asset valuation is another question that needs to be addressed  Thus  best educated opinions and available statistics are the primary sources of information  Nevertheless  risk assessment should produce such information for senior executives of the organization that the primary risks are easy to understand and that the risk management decisions may be prioritized within overall company goals  Thus  there have been several theories and attempts to quantify risks  Numerous different risk formulae exist  but perhaps the most widely accepted formula for risk quantification is  \"Rate  or probability  of occurrence multiplied by the impact of the event equals risk magnitude \" vague     Risk mitigation measures are usually formulated according to one or more of the following major risk options  which are     Later research 11  has shown that the financial benefits of risk management are less dependent on the formula used but are more dependent on the frequency and how risk assessment is performed     In business it is imperative to be able to present the findings of risk assessments in financial  market  or schedule terms  Robert Courtney Jr   IBM  1970  proposed a formula for presenting risks in financial terms  The Courtney formula was accepted as the official risk analysis method for the US governmental agencies  The formula proposes calculation of ALE  annualized loss expectancy  and compares the expected loss value to the security control implementation costs  cost-benefit analysis      Once risks have been identified and assessed  all techniques to manage the risk fall into one or more of these four major categories  12     Ideal use of these risk control strategies may not be possible   Some of them may involve trade-offs that are not acceptable to the organization or person making the risk management decisions  Another source  from the US Department of Defense  see link   Defense Acquisition University  calls these categories ACAT  for Avoid  Control  Accept  or Transfer   This use of the ACAT acronym is reminiscent of another ACAT  for Acquisition Category  used in US Defense industry procurements  in which Risk Management figures prominently in decision making and planning     This includes not performing an activity that could present risk  Refusing to purchase a property or business to avoid legal liability is one such example   Avoiding airplane flights for fear of hijacking   Avoidance may seem like the answer to all risks  but avoiding risks also means losing out on the potential gain that accepting  retaining  the risk may have allowed   Not entering a business to avoid the risk of loss also avoids the possibility of earning profits   Increasing risk regulation in hospitals has led to avoidance of treating higher risk conditions  in favor of patients presenting with lower risk  13     Risk reduction or \"optimization\" involves reducing the severity of the loss or the likelihood of the loss from occurring   For example  sprinklers are designed to put out a fire to reduce the risk of loss by fire   This method may cause a greater loss by water damage and therefore may not be suitable  Halon fire suppression systems may mitigate that risk  but the cost may be prohibitive as a strategy     Acknowledging that risks can be positive or negative  optimizing risks means finding a balance between negative risk and the benefit of the operation or activity  and between risk reduction and effort applied  By effectively applying Health  Safety and Environment  HSE  management standards  organizations can achieve tolerable levels of residual risk  14     Modern software development methodologies reduce risk by developing and delivering software incrementally   Early methodologies suffered from the fact that they only delivered software in the final phase of development  any problems encountered in earlier phases meant costly rework and often jeopardized the whole project   By developing in iterations  software projects can limit effort wasted to a single iteration     Outsourcing could be an example of risk sharing strategy if the outsourcer can demonstrate higher capability at managing or reducing risks  15  For example  a company may outsource only its software development  the manufacturing of hard goods  or customer support needs to another company  while handling the business management itself   This way  the company can concentrate more on business development without having to worry as much about the manufacturing process  managing the development team  or finding a physical location for a center     Briefly defined as \"sharing with another party the burden of loss or the benefit of gain  from a risk  and the measures to reduce a risk \"    The term of   risk transfer   is often used in place of risk sharing in the mistaken belief that you can transfer a risk to a third party through insurance or outsourcing  In practice if the insurance company or contractor go bankrupt or end up in court  the original risk is likely to still revert to the first party  As such  in the terminology of practitioners and scholars alike  the purchase of an insurance contract is often described as a \"transfer of risk \" However  technically speaking  the buyer of the contract generally retains legal responsibility for the losses \"transferred\"  meaning that insurance may be described more accurately as a post-event compensatory mechanism  For example  a personal injuries insurance policy does not transfer the risk of a car accident to the insurance company  The risk still lies with the policy holder namely the person who has been in the accident  The insurance policy simply provides that if an accident  the event  occurs involving the policy holder then some compensation may be payable to the policy holder that is commensurate with the suffering/damage     Methods of managing risk fall into multiple categories  Risk retention pools are technically retaining the risk for the group  but spreading it over the whole group involves transfer among individual members of the group  This is different from traditional insurance  in that no premium is exchanged between members of the group up front  but instead losses are assessed to all members of the group     Risk retention involves accepting the loss  or benefit of gain  from a risk when the incident occurs   True self-insurance falls in this category   Risk retention is a viable strategy for small risks where the cost of insuring against the risk would be greater over time than the total losses sustained  All risks that are not avoided or transferred are retained by default   This includes risks that are so large or catastrophic that either they cannot be insured against or the premiums would be infeasible  War is an example since most property and risks are not insured against war  so the loss attributed to war is retained by the insured   Also any amounts of potential loss  risk  over the amount insured is retained risk   This may also be acceptable if the chance of a very large loss is small or if the cost to insure for greater coverage amounts is so great that it would hinder the goals of the organization too much     Select appropriate controls or countermeasures to mitigate each risk  Risk mitigation needs to be approved by the appropriate level of management  For instance  a risk concerning the image of the organization should have top management decision behind it whereas IT management would have the authority to decide on computer virus risks     The risk management plan should propose applicable and effective security controls for managing the risks  For example  an observed high risk of computer viruses could be mitigated by acquiring and implementing antivirus software  A good risk management plan should contain a schedule for control implementation and responsible persons for those actions     According to ISO/IEC 27001  the stage immediately after completion of the risk assessment phase consists of preparing a Risk Treatment Plan  which should document the decisions about how each of the identified risks should be handled   Mitigation of risks often means selection of security controls  which should be documented in a Statement of Applicability  which identifies which particular control objectives and controls from the  nstandard have been selected  and why    \"Implementation follows all of the planned methods for mitigating the effect of the risks   Purchase insurance policies for the risks that it has been decided to transferred to an insurer  avoid all risks that can be avoided without sacrificing the entity s goals  reduce others  and retain the rest  n\"   Initial risk management plans will never be perfect  Practice  experience  and actual loss results will necessitate changes in the plan and contribute information to allow possible different decisions to be made in dealing with the risks being faced     Risk analysis results and management plans should be updated periodically  There are two primary reasons for this      Prioritizing the risk management processes too highly could keep an organization from ever completing a project or even getting started   This is especially true if other work is suspended until the risk management process is considered complete     It is also important to keep in mind the distinction between risk and uncertainty  Risk can be measured by impacts  probability     If risks are improperly assessed and prioritized  time can be wasted in dealing with risk of losses that are not likely to occur   Spending too much time assessing and managing unlikely risks is to be avoided   Unlikely events do occur but if the risk is unlikely enough to occur it may be better to simply retain the risk and deal with the result if the loss does in fact occur  Qualitative risk assessment is subjective and lacks consistency  The primary justification for a formal risk assessment process is legal and bureaucratic     As applied to corporate finance  risk management is the technique for measuring  monitoring and controlling the financial or operational risk on a firm  s balance sheet  a traditional measure is the value at risk  VaR   but there also other measures like profit at risk  PaR  or margin at risk  The Basel II framework breaks risks into market risk  price risk   credit risk and operational risk and also specifies methods for calculating capital requirements for each of these components     In Information Technology  risk management includes \"Incident Handling\"  an action plan for dealing with intrusions  cyber-theft  denial of service  fire  floods  and other security-related events  According to the SANS Institute  16  it is a six step process  Preparation  Identification  Containment  Eradication  Recovery  and Lessons Learned     The concept of \"contractual risk management\" emphasises the use of risk management techniques in contract deployment  i e  managing the risks which are accepted through entry into a contract  Norwegian academic Petri Keskitalo defines \"contractual risk management\" as \"a practical  proactive and systematical contracting method that uses contract planning and governance to manage risks connected to business activities\"  17  In an article by Samuel Greengard published in 2010  two US legal cases are mentioned which emphasise the importance of having a strategy for dealing with risk  18     Greengard recommends using industry-standard contract language as much as possible to reduce risk as much as possible and rely on clauses which have been in use and subject to established court interpretation over a number of years  18     In enterprise risk management  a risk is defined as a possible event or circumstance that can have negative influences on the enterprise in question   Its impact can be on the very existence  the resources  human and capital   the products and services  or the customers of the enterprise  as well as external impacts on society  markets  or the environment   In a financial institution  enterprise risk management is normally thought of as the combination of credit risk  interest rate risk or asset liability management  liquidity risk  market risk  and operational risk     In the more general case  every probable risk can have a pre-formulated plan to deal with its possible consequences  to ensure contingency if the risk becomes a liability      From the information above and the average cost per employee over time  or cost accrual ratio  a project manager can estimate     Risk in a project or process can be due either to Special Cause Variation or Common Cause Variation and requires appropriate treatment   That is to re-iterate the concern about extremal cases not being equivalent in the list immediately above     ESRM is a security program management approach that links security activities to an enterprise  s mission and business goals through risk management methods  The security leader  s role in ESRM is to manage risks of harm to enterprise assets in partnership with the business leaders whose assets are exposed to those risks  ESRM involves educating business leaders on the realistic impacts of identified risks  presenting potential strategies to mitigate those impacts  then enacting the option chosen by the business in line with accepted levels of business risk tolerance 21     For medical devices  risk management is a process for identifying  evaluating and mitigating risks associated with harm to people and damage to property or the environment  Risk management is an integral part of medical device design and development  production processes and evaluation of field experience  and is applicable to all types of medical devices  The evidence of its application is required by most regulatory bodies such as the US FDA  The management of risks for medical devices is described by the International Organization for Standardization  ISO  in ISO 14971 2019  Medical DevicesThe application of risk management to medical devices  a product safety standard  The standard provides a process framework and associated requirements for management responsibilities  risk analysis and evaluation  risk controls and lifecycle risk management  Guidance on the application of the standard is available via ISO/TR 24971 2020     The European version of the risk management standard was updated in 2009 and again in 2012 to refer to the Medical Devices Directive  MDD  and Active Implantable Medical Device Directive  AIMDD  revision in 2007  as well as the In Vitro Medical Device Directive  IVDD   The requirements of EN 14971 2012 are nearly identical to ISO 14971 2007  The differences include three \" informative \" Z Annexes that refer to the new MDD  AIMDD  and IVDD   These annexes indicate content deviations that include the requirement for risks to be reduced as far as possible  and the requirement that risks be mitigated by design and not by labeling on the medical device  i e   labeling can no longer be used to mitigate risk      Typical risk analysis and evaluation techniques adopted by the medical device industry include hazard analysis  fault tree analysis  FTA   failure mode and effects analysis  FMEA   hazard and operability study  HAZOP   and risk traceability analysis for ensuring risk controls are implemented and effective  i e  tracking risks identified to product requirements  design specifications  verification and validation results etc    FTA analysis requires diagramming software  FMEA analysis can be done using a spreadsheet program  There are also integrated medical device risk management solutions     Through a draft guidance  the FDA has introduced another method named \"Safety Assurance Case\" for medical device safety assurance analysis  The safety assurance case is structured argument reasoning about systems appropriate for scientists and engineers  supported by a body of evidence  that provides a compelling  comprehensible and valid case that a system is safe for a given application in a given environment  With the guidance  a safety assurance case is expected for safety critical devices  e g  infusion devices  as part of the pre-market clearance submission  e g  510 k   In 2013  the FDA introduced another draft guidance expecting medical device manufacturers to submit cybersecurity risk analysis information     Project risk management must be considered at the different phases of acquisition   In the beginning of a project  the advancement of technical developments  or threats presented by a competitor  s projects  may cause a risk or threat assessment and subsequent evaluation of alternatives  see Analysis of Alternatives   Once a decision is made  and the project begun  more familiar project management applications can be used  22  23  24     Megaprojects  sometimes also called \"major programs\"  are large-scale investment projects  typically costing more than $1 billion per project  Megaprojects include major bridges  tunnels  highways  railways  airports  seaports  power plants  dams  wastewater projects  coastal flood protection schemes  oil and natural gas extraction projects  public buildings  information technology systems  aerospace projects  and defense systems  Megaprojects have been shown to be particularly risky in terms of finance  safety  and social and environmental impacts  Risk management is therefore particularly pertinent for megaprojects and special methods and special education have been developed for such risk management  25     It is important to assess risk in regard to natural disasters like floods  earthquakes  and so on  Outcomes of natural disaster risk assessment are valuable when considering future repair costs  business interruption losses and other downtime  effects on the environment  insurance costs  and the proposed costs of reducing the risk  26  27  The Sendai Framework for Disaster Risk Reduction is a 2015 international accord that has set goals and targets for disaster risk reduction in response to natural disasters  28  There are regular International Disaster and Risk Conferences in Davos to deal with integral risk management     Several tools can be used to assess risk and risk management of natural disasters and other climate events  including geospatial modeling  a key component of land change science  This modeling requires an understanding of geographic distributions of people as well as an ability to calculate the likelihood of a natural disaster occurring     The management of risks to persons and property in wilderness and remote natural areas has developed with increases in outdoor recreation participation and decreased social tolerance for loss   Organizations providing commercial wilderness experiences can now align with national and international consensus standards for training and equipment such as ANSI/NASBLA 101-2017  boating   29  UIAA 152  ice climbing tools   30  and European Norm 13089 2015 + A1 2015  mountaineering equipment   31  32  The Association for Experiential Education offers accreditation for wilderness adventure programs  33  The Wilderness Risk Management Conference provides access to best practices  and specialist organizations provide wilderness risk management consulting and training  34  35  36  37     In his book  Outdoor Leadership and Education  climber  outdoor educator  and author Ari Schneider  notes that outdoor recreation is inherently risky  and there is no way to completely eliminate risk  However  he explains how that can be a good thing for outdoor education programs  According to Schneider  optimal adventure is achieved when real risk is managed and perceived risk is maintained in order to keep actual danger low and a sense of adventure high  38     The text Outdoor Safety - Risk Management for Outdoor Leaders  39  published by the New Zealand Mountain Safety Council  provides a view of wilderness risk management from the New Zealand perspective  recognizing the value of national outdoor safety legislation and devoting considerable attention to the roles of judgment and decision-making processes in wilderness risk management     Risk Management for Outdoor Programs  A Guide to Safety in Outdoor Education  Recreation and Adventure  40  published by Viristar  breaks down wilderness and experiential risk management into eight \"risk domains\" such as staff and equipment  and eleven \"risk management instruments\" such as incident reporting and risk transfer  before combining them all in a systems-thinking framework  41     One popular models for risk assessment is the Risk Assessment and Safety Management  RASM  Model developed by Rick Curtis  author of The Backpacker  s Field Manual  38  The formula for the RASM Model is  Risk = Probability of Accident  Severity of Consequences  The RASM Model weighs negative riskthe potential for loss  against positive riskthe potential for growth     IT risk is a risk related to information technology  This is a relatively new term due to an increasing awareness that information security is simply one facet of a multitude of risks that are relevant to IT and the real world processes it supports  \"Cybersecurity is tied closely to the advancement of technology  It lags only long enough for incentives like black markets to evolve and new exploits to be discovered  There is no end in sight for the advancement of technology  so we can expect the same from cybersecurity \" 42     ISACA  s Risk IT framework ties IT risk to enterprise risk management     Duty of Care Risk Analysis  DoCRA  43  evaluates risks and their safeguards and considers the interests of all parties potentially affected by those risks     For the offshore oil and gas industry  operational risk management is regulated by the safety case regime in many countries  Hazard identification and risk assessment tools and techniques are described in the international standard ISO 17776 2000  and organisations such as the IADC  International Association of Drilling Contractors  publish guidelines for Health  Safety and Environment  HSE  Case development which are based on the ISO standard  Further  diagrammatic representations of hazardous events are often expected by governmental regulators as part of risk management in safety case submissions  these are known as bow-tie diagrams  see Network theory in risk assessment   The technique is also used by organisations and regulators in mining  aviation  health  defence  industrial and finance     The principles and tools for quality risk management are increasingly being applied to different aspects of pharmaceutical quality systems  These aspects include development  manufacturing  distribution  inspection  and submission/review processes throughout the lifecycle of drug substances  drug products  biological and biotechnological products  including the use of raw materials  solvents  excipients  packaging and labeling materials in drug products  biological and biotechnological products   Risk management is also applied to the assessment of microbiological contamination in relation to pharmaceutical products and cleanroom manufacturing environments  44     Risk communication is a complex cross-disciplinary academic field related to core values of the targeted audiences  45  46  Problems for risk communicators involve how to reach the intended audience  how to make the risk comprehensible and relatable to other risks  how to pay appropriate respect to the audience  s values related to the risk  how to predict the audience  s response to the communication  etc  A main goal of risk communication is to improve collective and individual decision making  Risk communication is somewhat related to crisis communication  but there are clear distinctions  Risk communication deals with possible risks and aims to raise awareness of those risks to encourage or persuade changes in behavior to relieve threats in the long term  On the other hand  crisis communication is aimed at raising awareness of a specific type of threat  the magnitude  outcomes  and specific behaviors to adopt to reduce the threat  47  Some experts coincide that risk is not only enrooted in the communication process but also it cannot be dissociated from the use of language  Though each culture develops its own fears and risks  these construes apply only by the hosting culture     Risk communication and community engagement  RCCE  is a method that draws heavily on volunteers  frontline personnel and on people without prior training in this area  48     Category  By country     Atmospheric dispersion modeling is the mathematical simulation of how air pollutants disperse in the ambient atmosphere  It is performed with computer programs that include algorithms to solve the mathematical equations that govern the pollutant dispersion  The dispersion models are used to estimate the downwind ambient concentration of air pollutants or toxins emitted from sources such as industrial plants  vehicular traffic or accidental chemical releases  They can also be used to predict future concentrations under specific scenarios  i e  changes in emission sources   Therefore  they are the dominant type of model used in air quality policy making  They are most useful for pollutants that are dispersed over large distances and that may react in the atmosphere  For pollutants that have a very high spatio-temporal variability  i e  have very steep distance to source decay such as black carbon  and for epidemiological studies statistical land-use regression models are also used     Dispersion models are important to governmental agencies tasked with protecting and managing the ambient air quality  The models are typically employed to determine whether existing or proposed new industrial facilities are or will be in compliance with the National Ambient Air Quality Standards  NAAQS  in the United States and other nations  The models also serve to assist in the design of effective control strategies to reduce emissions of harmful air pollutants  During the late 1960s  the Air Pollution Control Office of the U S  EPA initiated research projects that would lead to the development of models for the use by urban and transportation planners  1  A major and significant application of a roadway dispersion model that resulted from such research was applied to the Spadina Expressway of Canada in 1971     Air dispersion models are also used by public safety responders and emergency management personnel for emergency planning of accidental chemical releases  Models are used to determine the consequences of accidental releases of hazardous or toxic materials  Accidental releases may result in fires  spills or explosions that involve hazardous materials  such as chemicals or radionuclides  The results of dispersion modeling  using worst case accidental release source terms and meteorological conditions  can provide an estimate of location impacted areas  ambient concentrations  and be used to determine protective actions appropriate in the event a release occurs  Appropriate protective actions may include evacuation or shelter in place for persons in the downwind direction  At industrial facilities  this type of consequence assessment or emergency planning is required under the Clean Air Act  United States   CAA  codified in Part 68 of Title 40 of the Code of Federal Regulations     The dispersion models vary depending on the mathematics used to develop the model  but all require the input of data that may include     Many of the modern  advanced dispersion modeling programs include a pre-processor module for the input of meteorological and other data  and many also include a post-processor module for graphing the output data and/or plotting the area impacted by the air pollutants on maps  The plots of areas impacted may also include isopleths showing areas of minimal to high concentrations that define areas of the highest health risk  The isopleths plots are useful in determining protective actions for the public and responders     The atmospheric dispersion models are also known as atmospheric diffusion models  air dispersion models  air quality models  and air pollution dispersion models     Discussion of the layers in the Earth  s atmosphere is needed to understand where airborne pollutants disperse in the atmosphere  The layer closest to the Earth  s surface is known as the troposphere  It extends from sea-level to a height of about 18 xa0km and contains about 80 percent of the mass of the overall atmosphere  The stratosphere is the next layer and extends from 18 xa0km to about 50 xa0km  The third layer is the mesosphere which extends from 50 xa0km to about 80 xa0km  There are other layers above 80 xa0km  but they are insignificant with respect to atmospheric dispersion modeling     The lowest part of the troposphere is called the atmospheric boundary layer  ABL  or the planetary boundary layer  PBL    The air temperature of the atmosphere decreases with increasing altitude until it reaches what is called an inversion layer  where the temperature increases with increasing altitude  that caps the Convective Boundary Layer  typically to about 1 5 to 2 0 xa0km in height  The upper part of the troposphere  i e   above the inversion layer  is called the free troposphere and it extends up to the tropopause  the boundary in the Earth  s atmosphere between the troposphere and the stratosphere   In tropical and mid-latitudes during daytime  the Free convective layer can comprise the entire troposphere  which is up to 10 xa0km to 18 xa0km in the Intertropical convergence zone    \"The ABL is of the most important with respect to the emission  transport and dispersion of airborne pollutants  The part of the ABL between the Earth s surface and the bottom of the inversion layer is known as the mixing layer  Almost all of the airborne pollutants emitted into the ambient atmosphere are transported and dispersed within the mixing layer  Some of the emissions penetrate the inversion layer and enter the free troposphere above the ABL  n\"  \"In summary  the layers of the Earth s atmosphere from the surface of the ground upwards are  the ABL made up of the mixing layer capped by the inversion layer  the free troposphere  the stratosphere  the mesosphere and others  Many atmospheric dispersion models are referred to as boundary layer models because they mainly model air pollutant dispersion within the ABL  To avoid confusion  models referred to as mesoscale models have dispersion modeling capabilities that extend horizontally up to a few hundred kilometres  It does not mean that they model dispersion in the mesosphere  n\"   The technical literature on air pollution dispersion is quite extensive and dates back to the 1930s and earlier  One of the early air pollutant plume dispersion equations was derived by Bosanquet and Pearson  2  Their equation did not assume Gaussian distribution nor did it include the effect of ground reflection of the pollutant plume     Sir Graham Sutton derived an air pollutant plume dispersion equation in 1947 3  which did include the assumption of Gaussian distribution for the vertical and crosswind dispersion of the plume and also included the effect of ground reflection of the plume     Under the stimulus provided by the advent of stringent environmental control regulations  there was an immense growth in the use of air pollutant plume dispersion calculations between the late 1960s and today  A great many computer programs for calculating the dispersion of air pollutant emissions were developed during that period of time and they were called \"air dispersion models\"  The basis for most of those models was the Complete Equation For Gaussian Dispersion Modeling Of Continuous  Buoyant Air Pollution Plumes shown below  4  5                         C        =                                                          Q                        u                                                                                    f                                                                                        y                                                                              2                                                                                                                                                                        g                                  1                                            +                              g                                  2                                            +                              g                                  3                                                                                                                      z                                                                              2                                                                                               displaystyle C=   frac     Q  u    cdot    frac     f    sigma _ y    sqrt  2  pi          cdot    frac     g_ 1 +g_ 2 +g_ 3     sigma _ z    sqrt  2  pi         n    The above equation not only includes upward reflection from the ground  it also includes downward reflection from the bottom of any inversion lid present in the atmosphere     The sum of the four exponential terms in                               g                      3                                   displaystyle g_ 3     converges to a final value quite rapidly  For most cases  the summation of the series with m = 1  m = 2 and m = 3 will provide an adequate solution                                                         z                                   displaystyle   sigma _ z     and                                                     y                                   displaystyle   sigma _ y     are functions of the atmospheric stability class  i e   a measure of the turbulence in the ambient atmosphere  and of the downwind distance to the receptor  The two most important variables affecting the degree of pollutant emission dispersion obtained are the height of the emission source point and the degree of atmospheric turbulence  The more turbulence  the better the degree of dispersion     Equations 6  7  for                                                     y                                   displaystyle   sigma _ y     and                                                     z                                   displaystyle   sigma _ z     are                                                         y                                   displaystyle   sigma _ y     x  = exp Iy + Jyln x  + Ky ln x  2                                                         z                                   displaystyle   sigma _ z     x  = exp Iz + Jzln x  + Kz ln x  2      units of                                                     z                                   displaystyle   sigma _ z      and                                                     y                                   displaystyle   sigma _ y      and x are in meters     The classification of stability class is proposed by F  Pasquill  8  The six stability classes are referred to  nA-extremely unstable nB-moderately unstable nC-slightly unstable nD-neutral nE-slightly stable nF-moderately stable    The resulting calculations for air pollutant concentrations are often expressed as an air pollutant concentration contour map in order to show the spatial variation in contaminant levels over a wide area under study   In this way the contour lines can overlay sensitive receptor locations and reveal the spatial relationship of air pollutants to areas of interest     Whereas older models rely on stability classes  see air pollution dispersion terminology  for the determination of                                                     y                                   displaystyle   sigma _ y     and                                                     z                                   displaystyle   sigma _ z      more recent models increasingly rely on the Monin-Obukhov similarity theory to derive these parameters    \"The Gaussian air pollutant dispersion equation  discussed above  requires the input of H which is the pollutant plume s centerline height above ground leveland H nis the sum of Hs  the actual physical height of the pollutant plume s emission source point  plus H  the plume rise due to the plume s buoyancy   n\"   To determine H  many if not most of the air dispersion models developed between the late 1960s and the early 2000s used what are known as \"the Briggs equations \" G A  Briggs first published his plume rise observations and comparisons in 1965  9  In 1968  at a symposium sponsored by CONCAWE  a Dutch organization   he compared many of the plume rise models then available in the literature  10  In that same year  Briggs also wrote the section of the publication edited by Slade 11  dealing with the comparative analyses of plume rise models   That was followed in 1969 by his classical critical review of the entire plume rise literature  12  in which he proposed a set of plume rise equations which have become widely known as \"the Briggs equations\"   Subsequently  Briggs modified his 1969 plume rise equations in 1971 and in 1972  13  14     Briggs divided air pollution plumes into these four general categories     Briggs considered the trajectory of cold jet plumes to be dominated by their initial velocity momentum  and the trajectory of hot  buoyant plumes to be dominated by their buoyant momentum to the extent that their initial velocity momentum was relatively unimportant   Although Briggs proposed plume rise equations for each of the above plume categories  it is important to emphasize that \"the Briggs equations\" which become widely used are those that he proposed for bent-over  hot buoyant plumes     In general  Briggs  s equations for bent-over  hot buoyant plumes are based on observations and data involving plumes from typical combustion sources such as the flue gas stacks from steam-generating boilers burning fossil fuels in large power plants   Therefore  the stack exit velocities were probably in the range of 20 to 100 xa0ft/s  6 to 30 xa0m/s  with exit temperatures ranging from 250 to 500 xa0F  120 to 260 xa0C      A logic diagram for using the Briggs equations 4  to obtain the plume rise trajectory of bent-over buoyant plumes is presented below     The above parameters used in the Briggs   equations are discussed in Beychok  s book  4     List of atmospheric dispersion models provides a more comprehensive list of models than listed below  It includes a very brief description of each model     Hydraulic shock  colloquial  water hammer  fluid hammer  is a pressure surge or wave caused  when a fluid in motion  usually a liquid but sometimes also a gas  is forced to stop or change direction suddenly  a momentum change  This phenomenon commonly occurs when a valve closes suddenly at an end of a pipeline system  and a pressure wave propagates in the pipe     This pressure wave can cause major problems  from noise and vibration to pipe rupture or collapse  It is possible to reduce the effects of the water hammer pulses with accumulators  expansion tanks  surge tanks  blowoff valves  and other features  The effects can be avoided by ensuring that no valves will close too quickly with significant flow  but there are many situations that can cause the effect     Rough calculations can be made using the Zhukovsky  Joukowsky  equation  1  or more accurate ones using the method of characteristics  2     In the 1st century B C   Marcus Vitruvius Pollio described the effect of water hammer in lead pipes and stone tubes of the Roman public water supply  3  4   Water hammer was exploited before there was even a word for it  in 1772  Englishman John Whitehurst built a hydraulic ram for a home in Cheshire  England  5  In  1796  French inventor Joseph Michel Montgolfier  17401810  built a hydraulic ram for his paper mill in Voiron  6   In French and Italian  the terms for \"water hammer\" come from the hydraulic ram   coup de blier  French  and colpo d  ariete  Italian  both mean \"blow of the ram\"  7   As the 19th century witnessed the installation of municipal water supplies  water hammer became a concern to civil engineers  8  9  10   Water hammer also interested physiologists who were studying the circulatory system  11     Although it was prefigured in work by Thomas Young  12  11  the theory of water hammer is generally considered to have begun in 1883 with the work of German physiologist Johannes von Kries  18531928   who was investigating the pulse in blood vessels  13  14   However  his findings went unnoticed by civil engineers  15  16   Kries  s findings were subsequently derived independently in 1898 by the Russian fluid dynamicist Nikolay Yegorovich Zhukovsky  18471921   1  17  in 1898 by the American civil engineer Joseph Palmer Frizell  18321910   18  19  and in 1902 by the Italian engineer Lorenzo Allievi  18561941   20     When a pipe with water flowing through it is suddenly closed at the outlet  downstream   the mass of water before the closure is still moving  thereby building up pressure and a resulting shock wave  In domestic plumbing this shock wave is experienced as a loud banging resembling a hammering noise  Water hammer can cause pipelines to break if the pressure is high enough  Air traps or stand pipes  open at the top  are sometimes added as dampers to water systems to absorb the potentially damaging forces caused by the moving water     In hydroelectric generating stations  the water traveling along the tunnel or pipeline may be prevented from entering a turbine by closing a valve  For example  if there is 14 xa0km  8 7 xa0mi  of tunnel of 7 7 xa0m  25 xa0ft  diameter full of water travelling at 3 75 xa0m/s  8 4 xa0mph   21  that represents approximately 8 000 megajoules  2 200 xa0kWh  of kinetic energy that must be arrested  This arresting is frequently achieved by a surge shaft 22  open at the top  into which the water flows  As the water rises up the shaft its kinetic energy is converted into potential energy  which causes the water in the tunnel to decelerate  At some hydroelectric power  HEP  stations  such as the Saxon Falls Hydro Power Plant In Michigan  what looks like a water tower is actually one of these devices  known in these cases as a surge drum  23     At home  a water hammer may occur when a dishwasher  washing machine or toilet shuts off water flow  The result may be heard as a loud bang  repetitive banging  as the shock wave travels back and forth in the plumbing system   or as some shuddering     On the other hand  when an upstream valve in a pipe closes  water downstream of the valve attempts to continue flowing creating a vacuum that may cause the pipe to collapse or implode  This problem can be particularly acute if the pipe is on a downhill slope  To prevent this  air and vacuum relief valves or air vents are installed just downstream of the valve to allow air to enter the line to prevent this vacuum from occurring     Other causes of water hammer are pump failure and check valve slam  due to sudden deceleration  a check valve may slam shut rapidly  depending on the dynamic characteristic of the check valve and the mass of the water between a check valve and tank   To alleviate this situation  it is recommended to install non-slam check valves as they do not rely on gravity or fluid flow for their closure  For vertical pipes  other suggestions include installing new piping that can be designed to include air chambers to alleviate the possible shockwave of water due to excess water flow  24     Water hammer can also occur when filling an empty pipe that has a restriction such as a partially open valve or an orifice that allows air to pass easily as the pipe rapidly fills  but once full the water suddenly encounters the restriction and the pressure spikes     Steam distribution systems may also be vulnerable to a situation similar to water hammer  known as steam hammer  In a steam system  this phenomenon most often occurs when some of the steam condenses into water in a horizontal section of the piping   The rest of the steam forces this liquid water along the pipe  forming a \"slug\"  and hurls this at high velocity into a pipe fitting  creating a loud hammering noise and greatly stressing the pipe   This condition is usually caused by a poor condensate drainage strategy  having more condensate in the pipe makes the slug easier to form  Vacuum caused by condensation from thermal shock can also cause a steam hammer     Steam hammer can be avoided by using sloped pipes and installing steam traps  Where air-filled traps are used  these eventually become depleted of their trapped air over a long period through absorption into the water  This can be cured by shutting off the supply  opening taps at the highest and lowest locations to drain the system  thereby restoring air to the traps   and then closing the taps and re-opening the supply     On turbocharged internal combustion engines  a \"gas hammer\" can take place when the throttle is closed while the turbocharger is forcing air into the engine  There is no shockwave but the pressure can still rapidly increase to damaging levels or cause compressor surge  A pressure relief valve placed before the throttle prevents the air from surging against the throttle body by diverting it elsewhere  thus protecting the turbocharger from pressure damage  This valve can either recirculate the air into the turbocharger  s intake  recirculation valve   or it can blow the air into the atmosphere and produce the distinctive hiss-flutter of an aftermarket turbocharger  blowoff valve      If a stream of high velocity water impinges on a surface  water hammer can quickly erode and destroy it  In the 2009 Sayano-Shushenskaya power station accident  the lid to a 640 MW turbine was ejected upwards  hitting the ceiling above  During the accident  the rotor was seen flying through the air  still spinning  about 3 meters  above the floor  Unrestrained  256 cubic metres  67 600 xa0US xa0gal  per second of water began to spray all over the generator hall  25  The geyser caused the structural failure of steel ceiling joists  precipitating a roof collapse around the failed turbine     When an explosion happens in an enclosed space  water hammer can cause the walls of the container to deform  However  it can also impart momentum to the enclosure if it is free to move  An underwater explosion in the SL-1 nuclear reactor vessel caused the water to accelerate upwards through 2 5 feet  0 76 xa0m  of air before it struck the vessel head at 160 feet per second  49 xa0m/s  with a pressure of 10 000 pounds per square inch  69 000 xa0kPa   This pressure wave caused the 26 000 pounds  12 000 xa0kg  steel vessel to jump 9 feet 1 inch  2 77 m  into the air before it dropped into its prior location  26  It is imperative to perform ongoing preventive maintenance to avoid water hammer  as the results of these powerful explosions have resulted in fatalities  27     Water hammer has caused accidents and fatalities  but usually damage is limited to breakage of pipes or appendages  An engineer should always assess the risk of a pipeline burst  Pipelines transporting hazardous liquids or gases warrant special care in design  construction  and operation  Hydroelectric power plants especially must be carefully designed and maintained because the water hammer can cause water pipes to fail catastrophically     The following characteristics may reduce or eliminate water hammer     One of the first to successfully investigate the water hammer problem was the Italian engineer Lorenzo Allievi     Water hammer can be analyzed by two different approachesrigid column theory  which ignores compressibility of the fluid and elasticity of the walls of the pipe  or by a full analysis that includes elasticity   When the time it takes a valve to close is long compared to the propagation time for a pressure wave to travel the  length of the pipe  then rigid column theory is appropriate  otherwise considering elasticity may be necessary  28  nBelow are two approximations for the peak pressure  one that considers elasticity  but assumes the valve closes instantaneously  and a second that neglects elasticity but includes a finite time for the valve to close     The pressure profile of the water hammer pulse can be calculated from the Joukowsky equation 29     So for a valve closing instantaneously  the maximal magnitude of the water hammer pulse is    where P is the magnitude of the pressure wave  Pa    is the density of the fluid  kg/m3   a0 is the speed of sound in the fluid  m/s   and v is the change in the fluid  s velocity  m/s   The pulse comes about due to Newton  s laws of motion and the continuity equation applied to the deceleration of a fluid element  30     As the speed of sound in a fluid is                     a        =                              B                          /                                                           displaystyle a=   sqrt  B/  rho        the peak pressure depends on the fluid compressibility if the valve is closed abruptly     where    When the valve is closed slowly compared to the transit time for a pressure wave to travel the length of the pipe  the elasticity can be neglected  and the phenomenon can be described in terms of inertance or rigid column theory     Assuming constant deceleration of the water column  dv/dt = v/t   this gives    where     The above formula becomes  for water and with imperial unit     For practical application  a safety factor of about 5 is recommended     where P1 is the inlet pressure in psi  V is the flow velocity in ft/s  t is the valve closing time in seconds  and L is the upstream pipe length in feet  31     Hence  we can say that the magnitude of the water hammer largely depends upon the time of closure  elastic components of pipe & fluid properties  32     When a valve with a volumetric flow rate Q is closed  an excess pressure P is created upstream of the valve  whose value is given by the Joukowsky equation     In this expression  33     The hydraulic impedance Z of the pipeline determines the magnitude of the water hammer pulse  It is itself defined by    where    The latter follows from a series of hydraulic concepts     Thus  the equivalent elasticity is the sum of the original elasticities     As a result  we see that we can reduce the water hammer by     The water hammer effect can be simulated by solving the following partial differential equations     where V is the fluid velocity inside pipe                                       displaystyle   rho     is the fluid density  B is the equivalent bulk modulus  and f is the DarcyWeisbach friction factor  34     Column separation is a phenomenon that can occur during a water-hammer event   If the pressure in a pipeline drops below the vapor pressure of the liquid  cavitation will occur  some of the liquid vaporizes  forming a bubble in the pipeline  keeping the pressure close to the vapor pressure    This is most likely to occur at specific locations such as closed ends  high points or knees  changes in pipe slope    When subcooled liquid flows into the space previously occupied by vapor the area of contact between the vapor and the liquid increases   This causes the vapor to condense into the liquid reducing the pressure in the vapor space   The liquid on either side of the vapor space is then accelerated into this space by the pressure difference   The collision of the two columns of liquid  or of one liquid column if at a closed end  causes a large and nearly instantaneous rise in pressure   This pressure rise can damage hydraulic machinery  individual pipes and supporting structures   Many repetitions of cavity formation and collapse may occur in a single water-hammer event  35     Most water hammer software packages use the method of characteristics 30  to solve the differential equations involved  This method works well if the wave speed does not vary in time due to either air or gas entrainment in a pipeline  The wave method  WM  is also used in various software packages  WM lets operators analyze large networks efficiently  Many commercial and non-commercial packages are available     Software packages vary in complexity  dependent on the processes modeled  The more sophisticated packages may have any of the following features     In the fields of medicine  biotechnology and pharmacology  drug discovery is the process by which new candidate medications are discovered  1     Historically  drugs were discovered by identifying the active ingredient from traditional remedies or by serendipitous discovery  as with penicillin  More recently  chemical libraries of synthetic small molecules  natural products or extracts were screened in intact cells or whole organisms to identify substances that had a desirable therapeutic effect in a process known as classical pharmacology  After sequencing of the human genome allowed rapid cloning and synthesis of large quantities of purified proteins  it has become common practice to use high throughput screening of large compounds libraries against isolated biological targets which are hypothesized to be disease-modifying in a process known as reverse pharmacology  Hits from these screens are then tested in cells and then in animals for efficacy  2     Modern drug discovery involves the identification of screening hits  3  medicinal chemistry 4  and optimization of those hits to increase the affinity  selectivity  to reduce the potential of side effects   efficacy/potency  metabolic stability  to increase the half-life   and oral bioavailability  Once a compound that fulfills all of these requirements has been identified  the process of drug development can continue  If successful  clinical trials are developed  5     Modern drug discovery is thus usually a capital-intensive process that involves large investments by pharmaceutical industry corporations as well as national governments  who provide grants and loan guarantees   Despite advances in technology and understanding of biological systems  drug discovery is still a lengthy  \"expensive  difficult  and inefficient process\" with low rate of new therapeutic discovery  6   In 2010  the research and development cost of each new molecular entity was about US$1 8 billion  7  In the 21st century  basic discovery research is funded primarily by governments and by philanthropic organizations  while late-stage development is funded primarily by pharmaceutical companies or venture capitalists  8  To be allowed to come to market  drugs must undergo several successful phases of clinical trials  and pass through a new drug approval process  called the New Drug Application in the United States     Discovering drugs that may be a commercial success  or a public health success  involves a complex interaction between investors  industry  academia  patent laws  regulatory exclusivity  marketing and the need to balance secrecy with communication  9  Meanwhile  for disorders whose rarity means that no large commercial success or public health effect can be expected  the orphan drug funding process ensures that people who experience those disorders can have some hope of pharmacotherapeutic advances     The idea that the effect of a drug in the human body is mediated by specific interactions of the drug molecule with biological macromolecules   proteins or nucleic acids in most cases  led scientists to the conclusion that individual chemicals are required for the biological activity of the drug  This made for the beginning of the modern era in pharmacology  as pure chemicals  instead of crude extracts of medicinal plants  became the standard drugs  Examples of drug compounds isolated from crude preparations are morphine  the active agent in opium  and digoxin  a heart stimulant originating from Digitalis lanata  Organic chemistry also led to the synthesis of many of the natural products isolated from biological sources     Historically  substances  whether crude extracts or purified chemicals  were screened for biological activity without knowledge of the biological target  Only after an active substance was identified was an effort made to identify the target  This approach is known as classical pharmacology  forward pharmacology  10  or phenotypic drug discovery  11     Later  small molecules were synthesized to specifically target a known physiological/pathological pathway  avoiding the mass screening of banks of stored compounds  This led to great success  such as the work of Gertrude Elion and George H  Hitchings on purine metabolism  12  13  the work of James Black 14  on beta blockers and cimetidine  and the discovery of statins by Akira Endo  15  Another champion of the approach of developing chemical analogues of known active substances was Sir David Jack at Allen and Hanbury  s  later Glaxo  who pioneered the first inhaled selective beta2-adrenergic agonist for asthma  the first inhaled steroid for asthma  ranitidine as a successor to cimetidine  and supported the development of the triptans  16     Gertrude Elion  working mostly with a group of fewer than 50 people on purine analogues  contributed to the discovery of the first anti-viral  the first immunosuppressant  azathioprine  that allowed human organ transplantation  the first drug to induce remission of childhood leukemia  pivotal anti-cancer treatments  an anti-malarial  an anti-bacterial  and a treatment for gout     Cloning of human proteins made possible the screening of large libraries of compounds against specific targets thought to be linked to specific diseases  This approach is known as reverse pharmacology and is the most frequently used approach today  17     A \"target\" is produced within the pharmaceutical industry  8  Generally  the \"target\" is the naturally existing cellular or molecular structure involved in the pathology of interest where the drug-in-development is meant to act  8  However  the distinction between a \"new\" and \"established\" target can be made without a full understanding of just what a \"target\" is  This distinction is typically made by pharmaceutical companies engaged in the discovery and development of therapeutics  8  In an estimate from 2011  435 human genome products were identified as therapeutic drug targets of FDA-approved drugs  18     \"Established targets\" are those for which there is a good scientific understanding  supported by a lengthy publication history  of both how the target functions in normal physiology and how it is involved in human pathology  2  This does not imply that the mechanism of action of drugs that are thought to act through a particular established target is fully understood  2  Rather  \"established\" relates directly to the amount of background information available on a target  in particular functional information   In general  \"new targets\" are all those targets that are not \"established targets\" but which have been or are the subject of drug discovery efforts  The majority of targets selected for drug discovery efforts are proteins  such as G-protein-coupled receptors  GPCRs  and protein kinases  19     The process of finding a new drug against a chosen target for a particular disease usually involves high-throughput screening  HTS   wherein large libraries of chemicals are tested for their ability to modify the target  For example  if the target is a novel GPCR  compounds will be screened for their ability to inhibit or stimulate that receptor  see antagonist and agonist   if the target is a protein kinase  the chemicals will be tested for their ability to inhibit that kinase  citation needed     Another important function of HTS is to show how selective the compounds are for the chosen target  as one wants to find a molecule which will interfere with only the chosen target  but not other  related targets  citation needed  To this end  other screening runs will be made to see whether the \"hits\" against the chosen target will interfere with other related targets  this is the process of cross-screening  citation needed  Cross-screening is important  because the more unrelated targets a compound hits  the more likely that off-target toxicity will occur with that compound once it reaches the clinic  citation needed     It is unlikely that a perfect drug candidate will emerge from these early screening runs  One of the first steps is to screen for compounds that are unlikely to be developed into drugs  for example compounds that are hits in almost every assay  classified by medicinal chemists as \"pan-assay interference compounds\"  are removed at this stage  if they were not already removed from the chemical library  20  21  22   It is often observed that several compounds are found to have some degree of activity  and if these compounds share common chemical features  one or more pharmacophores can then be developed  At this point  medicinal chemists will attempt to use structureactivity relationships  SAR  to improve certain features of the lead compound     This process will require several iterative screening runs  during which  it is hoped  the properties of the new molecular entities will improve  and allow the favoured compounds to go forward to in vitro and in vivo testing for activity in the disease model of choice     Amongst the physicochemical properties associated with drug absorption include ionization  pKa   and solubility  permeability can be determined by PAMPA and Caco-2  PAMPA is attractive as an early screen due to the low consumption of drug and the low cost compared to tests such as Caco-2  gastrointestinal tract  GIT  and Bloodbrain barrier  BBB  with which there is a high correlation     A range of parameters can be used to assess the quality of a compound  or a series of compounds  as proposed in the Lipinski  s Rule of Five  Such parameters include calculated properties such as cLogP to estimate lipophilicity  molecular weight  polar surface area and measured properties  such as potency  in-vitro measurement of enzymatic clearance etc  Some descriptors such as ligand efficiency 23   LE  and lipophilic efficiency 24  25   LiPE  combine such parameters to assess druglikeness     While HTS is a commonly used method for novel drug discovery  it is not the only method  It is often possible to start from a molecule which already has some of the desired properties  Such a molecule might be extracted from a natural product or even be a drug on the market which could be improved upon  so-called \"me too\" drugs   Other methods  such as virtual high throughput screening  where screening is done using computer-generated models and attempting to \"dock\" virtual libraries to a target  are also often used  citation needed     Another important method for drug discovery is de novo drug design  in which a prediction is made of the sorts of chemicals that might  e g   fit into an active site of the target enzyme  For example  virtual screening and computer-aided drug design are often used to identify new chemical moieties that may interact with a target protein  26  27  Molecular modelling 28  and molecular dynamics simulations can be used as a guide to improve the potency and properties of new drug leads  29  30  31     There is also a paradigm shift in the drug discovery community to shift away from HTS  which is expensive and may only cover limited chemical space  to the screening of smaller libraries  maximum a few thousand compounds   These include fragment-based lead discovery  FBDD  32  33  34  35  and protein-directed dynamic combinatorial chemistry  36  37  38  39  40  The ligands in these approaches are usually much smaller  and they bind to the target protein with weaker binding affinity than hits that are identified from HTS  Further modifications through organic synthesis into lead compounds are often required  Such modifications are often guided by protein X-ray crystallography of the protein-fragment complex  41  42  43  The advantages of these approaches are that they allow more efficient screening and the compound library  although small  typically covers a large chemical space when compared to HTS     Phenotypic screens have also provided new chemical starting points in drug discovery  44  45  xa0 A variety of models have been used including yeast  zebrafish  worms  immortalized cell lines  primary cell lines  patient-derived cell lines and whole animal models  xa0These screens are designed to find compounds which reverse a disease phenotype such as death  protein aggregation  mutant protein expression  or cell proliferation as examples in a more holistic cell model or organism  xa0Smaller screening sets are often used for these screens  especially when the models are expensive or time-consuming to run  46  xa0 In many cases  the exact mechanism of action of hits from these screens is unknown and may require extensive target deconvolution experiments to ascertain     Once a lead compound series has been established with sufficient target potency and selectivity and favourable drug-like properties  one or two compounds will then be proposed for drug development  The best of these is generally called the lead compound  while the other will be designated as the \"backup\"  citation needed  These important decisions are generally supported by computational modelling innovations   47  48  49     Traditionally  many drugs and other chemicals with biological activity have been discovered by studying chemicals that organisms create to affect the activity of other organisms for survival  50     Despite the rise of combinatorial chemistry as an integral part of lead discovery process  natural products still play a major role as starting material for drug discovery  51  A 2007 report 52  found that of the 974 small molecule new chemical entities developed between 1981 and 2006  63% were natural derived or semisynthetic derivatives of natural products  For certain therapy areas  such as antimicrobials  antineoplastics  antihypertensive and anti-inflammatory drugs  the numbers were higher  citation needed  In many cases  these products have been used traditionally for many years  citation needed     Natural products may be useful as a source of novel chemical structures for modern techniques of development of antibacterial therapies  53     Many secondary metabolites produced by plants have potential therapeutic medicinal properties  These secondary metabolites contain  bind to  and modify the function of proteins  receptors  enzymes  etc    Consequently  plant derived natural products have often been used as the starting point for drug discovery  54  55  56  57  58     Until the Renaissance  the vast majority of drugs in Western medicine were plant-derived extracts  59  This has resulted in a pool of information about the potential of plant species as important sources of starting materials for drug discovery  60  Botanical knowledge about different metabolites and hormones that are produced in different anatomical parts of the plant  e g  roots  leaves  and flowers  are crucial for correctly identifying bioactive and pharmacological plant properties  60  61  Identifying new drugs and getting them approved for market has proved to be a stringent process due to regulations set by national drug regulatory agencies  62     Jasmonates are important in responses to injury and intracellular signals  They induce apoptosis  63  64  and protein cascade via proteinase inhibitor  63  have defense functions  65  and regulate plant responses to different biotic and abiotic stresses  65  66  Jasmonates also have the ability to directly act on mitochondrial membranes by inducing membrane depolarization via release of metabolites  67     Jasmonate derivatives  JAD  are also important in wound response and tissue regeneration in plant cells  They have also been identified to have anti-aging effects on human epidermal layer  68  It is suspected that they interact with proteoglycans  PG  and glycosaminoglycan  GAG  polysaccharides  which are essential extracellular matrix  ECM  components to help remodel the ECM  69  The discovery of JADs on skin repair has introduced newfound interest in the effects of these plant hormones in therapeutic medicinal application  68     Salicylic acid  SA   a phytohormone  was initially derived from willow bark and has since been identified in many species  It is an important player in plant immunity  although its role is still not fully understood by scientists  70  They are involved in disease and immunity responses in plant and animal tissues  They have salicylic acid binding proteins  SABPs  that have shown to affect multiple animal tissues  70  The first discovered medicinal properties of the isolated compound was involved in pain and fever management  They also play an active role in the suppression of cell proliferation  63  They have the ability to induce death in lymphoblastic leukemia and other human cancer cells  63  One of the most common drugs derived from salicylates is aspirin  also known as acetylsalicylic acid  with anti-inflammatory and anti-pyretic properties  70  71     Microbes compete for living space and nutrients  To survive in these conditions  many microbes have developed abilities to prevent competing species from proliferating  Microbes are the main source of antimicrobial drugs  Streptomyces isolates have been such a valuable source of antibiotics  that they have been called medicinal molds  The classic example of an antibiotic discovered as a defense mechanism against another microbe is penicillin in bacterial cultures contaminated by Penicillium fungi in 1928  citation needed     Marine environments are potential sources for new bioactive agents  72  Arabinose nucleosides discovered from marine invertebrates in 1950s  demonstrated for the first time that sugar moieties other than ribose and deoxyribose can yield bioactive nucleoside structures  It took until 2004 when the first marine-derived drug was approved  citation needed  dubious   discuss  For example  the cone snail toxin ziconotide  also known as Prialt treats severe neuropathic pain  Several other marine-derived agents are now in clinical trials for indications such as cancer  anti-inflammatory use and pain  One class of these agents are bryostatin-like compounds  under investigation as anti-cancer therapy  citation needed     As above mentioned  combinatorial chemistry was a key technology enabling the efficient generation of large screening libraries for the needs of high-throughput screening  However  now  after two decades of combinatorial chemistry  it has been pointed out that despite the increased efficiency in chemical synthesis  no increase in lead or drug candidates has been reached  52  This has led to analysis of chemical characteristics of combinatorial chemistry products  compared to existing drugs or natural products  The chemoinformatics concept chemical diversity  depicted as distribution of compounds in the chemical space based on their physicochemical characteristics  is often used to describe the difference between the combinatorial chemistry libraries and natural products  The synthetic  combinatorial library compounds seem to cover only a limited and quite uniform chemical space  whereas existing drugs and particularly natural products  exhibit much greater chemical diversity  distributing more evenly to the chemical space  51  The most prominent differences between natural products and compounds in combinatorial chemistry libraries is the number of chiral centers  much higher in natural compounds   structure rigidity  higher in natural compounds  and number of aromatic moieties  higher in combinatorial chemistry libraries   Other chemical differences between these two groups include the nature of heteroatoms  O and N enriched in natural products  and S and halogen atoms more often present in synthetic compounds   as well as level of non-aromatic unsaturation  higher in natural products   As both structure rigidity and chirality are well-established factors in medicinal chemistry known to enhance compounds specificity and efficacy as a drug  it has been suggested that natural products compare favourably to today  s combinatorial chemistry libraries as potential lead molecules     Two main approaches exist for the finding of new bioactive chemical entities from natural sources    \"The first is sometimes referred to as random collection and screening of material  but the collection is far from random   Biological  often botanical  knowledge is often used to identify families that show promise   This approach is effective because only a small part of the earth s biodiversity has ever been tested for pharmaceutical activity   Also  organisms living in a species-rich environment need to evolve defensive and competitive mechanisms to survive   Those mechanisms might be exploited in the development of beneficial drugs  n\"   A collection of plant  animal and microbial samples from rich ecosystems can potentially give rise to novel biological activities worth exploiting in the drug development process  One example of successful use of this strategy is the screening for antitumor agents by the National Cancer Institute  which started in the 1960s  Paclitaxel was identified from Pacific yew tree Taxus brevifolia  Paclitaxel showed anti-tumour activity by a previously undescribed mechanism  stabilization of microtubules  and is now approved for clinical use for the treatment of lung  breast  and ovarian cancer  as well as for Kaposi  s sarcoma  Early in the 21st century  Cabazitaxel  made by Sanofi  a French firm   another relative of taxol has been shown effective against prostate cancer  also because it works by preventing the formation of microtubules  which pull the chromosomes apart in dividing cells  such as cancer cells   Other examples are   1  Camptotheca  Camptothecin  Topotecan  Irinotecan  Rubitecan  Belotecan   2  Podophyllum  Etoposide  Teniposide   3a  Anthracyclines  Aclarubicin  Daunorubicin  Doxorubicin  Epirubicin  Idarubicin  Amrubicin  Pirarubicin  Valrubicin  Zorubicin   3b  Anthracenediones  Mitoxantrone  Pixantrone      The second main approach involves ethnobotany  the study of the general use of plants in society  and ethnopharmacology  an area inside ethnobotany  which is focused specifically on medicinal uses     Artemisinin  an antimalarial agent from sweet wormtree Artemisia annua  used in Chinese medicine since 200BC is one drug used as part of combination therapy for multiresistant Plasmodium falciparum     The elucidation of the chemical structure is critical to avoid the re-discovery of a chemical agent that is already known for its structure and chemical activity  Mass spectrometry is a method in which individual compounds are identified based on their mass/charge ratio  after ionization  Chemical compounds exist in nature as mixtures  so the combination of liquid chromatography and mass spectrometry  LC-MS  is often used to separate the individual chemicals  Databases of mass spectras for known compounds are available and can be used to assign a structure to an unknown mass spectrum  Nuclear magnetic resonance spectroscopy is the primary technique for determining chemical structures of natural products  NMR yields information about individual hydrogen and carbon atoms in the structure  allowing detailed reconstruction of the molecule  s architecture     When a drug is developed with evidence throughout its history of research to show it is safe and effective for the intended use in the United States  the company can file an application  the New Drug Application  NDA   to have the drug commercialized and available for clinical application  73  NDA status enables the FDA to examine all submitted data on the drug to reach a decision on whether to approve or not approve the drug candidate based on its safety  specificity of effect  and efficacy of doses  73     Drug Discovery at Curlie            Molecular modelling encompasses all methods  theoretical and computational  used to model or mimic the behaviour of molecules  1  The methods are used in the fields of computational chemistry  drug design  computational biology and materials science to study molecular systems ranging from small chemical systems to large biological molecules and material assemblies  The simplest calculations can be performed by hand  but inevitably computers are required to perform molecular modelling of any reasonably sized system  The common feature of molecular modelling methods is the atomistic level description of the molecular systems  This may include treating atoms as the smallest individual unit  a molecular mechanics approach   or explicitly modelling protons and neutrons with its quarks  anti-quarks and gluons and electrons with its photons  a quantum chemistry approach      Molecular mechanics is one aspect of molecular modelling  as it involves the use of classical mechanics  Newtonian mechanics  to describe the physical basis behind the models  Molecular models typically describe atoms  nucleus and electrons collectively  as point charges with an associated mass  The interactions between neighbouring atoms are described by spring-like interactions  representing chemical bonds  and Van der Waals forces  The Lennard-Jones potential is commonly used to describe the latter  The electrostatic interactions are computed based on Coulomb  s law  Atoms are assigned coordinates in Cartesian space or in internal coordinates  and can also be assigned velocities in dynamical simulations  The atomic velocities are related to the temperature of the system  a macroscopic quantity  The collective mathematical expression is termed a potential function and is related to the system internal energy  U   a thermodynamic quantity equal to the sum of potential and kinetic energies  Methods which minimize the potential energy are termed energy minimization methods  e g   steepest descent and conjugate gradient   while methods that model the behaviour of the system with propagation of time are termed molecular dynamics     This function  referred to as a potential function  computes the molecular potential energy as a sum of energy terms that describe the deviation of bond lengths  bond angles and torsion angles away from equilibrium values  plus terms for non-bonded pairs of atoms describing van der Waals and electrostatic interactions  The set of parameters consisting of equilibrium bond lengths  bond angles  partial charge values  force constants and van der Waals parameters are collectively termed a force field  Different implementations of molecular mechanics use different mathematical expressions and different parameters for the potential function  2  The common force fields in use today have been developed by using  chemical theory  experimental reference data  and high level quantum calculations  The method  termed energy minimization  is used to find positions of zero gradient for all atoms  in other words  a local energy minimum  Lower energy states are more stable and are commonly investigated because of their role in chemical and biological processes  A molecular dynamics simulation  on the other hand  computes the behaviour of a system as a function of time  It involves solving Newton  s laws of motion  principally the second law                                F                =        m                  a                         displaystyle   mathbf  F  =m  mathbf  a       Integration of Newton  s laws of motion  using different integration algorithms  leads to atomic trajectories in space and time  The force on an atom is defined as the negative gradient of the potential energy function  The energy minimization method is useful to obtain a static picture for comparing between states of similar systems  while molecular dynamics provides information about the dynamic processes with the intrinsic inclusion of temperature effects     Molecules can be modelled either in vacuum  or in the presence of a solvent such as water  Simulations of systems in vacuum are referred to as gas-phase simulations  while those that include the presence of solvent molecules are referred to as explicit solvent simulations  In another type of simulation  the effect of solvent is estimated using an empirical mathematical expression  these are termed implicit solvation simulations     Most force fields are distance-dependent  making the most convenient expression for these Cartesian coordinates   Yet the comparatively rigid nature of bonds which occur between specific atoms  and in essence  defines what is meant by the designation molecule  make an internal coordinate system the most logical representation   In some fields the IC representation  bond length  angle between bonds  and twist angle of the bond as shown in the figure  is termed the Z-matrix or torsion angle representation  Unfortunately  continuous motions in Cartesian space often require discontinuous angular branches in internal coordinates  making it relatively hard to work with force fields in the internal coordinate representation  and conversely a simple displacement of an atom in Cartesian space may not be a straight line trajectory due to the prohibitions of the interconnected bonds   Thus  it is very common for computational optimizing programs to flip back and forth between representations during their iterations  This can dominate the calculation time of the potential itself and in long chain molecules introduce cumulative numerical inaccuracy  While all conversion algorithms produce mathematically identical results  they differ in speed and numerical accuracy  3   Currently  the fastest and most accurate torsion to Cartesian conversion is the Natural Extension Reference Frame  NERF  method  3     Molecular modelling methods are now used routinely to investigate the structure  dynamics  surface properties  and thermodynamics of inorganic  biological  and polymeric systems  The types of biological activity that have been investigated using molecular modelling include protein folding  enzyme catalysis  protein stability  conformational changes associated with biomolecular function  and molecular recognition of proteins  DNA  and membrane complexes  4     ACT-R  pronounced /kt r/  short for \"Adaptive Control of ThoughtRational\"  is a cognitive architecture mainly developed by John Robert Anderson and Christian Lebiere at Carnegie Mellon University   Like any cognitive architecture  ACT-R aims to define the basic and irreducible cognitive and perceptual operations that enable the human mind    nIn theory  each task that humans can perform should consist of a series of these discrete operations     Most of the ACT-R  s basic assumptions are also inspired by the progress of cognitive neuroscience  and ACT-R can be seen and described as a way of specifying how the brain itself is organized in a way that enables individual processing modules to produce cognition     ACT-R has been inspired by the work of Allen Newell  and especially by his lifelong championing the idea of unified theories as the only way to truly uncover the underpinnings of cognition  2  nIn fact  Anderson usually credits Newell as the major source of influence over his own theory     Like other influential cognitive architectures  including Soar  CLARION   and EPIC   the ACT-R theory has a computational implementation as an interpreter of a special coding language   The interpreter itself is written in Common Lisp  and might be loaded into any of the Common Lisp language distributions     This means that any researcher may download the ACT-R code from the ACT-R website  load it into a Common Lisp distribution  and gain full access to the theory in the form of the ACT-R interpreter     Also  this enables researchers to specify models of human cognition in the form of a script in the ACT-R language   The language primitives and data-types are designed to reflect the theoretical assumptions about human cognition   nThese assumptions are based on numerous facts derived from experiments in cognitive psychology and brain imaging     Like a programming language  ACT-R is a framework  for different tasks  e g   Tower of Hanoi  memory for text or for list of words  language comprehension  communication  aircraft controlling   researchers create \"models\"  i e   programs  in ACT-R  nThese models reflect the modelers   assumptions about the task within the ACT-R view of cognition    nThe model might then be run     Running a model automatically produces a step-by-step simulation of human behavior which specifies each individual cognitive operation  i e   memory encoding and retrieval  visual and auditory encoding  motor programming and execution  mental imagery manipulation     nEach step is associated with quantitative predictions of latencies and accuracies   nThe model can be tested by comparing its results with the data collected in behavioral experiments     In recent years  ACT-R has also been extended to make quantitative predictions of patterns of activation in the brain  as detected in experiments with fMRI  nIn particular  ACT-R has been augmented to predict the shape and time-course of the BOLD response of several brain areas  including the hand and mouth areas in the motor cortex  the left prefrontal cortex  the anterior cingulate cortex  and the basal ganglia     ACT-R  s most important assumption is that human knowledge can be divided into two irreducible kinds of representations  declarative and procedural      Within the ACT-R code  declarative knowledge is represented in the form of chunks  i e  vector representations of individual properties  each of them accessible from a labelled slot     Chunks are held and made accessible through buffers  which are the front-end of what are modules  i e  specialized and largely independent brain structures     There are two types of modules    \"All the modules can only be accessed through their buffers  The contents of the buffers at a given moment in time represents the state of ACT-R at that moment  The only exception to this rule is the procedural module  which stores and applies procedural knowledge  It does not have an accessible buffer and is actually used to access other module s contents  n\"   Procedural knowledge is represented in form of productions   The term \"production\" reflects the actual implementation of ACT-R as a production system  but  in fact  a production is mainly a formal notation to specify the information flow from cortical areas  i e  the buffers  to the basal ganglia  and back to the cortex     At each moment  an internal pattern matcher searches for a production that matches the current state of the buffers  Only one such production can be executed at a given moment  That production  when executed  can modify the buffers and thus change the state of the system  Thus  in ACT-R  cognition  nunfolds as a succession of production firings     In the cognitive sciences  different theories are usually ascribed to either the \"symbolic\" or the \"connectionist\" approach to cognition  ACT-R clearly belongs to the \"symbolic\" field and is classified as such in standard textbooks and collections  3  Its entities  chunks and productions  are discrete and its operations are syntactical  that is  not referring to the semantic content of the representations but only to their properties that deem them appropriate to participate in the computation s   This is seen clearly in the chunk slots and in the properties of buffer matching in productions  both of which function as standard symbolic variables     Members of the ACT-R community  including its developers  prefer to think of ACT-R as a general framework that specifies how the brain is organized  and how its organization gives birth to what is perceived  and  in cognitive psychology  investigated  as mind  going beyond the traditional symbolic/connectionist debate  None of this  naturally  argues against the classification of ACT-R as symbolic system  because all symbolic approaches to cognition aim to describe the mind  as a product of brain function  using a certain class of entities and systems to achieve that goal     A common misunderstanding suggests that ACT-R may not be a symbolic system because it attempts to characterize brain function  This is incorrect on two counts  First  all approaches to computational modeling of cognition  symbolic or otherwise  must in some respect characterize brain function  because the mind is brain function  And second  all such approaches  including connectionist approaches  attempt to characterize the mind at a cognitive level of description and not at the neural level  because it is only at the cognitive level at which important generalizations can be retained  4     Further misunderstandings arise because of the associative character of certain ACT-R properties  such as chunks spreading activation to each other  or chunks and productions carrying quantitative properties relevant to their selection  None of these properties counter the fundamental nature of these entities as symbolic  regardless of their role in unit selection and  ultimately  in computation     The importance of distinguishing between the theory itself and its implementation is usually highlighted by ACT-R developers    \"In fact  much of the implementation does not reflect the theory    nFor instance  the actual implementation makes use of additional  modules  that exist only for purely computational reasons  and are not supposed to reflect anything in the brain  e g   one computational module contains the pseudo-random number generator used to produce noisy parameters  while another holds naming routines for generating data structures accessible through variable names   n\"   Also  the actual implementation is designed to enable researchers to modify the theory  e g  by altering the standard parameters  or creating new modules  or partially modifying the behavior of the existing ones     Finally  while Anderson  s laboratory at CMU maintains and releases the official ACT-R code  other alternative implementations of the theory have been made available  These alternative implementations include jACT-R  5   written in Java by Anthony M  Harrison at the Naval Research Laboratory  and Python ACT-R  written in Python by Terrence C  Stewart and Robert L  West at Carleton University  Canada   6     Similarly  ACT-RN  now discontinued  was a full-fledged neural implementation of the 1993 version of the theory  7    nAll of these versions were fully functional  and models have been written and run with all of them     Because of these implementational degrees of freedom  the ACT-R community usually refers to the \"official\"  Lisp-based  version of the theory  when adopted in its original form and left unmodified  as \"Vanilla ACT-R\"     Over the years  ACT-R models have been used in more than 700 different scientific publications  and have been cited in many more     The ACT-R declarative memory system has been used to model human memory since its inception   In the course of years  it has been adopted to successfully model a large number of known effects   They include the fan effect of interference for associated information  8  primacy and recency effects for list memory  9  and serial recall  10     ACT-R has been used to model attentive and control processes in a number of cognitive paradigms   These include the Stroop task  11  12  task switching  13  14  the psychological refractory period  15  and multi-tasking  16     A number of researchers have been using ACT-R to model several aspects of natural language understanding and production   They include models of syntactic parsing  17  language understanding  18  language acquisition  19  and metaphor comprehension  20     ACT-R has been used to capture how humans solve complex problems like the Tower of Hanoi  21  or how people solve algebraic equations  22  It has also been used to model human behavior in driving and flying  23     With the integration of perceptual-motor capabilities  ACT-R has become increasingly popular as a modeling tool in human factors and human-computer interaction   In this domain  it has been adopted to model driving behavior under different conditions  24  25  menu selection and visual search on computer application  26  27  and web navigation  28     More recently  ACT-R has been used to predict patterns of brain activation during imaging experiments  29   In this field  ACT-R models have been successfully used to predict prefrontal and parietal activity in memory retrieval  30  anterior cingulate activity for control operations  31  and practice-related changes in brain activity  32     ACT-R has been often adopted as the foundation for cognitive tutors  33  34  These systems use an internal ACT-R model to mimic the behavior of a student and personalize his/her instructions and curriculum  trying to \"guess\" the difficulties that students may have and provide focused help     Such \"Cognitive Tutors\" are being used as a platform for research on learning and cognitive modeling as part of the Pittsburgh Science of Learning Center   Some of the most successful applications  like the Cognitive Tutor for Mathematics  are used in thousands of schools across the United States     ACT-R is the ultimate successor of a series of increasingly precise models of human cognition developed by John R  Anderson     Its roots can be backtraced to the original HAM  Human Associative Memory  model of memory  described by John R  Anderson and Gordon Bower in 1973  35   The HAM model was later expanded into the first version of the ACT theory  36    This was the first time the procedural memory was added to the original declarative memory system  introducing a computational dichotomy that was later proved to hold in human brain  37  The theory was then further extended into the ACT* model of human cognition  38     In the late eighties  Anderson devoted himself to exploring and outlining a mathematical approach to cognition that he named Rational analysis  39   The basic assumption of Rational Analysis is that cognition is optimally adaptive  and precise estimates of cognitive functions mirror statistical properties of the environment  40    Later on  he came back to the development of the ACT theory  using the Rational Analysis as a unifying framework for the underlying calculations   To highlight the importance of the new approach in the shaping of the architecture  its name was modified to ACT-R  with the \"R\" standing for \"Rational\"  41     In 1993  Anderson met with Christian Lebiere  a researcher in connectionist models mostly famous for  developing with Scott Fahlman the Cascade Correlation learning algorithm   Their joint work culminated in the release of ACT-R 4 0  42  Thanks to Mike Byrne  now at Rice University   version 4 0 also included optional perceptual and motor capabilities  mostly inspired from the EPIC architecture  which greatly expanded the possible applications of the theory     After the release of ACT-R 4 0  John Anderson became more and more interested in the underlying neural plausibility of his life-time theory  and began to use brain imaging techniques pursuing his own goal of understanding the computational underpinnings of human mind     The necessity of accounting for brain localization pushed for a major revision of the theory   ACT-R 5 0 introduced the concept of modules  specialized sets of procedural and declarative representations that could be mapped to known brain systems  43   In addition  the interaction between procedural and declarative knowledge was mediated by newly introduced buffers  specialized structures for holding temporarily active information  see the section above    Buffers were thought to reflect cortical activity  and a subsequent series of studies later confirmed that activations in cortical regions could be successfully related to computational operations over buffers     A new version of the code  completely rewritten  was presented in 2005 as ACT-R 6 0  It also included significant improvements in the ACT-R coding language   This included a new mechanism in ACT-R production specification called dynamic pattern matching  xa0 Unlike previous versions which required the pattern matched by a production to include specific slots for the information in the buffers  dynamic pattern matching allows the slots to be matched to also be specified by the buffer contents   A description and motivation for the ACT-R 6 0 is given in Anderson  2007   44     At the 2015 workshop  it was argued that software changes required an increment in the model numbering to ACT-R 7 0   A major software change was removal of the requirement that chunks must be specified based on predefined chunk-types  xa0 The chunk-type mechanism was not removed  but changed from being a required construct of the architecture to being an optional syntactic mechanism in the software  xa0 This allowed for more flexibility in knowledge representation for modeling tasks that require learning novel information and extended the functionality provided through dynamic pattern matching now allowing models to create new \"types\" of chunks  xa0 This also lead to a simplification of the syntax required for specifying the actions in a production because all the actions now have the same syntactic form  xa0 The ACT-R software has also been subsequently updated to include a remote interface based on JSON RPC 1 0  xa0 That interface was added to make it easier to build tasks for models and work with ACT-R from languages other than Lisp  and the tutorial included with the software has been updated to provide Python implementations for all of the example tasks performed by the tutorial models     The long development of the ACT-R theory gave birth to a certain number of parallel and related projects     The most important ones are the PUPS production system  an initial implementation of Anderson  s theory  later abandoned  and ACT-RN  7  a neural network implementation of the theory developed by Christian Lebiere     Lynne M  Reder  also at Carnegie Mellon University  developed in the early nineties SAC  a model of conceptual and perceptual aspects of memory that shares many features with the ACT-R core declarative system  although differing in some assumptions         The Environmental Protection Agency  EPA  is an independent executive agency of the United States federal government tasked with environmental protection matters  3  President Richard Nixon proposed the establishment of EPA on July 9  1970  it began operation on December 2  1970  after Nixon signed an executive order  4  The order establishing the EPA was ratified by committee hearings in the House and Senate  The agency is led by its administrator  who is appointed by the president and approved by the Senate  4  The current Administrator is Michael S  Regan  The EPA is not a Cabinet department  but the administrator is normally given cabinet rank     The EPA has its headquarters in Washington  D C   regional offices for each of the agency  s ten regions  and 27 laboratories  5  The agency conducts environmental assessment  research  and education  It has the responsibility of maintaining and enforcing national standards under a variety of environmental laws  in consultation with state  tribal  and local governments  It delegates some permitting  monitoring  and enforcement responsibility to U S  states and the federally recognized tribes  EPA enforcement powers include fines  sanctions  and other measures  The agency also works with industries and all levels of government in a wide variety of voluntary pollution prevention programs and energy conservation efforts     In 2018  the agency had 13 758 employees  1  More than half of EPA  s employees are engineers  scientists  and environmental protection specialists  other employees include legal  public affairs  financial  and information technologists     Many public health and environmental groups advocate for the agency and believe that it is creating a better world  Other critics believe that the agency commits government overreach by adding unnecessary regulations on business and property owners  6     Beginning in the late 1950s and through the 1960s  Congress reacted to increasing public concern about the impact that human activity could have on the environment  7  8  9  Senator James E  Murray introduced a bill  the Resources and Conservation Act  RCA  of 1959  in the 86th Congress  The 1962 publication of Silent Spring by Rachel Carson alerted the public about the detrimental effects on the environment of the indiscriminate use of pesticides  10     In the years following  similar bills were introduced and hearings were held to discuss the state of the environment and Congress  s potential responses  In 1968  a joint HouseSenate colloquium was convened by the chairmen of the Senate Committee on Interior and Insular Affairs  Senator Henry M  Jackson  and the House Committee on Science and Astronautics  Representative George P  Miller  to discuss the need for and means of implementing a national environmental policy  In the colloquium  some members of Congress expressed a continuing concern over federal agency actions affecting the environment  11     The National Environmental Policy Act of 1969  NEPA  12  was modeled on the Resources and Conservation Act of 1959  RCA   13  RCA would have established a Council on Environmental Quality in the Executive Office of the President  declared a national environmental policy  and required the preparation of an annual environmental report  14  15  16  17     President Nixon signed NEPA into law on January 1  1970  The law created the Council on Environmental Quality  CEQ  in the Executive Office of the President  7  18  NEPA required that a detailed statement of environmental impacts be prepared for all major federal actions significantly affecting the environment  The \"detailed statement\" would ultimately be referred to as an environmental impact statement  EIS   7     On July 9  1970  Nixon proposed an executive reorganization that consolidated many environmental responsibilities of the federal government under one agency  a new Environmental Protection Agency  19  This proposal included merging pollution control programs from a number of departments  such as the combination of pesticide programs from the United States Department of Agriculture and the United States Department of the Interior  20  5 After conducting hearings during that summer  the House and Senate approved the proposal  The EPA was created 90 days before it had to operate  20  11 and officially opened its doors on December 2  1970  The agency  s first administrator  William Ruckelshaus  took the oath of office on December 4  1970  9     EPA  s primary predecessor was the former Environmental Health Divisions of the U S  Public Health Service  PHS   and its creation caused one of a series of reorganizations of PHS that occurred during 19661973   From PHS  EPA absorbed the entire National Air Pollution Control Administration  as well as the Environmental Control Administration  s Bureau of Solid Waste Management  Bureau of Water Hygiene  and part of its Bureau of Radiological Health   It also absorbed the Federal Water Quality Administration  which had previously been transferred from PHS to the Department of the Interior in 1966   A few functions from other agencies were also incorporated into EPA  the formerly independent Federal Radiation Council was merged into it  pesticides programs were transferred from the Department of the Interior  Food and Drug Administration  and Agricultural Research Service  and some functions were transferred from the Council on Environmental Quality and Atomic Energy Commission  21  22     Upon its creation  EPA inherited 84 sites spread across 26 states  of which 42 sites were laboratories   The EPA consolidated these laboratories into 22 sites  23     In its first year  the EPA had a budget of $1 4 xa0billion and 5 800 employees  20  5 At its start  the EPA was primarily a technical assistance agency that set goals and standards  Soon  new acts and amendments passed by Congress gave the agency its regulatory authority  20  9 A major expansion of the Clean Air Act was approved later that month  24     EPA staff recall that in the early days there was \"an enormous sense of purpose and excitement\" and the expectation that \"there was this agency which was going to do something about a problem that clearly was on the minds of a lot of people in this country \" leading to tens of thousands of resumes from those eager to participate in the mighty effort to clean up America  s environment  25     When EPA first began operation  members of the private sector felt strongly that the environmental protection movement was a passing fad  Ruckelshaus stated that he felt pressure to show a public which was deeply skeptical about government  s effectiveness  that EPA could respond effectively to widespread concerns about pollution  26     The burning Cuyahoga River in 1969 had led to a national outcry  In December 1970 a federal grand jury investigation led by U S  Attorney Robert W  Jones began  of water pollution allegedly being caused by about 12 companies in northeastern Ohio  27  It was the first grand jury investigation of water pollution in the area  The attorney general of the United States  John N  Mitchell  held a press conference on December 18  1970  referencing new pollution control litigation  with particular reference to work with the new Environmental Protection Agency  and announcing the filing of a lawsuit that morning against the Jones and Laughlin Steel Corporation for discharging substantial quantities of cyanide into the Cuyahoga River near Cleveland  28  Jones filed the misdemeanor charges in District Court  alleging violations of the 1899 Rivers and Harbors Act  29     Partly based on such litigation experience  Congress enacted the Federal Water Pollution Control Act Amendments of 1972  better known as the Clean Water Act  CWA   30  The CWA established a national framework for addressing water quality  including mandatory pollution control standards  to be implemented by the agency in partnership with the states  31  Congress also amended the Federal Insecticide  Fungicide  and Rodenticide Act  FIFRA  in 1972  requiring EPA to measure every pesticide  s risks against its potential benefits  32  33     Congress passed the Safe Drinking Water Act in 1974  requiring EPA to develop mandatory federal standards for all public water systems  which serve 90% of the US population  The law required EPA to enforce the standards with the cooperation of state agencies  34  35     In October 1976  Congress passed the Toxic Substances Control Act  TSCA  which  like FIFRA  related to the manufacture  labeling and usage of commercial products rather than pollution  36  37  This act gave the EPA the authority to gather information on chemicals and require producers to test them  gave it the ability to regulate chemical production and use  with specific mention of PCBs   and required the agency to create the National Inventory listing of chemicals  37     Congress also enacted the Resource Conservation and Recovery Act  RCRA  in 1976  significantly amending the Solid Waste Disposal Act of 1965  38  It tasked the EPA with setting national goals for waste disposal  conserving energy and natural resources  reducing waste  and ensuring environmentally sound management of waste  Accordingly  the agency developed regulations for solid and hazardous waste that were to be implemented in collaboration with states  39     To manage the agency  s expanding legal mandates and workload  by the end of 1979 the budget grew to about $5 4 billion and the workforce size increased to about 13 000  40     In 1980  following the discovery of many abandoned or mismanaged hazardous waste sites such as Love Canal  Congress passed the Comprehensive Environmental Response  Compensation  and Liability Act  nicknamed Superfund  The new law authorized EPA to cast a wider net for parties responsible for sites contaminated by previous hazardous waste disposal and established a funding mechanism for assessment and cleanup  41     Anne Gorsuch was appointed EPA Administrator in 1981 by President Ronald Reagan  42  Gorsuch based her administration of EPA on the New Federalism approach of downsizing federal agencies by delegating their functions and services to the individual states  43  She believed that EPA was over-regulating business and that the agency was too large and not cost-effective  During her 22 months as agency head  she cut the budget of the EPA by 22%  reduced the number of cases filed against polluters  relaxed Clean Air Act regulations  and facilitated the spraying of restricted-use pesticides  She cut the total number of agency employees  and hired staff from the industries they were supposed to be regulating  44  Environmentalists contended that her policies were designed to placate polluters  and accused her of trying to dismantle the agency  45     Following her mismanagement of the Superfund program  Assistant Administrator Rita Lavelle was fired by Reagan in February 1983  46  Lavelle was later convicted of perjury  47  Gorsuch had increasing confrontations with Congress over Superfund and other programs  including her refusal to submit subpoeaned documents  Gorsuch  who had recently remarried  becoming Anne Gorsuch Burford  resigned in March 1983  followed by resignations of her Deputy Administrator and most of her Assistant Administrators  46  48  49   See Fiscal mismanagement  1983   Reagan then appointed William Ruckelshaus as EPA Administrator for a second term  Lee M  Thomas succeeded Ruckelshaus as Administrator in 1985  42     In April 1986  when the Chernobyl disaster occurred in Ukraine  the EPA was tasked with identifying any impacts on the United States and keeping the public informed  Administrator Lee Thomas assembled an interagency team  including personnel from the Nuclear Regulatory Commission  National Oceanic and Atmospheric Administration  and the Department of Energy to monitor the situation  They held press conferences for 10 days  50  9 That same year Congress passed the Emergency Planning and Community Right-to-Know Act  which authorized the EPA to gather data on toxic chemicals and share this information with the public  37     EPA also researched the implications of stratospheric ozone depletion  Under Administrator Thomas  EPA joined with several international organizations to perform a risk assessment of stratospheric ozone  which helped provide motivation for the Montreal Protocol  which was agreed to in August 1987  50  14    In 1988  during his first presidential campaign  George H  W  Bush was vocal about environmental issues  Following his election victory  he appointed William K  Reilly  an environmentalist  as EPA Administrator  Under Reilly  s leadership  the EPA implemented voluntary programs and initiated the development of a \"cluster rule\" for multimedia regulation of the pulp and paper industry  51  At the time  the environment was increasingly being recognized as a regional issue  which was reflected in 1990 amendment of the Clean Air Act and new approaches by the agency  52  53     In 1992 EPA and the Department of Energy launched the Energy Star program  a voluntary program that fosters energy efficiency  54     Carol Browner was appointed EPA Administrator by President Bill Clinton and served from 1993 to 2001  55     Since the passage of the Superfund law in 1980  a special tax had been levied on the chemical and petroleum industries  to support the cleanup trust fund  Congressional authorization of the tax was due to expire in 1995  Although Browner and the Clinton Administration supported continuation of the tax  Congress declined to reauthorize it  Subsequently  the Superfund program has been supported only by annual appropriations  greatly reducing the number of waste sites that are remediated in a given year  56     Major legislative updates during the Clinton Administration were the Food Quality Protection Act 57  and the 1996 amendments to the Safe Drinking Water Act  58     The EPA is led by the administrator  appointed following nomination by the president and approval from Congress  Michael S  Regan began serving as Administrator on March 11  2021  59     Creating 10 EPA regions was an initiative that came from President Richard Nixon  75  See Standard Federal Regions    \"Each EPA regional office is responsible within its states for implementing the agency s programs  except those programs that have been specifically delegated to states  n\"   Each regional office also implements programs on Indian Tribal lands  except those programs delegated to tribal authorities     The Environmental Protection Agency can only act pursuant to statutesthe laws passed by Congress  Appropriations statutes authorize how much money the agency can spend each year to carry out the approved statutes  The agency has the power to issue regulations  A regulation interprets a statute  and EPA applies its regulations to various environmental situations and enforces the requirements  The agency must include a rationale of why a regulation is needed   See Administrative Procedure Act   Regulations can be challenged in federal courts  either district court or appellate court  depending on the particular statutory provision  77     EPA has principal implementation authority for the following federal environmental laws     There are additional laws where EPA has a contributing role or provides assistance to other agencies  Among these laws are     EPA established its major programs pursuant to the primary missions originally articulated in the laws passed by Congress  Additional programs have been developed to interpret the primary missions  Some of the newer programs have been specifically authorized by Congress  78     Former Administrator William Ruckelshaus observed in 2016 that a danger for EPA was that air  water  waste and other programs would be unconnected  placed in \"silos \" a problem that persists more than 50 years later  albeit less so than at the start  79     The Radiation Protection Program comprises seven project groups  85     In an EPA Enforcement report submitted by the Environmental Data & Governance Initiative  EDGI  it compared EPA statistics over time  113  The number of civil cases have gradually decreased and  in 2018  the criminal and civil penalties from EPA claims dropped over four times their amounts in 2013  2016  and 2017  114  In 2016  an amount of $6 307 833 117 of penalties were administered through EPA violations  115  In 2018  an amount of $184 768 000 of penalties were administered  116  Furthermore  federal inspection and evaluations conducted by the EPA have steadily decreased from 2015-2018  116  EPA Enforcement has decreased partially due to budget cuts within the Environmental Protection Agency  117     In 1982 Congress charged that EPA had mishandled the $1 6 xa0billion Superfund program and demanded records from EPA Administrator Anne Gorsuch  She refused and became the first agency director in U S  history to be cited for contempt of Congress  EPA turned the documents over to Congress several months later  after the White House abandoned its court claim that the documents could not be subpoenaed by Congress because they were covered by executive privilege  Six congressional committees were investigating the Superfund program  and the Federal Bureau of Investigation was exploring whether documents had been destroyed  50  4 Gorsuch resigned her post in 1983  citing pressures caused by the media and the congressional investigation  137  Critics charged that the EPA was in \"a shambles\" at that time  138     TSCA enables the EPA to require industry to conduct testing of chemicals  but the agency must balance such requirements with obligations to provide information to the public and ensure the protection of trade secrets and confidential business information  the legal term for proprietary information   Arising issues and problems from these overlapping obligations have been the subject of multiple critical reports by the Government Accountability Office  How much information the agency should have access to from industry  how much it should keep confidential  and how much it should reveal to the public is still contested  For example  according to TSCA  state officials are not allowed access to confidential business information collected by the EPA  37     In April 2008  the Union of Concerned Scientists said that more than half of the nearly 1 600 EPA staff scientists who responded online to a detailed questionnaire reported they had experienced incidents of political interference in their work  The survey included chemists  toxicologists  engineers  geologists and experts in other fields of science  About 40% of the scientists reported that the interference had been more prevalent in the last five years than in previous years  The highest number of complaints came from scientists who were involved in determining the risks of cancer by chemicals used in food and other aspects of everyday life  139     EPA research has also been suppressed by career managers  140  Supervisors at EPA  s National Center for Environmental Assessment required several paragraphs to be deleted from a peer-reviewed journal article about EPA  s integrated risk information system  which led two co-authors to have their names removed from the publication  and the corresponding author  Ching-Hung Hsu  to leave EPA \"because of the draconian restrictions placed on publishing\"  141  EPA subjects employees who author scientific papers to prior restraint  even if those papers are written on personal time  142     EPA employees have reported difficulty in conducting and reporting the results of studies on hydraulic fracturing due to industry 143  144  145  and governmental pressure  and are concerned about the censorship of environmental reports  143  146  147     In February 2017  U S  representative Matt Gaetz  R-Fla   sponsored H R  861  a bill 148  to abolish the EPA by 2018  According to Gaetz  \"The American people are drowning in rules and regulation promulgated by unelected bureaucrats  And the Environmental Protection Agency has become an extraordinary offender \" The bill was co-sponsored by Thomas Massie  R-Ky    Steven Palazzo  R-Ms   and Barry Loudermilk  R-Ga    149     In July 2005  an EPA report showing that auto companies were using loopholes to produce less fuel-efficient cars was delayed  The report was supposed to be released the day before a controversial energy bill was passed and would have provided backup for those opposed to it  but the EPA delayed its release at the last minute  150     In 2007  the state of California sued the EPA for its refusal to allow California and 16 other states to raise fuel economy standards for new cars  151  EPA Administrator Stephen L  Johnson claimed that the EPA was working on its own standards  but the move has been widely considered an attempt to shield the auto industry from environmental regulation by setting lower standards at the federal level  which would then preempt state laws  152  153  154  California governor Arnold Schwarzenegger  along with governors from 13 other states  stated that the EPA  s actions ignored federal law  and that existing California standards  adopted by many states in addition to California  were almost twice as effective as the proposed federal standards  155  It was reported that Stephen Johnson ignored his own staff in making this decision  156     After the federal government had bailed out General Motors and Chrysler in the Automotive industry crisis of 20082010  the 2010 Chevrolet Equinox was released with an EPA fuel economy rating abnormally higher than its competitors  Independent road tests 157  158  159  160  found that the vehicle did not out-perform its competitors  which had much lower fuel economy ratings  Later road tests found better  but inconclusive  results  161  162     In March 2005  nine states  California  New York  New Jersey  New Hampshire  Massachusetts  Maine  Connecticut  New Mexico and Vermont  sued the EPA  The EPA  s inspector general had determined that the EPA  s regulation of mercury emissions did not follow the Clean Air Act  and that the regulations were influenced by top political appointees  163  164  The EPA had suppressed a study it commissioned by Harvard University which contradicted its position on mercury controls  165  The suit alleged that the EPA  s rule exempting coal-fired power plants from \"maximum available control technology\" was illegal  and additionally charged that the EPA  s system of cap-and-trade to lower average mercury levels would allow power plants to forego reducing mercury emissions  which they objected would lead to dangerous local hotspots of mercury contamination even if average levels declined  166  Several states also began to enact their own mercury emission regulations  Illinois  s proposed rule would have reduced mercury emissions from power plants by an average of 90% by 2009  167  In 2008by which point a total of fourteen states had joined the suitthe U S  Court of Appeals for the District of Columbia ruled that the EPA regulations violated the Clean Air Act  168     In response  EPA announced plans to propose such standards to replace the vacated Clean Air Mercury Rule  and did so on March 16  2011  169     In December 2007  EPA Administrator Stephen L  Johnson approved a draft of a document that declared that climate change imperiled the public welfarea decision that would trigger the first national mandatory global-warming regulations  Associate Deputy Administrator Jason Burnett e-mailed the draft to the White House  White House aideswho had long resisted mandatory regulations as a way to address climate changeknew the gist of what Johnson  s finding would be  Burnett said  They also knew that once they opened the attachment  it would become a public record  making it controversial and difficult to rescind  So they did not open it  rather  they called Johnson and asked him to take back the draft  Johnson rescinded the draft  in July 2008  he issued a new version which did not state that global warming was danger to public welfare  Burnett resigned in protest  170     A $3 xa0million mapping study on sea level rise was suppressed by EPA management during both the Bush and Obama administrations  and managers changed a key interagency report to reflect the removal of the maps  171     On April 28  2017  multiple climate change subdomains at EPA gov began redirecting to a notice stating \"this page is being updated \" 172  The EPA issued a statement announcing the overhaul of its website to \"reflect the agency  s new direction under President Donald Trump and Administrator Scott Pruitt \" 173  The removed EPA climate change domains included extensive information on the EPA  s work to mitigate climate change  as well as details of data collection efforts and indicators for climate change  174     In August 2015  the 2015 Gold King Mine waste water spill occurred when EPA contractors examined the level of pollutants such as lead and arsenic in a Colorado mine  175  and accidentally released over three million gallons of waste water into Cement Creek and the Animas River  176     In 2015  the International Agency for Research on Cancer  IARC   a branch of the World Health Organization  cited research linking glyphosate  an ingredient of the weed killer Roundup manufactured by the chemical company Monsanto  to non-Hodgkin  s lymphoma   In March 2017  the presiding judge in a litigation brought about by people who claim to have developed glyphosate-related non-Hodgkin  s lymphoma opened Monsanto emails and other documents related to the case  including email exchanges between the company and federal regulators   According to an article in The New York Times  the \"records suggested that Monsanto had ghostwritten research that was later attributed to academics and indicated that a senior official at the Environmental Protection Agency had worked to quash a review of Roundups main ingredient  glyphosate  that was to have been conducted by the United States Department of Health and Human Services \"  The records show that Monsanto was able to prepare \"a public relations assault\" on the finding after they were alerted to the determination by Jess Rowland  the head of the EPA  s cancer assessment review committee at that time   months in advance   Emails also showed that Rowland \"had promised to beat back an effort by the Department of Health and Human Services to conduct its own review \" 177  178  179     On February 17  2017  Scott Pruitt was appointed administrator by President Donald Trump  The Democratic Party saw the appointment as a controversial move  as Pruitt had spent most of his career challenging environmental regulations and policies  He did not have previous experience in the environmental protection field and had received financial support from the fossil fuel industry  180  In 2017  the Trump administration proposed a 31% cut to the EPA  s budget to $5 7 xa0billion from $8 1 xa0billion and to eliminate a quarter of the agency jobs  181  However  this cut was not approved by Congress  53     Pruitt resigned from the position on July 5  2018  citing \"unrelenting attacks\" due to ongoing ethics controversies  182     The EPA has been criticized for its lack of progress towards environmental justice  Administrator Christine Todd Whitman was criticized for her changes to President Bill Clinton  s Executive Order 12898 during 2001  removing the requirements for government agencies to take the poor and minority populations into special consideration when making changes to environmental legislation  and therefore defeating the spirit of the Executive Order  183  In a March 2004 report  the inspector general of the agency concluded that the EPA \"has not developed a clear vision or a comprehensive strategic plan  and has not established values  goals  expectations  and performance measurements\" for environmental justice in its daily operations  Another report in September 2006 found the agency still had failed to review the success of its programs  policies and activities towards environmental justice  184  Studies have also found that poor and minority populations were underserved by the EPA  s Superfund program  and that this situation was worsening  183    \"Many environmental justice issues are local  and therefore difficult to address by a federal agency  such as the EPA  Without strong media attention  political interest  or  crisis  status  local issues are less likely to be addressed at the federal level compared to larger  well publicized incidents  n\"   Conflicting political powers in successive administrations  The White House maintains direct control over the EPA  and its enforcement actions are subject to the political agenda of who is in power  Republicans and Democrats differ in their approaches to environmental justice  While President Bill Clinton signed Executive Order 12898  the Bush administration did not develop a clear plan or establish goals for integrating environmental justice into everyday practices  affecting the motivation for environmental enforcement  185  page xa0needed     The EPA is responsible for preventing and detecting environmental crimes  informing the public of environmental enforcement  and setting and monitoring standards of air pollution  water pollution  hazardous wastes and chemicals  \"It is difficult to construct a specific mission statement given its wide range of responsibilities \" 186  page xa0needed  It is impossible to address every environmental crime adequately or efficiently if there is no specific mission statement to refer to  The EPA answers to various groups  competes for resources  and confronts a wide array of harms to the environment  All of these present challenges  including a lack of resources  its self-policing policy  and a broadly defined legislation that creates too much discretion for EPA officers  187  page xa0needed     The EPA \"does not have the authority or resources to address injustices without an increase in federal mandates\" requiring private industries to consider the environmental ramifications of their activities  188     The Environmental Protection Agency  EPA  has developed a multitude of laws such as the Clean Air Act  CAA   189  the Resource Conservation and Recovery Act  RCRA   190  and the Comprehensive Environmental Response  Compensation  and Liability Act  CERCLA   191  which all attempt to prevent and reconcile environmental damages  The EPA  starting in 2018 under Andrew Wheeler  has redeveloped pollution standards resulting in less overall regulation  192  Furthermore  the CAA  s discretionary application 193  194  has caused a varied application of the law within Louisiana  In 1970  Louisiana deployed the Comprehensive Toxic Air Pollutant Emission Control Program to satisfy the Federal Act  195  This program does not require monitoring that is equivalent to other states  196     In 2021  President Joe Biden selected Michael Regan to serve as the EPA Chief  Michael Regan claimed that he looked to push aggressively on key environmental issues  which starkly contrasts Andrew Wheeler  s official EPA policy from 2018  During Michael Regan  s Senate Confirmation hearing  Senator Cory Booker specifically mentioned Cancer Alley in St  John  s Parish as a place where there is harm being done to low income communities of color  197     In the latest Center for Effective Government analysis of 15 federal agencies which receive the most Freedom of Information Act FOIA requests  published in 2015  using 2012 and 2013 data  the most recent years available   the EPA earned a D by scoring 67 out of a possible 100 points  i e  did not earn a satisfactory overall grade  198     On July 17  2019  the top scientific integrity official from the EPA  Francesca Grifo  was not permitted to testify by the EPA in front of a House committee hearing  The EPA offered to send a different representative in place of Grifo and accused the committee of \"dictating to the agency who they believe was qualified to speak \" The hearing was to discuss the importance of allowing federal scientists and other employees to speak freely when and to whom they want to about their research without having to worry about any political consequences  199     The DSSAM Model  Dynamic Stream Simulation and Assessment Model  is a computer simulation developed for the Truckee River to analyze water quality impacts from land use and wastewater management decisions in the Truckee River Basin  This area includes the cities of Reno and Sparks  Nevada as well as the Lake Tahoe Basin  The model is historically and alternatively called the Earth Metrics Truckee River Model   Since original development in 1984-1986 under contract to the U S  Environmental Protection Agency  EPA   1  the model has been refined and successive versions have been dubbed DSSAM II and DSSAM III   This hydrology transport model is based upon a pollutant loading metric called Total maximum daily load  TMDL   The success of this flagship model contributed to the Agencys broadened commitment to the use of the underlying TMDL protocol in its national policy for management of most river systems in the United States  2     The Truckee River has a length of over 115 miles  185 xa0km  and drains an area of approximately 3120 square miles  3  not counting the extent of its Lake Tahoe sub-basin  The DSSAM model establishes numerous stations along the entire river extent as well as a considerable number of monitoring points inside the  Great Basin  s Pyramid Lake  the receiving waters of this closed hydrological system   Although the region is sparsely populated  it is important because Lake Tahoe is visited by 20 million persons per annum and Truckee River water quality affects at least two endangered species  the Cui-ui sucker fish and the Lahontan cutthroat trout     Impetus to derive a quantitative prediction model arose from a trend of historically decreasing river flow rates coupled with jurisdictional and tribal conflicts over water rights as well as concern for river biota   When expansion of the Reno-Sparks Wastewater Treatment Plant was proposed  the EPA decided to fund a large scale research effort to create  simulation software and a parallel program to collect field data in the Truckee River and Pyramid Lake   For river stations water quality measurements were made in the benthic zone as well as the topic zone  in the case of Pyramid Lake boats were used to collect grab samples at varying depths and locations   Earth Metrics conducted the software development for the first generation computer model and collected field data on water quality and flow rates in the Truckee River   After model calibration  runs were made to evaluate impacts of alternative land use controls and discharge parameters for treated effluent      The DSSAM Model is constructed to allow dynamic decay of most pollutants  for example  total nitrogen and phosphorus are allowed to be consumed by benthic algae in each time step  and the algal communities are given a separate population dynamic in each river reach  e g metabolic rate based upon river temperature    Sources throughout the watershed include non-point agricultural and urban stormwater as well as a multiplicity of point source discharges of treated municipal wastewater effluent     Subsequent to the first generation of DSSAM  model development  calibration and application  later refinements were made   These augmentations to model functionality focussed on increased flexibility in modeling the diel cycle and also allowed inclusion of analyzing particulate nitrogen and phosphorus   In developing DSSAM III several changes in the model operation and scope were performed  4     Numerous different uses of the model have been made including  a analysis of public policies for urban stormwater runoff   b  researching agricultural methods for surface runoff minimization   c  innovative solutions for non-point source control and d engineering aspects of treated wastewater discharge   Regarding stormwater runoff in Washoe County  the specific elements within a new xeriscape ordinance were analyzed for efficacy using the model   For the varied agricultural uses in the watershed  the model was run to understand the principal sources of adverse impact  and management practices were developed to reduce in river pollution  Use of the model has specifically been conducted to analyze survival of two endangered species found in the Truckee River and Pyramid Lake  the Cui-ui sucker fish  endangered 1967  and the Lahontan cutthroat trout  threatened 1970    When the model is used for surface runoff reaching a stream  this pollutant input can be viewed as a line source  e g   a continuous linear source of pollution entering the waterway      The United States Environmental Protection Agency  EPA  Storm Water Management Model  SWMM  1  2  3  4  5  6  7  is a dynamic rainfallrunoffsubsurface runoff simulation model used for single-event to long-term  continuous  simulation of the surface/subsurface hydrology quantity and quality from primarily urban/suburban areas  It can simulate the Rainfall- runoff  runoff  evaporation  infiltration and groundwater connection for roots  streets  grassed areas  rain gardens and ditches and pipes  for example   The hydrology component of SWMM operates on a collection of subcatchment areas divided into impervious and pervious areas with and without depression storage to predict runoff  and pollutant loads from precipitation  evaporation and infiltration losses from each of the subcatchment   Besides  low impact development  LID  and best management practice areas on the subcatchment can be modeled to reduce the impervious and pervious runoff   The routing or hydraulics section of SWMM transports this water and possible associated water quality constituents through a system of closed pipes  open channels  storage/treatment devices  ponds  storages  pumps  orifices  weirs  outlets  outfalls and other regulators  SWMM tracks the quantity and quality of the flow generated within each subcatchment  and the flow rate  flow depth  and quality of water in each pipe and channel during a simulation period composed of multiple fixed or variable time steps  The water quality constituents such as water quality constituents can be simulated from buildup on the subcatchments through washoff to a hydraulic network with optional first order decay and linked pollutant removal  best management practice and low-impact development  LID  removal and treatment can be simulated at selected storage nodes  SWMM is one of the hydrology transport models which the EPA and other agencies have applied widely throughout North America and through consultants and universities throughout the world   The latest update notes and new features can be found on the EPA website in the download section   Recently added in November 2015 were the EPA SWMM 5 1 Hydrology Manual  Volume I  and in 2016 the EPA SWMM 5 1 Hydraulic Manual  Volume II  and EPA SWMM 5 1 Water Quality  including LID Modules  Volume  III  + Errata    The EPA storm water management model  SWMM  is a dynamic rainfall-runoff-routing simulation model used for single event or long-term  continuous  simulation of runoff quantity and quality from primarily urban areas  The runoff component of SWMM operates on a collection of subcatchment areas that receive precipitation and generate runoff and pollutant loads  The routing portion of SWMM transports this runoff through a system of pipes  channels  storage/treatment devices  pumps  and regulators  SWMM tracks the quantity and quality of runoff generated within each subcatchment  and the flow rate  flow depth  and quality of water in each pipe and channel during a simulation period divided into multiple time steps     SWMM accounts for various hydrologic processes that produce runoff from urban areas  These include     SWMM also contains a flexible set of hydraulic modeling capabilities used to route runoff and external inflows through the drainage system network of pipes  channels  storage/treatment units and diversion structures  These include the ability to     Spatial variability in all of these processes is achieved by dividing a study area into a collection of smaller  homogeneous subcatchment areas  each containing its own fraction of pervious and impervious sub-areas  Overland flow can be routed between sub-areas  between subcatchments  or between entry points of a drainage system     Since its inception  SWMM has been used in thousands of sewer and stormwater studies throughout the world  Typical applications include     EPA SWMM is public domain software that may be freely copied and distributed   The SWMM 5 public domain consists of C engine code and Delphi SWMM 5 graphical user interface code  The C code and Delphi code are easily edited and can be recompiled by students and professionals for custom features or extra output features     SWMM was first developed between 19691971 and has undergone four major upgrades since those years  The major upgrades were   1  Version 2 in 1973-1975   2  Version 3 in 1979-1981   3  Version 4 in 1985-1988 and  4  Version 5 in 2001-2004  A list of the major changes and post-2004 changes are shown in Table 1  The current SWMM edition  Version 5/5 1 012  is a complete re-write of the previous Fortran releases in the programming language C  and it can be run under Windows XP  Windows Vista  Windows 7  Windows 8  Windows 10 and also with a recompilation under Unix   The code for SWMM5 is open source and public domain code that can be downloaded from the EPA Web Site     EPA SWMM 5 provides an integrated graphical environment for editing watershed input data  running hydrologic  hydraulic  real time control and water quality simulations  and viewing the results in a variety of graphical formats  These include color-coded thematic drainage area maps  time series graphs and tables  profile plots  scatter plots and statistical frequency analyses     The last rewrite of EPA SWMM was produced by the Water Supply and Water Resources Division of the U S  Environmental Protection Agency  s National Risk Management Research Laboratory with assistance from the consulting firm of CDM Inc under a Cooperative Research and Development Agreement  CRADA   SWMM 5 is used as the computational engine for many modeling packages plus components of SWMM5 are in other modeling packages   The major modeling packages that use all or some of the SWMM5 components are shown in the Vendor section  The update history of SWMM 5 from the original SWMM 5 0 001 to the current version SWMM 5 1 012 can be found at the EPA Download in the file epaswmm5_updates txt   SWMM 5 was approved FEMA Model Approval Page in May 2005 with this note about the versions that are approved on the FEMA Approval Page SWMM 5 Version 5 0 005  May 2005  and up for NFIP modeling   SWMM 5 is used as the computational engine for many modeling packages  see the SWMM 5 Platform Section of this article  and some components of SWMM5 are in other modeling packages  see the SWMM 5 Vendor Section of this article      SWMM conceptualizes a drainage system as a series of water and material flows between several major environmental compartments  These compartments and the SWMM objects they contain include     The Atmosphere compartment  from which precipitation falls and pollutants are deposited onto the land surface compartment  SWMM uses Rain Gage objects to represent rainfall inputs to the system   The rain gage objects can use time series  external text files or NOAA rainfall data files   The Rain Gage objects can use precipitation for thousands of years  Using the SWMM-CAT Addon to SWMM5 climate change can now be simulated using modified temperature  evaporation or rainfall     The Land Surface compartment  which is represented by one or more Subcatchment objects  It receives precipitation from the Atmospheric compartment in the form of rain or snow  it sends outflow in the form of infiltration to the Groundwater compartment and also as surface runoff and pollutant loadings to the Transport compartment   The Low Impact Development  LID  controls are part of the Subcatchments and store  infiltrate or evaporate the runoff     The Groundwater compartment receives Infiltration  hydrology  from the Land Surface compartment and transfers a portion of this inflow to the Transport compartment  This compartment is modeled using Aquifer objects   The connection to the Transport compartment can be either a static boundary or a dynamic depth in the channels  The links in the Transport compartment now also have seepage and evaporation     The Transport compartment contains a network of conveyance elements  channels  pipes  pumps  and regulators  and storage/treatment units that transport water to outfalls or to treatment facilities  Inflows to this compartment can come from surface runoff  groundwater interflow  sanitary dry weather flow  or from user-defined hydrographs  The components of the Transport compartment are modeled with Node and Link objects     Not all compartments need to appear in a particular SWMM model  For example  one could model just the transport compartment  using pre-defined hydrographs as inputs  If kinematic wave routing is used  then the nodes do not need to contain an outfall     The simulated model parameters for subcatchments are surface roughness  depression storage  slope  flow path length  for Infiltration  Horton  max/min rates and decay constant  Green-Ampt  hydraulic conductivity  initial moisture deficit and suction head  Curve Number  NRCS  SCS  Curve number  All  time for saturated soil to fully drain   for Conduits  Mannings roughness  for Water Quality  buildup/washoff function coefficients  first-order decay coefficients  removal equations   A study area can be divided into any number of individual subcatchments  each of which drains to a single point  Study areas can range in size from a small portion of a single lot up to thousands of acres  SWMM uses hourly or more frequent rainfall data as input and can be run for single events or in a continuous fashion for any number of years     SWMM 5 accounts for various hydrologic processes that produce surface and subsurface runoff from urban areas  These include     Spatial variability in all of these processes is achieved by dividing a study area into a collection of smaller  homogeneous watershed or subcatchment areas  each containing its fraction of pervious and impervious sub-areas  Overland flow can be routed between sub-areas  between subcatchments  or between entry points of a drainage system     SWMM also contains a flexible set of hydraulic modeling capabilities used to route runoff and external inflows through the drainage system network of pipes  channels  storage/treatment units and diversion structures  These include the ability to     Infiltration is the process of rainfall penetrating the ground surface into the unsaturated soil zone of pervious subcatchments areas  SWMM5 offers four choices for modeling infiltration     Classical infiltration method    This method is based on empirical observations showing that infiltration decreases exponentially from an initial maximum rate to some minimum rate over the course of a long rainfall event  Input parameters required by this method include the maximum and minimum infiltration rates  a decay coefficient that describes how fast the rate decreases over time  and the time it takes a fully saturated soil to completely dry  used to compute the recovery of infiltration rate during dry periods      Modified Horton Method    This is a modified version of the classical Horton Method that uses the cumulative infiltration in excess of the minimum rate as its state variable  instead of time along the Horton curve   providing a more accurate infiltration estimate when low rainfall intensities occur  It uses the same input parameters as does the traditional Horton Method     GreenAmpt method   \"This method for modeling infiltration assumes that a sharp wetting front exists in the soil column  separating soil with some initial moisture content below from saturated soil above  The input parameters required are the initial moisture deficit of the soil  the soil s hydraulic conductivity  and the suction head at the wetting front  The recovery rate of moisture deficit during dry periods is empirically related to the hydraulic conductivity  n\"   Curve number method   \"This approach is adopted from the NRCS  SCS  curve number method for estimating runoff  It assumes that the total infiltration capacity of a soil can be found from the soil s tabulated curve number  During a rain event this capacity is depleted as a function of cumulative rainfall and remaining capacity  The input parameters for this method are the curve number and the time it takes a fully saturated soil to completely dry  used to compute the recovery of infiltration capacity during dry periods   n\"  \"SWMM also allows the infiltration recovery rate to be adjusted by a fixed amount on a monthly basis to account for seasonal variation in such factors as evaporation rates and groundwater levels  This optional monthly soil recovery pattern is specified as part of a project s evaporation data  n\"   In addition to modeling the generation and transport of runoff flows  SWMM can also estimate the production of pollutant loads associated with this runoff  The following processes can be modeled for any number of user-defined water quality constituents     Rain Gages in SWMM5 supply precipitation data for one or more subcatchment areas in a study region  The rainfall data can be either a user-defined time series or come from an external file  Several different popular rainfall file formats currently in use are supported  as well as a standard user-defined format  nThe principal input properties of rain gages include     The other principal input parameters for the subcatchments include     Steady-flow routing represents the simplest type of routing possible  actually no routing  by assuming that within each computational time step flow is uniform and steady  Thus it simply translates inflow hydrographs at the upstream end of the conduit to the downstream end  with no delay or change in shape  The normal flow equation is used to relate flow rate to flow area  or depth      This type of routing cannot account for channel storage  backwater effects  entrance/exit losses  flow reversal or pressurized flow  It can only be used with dendritic conveyance networks  where each node has only a single outflow link  unless the node is a divider in which case two outflow links are required   This form of routing is insensitive to the time step employed and is really only appropriate for preliminary analysis using long-term continuous simulations  nKinematic wave routing solves the continuity equation along with a simplified form of the momentum equation in each conduit  The latter requires that the slope of the water surface equal the slope of the conduit     The maximum flow that can be conveyed through a conduit is the full normal flow value  Any flow in excess of this entering the inlet node is either lost from the system or can pond atop the inlet node and be re-introduced into the conduit as capacity becomes available     Kinematic wave routing allows flow and area to vary both spatially and temporally within a conduit  This can result in attenuated and delayed outflow hydrographs as inflow is routed through the channel  However this form of routing cannot account for backwater effects  entrance/exit losses  flow reversal  or pressurized flow  and is also restricted to dendritic network layouts  It can usually maintain numerical stability with moderately large time steps  on the order of 1 to 5 minutes  If the aforementioned effects are not expected to be significant then this alternative can be an accurate and efficient routing method  especially for long-term simulations     Dynamic wave routing solves the complete one-dimensional Saint Venant flow equations and therefore produces the most theoretically accurate results  These equations consist of the continuity and momentum equations for conduits and a volume continuity equation at nodes     With this form of routing it is possible to represent pressurized flow when a closed conduit becomes full  such that flows can exceed the full normal flow value  Flooding occurs when the water depth at a node exceeds the maximum available depth  and the excess flow is either lost from the system or can pond atop the node and re-enter the drainage system     Dynamic wave routing can account for channel storage  backwater  entrance/exit losses  flow reversal  and pressurized flow  Because it couples together the solution for both water levels at nodes and flow in conduits it can be applied to any general network layout  even those containing multiple downstream diversions and loops  It is the method of choice for systems subjected to significant backwater effects due to downstream flow restrictions and with flow regulation via weirs and orifices  This generality comes at a price of having to use much smaller time steps  on the order of a minute or less  SWMM can automatically reduce the user-defined maximum time step as needed to maintain numerical stability      One of the great advances in SWMM 5 was the integration of urban/suburban subsurface flow with the hydraulic computations of the drainage network   This advance is a tremendous improvement over the separate subsurface hydrologic and hydraulic computations of the previous versions of SWMM because it allows the modeler to conceptually model the same interactions that occur physically in the real open channel/shallow aquifer environment   The SWMM 5 numerical engine calculates the surface runoff  subsurface hydrology and assigns the current climate data at either the wet or dry hydrologic time step   The hydraulic calculations for the links  nodes  control rules and boundary conditions of the network are then computed at either a fixed or variable time step within the hydrologic time step by using interpolation routines and the simulated hydrologic starting and ending values  The versions of SWMM 5 greater than SWMM 5 1 007 allow the modeler to simulate climate changes by globally changing the rainfall  temperature  and evaporation using monthly adjustments     An example of this integration was the collection of the different SWMM 4 link types in the runoff  transport and Extran blocks to one unified group of closed conduit and open channel link types in SWMM 5 and a collection of node types  Figure 2       nSWMM contains a flexible set of hydraulic modeling capabilities used to route runoff and external inflows through the drainage system network of pipes  channels  storage/treatment units  and diversion structures  These include the ability to do the following     Handle drainage networks of unlimited size  nUse a wide variety of standard closed and open conduit shapes as well as natural channels  nModel special elements  such as storage/treatment units  flow dividers  pumps  weirs  and orifices  nApply external flows and water quality inputs from surface runoff  groundwater interflow  rainfall-dependent infiltration/inflow  dry weather sanitary flow  and user-defined inflows  nUtilize either kinematic wave or full dynamic wave flow routing methods  nModel various flow regimes  such as backwater  surcharging  reverse flow  and surface ponding  apply user-defined dynamic control rules to simulate the operation of pumps  orifice openings  and weir crest levels  nPercolation of infiltrated water into groundwater layers  nInterflow between groundwater and the drainage system  nNonlinear reservoir routing of overland flow  Runoff reduction via LID controls  8     The low-impact development  LID  function was new to SWMM 5 0 019/20/21/22 and SWMM 5 1+  It is integrated within the subcatchment and allows further refinement of the overflows  infiltration flow and evaporation in rain barrel  swales  permeable paving  green roof  rain garden  bioretention and infiltration trench  The term Low-impact development  Canada/US  is used in Canada and the United States to describe a land planning and engineering design approach to managing stormwater runoff  In recent years many states in the US have adopted LID concepts and standards to enhance their approach to reducing the harmful potential for storm water pollution in new construction projects  LID takes many forms but can generally be thought of as an effort to minimize or prevent concentrated flows of storm water leaving a site  To do this the LID practice suggests that when impervious surfaces  concrete  etc   are used  they are periodically interrupted by pervious areas which can allow the storm water to infiltrate  soak into the earth     A variety of sub-processes in each LID can be defined in SWMM5 such as  surface  pavement  soil  storage  drainmat and drain     Each type of LID has limitations on the type of sub process allowed by SWMM 5  It has a good report feature and a LID summary report can be in the rpt file and an external report file in which the surface depth can be seen  soil moisture  storage depth  surface inflow  evaporation  surface infiltration  soil percolation  storage infiltration  surface outflow and the LID continuity error  There can be multiple LID  s per subcatchment and no issues have been had because of having many complicated LID sub networks and processes inside the Subcatchments of SWMM 5 or any continuity issues not solvable by a smaller wet hydrology time step  The types of SWMM 5 LID compartments are   storage  underdrain  surface  pavement and soil  a bio retention cell has storage  underdrain and surface compartments   an infiltration trench lid has storage  underdrain and surface compartments   A porous pavement LID has storage  underdrain and pavement compartments  A rain barrel has only storage and underdrain compartments and a vegetative swale LID has a single surface compartment  Each type of LID shares different underlying compartment objects in SWMM 5 which are called layers     This set of equations can be solved numerically at each runoff time step to determine how an inflow hydrograph to the LID unit is converted into some combination of runoff hydrograph  sub-surface storage  sub-surface drainage  and infiltration into the surrounding native soil  In addition to Street Planters and Green Roofs  the bio-retention model just described can be used to represent Rain Gardens by eliminating the storage layer and also Porous Pavement systems by replacing the soil layer with a pavement layer     The surface layer of the LID receives both direct rainfall and runon from other areas  It loses water through infiltration into the soil layer below it  by evapotranspiration  ET  of any water stored in depression storage and vegetative capture  and by any surface runoff that might occur  The soil layer contains an amended soil mix that can support vegetative growth  It receives infiltration from the surface layer and loses water through ET and by percolation into the storage layer below it  The storage layer consists of coarse crushed stone or gravel  It receives percolation from the soil zone above it and loses water by either infiltration into the underlying natural soil or by outflow through a perforated pipe underdrain system     New as of July 2013  the EPA  s  National Stormwater Calculator is a Windows desktop application that estimates the annual amount of rainwater and frequency of runoff from a specific site anywhere in the United States  Estimates are based on local soil conditions  land cover  and historic rainfall records  The calculator accesses several national databases that provide soil  topography  rainfall  and evaporation information for the chosen site  The user supplies information about the site  s land cover and selects the types of low impact development  LID  controls they would like to use on site   The LID Control features in SWMM 5 1 013 include the following among types of Green infrastructure     StreetPlanter  Bio-retention Cells are depressions that contain vegetation grown in an engineered soil mixture placed above a gravel drainage bed  They provide storage  infiltration and evaporation of both direct rainfall and runoff captured from surrounding areas  Street planters consist of concrete boxes filled with an engineered soil that supports vegetative growth  Beneath the soil is a gravel bed that provides additional storage  The walls of a planter extend 3 to 12 inches above the soil bed to allow for ponding within the unit  The thickness of the soil growing medium ranges from 6 to 24 inches while gravel beds are 6 to 18 inches in depth  The planter  s capture ratio is the ratio of its area to the impervious area whose runoff it captures     Raingarden Rain Gardens are a type of bio-retention cell consisting of just the engineered soil layer with no gravel bed below it  Rain Gardens are shallow depressions filled with an engineered soil mix that supports vegetative growth  They are usually used on individual home lots to capture roof runoff  Typical soil depths range from 6 to 18 inches  nThe capture ratio is the ratio of the rain garden  s area to the impervious area that drains onto it     GreenRoof  Green Roofs are another variation of a bio-retention cell that have a soil layer laying atop a special drainage mat material that conveys excess percolated rainfall off of the roof  Green Roofs  also known as Vegetated Roofs  are bio-retention systems placed on roof surfaces that capture and temporarily store rainwater in a soil growing medium  They consist of a layered system of roofing designed to support plant growth and retain water for plant uptake while preventing ponding on the roof surface  nThe thickness used for the growing medium typically ranges from 3 to 6 inches     InfilTrench  infiltration trenches are narrow ditches filled with gravel that intercept runoff from upslope impervious areas  They provide storage volume and additional time for captured runoff to infiltrate the native soil below      PermPave or Permeable Pavements nContinuous Permeable Pavement systems are excavated areas filled with gravel and paved over with a porous concrete or asphalt mix   Continuous Permeable Pavement systems are excavated areas filled with gravel and paved over with a porous concrete or asphalt mix  Modular Block systems are similar except that permeable block pavers are used instead  Normally all rainfall will immediately pass through the pavement into the gravel storage layer below it where it can infiltrate at natural rates into the site  s native soil  Pavement layers are usually 4 to 6 inches in height while the gravel storage layer is typically 6 to 18 inches high  The Capture Ratio is the percent of the treated area  street or parking lot  that is replaced with permeable pavement      Cistern  Rain Barrels  or Cisterns  are containers that collect roof runoff during storm events and can either release or re-use the rainwater during dry periods  Rain harvesting systems collect runoff from rooftops and convey it to a cistern tank where it can be used for non-potable water uses and on-site infiltration  The harvesting system is assumed to consist of a given number of fixed-sized cisterns per 1000 square feet of rooftop area captured  The water from each cistern is withdrawn at a constant rate and is assumed to be consumed or infiltrated entirely on-site     VegSwale  Vegetative swales are channels or depressed areas with sloping sides covered with grass and other vegetation  They slow down the conveyance of collected runoff and allow it more time to infiltrate the native soil beneath it  Infiltration basins are shallow depressions filled with grass or other natural vegetation that capture runoff from adjoining areas and allow it to infiltrate into the soil     Wet ponds are frequently used for water quality improvement  groundwater recharge  flood protection  aesthetic improvement or any combination of these  Sometimes they act as a replacement for the natural absorption of a forest or other natural process that was lost when an area is developed  As such  these structures are designed to blend into neighborhoods and viewed as an amenity     Dry ponds temporarily stores water after a storm  but eventually empties out at a controlled rate to a downstream water body     Sand filters generally control runoff water quality  providing very limited flow rate control  A typical sand filter system consists of two or three chambers or basins  The first is the sedimentation chamber  which removes floatables and heavy sediments  The second is the filtration chamber  which removes additional pollutants by filtering the runoff through a sand bed  The third is the discharge chamber  nInfiltration trench  is a type of best management practice  BMP  that is used to manage stormwater runoff  prevent flooding and downstream erosion  and improve water quality in an adjacent river  stream  lake or bay  It is a shallow excavated trench filled with gravel or crushed stone that is designed to infiltrate stormwater though permeable soils into the groundwater aquifer     A Vegatated filter strip is a type of buffer strip that is an area of vegetation  generally narrow and long  that slows the rate of runoff  allowing sediments  organic matter  and other pollutants that are being conveyed by the water to be removed by settling out  Filter strips reduce erosion and the accompanying stream pollution  and can be a best management practice     Other LID like concepts around the world include sustainable drainage system  SUDS   The idea behind SUDS is to try to replicate natural systems that use cost effective solutions with low environmental impact to drain away dirty and surface water run-off through collection  storage  and cleaning before allowing it to be released slowly back into the environment  such as into water courses     In addition the following features can also be simulated using the features of SWMM 5  storage ponds  seepage  orifices  Weirs  seepage and evaporation from natural channels   constructed wetlands  wet ponds  dry ponds  infiltration basin  non-surface sand filters  vegetated filterstrips  vegetated filterstrip and infiltration basin   A  WetPark would be a combination of wet and dry ponds and LID features   A WetPark is also considered a constructed wetland     The SWMM 5 0 001 to 5 1 013 main components are  rain gages  watersheds  LID controls or BMP features such as Wet and Dry Ponds  nodes  links  pollutants  landuses  time patterns  curves  time series  controls  transects  aquifers  unit hydrographs  snowmelt and shapes  Table 3   Other related objects are the types of Nodes and the Link Shapes  The purpose of the objects is to simulate the major components of the hydrologic cycle  the hydraulic components of the drainage  sewer or stormwater network and the buildup/washoff functions that allow the simulation of water quality constituents   A watershed simulation starts with a precipitation time history  SWMM 5 has many types of open and closed pipes and channels  dummy  circular  filled circular  rectangular closed  rectangular open  trapezoidal  triangular  parabolic  power function  rectangular triangle  rectangle round  modified baskethandle  horizontal ellipse  vertical ellipse  arch  eggshaped  horseshoe  gothic  catenary  semielliptical  baskethandle  semicircular  irregular  custom and force main     The major objects or hydrology and hydraulic components in SWMM 5 are     The major overall components are called in the SWMM 5 input file and C code of the simulation engine  gage  subcatch  node  link  pollut  landuse  timepattern  curve  tseries  control  transect  aquifer  unithyd  snowmelt  shape and lid  The subsets of possible nodes are  junction  outfall  storage and divider   Storage Nodes are either tabular with a depth/area table or a functional relationship between area and depth   Possible node inflows include  external_inflow  dry_weather_inflow  wet_weather_inflow  groundwater_inflow  rdii_inflow  flow_inflow  concen_inflow  and mass_inflow   The dry weather inflows can include the possible patterns   monthly_pattern  daily_pattern  hourly_pattern and weekend_pattern     The SWMM 5 component structure allows the user to choose which major hydrology and hydraulic components are using during the simulation     The SWMM 3 and SWMM 4 converter can convert up to two files from the earlier SWMM 3 and 4 versions at one time to SWMM 5  Typically one would convert a Runoff and Transport file to SWMM 5 or a Runoff and Extran File to SWMM 5  If  there is a combination of a SWMM 4 Runoff  Transport and Extran network then it will have to be converted in pieces and the two data sets will have to be copied and pasted together to make one SWMM 5 data set  The x y coordinate file is only necessary if there are not existing x  y coordinates on the D1 line of the SWMM 4 Extran input data  set  The command File=>Define Ini File can be used to define the location of the ini file  The ini file will save the conversion project input data files and directories     The SWMMM3 and SWMM 3 5 files are fixed format   The SWMM 4 files are free format   The converter will detect which version of SWMM is being used   The converted files can be combined using a text editor to merge the created inp files     The Storm Water Management Model Climate Adjustment Tool  SWMM-CAT  is a new addition to SWMM5  December 2014    It is a simple to use software utility that allows future climate change projections to be incorporated into the Storm Water Management Model  SWMM   SWMM was recently updated to accept a set of monthly adjustment factors for each of these time series that could represent the impact of future changes in climatic conditions  SWMM-CAT provides a set of location-specific adjustments that derived from global climate change models run as part of the World Climate Research Programme  WCRP  Coupled Model Intercomparison Project Phase 3  CMIP3  archive  Figure 4    SWMM-CAT is a utility that adds location-specific climate change adjustments to a Storm Water Management Model  SWMM  project file  Adjustments can be applied on a monthly basis to air temperature  evaporation rates  and precipitation  as well as to the 24-hour design storm at different recurrence intervals  The source of these adjustments are global climate change models run as part of the World Climate Research Programme  WCRP  Coupled Model Intercomparison Project Phase 3  CMIP3  archive  Downscaled results from this archive were generated and converted into changes with respect to historical values by USEPA  s CREAT project  http //water epa gov/infrastructure/watersecurity/climate/creat cfm      The following steps are used to select a set of adjustments to apply to SWMM5     1  Enter the latitude and longitude coordinates of the location if available or its 5-digit zip code  SWMM-CAT will display a range of climate change outcomes for the CMIP3 results closest to the location     2  Select whether to use climate change projections based on either a near term or far term projection period  The displayed climate change outcomes will be updated to reflect the chosen choice     3  Select a climate change outcome to save to SWMM  There are three choices that span the range of outcomes produced by the different global climate models used in the CMIP3 project  The Hot/Dry outcome represents a model whose average temperature change was on the high end and whose average rainfall change was on the lower end of all model projections  The Warm/Wet outcome represents a model whose average temperature change was on the lower end and whose average rainfall change was on the wetter end of the spectrum  The Median outcome is for a model whose temperature and rainfall changes were closest to the median of all models     4  Click the Save Adjustments to SWMM link to bring up a dialog form that will allow the selection of an existing SWMM project file to save the adjustments to  The form will also allow the selection of which type of adjustments  monthly temperature  evaporation  rainfall  or 24-hour design storm  to save  Conversion of temperature and evaporation units is automatically handled depending on the unit system  US or SI  detected in the SWMM file     Other external programs that aid in the generation of data for the EPA SWMM 5 model include   SUSTAIN  BASINS   SSOAP and the EPAs National Stormwater Calculator  SWC  which is a desktop application that estimates the annual amount of rainwater and frequency of runoff from a specific site anywhere in the United States  including Puerto Rico   The estimates are based on local soil conditions  land cover  and historic rainfall records  Figure 5      A number of software packages use the SWMM5 engine  including many commercial software packages  9  Some of these software packages include         Category  By country     Air pollution is the presence of substances in the atmosphere that are harmful to the health of humans and other living beings  or cause damage to the climate or to materials  There are many different types of air pollutants  such as gases  such as ammonia  carbon monoxide  sulfur dioxide  nitrous oxides  methane  carbon dioxide and chlorofluorocarbons   particulates  both organic and inorganic   and biological molecules  Air pollution may cause diseases  allergies and even death to humans  it may also cause harm to other living organisms such as animals and food crops  and may damage the natural environment  for example  climate change  ozone depletion or habitat degradation  or built environment  for example  acid rain   Both human activity and natural processes can generate air pollution     Air pollution is a significant risk factor for a number of pollution-related diseases  including respiratory infections  heart disease  COPD  stroke and lung cancer  1  The human health effects of poor air quality are far reaching  but principally affect the body  s respiratory system and the cardiovascular system  Individual reactions to air pollutants depend on the type of pollutant a person is exposed to  the degree of exposure  and the individual  s health status and genetics  2  Indoor air pollution and poor urban air quality are listed as two of the world  s worst toxic pollution problems in the 2008 Blacksmith Institute World  s Worst Polluted Places report  3  Outdoor air pollution alone causes 2 1 4  5  to 4 21 million deaths annually  1  6  Overall  air pollution causes the deaths of around 7 million people worldwide each year  and is the world  s largest single environmental health risk  1  7  8  The scope of the air pollution crisis is enormous  90% of the world  s population breathes dirty air to some degree  Although the health consequences are extensive  the way the problem is handled is often haphazard  9  10  11     Productivity losses and degraded quality of life caused by air pollution are estimated to cost the world economy $5 trillion per year  12  13  14  Various pollution control technologies and strategies are available to reduce air pollution  15  16  To reduce the impacts of air pollution  both international and national legislation and regulation have been implemented to regulate air pollution  Local laws where well enforced in cities have led to strong public health improvements  At the international level some of these efforts have been successful  for example the Montreal Protocol which successful at reducing release of harmful ozone depleting chemicals or 1985 Helsinki Protocol which reduced sulfur emissions  while other attempts have been less rapid in implementation  such as international action on climate change     An air pollutant is a material in the air that can have adverse effects on humans and the ecosystem  The substance can be solid particles  liquid droplets  or gases  A pollutant can be of natural origin or man-made  nPollutants are classified as primary or secondary  Primary pollutants are usually produced by processes such as ash from a volcanic eruption  Other examples include carbon monoxide gas from motor vehicle exhausts or sulfur dioxide released from factories  Secondary pollutants are not emitted directly  Rather  they form in the air when primary pollutants react or interact  Ground level ozone is a prominent example of a secondary pollutant  Some pollutants may be both primary and secondary  they are both emitted directly and formed from other primary pollutants     Pollutants emitted into the atmosphere by human activity include     Secondary pollutants include     Minor air pollutants include     Persistent organic pollutants  POPs  are organic compounds that are resistant to environmental degradation through chemical  biological  and photolytic processes  Because of this  they have been observed to persist in the environment  to be capable of long-range transport  bioaccumulate in human and animal tissue  biomagnify in food chains  and to have potentially significant impacts on human health and the environment     These are mostly related to the burning of fuel     There are also sources from processes other than combustion    Air pollutant emission factors are reported representative values that attempt to relate the quantity of a pollutant released to the ambient air with an activity associated with the release of that pollutant  These factors are usually expressed as the weight of pollutant divided by a unit weight  volume  distance  or duration of the activity emitting the pollutant  e g   kilograms of particulate emitted per tonne of coal burned   Such factors facilitate estimation of emissions from various sources of air pollution  In most cases  these factors are simply averages of all available data of acceptable quality  and are generally assumed to be representative of long-term averages     There are 12 compounds in the list of persistent organic pollutants  Dioxins and furans are two of them and intentionally created by combustion of organics  like open burning of plastics  These compounds are also endocrine disruptors and can mutate the human genes     The United States Environmental Protection Agency has published a compilation of air pollutant emission factors for a wide range of industrial sources  38  The United Kingdom  Australia  Canada and many other countries have published similar compilations  as well as the European Environment Agency  39  40  41  42     Air pollution risk is a function of the hazard of the pollutant and the exposure to that pollutant  Air pollution exposure can be expressed for an individual  for certain groups  e g  neighborhoods or children living in a country   or for entire populations  For example  one may want to calculate the exposure to a hazardous air pollutant for a geographic area  which includes the various microenvironments and age groups  This can be calculated 2  as an inhalation exposure  This would account for daily exposure in various settings  e g  different indoor micro-environments and outdoor locations   The exposure needs to include different age and other demographic groups  especially infants  children  pregnant women and other sensitive subpopulations  The exposure to an air pollutant must integrate the concentrations of the air pollutant with respect to the time spent in each setting and the respective inhalation rates for each subgroup for each specific time that the subgroup is in the setting and engaged in particular activities  playing  cooking  reading  working  spending time in traffic  etc    For example  a small child  s inhalation rate will be less than that of an adult  A child engaged in vigorous exercise will have a higher respiration rate than the same child in a sedentary activity  The daily exposure  then  needs to reflect the time spent in each micro-environmental setting and the type of activities in these settings  The air pollutant concentration in each microactivity/microenvironmental setting is summed to indicate the exposure  2  For some pollutants such as black carbon  traffic related exposures may dominate total exposure despite short exposure times since high concentrations coincide with proximity to major roads or participation to  motorized  traffic  44  A large portion of total daily exposure occurs as short peaks of high concentrations  but it remains unclear how to define peaks and determine their frequency and health impact  45     A lack of ventilation indoors concentrates air pollution where people often spend the majority of their time  Radon  Rn  gas  a carcinogen  is exuded from the Earth in certain locations and trapped inside houses  Building materials including carpeting and plywood emit formaldehyde  H2CO  gas  Paint and solvents give off volatile organic compounds  VOCs  as they dry  Lead paint can degenerate into dust and be inhaled  Intentional air pollution is introduced with the use of air fresheners  incense  and other scented items  Controlled wood fires in cook stoves and fireplaces can add significant amounts of harmful smoke particulates into the air  inside and out  46  47  Indoor pollution fatalities may be caused by using pesticides and other chemical sprays indoors without proper ventilation     Carbon monoxide poisoning and fatalities are often caused by faulty vents and chimneys  or by the burning of charcoal indoors or in a confined space  such as a tent  48  Chronic carbon monoxide poisoning can result even from poorly-adjusted pilot lights  Traps are built into all domestic plumbing to keep sewer gas and hydrogen sulfide  out of interiors  Clothing emits tetrachloroethylene  or other dry cleaning fluids  for days after dry cleaning     Though its use has now been banned in many countries  the extensive use of asbestos in industrial and domestic environments in the past has left a potentially very dangerous material in many localities  Asbestosis is a chronic inflammatory medical condition affecting the tissue of the lungs  It occurs after long-term  heavy exposure to asbestos from asbestos-containing materials in structures  Sufferers have severe dyspnea  shortness of breath  and are at an increased risk regarding several different types of lung cancer  As clear explanations are not always stressed in non-technical literature  care should be taken to distinguish between several forms of relevant diseases  According to the World Health Organization  WHO   these may be defined as asbestosis  lung cancer  and peritoneal mesothelioma  generally a very rare form of cancer  when more widespread it is almost always associated with prolonged exposure to asbestos      Biological sources of air pollution are also found indoors  as gases and airborne particulates  Pets produce dander  people produce dust from minute skin flakes and decomposed hair  dust mites in bedding  carpeting and furniture produce enzymes and micrometre-sized fecal droppings  inhabitants emit methane  mold forms on walls and generates mycotoxins and spores  air conditioning systems can incubate Legionnaires   disease and mold  and houseplants  soil and surrounding gardens can produce pollen  dust  and mold  Indoors  the lack of air circulation allows these airborne pollutants to accumulate more than they would otherwise occur in nature     Even at levels lower than those considered safe by United States regulators  exposure to three components of air pollution  fine particulate matter  nitrogen dioxide and ozone  correlates with cardiac and respiratory illness  49  In 2012  air pollution caused premature deaths on average of 1 year in Europe  and was a significant risk factor for a number of pollution-related diseases  including respiratory infections  heart disease  COPD  stroke and lung cancer  1  The health effects caused by air pollution may include difficulty in breathing  wheezing  coughing  asthma 50  and worsening of existing respiratory and cardiac conditions  These effects can result in increased medication use  increased doctor or emergency department visits  more hospital admissions and premature death  The human health effects of poor air quality are far reaching  but principally affect the body  s respiratory system and the cardiovascular system  Individual reactions to air pollutants depend on the type of pollutant a person is exposed to  the degree of exposure  and the individual  s health status and genetics  2  nThe most common sources of air pollution include particulates  ozone  nitrogen dioxide  and sulfur dioxide  Children aged less than five years that live in developing countries are the most vulnerable population in terms of total deaths attributable to indoor and outdoor air pollution  51     The World Health Organization estimated in 2014 that every year air pollution causes the premature death of some 7 million people worldwide  1  Studies published in March 2019 indicated that the number may be around 8 8 million  53     India has the highest death rate due to air pollution  54  India also has more deaths from asthma than any other nation according to the World Health Organization  In December 2013 air pollution was estimated to kill 500 000 people in China each year  55  There is a positive correlation between pneumonia-related deaths and air pollution from motor vehicle emissions  56     Annual premature European deaths caused by air pollution are estimated at 430 000 57 -800 000 53  An important cause of these deaths is nitrogen dioxide and other nitrogen oxides  NOx  emitted by road vehicles  57  In a 2015 consultation document the UK government disclosed that nitrogen dioxide is responsible for 23 500 premature UK deaths per annum  58  Across the European Union  air pollution is estimated to reduce life expectancy by almost nine months  59  Causes of deaths include strokes  heart disease  COPD  lung cancer  and lung infections  1     Urban outdoor air pollution is estimated to cause 1 3 million deaths worldwide per year  Children are particularly at risk due to the immaturity of their respiratory organ systems  60     The US EPA estimated in 2004 that a proposed set of changes in diesel engine technology  Tier 2  could result in 12 000 fewer premature mortalities  15 000 fewer heart attacks  6 000 fewer emergency department visits by children with asthma  and 8 900 fewer respiratory-related hospital admissions each year in the United States  61     The US EPA has estimated that limiting ground-level ozone concentration to 65 parts per billion  would avert 1 700 to 5 100 premature deaths nationwide in 2020 compared with the 75-ppb standard  The agency projected the more protective standard would also prevent an additional 26 000 cases of aggravated asthma  and more than a million cases of missed work or school  62  63  Following this assessment  the EPA acted to protect public health by lowering the National Ambient Air Quality Standards  NAAQS  for ground-level ozone to 70 parts per billion  ppb   64     A new economic study of the health impacts and associated costs of air pollution in the Los Angeles Basin and San Joaquin Valley of Southern California shows that more than 3 800 people die prematurely  approximately 14 years earlier than normal  each year because air pollution levels violate federal standards  The number of annual premature deaths is considerably higher than the fatalities related to auto collisions in the same area  which average fewer than 2 000 per year  65  66  67     Diesel exhaust  DE  is a major contributor to combustion-derived particulate matter air pollution  In several human experimental studies  using a well-validated exposure chamber setup  DE has been linked to acute vascular dysfunction and increased thrombus formation  68  69     The mechanisms linking air pollution to increased cardiovascular mortality are uncertain  but probably include pulmonary and systemic inflammation  70     A study by Greenpeace estimates there are 4 5 million annual premature deaths worldwide because of pollutants released by high-emission power stations and vehicle exhausts  65 000 deaths occur in the Middle East each year due to pollution  71  A study by scientists of U K  and U S  universities that uses a high spatial resolution model and an updated concentration-response function concluded in 2021 that 10 2 million global excess deaths in 2012 and 8 7 million in 2018  or a fifth dubious   discuss   were due to air pollution generated by fossil fuel combustion  significantly higher than earlier estimates and with spatially subdivided mortality impacts  72  73     A 2007 review of evidence found that  ambient air pollution exposure is a risk factor correlating with increased total mortality from cardiovascular events  range  12% to 14% per 10 xa0g/m3 increase   74  clarification needed     Air pollution is also emerging as a risk factor for stroke  particularly in developing countries where pollutant levels are highest  75  A 2007 study found that in women  air pollution is not associated with hemorrhagic but with ischemic stroke  76  Air pollution was also found to be associated with increased incidence and mortality from coronary stroke in a cohort study in 2011  77  Associations are believed to be causal and effects may be mediated by vasoconstriction  low-grade inflammation and atherosclerosis 78  Other mechanisms such as autonomic nervous system imbalance have also been suggested  79  80     Research has demonstrated increased risk of developing asthma 81  and COPD 82  from increased exposure to traffic-related air pollution  Additionally  air pollution has been associated with increased hospitalization and mortality from asthma and COPD  83  84  Chronic obstructive pulmonary disease  COPD  includes diseases such as chronic bronchitis and emphysema  85     A study conducted in 19601961 in the wake of the Great Smog of 1952 compared 293 London residents with 477 residents of Gloucester  Peterborough  and Norwich  three towns with low reported death rates from chronic bronchitis  All subjects were male postal truck drivers aged 40 to 59  Compared to the subjects from the outlying towns  the London subjects exhibited more severe respiratory symptoms  including cough  phlegm  and dyspnea   reduced lung function  FEV1 and peak flow rate   and increased sputum production and purulence  The differences were more pronounced for subjects aged 50 to 59  The study controlled for age and smoking habits  so concluded that air pollution was the most likely cause of the observed differences  86  nMore recent studies have shown that air pollution exposure from traffic reduces lung function development in children 87  and lung function may be compromised by air pollution even at low concentrations  88  Air pollution exposure also cause lung cancer in non smokers     It is believed that much like cystic fibrosis  by living in a more urban environment serious health hazards become more apparent  Studies have shown that in urban areas patients suffer mucus hypersecretion  lower levels of lung function  and more self-diagnosis of chronic bronchitis and emphysema  89     A review of evidence regarding whether ambient air pollution exposure is a risk factor for cancer in 2007 found solid data to conclude that long-term exposure to PM2 5  fine particulates  increases the overall risk of non-accidental mortality by 6% per a 10 microg/m3 increase  Exposure to PM2 5 was also associated with an increased risk of mortality from lung cancer  range  15% to 21% per 10 microg/m3 increase  and total cardiovascular mortality  range  12% to 14% per a 10 microg/m3 increase   The review further noted that living close to busy traffic appears to be associated with elevated risks of these three outcomes xa0 increase in lung cancer deaths  cardiovascular deaths  and overall non-accidental deaths  The reviewers also found suggestive evidence that exposure to PM2 5 is positively associated with mortality from coronary heart diseases and exposure to SO2 increases mortality from lung cancer  but the data was insufficient to provide solid conclusions  92  Another investigation showed that higher activity level increases deposition fraction of aerosol particles in human lung and recommended avoiding heavy activities like running in outdoor space at polluted areas  93     In 2011  a large Danish epidemiological study found an increased risk of lung cancer for patients who lived in areas with high nitrogen oxide concentrations  In this study  the association was higher for non-smokers than smokers  94  An additional Danish study  also in 2011  likewise noted evidence of possible associations between air pollution and other forms of cancer  including cervical cancer and brain cancer  95     In the United States  despite the passage of the Clean Air Act in 1970  in 2002 at least 146 million Americans were living in non-attainment areasregions in which the concentration of certain air pollutants exceeded federal standards  96  These dangerous pollutants are known as the criteria pollutants  and include ozone  particulate matter  sulfur dioxide  nitrogen dioxide  carbon monoxide  and lead  Protective measures to ensure children  s health are being taken in cities such as New Delhi  India where buses now use compressed natural gas to help eliminate the \"pea-soup\" smog  97  A recent study in Europe has found that exposure to ultrafine particles can increase blood pressure in children  98  nAccording to a WHO report-2018  polluted air leads to the poisoning of millions of children under the age of 15  resulting in the death of some six hundred thousand children annually  99     Prenatal exposure to polluted air has been linked to a variety of neurodevelopmental disorders in children  For example  exposure to polycyclic aromatic hydrocarbons  PAH  was associated with reduced IQ scores and symptoms of anxiety and depression  100  A 2014 study found that PAHs might play a role in the development of childhood Attention Deficit Hyperactivity Disorder  ADHD   101  Researchers have also begun to find evidence for air pollution as a risk factor for autism spectrum disorder  ASD   In Los Angeles  children who were living in areas with high levels of traffic-related air pollution were more likely to be diagnosed with autism between 3-5 years of age  102  The connection between air pollution and neurodevelopmental disorders in children is thought to be related to epigenetic dysregulation of the primordial germ cells  embryo  and fetus during a critical period  Some PAHs are considered endocrine disruptors and are lipid soluble  When they build up in adipose tissue  they can be transferred across the placenta  103     Ambient levels of air pollution have been associated with preterm birth and low birth weight  A 2014 WHO worldwide survey on maternal and perinatal health found a statistically significant association between low birth weights  LBW  and increased levels of exposure to PM2 5  Women in regions with greater than average PM2 5 levels had statistically significant higher odds of pregnancy resulting in a low-birth weight infant even when adjusted for country-related variables  104  The effect is thought to be from stimulating inflammation and increasing oxidative stress     A study by the University of York found that in 2010 exposure to PM2 5 was strongly associated with 18% of preterm births globally  which was approximately 2 7 million premature births  The countries with the highest air pollution associated preterm births were in South and East Asia  the Middle East  North Africa  and West sub-Saharan Africa  105     The source of PM 2 5 differs greatly by region  In South and East Asia  pregnant women are frequently exposed to indoor air pollution because of wood and other biomass fuels being used for cooking  which are responsible for more than 80% of regional pollution  In the Middle East  North Africa and West sub-Saharan Africa  fine PM comes from natural sources  such as dust storms  105  The United States had an estimated 50 000 preterm births associated with exposure to PM2 5 in 2010  105     A study performed by Wang  et al  between the years of 1988 and 1991 has found a correlation between sulfur dioxide  SO2  and total suspended particulates  TSP  and preterm births and low birth weights in Beijing  A group of 74 671 pregnant women  in four separate regions of Beijing  were monitored from early pregnancy to delivery along with daily air pollution levels of sulfur Dioxide and TSP  along with other particulates   The estimated reduction in birth weight was 7 3 g for every 100 xa0g/m3 increase in SO2 and 6 9 g for each 100 xa0g/m3 increase in TSP  These associations were statistically significant in both summer and winter  although  summer was greater  The proportion of low birth weight attributable to air pollution  was 13%  This is the largest attributable risk ever reported for the known risk factors of low birth weight  106  Coal stoves  which are in 97% of homes  are a major source of air pollution in this area     Brauer et al  studied the relationship between air pollution and proximity to a highway with pregnancy outcomes in a Vancouver cohort of pregnant woman using addresses to estimate exposure during pregnancy  Exposure to NO  NO2  CO PM10 and PM2 5 were associated with infants born small for gestational age  SGA   Women living less than 50 meters away from an expressway or highway were 26% more likely to give birth to a SGA infant  107     Even in areas with relatively low levels of air pollution  public health effects can be significant and costly  since a large number of people breathe in such pollutants  A study published in 2017 found that even in areas of the U S  where ozone and PM2 5 meet federal standards  Medicare recipients who are exposed to more air pollution have higher mortality rates  108  A 2005 scientific study for the British Columbia Lung Association showed that a small improvement in air quality  1% reduction of ambient PM2 5 and ozone concentrations  would produce $29 million in annual savings in the Metro Vancouver region in 2010  109  This finding is based on health valuation of lethal  death  and sub-lethal  illness  affects     In 2020  scientists found that the boundary layer air over the Southern Ocean around Antarctica is unpolluted by humans  110     Data is accumulating that air pollution exposure also affects the central nervous system  111     In a June 2014 study conducted by researchers at the University of Rochester Medical Center  published in the journal Environmental Health Perspectives  it was discovered that early exposure to air pollution causes the same damaging changes in the brain as autism and schizophrenia  The study also shows that air pollution also affected short-term memory  learning ability  and impulsivity  Lead researcher Professor Deborah Cory-Slechta said that \"When we looked closely at the ventricles  we could see that the white matter that normally surrounds them hadn  t fully developed  It appears that inflammation had damaged those brain cells and prevented that region of the brain from developing  and the ventricles simply expanded to fill the space  Our findings add to the growing body of evidence that air pollution may play a role in autism  as well as in other neurodevelopmental disorders \" In a study of mice  air pollution also has a more significant negative effect on males than on females  112  113  114     In 2015  experimental studies reported the detection of significant episodic  situational  cognitive impairment from impurities in indoor air breathed by test subjects who were not informed about changes in the air quality  Researchers at the Harvard University and SUNY Upstate Medical University and Syracuse University measured the cognitive performance of 24 participants in three different controlled laboratory atmospheres that simulated those found in \"conventional\" and \"green\" buildings  as well as green buildings with enhanced ventilation  Performance was evaluated objectively using the widely used Strategic Management Simulation software simulation tool  which is a well-validated assessment test for executive decision-making in an unconstrained situation allowing initiative and improvisation  Significant deficits were observed in the performance scores achieved in increasing concentrations of either volatile organic compounds  VOCs  or carbon dioxide  while keeping other factors constant  The highest impurity levels reached are not uncommon in some classroom or office environments  115  116  Air pollution increases the risk of dementia in people over 50 years old  117     In India in 2014  it was reported that air pollution by black carbon and ground level ozone had reduced crop yields in the most affected areas by almost half in 2011 when compared to 1980 levels  118     Air pollution costs the world economy $5 trillion per year as a result of productivity losses and degraded quality of life  according to a joint study by the World Bank and the Institute for Health Metrics and Evaluation  IHME  at the University of Washington  12  13  14  These productivity losses are caused by deaths due to diseases caused by air pollution  One out of ten deaths in 2013 was caused by diseases associated with air pollution and the problem is getting worse  The problem is even more acute in the developing world  \"Children under age 5 in lower-income countries are more than 60 times as likely to die from exposure to air pollution as children in high-income countries \" 12  13  The report states that additional economic losses caused by air pollution  including health costs 119  and the adverse effect on agricultural and other productivity were not calculated in the report  and thus the actual costs to the world economy are far higher than $5 trillion     Artificial air pollution may be detectable on Earth from distant vantage points such as other planetary systems via atmospheric SETI  including NO2 pollution levels and with telescopic technology close to today  It may also be possible to detect extraterrestrial civilizations this way  120  121  122     The world  s worst short-term civilian pollution crisis was the 1984 Bhopal Disaster in India  123  Leaked industrial vapours from the Union Carbide factory  belonging to Union Carbide  Inc   U S A   later bought by Dow Chemical Company   killed at least 3787 people and injured from 150 000 to 600 000  The United Kingdom suffered its worst air pollution event when the 4 December Great Smog of 1952 formed over London  In six days more than 4 000 died and more recent estimates put the figure at nearer 12 000  124  An accidental leak of anthrax spores from a biological warfare laboratory in the former USSR in 1979 near Sverdlovsk is believed to have caused at least 64 deaths  125  The worst single incident of air pollution to occur in the US occurred in Donora  Pennsylvania in late October 1948  when 20 people died and over 7 000 were injured  126     There are now practical alternatives to the principal causes of air pollution     Various pollution control technologies and strategies are available to reduce air pollution  15  16  At its most basic level  land-use planning is likely to involve zoning and transport infrastructure planning  In most developed countries  land-use planning is an important part of social policy  ensuring that land is used efficiently for the benefit of the wider economy and population  as well as to protect the environment     Because a large share of air pollution is caused by combustion of fossil fuels such as coal and oil  the reduction of these fuels can reduce air pollution drastically  Most effective is the switch to clean power sources such as wind power  solar power  hydro power which don  t cause air pollution  133  Efforts to reduce pollution from mobile sources includes primary regulation  many developing countries have permissive regulations   citation needed  expanding regulation to new sources  such as cruise and transport ships  farm equipment  and small gas-powered equipment such as string trimmers  chainsaws  and snowmobiles   increased fuel efficiency  such as through the use of hybrid vehicles   conversion to cleaner fuels or conversion to electric vehicles     Titanium dioxide has been researched for its ability to reduce air pollution  Ultraviolet light will release free electrons from material  thereby creating free radicals  which break up VOCs and NOx gases  One form is superhydrophilic  134     In 2014  Prof  Tony Ryan and Prof  Simon Armitage of University of Sheffield prepared a 10 meter by 20 meter-sized poster coated with microscopic  pollution-eating nanoparticles of titanium dioxide  Placed on a building  this giant poster can absorb the toxic emission from around 20 cars each day  135     A very effective means to reduce air pollution is the transition to renewable energy  According to a study published in Energy and Environmental Science in 2015 the switch to 100% renewable energy in the United States would eliminate about 62 000 premature mortalities per year and about 42 000 in 2050  if no biomass were used  This would save about $600 billion in health costs a year due to reduced air pollution in 2050  or about 3 6% of the 2014 U S  gross domestic product  133     There is limited evidence that efforts to reduce particulate matter in the air can result in better health in Africa  the Middle East  Eastern Europe  Central Asia  and Southeast Asia  136     The following items are commonly used as pollution control devices in industry and transportation  They can either destroy contaminants or remove them from an exhaust stream before it is emitted into the atmosphere     In general  there are two types of air quality standards  The first class of standards  such as the U S  National Ambient Air Quality Standards and E U  Air Quality Directive  set maximum atmospheric concentrations for specific pollutants  Environmental agencies enact regulations which are intended to result in attainment of these target levels  The second class  such as the North American air quality index  take the form of a scale with various thresholds  which is used to communicate to the public the relative risk of outdoor activity  The scale may or may not distinguish between different pollutants     In Canada  air pollution and associated health risks are measured with the Air Quality Health Index or  AQHI   It is a health protection tool used to make decisions to reduce short-term exposure to air pollution by adjusting activity levels during increased levels of air pollution     The Air Quality Health Index or \"AQHI\" is a federal program jointly coordinated by Health Canada and Environment Canada  However  the AQHI program would not be possible without the commitment and support of the provinces  municipalities and NGOs  From air quality monitoring to health risk communication and community engagement  local partners are responsible for the vast majority of work related to AQHI implementation  The AQHI provides a number from 1 to 10+ to indicate the level of health risk associated with local air quality  Occasionally  when the amount of air pollution is abnormally high  the number may exceed 10  The AQHI provides a local air quality current value as well as a local air quality maximums forecast for today  tonight and tomorrow and provides associated health advice     As it is now known that even low levels of air pollution can trigger discomfort for the sensitive population  the index has been developed as a continuum  The higher the number  the greater the health risk and need to take precautions  The index describes the level of health risk associated with this number as   low      moderate      high   or   very high    and suggests steps that can be taken to reduce exposure  137     The measurement is based on the observed relationship of nitrogen dioxide  NO2   ground-level ozone  O3  and particulates  PM2 5  with mortality  from an analysis of several Canadian cities  Significantly  all three of these pollutants can pose health risks  even at low levels of exposure  especially among those with pre-existing health problems     When developing the AQHI  Health Canada  s original analysis of health effects included five major air pollutants  particulates  ozone  and nitrogen dioxide  NO2   as well as sulfur dioxide  SO2   and carbon monoxide  CO   The latter two pollutants provided little information in predicting health effects and were removed from the AQHI formulation     The AQHI does not measure the effects of odour  pollen  dust  heat or humidity     TA Luft is the German air quality regulation     Air pollution hotspots are areas where air pollution emissions expose individuals to increased negative health effects  139  They are particularly common in highly populated  urban areas  where there may be a combination of stationary sources  e g  industrial facilities  and mobile sources  e g  cars and trucks  of pollution  Emissions from these sources can cause respiratory disease  childhood asthma  50  cancer  and other health problems  Fine particulate matter such as diesel soot  which contributes to more than 3 2 million premature deaths around the world each year  is a significant problem  It is very small and can lodge itself within the lungs and enter the bloodstream  Diesel soot is concentrated in densely populated areas  and one in six people in the U S  live near a diesel pollution hot spot  140     While air pollution hotspots affect a variety of populations  some groups are more likely to be located in hotspots  Previous studies have shown disparities in exposure to pollution by race and/or income  Hazardous land uses  toxic storage and disposal facilities  manufacturing facilities  major roadways  tend to be located where property values and income levels are low  Low socioeconomic status can be a proxy for other kinds of social vulnerability  including race  a lack of ability to influence regulation and a lack of ability to move to neighborhoods with less environmental pollution  These communities bear a disproportionate burden of environmental pollution and are more likely to face health risks such as cancer or asthma  142     Studies show that patterns in race and income disparities not only indicate a higher exposure to pollution but also higher risk of adverse health outcomes  143  Communities characterized by low socioeconomic status and racial minorities can be more vulnerable to cumulative adverse health impacts resulting from elevated exposure to pollutants than more privileged communities  143  Blacks and Latinos generally face more pollution than whites and Asians  and low-income communities bear a higher burden of risk than affluent ones  142  Racial discrepancies are particularly distinct in suburban areas of the Southern United States and metropolitan areas of the Midwestern and Western United States  144  Residents in public housing  who are generally low-income and cannot move to healthier neighborhoods  are highly affected by nearby refineries and chemical plants  145     Air pollution is usually concentrated in densely populated metropolitan areas  especially in developing countries where environmental regulations are relatively lax or nonexistent  147  However  even populated areas in developed countries attain unhealthy levels of pollution  with Los Angeles and Rome being two examples  148  Between 2002 and 2011 the incidence of lung cancer in Beijing near doubled  While smoking remains the leading cause of lung cancer in China  the number of smokers is falling while lung cancer rates are rising  149     In Europe  Council Directive 96/62/EC on ambient air quality assessment and management provides a common strategy against which member states can \"set objectives for ambient air quality in order to avoid  prevent or reduce harmful effects on human health and the environment     and improve air quality where it is unsatisfactory\"  150     On 25 July 2008 in the case Dieter Janecek v Freistaat Bayern  the European Court of Justice ruled that under this directive 150  citizens have the right to require national authorities to implement a short term action plan that aims to maintain or achieve compliance to air quality limit values  151  152     This important case law appears to confirm the role of the EC as centralised regulator to European nation-states as regards air pollution control  It places a supranational legal obligation on the UK to protect its citizens from dangerous levels of air pollution  furthermore superseding national interests with those of the citizen     In 2010  the European Commission  EC  threatened the UK with legal action against the successive breaching of PM10 limit values  153  The UK government has identified that if fines are imposed  they could cost the nation upwards of 300 million per year  154     In March 2011  the Greater London Built-up Area remains the only UK region in breach of the EC  s limit values  and has been given 3 months to implement an emergency action plan aimed at meeting the EU Air Quality Directive  155  The City of London has dangerous levels of PM10 concentrations  estimated to cause 3000 deaths per year within the city  156  As well as the threat of EU fines  in 2010 it was threatened with legal action for scrapping the western congestion charge zone  which is claimed to have led to an increase in air pollution levels  157     In response to these charges  Boris Johnson  Mayor of London  has criticised the current need for European cities to communicate with Europe through their nation state  s central government  arguing that in future \"A great city like London\" should be permitted to bypass its government and deal directly with the European Commission regarding its air quality action plan  155     This can be interpreted as recognition that cities can transcend the traditional national government organisational hierarchy and develop solutions to air pollution using global governance networks  for example through transnational relations  Transnational relations include but are not exclusive to national governments and intergovernmental organisations  158  allowing sub-national actors including cities and regions to partake in air pollution control as independent actors     Particularly promising at present are global city partnerships  159  These can be built into networks  for example the C40 Cities Climate Leadership Group  of which London is a member  The C40 is a public   non-state   network of the world  s leading cities that aims to curb their greenhouse emissions  159  The C40 has been identified as   governance from the middle   and is an alternative to intergovernmental policy  160  It has the potential to improve urban air quality as participating cities \"exchange information  learn from best practices and consequently mitigate carbon dioxide emissions independently from national government decisions\"  159  A criticism of the C40 network is that its exclusive nature limits influence to participating cities and risks drawing resources away from less powerful city and regional actors     According to one projection  by 2030 half of the world  s pollution emissions could be generated by Africa  161  Potential contributors to such an outcome include increased burning activities  such as the burning of open waste   traffic  agri-food and chemical industries  sand dust from the Sahara  and overall population growth      xa0Global warming portal xa0Environment portal    Air pollution  its cruel impact and preventions    An hydrological transport model is a mathematical model used to simulate the flow of rivers   streams  groundwater movement or  drainage front displacement  and calculate water quality parameters  These models generally came into use in the 1960s and 1970s when demand for numerical forecasting of water quality and drainage was driven by environmental legislation  and at a similar time widespread access to significant computer power became available  Much of the original model development took place in the United States and United Kingdom  but today these models are refined and used worldwide     There are dozens of different transport models that can be generally grouped by pollutants addressed  complexity of pollutant sources  whether the model is steady state or dynamic  and time period modeled  Another important designation is whether the model is distributed  i e  capable of predicting multiple points within a river  or lumped  In a basic model  for example  only one pollutant might be addressed from a simple point discharge into the receiving waters  In the most complex of models  various line source inputs from surface runoff might be added to multiple point sources  treating a variety of chemicals plus sediment in a dynamic environment including vertical river stratification and interactions of pollutants with in-stream biota  In addition watershed groundwater may also be included  The model is termed \"physically based\" if its parameters can be measured in the field     Often models have separate modules to address individual steps in the simulation process  The most common module is a subroutine for calculation of surface runoff  allowing variation in land use type  topography  soil type  vegetative cover  precipitation and land management practice  such as the application rate of a fertilizer   The concept of hydrological modeling can be extended to other environments such as the oceans  but most commonly  and in this article  the subject of a river watershed is generally implied     In 1850  T  J  Mulvany was probably the first investigator to use mathematical modeling in a stream hydrology context  although there was no chemistry involved  1  By 1892 M E  Imbeau had conceived an event model to relate runoff to peak rainfall  again still with no chemistry  2  Robert E  Hortons seminal work 3  on surface runoff along with his coupling of quantitative treatment of erosion 4  laid the groundwork for modern chemical transport hydrology     Physically based models  sometimes known as deterministic  comprehensive or process-based models  try to represent the physical processes observed in the real world  Typically  such models contain representations of surface runoff  subsurface flow  evapotranspiration  and channel flow  but they can be far more complicated  \"Large scale simulation experiments were begun by the U S  Army Corps of Engineers in 1953 for reservoir management on the main stem of the Missouri River\"  This  5  and other early work that dealt with the River Nile 6  7  and the Columbia River 8  are discussed  in a wider context  in a book published by the Harvard Water Resources Seminar  that contains the sentence just quoted  9  nAnother early model that integrated many submodels for basin chemical hydrology was the Stanford Watershed Model  SWM   10  The SWMM  Storm Water Management Model   the HSPF  Hydrological Simulation Program  FORTRAN  and other modern American derivatives are successors to this early work     In Europe a favoured comprehensive model is the Systme Hydrologique Europen  SHE   11  12  which has been succeeded by MIKE SHE and SHETRAN  MIKE SHE is a watershed-scale physically based  spatially distributed model for water flow and sediment transport  Flow and transport processes are represented by either finite difference representations of partial differential equations or by derived empirical equations  The following principal submodels are involved     This model can analyze effects of land use and climate changes upon in-stream water quality  with consideration of groundwater interactions     Worldwide a number of basin models have been developed  among them RORB  Australia   Xinanjiang  China   Tank model  Japan   ARNO  Italy   TOPMODEL  Europe   UBC  Canada  and HBV  Scandinavia   MOHID Land  Portugal   However  not all of these models have a chemistry component  Generally speaking  SWM  SHE and TOPMODEL have the most comprehensive stream chemistry treatment and have evolved to accommodate the latest data sources including remote sensing and geographic information system data     In the United States  the Corps of Engineers  Engineer Research and Development Center in conjunction with a researchers at a number of universities have developed the Gridded Surface/Subsurface Hydrologic Analysis GSSHA model  13  14  15  GSSHA is widely used in the U S  for research and analysis by U S  Army Corps of Engineers districts and larger consulting companies to compute flow  water levels  distributed erosion  and sediment delivery in complex engineering designs  A distributed nutrient and contaminant fate and transport component is undergoing testing  GSSHA input/output processing and interface with GIS is facilitated by the Watershed Modeling System  WMS   16     Another model used in the United States and worldwide is Vflo  a physics-based distributed hydrologic model developed by Vieux & Associates  Inc  17  Vflo employs radar rainfall and GIS data to compute spatially distributed overland flow and channel flow  Evapotranspiration  inundation  infiltration  and snowmelt modeling capabilities are included  Applications include civil infrastructure operations and maintenance  stormwater prediction and emergency management  soil moisture monitoring  land use planning  water quality monitoring  and others     These models based on data are black box systems  using mathematical and statistical concepts to link a certain input  for instance rainfall  to the model output  for instance runoff   Commonly used techniques are regression  transfer functions  neural networks and system identification  These models are known as stochastic hydrology models  Data based models have been used within hydrology to simulate the rainfall-runoff relationship  represent the impacts of antecedent moisture and perform real-time control on systems     A key component of a hydrological transport model is the surface runoff element  which allows assessment of sediment  fertilizer  pesticide and other chemical contaminants  Building on the work of Horton  the unit hydrograph theory was developed by Dooge in 1959  18  It required the presence of the National Environmental Policy Act and kindred other national legislation to provide the impetus to integrate water chemistry to hydrology model protocols  In the early 1970s the U S  Environmental Protection Agency  EPA  began sponsoring a series of water quality models in response to the Clean Water Act  An example of these efforts was developed at the Southeast Water Laboratory  19  one of the first attempts to calibrate a surface runoff model with field data for a variety of chemical contaminants     The attention given to surface runoff contaminant models has not matched the emphasis on pure hydrology models  in spite of their role in the generation of stream loading contaminant data  In the United States the EPA has had difficulty interpreting 20  diverse proprietary contaminant models and has to develop its own models more often than conventional resource agencies  who  focused on flood forecasting  have had more of a centroid of common basin models     Liden applied the HBV model to estimate the riverine transport of three different substances  nitrogen  phosphorus and suspended sediment 21  in four different countries  Sweden  Estonia  Bolivia and Zimbabwe  The relation between internal hydrological model variables and nutrient transport was assessed  A model for nitrogen sources was developed and analysed in comparison with a statistical method  A model for suspended sediment transport in tropical and semi-arid regions was developed and tested  It was shown that riverine total nitrogen could be well simulated in the Nordic climate and riverine suspended sediment load could be estimated fairly well in tropical and semi-arid climates  The HBV model for material transport generally estimated material transport loads well  The main conclusion of the study was that the HBV model can be used to predict material transport on the scale of the drainage basin during stationary conditions  but cannot be easily generalised to areas not specifically calibrated  In a different work  Castanedo et al  applied an evolutionary algorithm to automated watershed model calibration  22     The United States EPA developed the DSSAM Model to analyze water quality impacts from land use and wastewater management decisions in the Truckee River basin  an area which include the cities of Reno and Sparks  Nevada as well as the Lake Tahoe basin  The model 23  satisfactorily predicted nutrient  sediment and dissolved oxygen parameters in the river  It is based on a pollutant loading metric called \"Total Maximum Daily Load\"  TMDL   The success of this model contributed to the EPA  s commitment to the use of the underlying TMDL protocol in EPA  s national policy for management of many river systems in the United States  24     The DSSAM Model is constructed to allow dynamic decay of most pollutants  for example  total nitrogen and phosphorus are allowed to be consumed by benthic algae in each time step  and the algal communities are given a separate population dynamic in each river reach  e g  based upon river temperature   Regarding stormwater runoff in Washoe County  the specific elements within a new xeriscape ordinance were analyzed for efficacy using the model  For the varied agricultural uses in the watershed  the model was run to understand the principal sources of impact  and management practices were developed to reduce in-river pollution  Use of the model has specifically been conducted to analyze survival of two endangered species found in the Truckee River and Pyramid Lake  the Cui-ui sucker fish   endangered 1967  and the Lahontan cutthroat trout  threatened 1970              Trout are species of freshwater fish belonging to the genera Oncorhynchus  Salmo and Salvelinus  all of the  subfamily Salmoninae of the family Salmonidae   The word trout is also used as part of the name of some non-salmonid fish such as Cynoscion nebulosus  the spotted seatrout or speckled trout     Trout are closely related to salmon and char  or charr   species termed salmon and char occur in the same genera as do fish called trout  Oncorhynchus  Pacific salmon and trout   Salmo  Atlantic salmon and various trout  Salvelinus  char and trout      Lake trout and most other trout live in freshwater lakes and rivers exclusively  while there are others  such as the steelhead  a form of the coastal rainbow trout  that can spend two or three years at sea before returning to fresh water to spawn  a habit more typical of salmon   Arctic char and brook trout are part of the char family  Trout are an important food source for humans and wildlife  including brown bears  birds of prey such as eagles  and other animals  They are classified as oily fish  1     The name \"trout\" is commonly used for some species in three of the seven genera in the subfamily Salmoninae  Salmo  Atlantic species  Oncorhynchus  Pacific species  and Salvelinus  which includes fish also sometimes called char or charr  Fish referred to as trout include     Trout that live in different environments can have dramatically different colorations and patterns  Mostly  these colors and patterns form as camouflage  based on the surroundings  and will change as the fish moves to different habitats  Trout in  or newly returned from the sea  can look very silvery  while the same fish living in a small stream or in an alpine lake could have pronounced markings and more vivid coloration  it is also possible that in some species  this signifies that they are ready to mate  In general  trout that are about to breed have extremely intense coloration and can look like an entirely different fish outside of spawning season  It is virtually impossible to define a particular color pattern as belonging to a specific breed  however  in general  wild fish are claimed to have more vivid colors and patterns     Trout have fins entirely without spines  and all of them have a small adipose fin along the back  near the tail  The pelvic fins sit well back on the body  on each side of the anus  The swim bladder is connected to the esophagus  allowing for gulping or rapid expulsion of air  a condition known as physostome  Unlike many other physostome fish  trout do not use their bladder as an auxiliary device for oxygen uptake  relying solely on their gills     There are many species  and even more populations  that are isolated from each other and morphologically different  However  since many of these distinct populations show no significant genetic differences  what may appear to be a large number of species is considered a much smaller number of distinct species by most ichthyologists  The trout found in the eastern United States are a good example of this  The brook trout  the aurora trout  and the  extinct  silver trout all have physical characteristics and colorations that distinguish them  yet genetic analysis shows that they are one species  Salvelinus fontinalis     Lake trout  Salvelinus namaycush   like brook trout  belong to the char genus  Lake trout inhabit many of the larger lakes in North America  and live much longer than rainbow trout  which have an average maximum lifespan of 7 years  Lake trout can live many decades  and can grow to more than 30 kilograms  66 xa0lb      Trout are usually found in cool  5060 xa0F or 1016 xa0C   clear streams and lakes  although many of the species have anadromous strains as well  Young trout are referred to as troutlet  troutling or fry  They are distributed naturally throughout North America  northern Asia and Europe  Several species of trout were introduced to Australia and New Zealand by amateur fishing enthusiasts in the 19th century  effectively displacing and endangering several upland native fish species  The introduced species included brown trout from England and rainbow trout from California  The rainbow trout were a steelhead strain  generally accepted as coming from Sonoma Creek  The rainbow trout of New Zealand still show the steelhead tendency to run up rivers in winter to spawn  2     In Australia the rainbow trout was introduced in 1894 from New Zealand and is an extremely popular gamefish in recreational angling  3  nDespite severely impacting the distribution and abundance of native Australian fish  such as the climbing galaxias  millions of rainbow and other trout species are released annually from government and private hatcheries  3     The closest resemblance of seema trout and other trout family can be found in the Himalayan Region of India  Nepal  Bhutan  Pakistan and in Tian Shan mountains of Kyrgyzstan  clarification needed     Trout generally feed on other fish  and soft bodied aquatic invertebrates  such as flies  mayflies  caddisflies  stoneflies  mollusks and dragonflies  In lakes  various species of zooplankton often form a large part of the diet  In general  trout longer than about 300 millimetres  12 xa0in  prey almost exclusively on fish  where they are available  Adult trout will devour smaller fish up to 1/3 their length  Trout  may feed on shrimp  mealworms  bloodworms  insects  small animal parts  and eel     Trout who swim the streams love to feed on land animals  aquatic life  and flies  4  Most of their diet comes from macroinvertebrates  or animals that do not have a backbone like snails  worms  or insects  They also eat flies  and most people who try to use lures to fish trout mimic flies because they are one of trout  s most fed on meals  4  Trout enjoy certain land animals  including insects like grasshoppers  They also eat small animals like mice when they fall in   Although only large trout have mouths capable of eating mice   They consume a diet of aquatic life like minnows or crawfish as well  Trout have a diverse diet they follow  they have plenty of different oppositions  4     As a group  trout are somewhat bony  but the flesh is generally considered to be tasty   The flavor of the flesh is heavily influenced by the diet of the fish  For example  trout that have been feeding on crustaceans tend to be more flavorful than those feeding primarily on insect life   Additionally  they provide a good fight when caught with a hook and line  and are sought after recreationally   Because of their popularity  trout are often raised on fish farms and planted into heavily fished waters  in an effort to mask the effects of overfishing  Farmed trout and char are also sold commercially as food fish  Trout is sometimes prepared by smoking  5     1 fillet of trout  79 xa0g  contains  6     While trout can be caught with a normal rod and reel  fly fishing is a distinctive method developed primarily for trout  and now extended to other species  Understanding how moving water shapes the stream channel makes it easier to find trout  In most streams  the current creates a riffle-run-pool pattern that repeats itself over and over  A deep pool may hold a big brown trout  but rainbows and smaller browns are likely found in runs  Riffles are where fishers will find small trout  called troutlet  during the day and larger trout crowding in during morning and evening feeding periods  7     Fishing for trout under the ice generally occurs in depths of 4 to 8 feet  Because trout are cold water fish  during the winter they move from up-deep to the shallows  replacing the small fish that inhabit the area during the summer  Trout in winter constantly cruise in shallow depths looking for food  usually traveling in groups  although bigger fish may travel alone and in water that  s somewhat deeper  around 12 feet  Rainbow  Brown  and Brook trout are the most common trout species caught through the ice  9     By information from International Game Fish Association  IGFA   the most outstanding records are  10     Waxworms are used as live-bait for trout fishing     Corn worms are also excellent live-bait when trout fishing     Nymph of a golden stonefly are used as live-bait for trout fishing     Nymph mayfly    salmon roe  Red caviar     Worms are cheap and a great bait to use for trout and most types of fish    Wooly buggers can be tied in every color imaginable    Egg patterns work great for steelhead and trout in rivers    Muddler minnow    Salmonid populations in general have been declining due to numerous factors  Non-native  invasive species  hybridization  wildfires  and climate change are just a few examples  Native salmonid fish in the western and southwestern United States are threatened by non-native species that were introduced decades ago  Non-native salmonids were introduced to enrich recreational fishing  11  however  they quickly started outcompeting and displacing native salmonids upon their arrival  Non-native  invasive species are quick to adapt to their new environment and learn to outcompete any native species  making them a force the native salmon and trout have to reckon with  Not only do the non-native fish drive the native fish to occupy new niches  but they also try to hybridize with them  contaminating the native gene construction  As more hybrids between native and non-native fish are formed  the lineage of the pure fish is continuously being contaminated by other species and soon may no longer represent the sole native species  The Rio Grande Cutthroat trout  Oncorhynchus clarki virginalis  are susceptible to hybridization with other salmonids such as rainbow trout  Oncorhynchus mykiss  and yield a new cut-bow trout  which is a contamination of both lineages genes  One solution to this issue is implemented by New Mexico Game and Fish hatcheries  stock only sterile fish in river streams  Hatcheries serve as a reservoir of fish for recreational activities but growing and stocking non-sterile fish would worsen the hybridization issue on a quicker  more magnified time scale  By stocking sterile fish  the native salmonids can  t share genes with the non-native hatchery fish  thus  preventing further gene contamination of the native trout in New Mexico  Fire is also a factor in deteriorating Gila trout  Oncorhynchus gilae  populations because of the ash and soot that can enter streams following fires  12  The ash lowers water quality  making it more difficult for the Gila trout to survive  In some New Mexico streams  the native Gila trout will be evacuated from streams that are threatened by nearby fires and be reintroduced after the threat is resolved  Again  climate change is also dwindling native salmonid populations  Climate change continually affects various cold-water fish  including trout  With an increase of temperature along with changes in spawning river flow  an abundance of trout species are effected negatively  In the past  a mere 8 xa0F increase was predicted to eliminate half of the native brook trout in the Southern Appalachian Mountains  13  Trout prefer cold water  50-60 xa0F  streams to spawn and live  but warming water temperatures are altering this ecosystem and further deteriorative native populations     Population dynamics is the type of mathematics used to model and study the size and age composition of populations as dynamical systems     Population dynamics has traditionally been the dominant branch of mathematical biology  which has a history of more than 220 years  1  although over the last century the scope of mathematical biology has greatly expanded     The beginning of population dynamics is widely regarded as the work of Malthus  formulated as the Malthusian growth model  According to Malthus  assuming that the conditions  the environment  remain constant  ceteris paribus   a population will grow  or decline  exponentially  2  18 This principle provided the basis for the subsequent predictive theories  such as the demographic studies such as the work of Benjamin Gompertz 3  and Pierre Franois Verhulst in the early 19th century  who refined and adjusted the Malthusian demographic model  4     A more general model formulation was proposed by F  J  Richards in 1959  5  further expanded by Simon Hopkins  in which the models of Gompertz  Verhulst and also Ludwig von Bertalanffy are covered as special cases of the general formulation  The LotkaVolterra predator-prey equations are another famous example  6  7  8  9  10  11  12  13  as well as the alternative ArditiGinzburg equations  14  15     Simplified population models usually start with four key variables  four demographic processes  including death  birth  immigration  and emigration  Mathematical models used to calculate changes in population demographics and evolution hold the assumption    null hypothesis    of no external influence  Models can be more mathematically complex where \"   several competing hypotheses are simultaneously confronted with the data \" 16  For example  in a closed system where immigration and emigration does not take place  the rate of change in the number of individuals in a population can be described as     where N is the total number of individuals in the specific experimental population being studied  B is the number of births and D is the number of deaths per individual in a particular experiment or model  The algebraic symbols b  d and r stand for the rates of birth  death  and the rate of change per individual in the general population  the intrinsic rate of increase  This formula can be read as the rate of change in the population  dN/dT  is equal to births minus deaths  B - D   2  13  17     Using these techniques  Malthus   population principle of growth was later transformed into a mathematical model known as the logistic equation     where N is the biomass density  a is the maximum per-capita rate of change  and K is the carrying capacity of the population  The formula can be read as follows  the rate of change in the population  dN/dT  is equal to growth  aN  that is limited by carrying capacity  1-N/K   From these basic mathematical principles the discipline of population ecology expands into a field of investigation that queries the demographics of real populations and tests these results against the statistical models  The field of population ecology often uses data on life history and matrix algebra to develop projection matrices on fecundity and survivorship  This information is used for managing wildlife stocks and setting harvest quotas  13  17     The rate at which a population increases in size if there are no density-dependent forces regulating the population is known as the intrinsic rate of increase  It is    where  the derivative                     d        N                  /                d        t                 displaystyle dN/dt    is the rate of increase of the population  N is the population size  and r is the intrinsic rate of increase  Thus r is the maximum theoretical rate of increase of a population per individual  that is  the maximum population growth rate  The concept is commonly used in insect population ecology or management to determine how environmental factors affect the rate at which pest populations increase  See also exponential population growth and logistic population growth  18     Population dynamics overlap with another active area of research in mathematical biology  mathematical epidemiology  the study of infectious disease affecting populations  Various models of viral spread have been proposed and analysed  and provide important results that may be applied to health policy decisions     The mathematical formula below can used to model geometric populations  Geometric populations grow in discrete reproductive periods between intervals of abstinence  as opposed to populations which grow without designated periods for reproduction  Say that N denotes the number of individuals in each generation of a population that will reproduce  20                                   N                      t                          +        1        =                  N                      t                          +                  B                      t                          +                  I                      t                                            D                      t                                            E                      t                                   displaystyle N^ t +1=N^ t +B^ t +I^ t -D^ t -E^ t     n    Where  Nt is the population size in generation t  and Nt+1 is the population size in the generation directly after Nt  Bt is the sum of births in the population between generations t and t+1  i e  the birth rate   It is the sum of immigrants added to the population between generations  Dt is the sum of deaths between generations  death rate   and Et is the sum of emigrants moving out of the population between generations     When there is no migration to or from the population   n                              N                      t                          +        1        =                  N                      t                          +                  B                      t                                            D                      t                                   displaystyle N^ t +1=N^ t +B^ t -D^ t     n    Assuming in this case that the birth and death rates are constants  then the birth rate minus the death rate equals R  the geometric rate of increase     Nt+1 = Nt + RNt    Nt+1 =  Nt + RNt     Take the term Nt out of the brackets again     Nt+1 =  1 + R Nt    1 + R =   where  is the finite rate of increase     Nt+1 = Nt    Therefore     Nt = tN0    The doubling time  td  of a population is the time required for the population to grow to twice its size  24  We can calculate the doubling time of a geometric population using the equation  Nt = tN0 by exploiting our knowledge of the fact that the population  N  is twice its size  2N  after the doubling time  20     Ntd = td  N0    2N0 = td  N0 n    td = 2N0 /  N0    td = 2    The doubling time can be found by taking logarithms  For instance     td  log2   = log2 2     log2 2  = 1    td  log2   = 1    td = 1 / log2      Or     td  ln   = ln 2     td = ln 2  / ln      td = 0 693    / ln      Therefore     td = 1 / log2   = 0 693    / ln       The half-life of a population is the time taken for the population to decline to half its size  We can calculate the half-life of a geometric population using the equation  Nt = tN0 by exploiting our knowledge of the fact that the population  N  is half its size  0 5N  after a half-life  20     Nt1/2 = t1/2  N0 n    0 5N0 = t1/2  N0    t1/2 = 0 5N0 /  N0    t1/2 = 0 5    The half-life can be calculated by taking logarithms  see above      t1/2 = 1 / log0 5   = ln 0 5  / ln       R = b - d    Nt+1 = Nt + RNt    Nt+1 - Nt = RNt    Nt+1 - Nt = N    N = RNt    N/Nt = R    1 + R =     Nt+1 = Nt     = Nt+1 / Nt    In geometric populations  R and  represent growth constants  see 2 and 2 3   In logistic populations however  the intrinsic growth rate  also known as intrinsic rate of increase  r  is the relevant growth constant  Since generations of reproduction in a geometric population do not overlap  e g  reproduce once a year  but do in an exponential population  geometric and exponential populations are usually considered to be mutually exclusive  25  However  both sets of constants share the mathematical relationship below  20     The growth equation for exponential populations is    Nt = N0ert    Assumption  Nt  of a geometric population  = Nt  of a logistic population      Therefore     N0ert = N0t    N0 cancels on both sides     N0ert / N0 = t    ert = t    Take the natural logarithms of the equation  Using natural logarithms instead of base 10 or base 2 logarithms simplifies the final equation as ln e  = 1     rt  ln e  = t  ln      In this case  e1 = e therefore ln e  = 1     rt  1 = t  ln      rt = t  ln      t cancels on both sides     rt / t = ln      The results     r = ln      and    er =     Evolutionary game theory was first developed by Ronald Fisher in his 1930 article The Genetic Theory of Natural Selection  26  In 1973 John Maynard Smith formalised a central concept  the evolutionarily stable strategy  27     Population dynamics have been used in several control theory applications  Evolutionary game theory can be used in different industrial or other contexts  Industrially  it is mostly used in multiple-input-multiple-output  MIMO  systems  although it can be adapted for use in single-input-single-output  SISO  systems  Some other examples of applications are military campaigns  water distribution  dispatch of distributed generators  lab experiments  transport problems  communication problems  among others     The computer game SimCity  Sim Earth and the MMORPG Ultima Online  among others  tried to simulate some of these population dynamics          n    all other Oncorhynchus and Salmo species    Salmon /smn/ is the common name for several species of ray-finned fish in the family Salmonidae  Other fish in the same family include trout  char  grayling  and whitefish  Salmon are native to tributaries of the North Atlantic  genus Salmo  and Pacific Ocean  genus Oncorhynchus   Many species of salmon have been introduced into non-native environments such as the Great Lakes of North America and Patagonia in South America  Salmon are intensively farmed  in many parts of the world  1     Typically  salmon are anadromous  they hatch in fresh water  migrate to the ocean  then return to fresh water to reproduce  However  populations of several species are restricted to fresh water throughout their lives  Folklore has it that the fish return to the exact spot where they hatched to spawn  Tracking studies have shown this to be mostly true  A portion of a returning salmon run may stray and spawn in different freshwater systems  the percent of straying depends on the species of salmon  2  Homing behavior has been shown to depend on olfactory memory  3  4  Salmon date back to the Neogene  citation needed     The term \"salmon\" comes from the Latin salmo  which in turn might have originated from salire  meaning \"to leap\"  5  The nine commercially important species of salmon occur in two genera  The genus Salmo contains the Atlantic salmon  found in the North Atlantic  as well as many species commonly named trout  The genus Oncorhynchus contains eight species which occur naturally only in the North Pacific  As a group  these are known as Pacific salmon  Chinook salmon have been introduced in New Zealand and Patagonia  Coho  freshwater sockeye  and Atlantic salmon have been established in Patagonia  as well  6       xa0  xa0  Both the Salmo and Oncorhynchus genera also contain a number of species referred to as trout  Within Salmo  additional minor taxa have been called salmon in English  i e  the Adriatic salmon  Salmo obtusirostris  and Black Sea salmon  Salmo labrax   The steelhead anadromous form of the rainbow trout migrates to sea  but it is not termed \"salmon\"     Also  there are several other species which are not true salmon  as in the above list but have common names which refer to them as being salmon  Of those listed below  the Danube salmon or huchen is a large freshwater salmonid related to the salmon above  but others are marine fishes of the unrelated Perciformes order     Eosalmo driftwoodensis  the oldest known salmon in the fossil record  helps scientists figure how the different species of salmon diverged from a common ancestor  The British Columbia salmon fossil provides evidence that the divergence between Pacific and Atlantic salmon had not yet occurred 40 million years ago  Both the fossil record and analysis of mitochondrial DNA suggest the divergence occurred 10 to 20 million years ago  This independent evidence from DNA analysis and the fossil record indicate that salmon divergence occurred long before the glaciers  of Quaternary glaciation  began their cycle of advance and retreat  39     Salmon eggs are laid in freshwater streams typically at high latitudes  The eggs hatch into alevin or sac fry  The fry quickly develop into parr with camouflaging vertical stripes  The parr stay for six months to three years in their natal stream before becoming smolts  which are distinguished by their bright  silvery colour with scales that are easily rubbed off  Only 10% of all salmon eggs are estimated to survive to this stage  50     The smolt body chemistry changes  allowing them to live in salt water  While a few species of salmon remain in fresh water throughout their life cycle  the majority are anadromous and migrate to the ocean for maturation  in these species  smolts spend a portion of their out-migration time in brackish water  where their body chemistry becomes accustomed to osmoregulation in the ocean  This body chemistry change is hormone-driven  causing physiological adjustments in the function of osmoregulatory organs such as the gills  which leads to large increases in their ability to secrete salt  51  Hormones involved in increasing salinity tolerance include insulin-like growth factor I  cortisol  and thyroid hormones  52  which permits the fish to endure the transition from a freshwater environment to the ocean     The salmon spend about one to five years  depending on the species  in the open ocean  where they gradually become sexually mature  The adult salmon then return primarily to their natal streams to spawn  Atlantic salmon spend between one and four years at sea  When a fish returns after just one year  s sea feeding  it is called a grilse in Canada  Britain  and Ireland  Grilse may be present at spawning  and go unnoticed by large males  releasing their own sperm on the eggs  53  page xa0needed     Prior to spawning  depending on the species  salmon undergo changes  They may grow a hump  develop canine-like teeth  or develop a kype  a pronounced curvature of the jaws in male salmon   All change from the silvery blue of a fresh-run fish from the sea to a darker colour  Salmon can make amazing journeys  sometimes moving hundreds of miles upstream against strong currents and rapids to reproduce  Chinook and sockeye salmon from central Idaho  for example  travel over 1 400 xa0km  900 xa0mi  and climb nearly 2 100 xa0m  7 000 xa0ft  from the Pacific Ocean as they return to spawn  Condition tends to deteriorate the longer the fish remain in fresh water  and they then deteriorate further after they spawn  when they are known as kelts  In all species of Pacific salmon  the mature individuals die within a few days or weeks of spawning  a trait known as semelparity  Between 2 and 4% of Atlantic salmon kelts survive to spawn again  all females  However  even in those species of salmon that may survive to spawn more than once  iteroparity   postspawning mortality is quite high  perhaps as high as 40 to 50%      To lay her roe  the female salmon uses her tail  caudal fin   to create a low-pressure zone  lifting gravel to be swept downstream  excavating a shallow depression  called a redd  The redd may sometimes contain 5 000 eggs covering 2 8 xa0m2  30 xa0sq xa0ft   54  The eggs usually range from orange to red  One or more males approach the female in her redd  depositing sperm  or milt  over the roe  49  The female then covers the eggs by disturbing the gravel at the upstream edge of the depression before moving on to make another redd  The female may make as many as seven redds before her supply of eggs is exhausted  49     Each year  the fish experiences a period of rapid growth  often in summer  and one of slower growth  normally in winter  This results in ring formation around an earbone called the otolith  annuli   analogous to the growth rings visible in a tree trunk  Freshwater growth shows as densely crowded rings  sea growth as widely spaced rings  spawning is marked by significant erosion as body mass is converted into eggs and milt     Freshwater streams and estuaries provide important habitat for many salmon species  They feed on terrestrial and aquatic insects  amphipods  and other crustaceans while young  and primarily on other fish when older  Eggs are laid in deeper water with larger gravel  and need cool water and good water flow  to supply oxygen  to the developing embryos  Mortality of salmon in the early life stages is usually high due to natural predation and human-induced changes in habitat  such as siltation  high water temperatures  low oxygen concentration  loss of stream cover  and reductions in river flow  Estuaries and their associated wetlands provide vital nursery areas for the salmon prior to their departure to the open ocean  Wetlands not only help buffer the estuary from silt and pollutants  but also provide important feeding and hiding areas     Salmon not killed by other means show greatly accelerated deterioration  phenoptosis  or \"programmed aging\"  at the end of their lives  Their bodies rapidly deteriorate right after they spawn as a result of the release of massive amounts of corticosteroids     Juvenile salmon  parr  grow up in the relatively protected natal river    The parr lose their camouflage bars and become smolt as they become ready for the transition to the ocean     Male ocean-phase adult sockeye    Male spawning-phase adult sockeye    In the Pacific Northwest and Alaska  salmon are keystone species  supporting wildlife such as birds  bears and otters  55  The bodies of salmon represent a transfer of nutrients from the ocean  rich in nitrogen  sulfur  carbon and phosphorus  to the forest ecosystem     Grizzly bears function as ecosystem engineers  capturing salmon and carrying them into adjacent wooded areas  There they deposit nutrient-rich urine and feces and partially eaten carcasses  Bears are estimated to leave up to half the salmon they harvest on the forest floor  56  57  in densities that can reach 4 000 kilograms per hectare  58  providing as much as 24% of the total nitrogen available to the riparian woodlands  The foliage of spruce trees up to 500 xa0m  1 600 xa0ft  from a stream where grizzlies fish salmon have been found to contain nitrogen originating from fished salmon  59     Beavers also function as ecosystem engineers  in the process of clear-cutting and damming  beavers alter their ecosystems extensively  Beaver ponds can provide critical habitat for juvenile salmon  An example of this was seen in the years following 1818 in the Columbia River Basin     In 1818  the British government made an agreement with the U S  government to allow U S  citizens access to the Columbia catchment  see Treaty of 1818   At the time  the Hudson  s Bay Company sent word to trappers to extirpate all furbearers from the area in an effort to make the area less attractive to U S  fur traders  In response to the elimination of beavers from large parts of the river system  salmon runs plummeted  even in the absence of many of the factors usually associated with the demise of salmon runs  Salmon recruitment can be affected by beavers   dams because dams can  60  61  62     Beavers   dams are able to nurture salmon juveniles in estuarine tidal marshes where the salinity is less than 10 xa0ppm  Beavers build small dams of generally less than 60 xa0cm  2 xa0ft  high in channels in the myrtle zone clarification needed   These dams can be overtopped at high tide and hold water at low tide  This provides refuges for juvenile salmon so they do not have to swim into large channels where they are subject to predation  63     It has been discovered that rivers which have seen a decline or disappearance of anadromous lampreys  loss of the lampreys also affects the salmon in a negative way  Like salmon  anadromous lampreys stop feeding and die after spawning  and their decomposing bodies release nutrients into the stream  Also  along with species like rainbow trout and Sacramento sucker  lampreys clean the gravel in the rivers during spawning  64  Their larvae  called ammocoetes  are filter feeders which contribute to the health of the waters  They are also a food source for the young salmon  and being fattier and oilier  it is assumed predators prefer them over salmon offspring  taking off some of the predation pressure on smolts  65  unreliable source?  Adult lampreys are also the preferred prey of seals and sea lions  which can eat 30 lampreys to every salmon  allowing more adult salmon to enter the rivers to spawn without being eaten by the marine mammals  66  67     According to Canadian biologist Dorothy Kieser  the myxozoan parasite Henneguya salminicola is commonly found in the flesh of salmonids  It has been recorded in the field samples of salmon returning to the Haida Gwaii Islands  The fish responds by walling off the parasitic infection into a number of cysts that contain milky fluid  This fluid is an accumulation of a large number of parasites     Henneguya and other parasites in the myxosporean group have complex life cycles  where the salmon is one of two hosts  The fish releases the spores after spawning  In the Henneguya case  the spores enter a second host  most likely an invertebrate  in the spawning stream  When juvenile salmon migrate to the Pacific Ocean  the second host releases a stage infective to salmon  The parasite is then carried in the salmon until the next spawning cycle  The myxosporean parasite that causes whirling disease in trout has a similar life cycle  68  However  as opposed to whirling disease  the Henneguya infestation does not appear to cause disease in the host salmoneven heavily infected fish tend to return to spawn successfully     According to Dr  Kieser  a lot of work on Henneguya salminicola was done by scientists at the Pacific Biological Station in Nanaimo in the mid-1980s  in particular  an overview report 69  which states  \"the fish that have the longest fresh water residence time as juveniles have the most noticeable infections  Hence in order of prevalence  coho are most infected followed by sockeye  chinook  chum and pink  As well  the report says  at the time the studies were conducted  stocks from the middle and upper reaches of large river systems in British Columbia such as Fraser  Skeena  Nass and from mainland coastal streams in the southern half of B C   \"are more likely to have a low prevalence of infection \" The report also states  \"It should be stressed that Henneguya  economically deleterious though it is  is harmless from the view of public health  It is strictly a fish parasite that cannot live in or affect warm blooded animals  including man\"     According to Klaus Schallie  Molluscan Shellfish Program Specialist with the Canadian Food Inspection Agency  \"Henneguya salminicola is found in southern B C  also and in all species of salmon  I have previously examined smoked chum salmon sides that were riddled with cysts and some sockeye runs in Barkley Sound  southern B C   west coast of Vancouver Island  are noted for their high incidence of infestation \" citation needed     Sea lice  particularly Lepeophtheirus salmonis and various Caligus species  including C  clemensi and C  rogercresseyi  can cause deadly infestations of both farm-grown and wild salmon  70  71  Sea lice are ectoparasites which feed on mucus  blood  and skin  and migrate and latch onto the skin of wild salmon during free-swimming  planktonic nauplii and copepodid larval stages  which can persist for several days  72  73  74     Large numbers of highly populated  open-net salmon farms A  ncan create exceptionally large concentrations of sea lice  when exposed in river estuaries containing large numbers of open-net farms  many young wild salmon are infected  and do not survive as a result  76  77  Adult salmon may survive otherwise critical numbers of sea lice  but small  thin-skinned juvenile salmon migrating to sea are highly vulnerable  On the Pacific coast of Canada  the louse-induced mortality of pink salmon in some regions is commonly over 80%  78     The risk of injury caused by underwater pile driving has been studied by Dr  Halvorsen and her co-workers  79  The study concluded that the fish are at risk of injury if the cumulative sound exposure level exceeds 210 dB relative to 1 Pa2 s  clarification needed     As can be seen from the production chart at the left  the global capture reported by different countries to the FAO of commercial wild salmon has remained fairly steady since 1990 at about one million tonnes per year  This is in contrast to farmed salmon  below  which has increased in the same period from about 0 6 million tonnes to well over two million tonnes  40     Nearly all captured wild salmon are Pacific salmon  The capture of wild Atlantic salmon has always been relatively small  and has declined steadily since 1990  In 2011 only 2 500 tonnes were reported  8  In contrast about half of all farmed salmon are Atlantic salmon     Recreational salmon fishing can be a technically demanding kind of sport fishing  not necessarily congenial for beginning fishermen  80  A conflict exists between commercial fishermen and recreational fishermen for the right to salmon stock resources  Commercial fishing in estuaries and coastal areas is often restricted so enough salmon can return to their natal rivers where they can spawn and be available for sport fishing  On parts of the North American west coast sport salmon fishing completely replaces inshore commercial fishing  81  In most cases  the commercial value of a salmon can be several times less than the value attributed to the same fish caught by a sport fisherman  This is \"a powerful economic argument for allocating stock resources preferentially to sport fishing \" 81     Salmon aquaculture is a major contributor to the world production of farmed finfish  representing about US$10 xa0billion annually  Other commonly cultured fish species include tilapia  catfish  sea bass  carp and bream  Salmon farming is significant in Chile  Norway  Scotland  Canada and the Faroe Islands  it is the source for most salmon consumed in the United States and Europe  Atlantic salmon are also  in very small volumes  farmed in Russia and Tasmania  Australia     Salmon are carnivorous  They are fed a meal produced from catching other wild fish and other marine organisms  Salmon farming leads to a high demand for wild forage fish  Salmon require large nutritional intakes of protein  and farmed salmon consume more fish than they generate as a final product  On a dry weight basis  24 xa0kg of wild-caught fish are needed to produce one kg of salmon  82  As the salmon farming industry expands  it requires more wild forage fish for feed  at a time when 75% of the world  s monitored fisheries are already near to or have exceeded their maximum sustainable yield  83  The industrial-scale extraction of wild forage fish for salmon farming affects the survivability of the wild predator fish which rely on them for food     Work continues on substituting vegetable proteins for animal proteins in the salmon diet  This substitution results in lower levels of the highly valued omega-3 fatty acid content in the farmed product     Intensive salmon farming uses open-net cages  which have low production costs  It has the drawback of allowing disease and sea lice to spread to local wild salmon stocks  84     Another form of salmon production  which is safer but less controllable  is to raise salmon in hatcheries until they are old enough to become independent  They are released into rivers in an attempt to increase the salmon population  This system is referred to as ranching  It was very common in countries such as Sweden  before the Norwegians developed salmon farming  but is seldom done by private companies  As anyone may catch the salmon when they return to spawn  a company is limited in benefiting financially from their investment     Because of this  the ranching method has mainly been used by various public authorities and nonprofit groups  such as the Cook Inlet Aquaculture Association  as a way to increase salmon populations in situations where they have declined due to overharvesting  construction of dams  and habitat destruction or fragmentation  Negative consequences to this sort of population manipulation include genetic \"dilution\" of the wild stocks  Many jurisdictions are now beginning to discourage supplemental fish planting in favour of harvest controls  and habitat improvement and protection     A variant method of fish stocking  called ocean ranching  is under development in Alaska  There  the young salmon are released into the ocean far from any wild salmon streams  When it is time for them to spawn  they return to where they were released  where fishermen can catch them     An alternative method to hatcheries is to use spawning channels  These are artificial streams  usually parallel to an existing stream  with concrete or rip-rap sides and gravel bottoms  Water from the adjacent stream is piped into the top of the channel  sometimes via a header pond  to settle out sediment  Spawning success is often much better in channels than in adjacent streams due to the control of floods  which in some years can wash out the natural redds  Because of the lack of floods  spawning channels must sometimes be cleaned out to remove accumulated sediment  The same floods that destroy natural redds also clean the regular streams  Spawning channels preserve the natural selection of natural streams  as there is no benefit  as in hatcheries  to use prophylactic chemicals to control diseases  citation needed     Farm-raised salmon are fed the carotenoids astaxanthin and canthaxanthin to match their flesh colour to wild salmon 85  to improve their marketability  86  Wild salmon get these carotenoids  primarily astaxanthin  from eating shellfish and krill     One proposed alternative to the use of wild-caught fish as feed for the salmon  is the use of soy-based products  This should be better for the local environment of the fish farm  but producing soy beans has a high environmental cost for the producing region  The fish omega-3 fatty acid content would be reduced compared to fish-fed salmon     Another possible alternative is a yeast-based coproduct of bioethanol production  proteinaceous fermentation biomass  Substituting such products for engineered feed can result in equal  sometimes enhanced  growth in fish  87  With its increasing availability  this would address the problems of rising costs for buying hatchery fish feed     Yet another attractive alternative is the increased use of seaweed  Seaweed provides essential minerals and vitamins for growing organisms  It offers the advantage of providing natural amounts of dietary fiber and having a lower glycemic load than grain-based fish meal  87  In the best-case scenario  widespread use of seaweed could yield a future in aquaculture that eliminates the need for land  freshwater  or fertilizer to raise fish  88  failed verification     Salmon population levels are of concern in the Atlantic and in some parts of the Pacific  90  The population of wild salmon declined markedly in recent decades  especially North Atlantic populations  which spawn in the waters of western Europe and eastern Canada  and wild salmon in the Snake and Columbia River systems in northwestern United States     Alaska fishery stocks are still abundant  and catches have been on the rise in recent decades  after the state initiated limitations in 1972  91  92  citation needed  Some of the most important Alaskan salmon sustainable wild fisheries are located near the Kenai River  Copper River  and in Bristol Bay  Fish farming of Pacific salmon is outlawed in the United States Exclusive Economic Zone  93  however  there is a substantial network of publicly funded hatcheries  94  and the State of Alaska  s fisheries management system is viewed as a leader in the management of wild fish stocks     In Canada  returning Skeena River wild salmon support commercial  subsistence and recreational fisheries  as well as the area  s diverse wildlife on the coast and around communities hundreds of miles inland in the watershed  The status of wild salmon in Washington is mixed  Of 435 wild stocks of salmon and steelhead  only 187 of them were classified as healthy  113 had an unknown status  one was extinct  12 were in critical condition and 122 were experiencing depressed populations  95     The commercial salmon fisheries in California have been either severely curtailed or closed completely in recent years  due to critically low returns on the Klamath and or Sacramento rivers  causing millions of dollars in losses to commercial fishermen  96  Both Atlantic and Pacific salmon are popular sportfish     Salmon populations have been established in all the Great Lakes  Coho stocks were planted by the state of Michigan in the late 1960s to control the growing population of non-native alewife  Now Chinook  king   Atlantic  and coho  silver  salmon are annually stocked in all Great Lakes by most bordering states and provinces  These populations are not self-sustaining and do not provide much in the way of a commercial fishery  but have led to the development of a thriving sport fishery     Wild  self sustaining Pacific salmon populations have been established in New Zealand  Chile  and Argentina  97  They are highly prized by sport fishers  but others worry about displacing native fish species  98   Also  and especially in Chile  Aquaculture in Chile   both Atlantic and Pacific salmon are used in net pen farming     In 2020 researchers reported widespread declines in the sizes of four species of wild Pacific salmon  Chinook  chum  coho  and sockeye  These declines have been occurring for 30 years  and are thought to be associated with climate change and competition with growing numbers of pink and hatchery salmon  99  89     Salmon is a popular food  Classified as an oily fish  100  salmon is considered to be healthy due to the fish  s high protein  high omega-3 fatty acids  and high vitamin D 101  content  Salmon is also a source of cholesterol  with a range of 23214 xa0mg/100 xa0g depending on the species  102  According to reports in the journal Science  farmed salmon may contain high levels of dioxins  medical citation needed  PCB  polychlorinated biphenyl  levels may be up to eight times higher in farmed salmon than in wild salmon  103  but still well below levels considered dangerous  104  105  Nonetheless  according to a 2006 study published in the Journal of the American Medical Association  the benefits of eating even farmed salmon still outweigh any risks imposed by contaminants  106  Farmed salmon has a high omega 3 fatty acid content comparable to wild salmon  107  The type of omega-3 present may not be a factor for other important health functions  vague     Salmon flesh is generally orange to red  although white-fleshed wild salmon with white-black skin colour occurs  The natural colour of salmon results from carotenoid pigments  largely astaxanthin  but also canthaxanthin  in the flesh  108  Wild salmon get these carotenoids from eating krill and other tiny shellfish     The vast majority of Atlantic salmon available around the world are farmed  almost 99%   109  whereas the majority of Pacific salmon are wild-caught  greater than 80%   Canned salmon in the US is usually wild Pacific catch  though some farmed salmon is available in canned form  Smoked salmon is another popular preparation method  and can either be hot or cold smoked  Lox can refer to either cold-smoked salmon or salmon cured in a brine solution  also called gravlax    Traditional canned salmon includes some skin  which is harmless  and bone  which adds calcium   Skinless and boneless canned salmon is also available     Raw salmon flesh may contain Anisakis nematodes  marine parasites that cause anisakiasis  Before the availability of refrigeration  the Japanese did not consume raw salmon  Salmon and salmon roe have only recently come into use in making sashimi  raw fish  and sushi  110     To the Indigenous peoples of the Pacific Northwest Coast  salmon is considered a vital part of the diet  Specifically  the indigenous peoples of Haida Gwaii  located near former Queen Charlotte Island in British Columbia  rely on salmon as one of their main sources of food  although many other bands have fished Pacific waters for centuries  111  Salmon are not only ancient and unique  but it is important because it is expressed in culture  art forms  and ceremonial feasts  Annually  salmon spawn in Haida  feeding on everything on the way upstream and down  111  Within the Haida nation  salmon is referred to as \"tsiin\"  111  and is prepared in several ways including smoking  baking  frying  and making soup     Historically  there has always been enough salmon  as people would not overfish  and only took what they needed  112  In 2003  a report on First Nation participation in commercial fisheries  including salmon  commissioned by BC  s Ministry of Agriculture  Food and Fisheries found that there were 595 nFirst Nation-owned and operated commercial vessels in the province  Of those vessels  First Nations   members owned 564  112  However  employment within the industry has decreased overall by 50% in the last decade  with 8 142 registered commercial fishermen in 2003  This has affected employment for many fisherman  who rely on salmon as a source of income  relevant?     Black bears also rely on salmon as food  The leftovers the bears leave behind are considered important nutrients for the Canadian forest  such as the soil  trees  and plants  In this sense  the salmon feed the forest and in return receive clean water and gravel in which to hatch and grow  sheltered from extremes of temperature and water flow in times of high and low rainfall  111  However  the condition of the salmon in Haida has been affected in recent decades  Due to logging and development  much of the salmon  s habitat  i e   Ain River  has been destroyed  resulting in the fish being close to endangered  111  For residents  this has resulted in limits on catches  in turn  has affected families diets  and cultural events such as feasts  Some of the salmon systems in danger include  the Davidon  Naden  Mamim  and Mathers  111  It is clear that further protection is needed for salmon  such as their habitats  where logging commonly occurs     The salmon has long been at the heart of the culture and livelihood of coastal dwellers  which can be traced as far back as 5 000 years when archeologists discovered Nisqually tribes remnants  113  The original distribution of the Genus Oncorhynchus covered the Pacific Rim coastline  114  History shows salmon used tributaries  rivers and estuaries without regard to jurisdiction for 1822 million years  Baseline data is near impossible to recreate based on the inconsistent historical data  but confirmed there have been massive depletion since the 1900s  The Pacific Northwest was once sprawled with native inhabitants who practiced eco management  to ensure little degradation was caused by their actions to salmon habitats  xa0 As animists  the indigenous people relied not only for salmon for food  but spiritual guidance  The role of the salmon spirit guided the people to respect ecological systems such as the rivers and tributaries the salmon used for spawning  Natives often used the entire fish and left no waste by creating items such turning the bladder into glue  bones for toys  and skin for clothing and shoes  The first salmon ceremony was introduced by indigenous tribes on the pacific coast  which consists of three major parts  First is the welcoming of the first catch  then comes the cooking and lastly  the return of the bones to the Sea to induce hospitality so that other salmon would give their lives to the people of that village  115      Many tribes such as the Yurok had a taboo against harvesting the first fish that swam upriver in summer  but once they confirmed that the salmon had returned in abundance they would begin to catch them in plentiful  116  The indigenous practices were guided by deep ecological wisdom  which was eradicated when Euro-American settlements began to be developed  117  Salmon have a much grander history than what is presently shown today  The Salmon that once dominated the Pacific Ocean are now just a fraction in population and size  The Pacific salmon population is now less than 13% of what it was when Lewis and Clark arrived at the region  118  In his 1908 State of the Union address  U S  President Theodore Roosevelt observed that the fisheries were in significant decline  119  120     The salmon fisheries of the Columbia River are now but a fraction of what they were twenty-five years ago  and what they would be now if the United States Government had taken complete charge of them by intervening between Oregon and Washington  During these twenty-five years the fishermen of each State have naturally tried to take all they could get  and the two legislatures have never been able to agree on joint action of any kind adequate in degree for the protection of the fisheries  At the moment the fishing on the Oregon side is practically closed  while there is no limit on the Washington side of any kind  and no one can tell what the courts will decide as to the very statutes under which this action and non-action result  Meanwhile very few salmon reach the spawning grounds  and probably four years hence the fisheries will amount to nothing  and this comes from a struggle between the associated  or gill-net  fishermen on the one hand  and the owners of the fishing wheels up the river     On the Columbia River the Chief Joseph Dam completed in 1955 completely blocks salmon migration to the upper Columbia River system     The Fraser River salmon population was affected by the 1914 slide caused by the Canadian Pacific Railway at Hells Gate  The 1917 catch was one quarter of the 1913 catch  121     The origin of the word for \"salmon\" was one of the arguments about the location of the origin of the Indo-European languages     The salmon is an important creature in several strands of Celtic mythology and poetry  which often associated them with wisdom and venerability  In Irish folklore  fishermen associated salmon with fairies and thought it was unlucky to refer to them by name  122  In Irish mythology  a creature called the Salmon of Knowledge 123  plays key role in the tale The Boyhood Deeds of Fionn  In the tale  the Salmon will grant powers of knowledge to whoever eats it  and is sought by poet Finn Eces for seven years  Finally Finn Eces catches the fish and gives it to his young pupil  Fionn mac Cumhaill  to prepare it for him  However  Fionn burns his thumb on the salmon  s juices  and he instinctively puts it in his mouth  In so doing  he inadvertently gains the Salmon  s wisdom  Elsewhere in Irish mythology  the salmon is also one of the incarnations of both Tuan mac Cairill 124  and Fintan mac Bchra  125     Salmon also feature in Welsh mythology  In the prose tale Culhwch and Olwen  the Salmon of Llyn Llyw is the oldest animal in Britain  and the only creature who knows the location of Mabon ap Modron  After speaking to a string of other ancient animals who do not know his whereabouts  King Arthur  s men Cai and Bedwyr are led to the Salmon of Llyn Llyw  who lets them ride its back to the walls of Mabon  s prison in Gloucester  126     In Norse mythology  after Loki tricked the blind god Hr into killing his brother Baldr  Loki jumped into a river and transformed himself into a salmon to escape punishment from the other gods  When they held out a net to trap him he attempted to leap over it but was caught by Thor who grabbed him by the tail with his hand  and this is why the salmon  s tail is tapered  127     Salmon are central spiritually and culturally to Native American mythology on the Pacific coast  from the Haida and Coast Salish peoples  to the Nuu-chah-nulth peoples in British Columbia  128     Category  By country     Thermal pollution  sometimes called \"thermal enrichment \" is the degradation of water quality by any process that changes ambient water temperature  Thermal pollution is the rise or fall in the temperature of a natural body of water caused by human influence  Thermal pollution  unlike chemical pollution  results in a change in the physical properties of water   A common cause of thermal pollution is the use of water as a coolant by power plants and industrial manufacturers  Urban runoffstormwater discharged to surface waters from roads and parking lotsand reservoirs can also be a source of thermal pollution  4  Thermal pollution can also be caused by the release of very cold water from the base of reservoirs into warmer rivers       When water used as a coolant is returned to the natural environment at a higher temperature  the sudden change in temperature decreases oxygen supply and affects ecosystem composition  Fish and other organisms adapted to particular temperature range can be killed by an abrupt change in water temperature  either a rapid increase or decrease  known as \"thermal shock \" Warm coolant water can also have long term effects on water temperature  increasing the overall temperature of water bodies  including deep water  Seasonality effects how these temperature increases are distributed throughout the water column  Elevated water temperatures decrease oxygen levels  which can kill fish and alter food chain composition  reduce species biodiversity  and foster invasion by new thermophilic species  5  6  375       In the United States  about 75 to 82 percent of thermal pollution is generated by power plants  7  335 The remainder is from industrial sources such as petroleum refineries  pulp and paper mills  chemical plants  steel mills and smelters  8  42  9     Heated water from these sources may be controlled with     One of the largest contributors to thermal pollution are once-through cooling  OTC  systems which do not reduce temperature as effectively as the above systems  A large power plant may withdraw and export as many as 500 million gallons per day  11  These systems produce water 10C warmer on average  12  For example  the Potrero Generating Station in San Francisco  closed in 2011   used OTC and discharged water to San Francisco Bay approximately 10 xa0C  20 xa0F  above the ambient bay temperature  13  Over 1 200 facilities in the United States use OTC systems as of 2014  8  44    Temperatures can be taken through remote sensing techniques to continually monitor plants   pollution  14  This aids in quantifying each plants   specific effects  and allows for tighter regulation of thermal pollution     Converting facilities from once-through cooling to closed-loop systems can significantly decrease the thermal pollution emitted  11  These systems release water at a temperature more comparable to the natural environment      nAs water stratifies within man-made dams  the temperature at the bottom drops dramatically  Many dams are constructed to release this cold water from the bottom into the natural systems  15  This may be mitigated by designing the dam to release warmer surface waters instead of the colder water at the bottom of the reservoir  16     During warm weather  urban runoff can have significant thermal impacts on small streams  As storm water passes over hot parking lots  roads and sidewalks it absorbs some of the heat  an effect of the urban heat island  Storm water management facilities that absorb runoff or direct it into groundwater  such as bioretention systems and infiltration basins  reduce these thermal effects by allowing the water more time to release excess heat before entering the aquatic environment  These related systems for managing runoff are components of an expanding urban design approach commonly called green infrastructure  17     Retention basins  stormwater ponds  tend to be less effective at reducing runoff temperature  as the water may be heated by the sun before being discharged to a receiving stream  18     Elevated temperature typically decreases the level of dissolved oxygen and of water  as gases are less soluble in hotter liquids  This can harm aquatic animals such as fish  amphibians and other aquatic organisms  Thermal pollution may also increase the metabolic rate of aquatic animals  as enzyme activity  resulting in these organisms consuming more food in a shorter time than if their environment were not changed  5  179 An increased metabolic rate may result in fewer resources  the more adapted organisms moving in may have an advantage over organisms that are not used to the warmer temperature  As a result  food chains of the old and new environments may be compromised  Some fish species will avoid stream segments or coastal areas adjacent to a thermal discharge  Biodiversity can be decreased as a result  21  41517 7  340    High temperature limits oxygen dispersion into deeper waters  contributing to anaerobic conditions  This can lead to increased bacteria levels when there is ample food supply  Many aquatic species will fail to reproduce at elevated temperatures  5  17980    Primary producers  e g  plants  cyanobacteria  are affected by warm water because higher water temperature increases plant growth rates  resulting in a shorter lifespan and species overpopulation  The increased temperature can also change the balance of microbial growth  including the rate of algae blooms which reduce dissolved oxygen concentrations  22     Temperature changes of even one to two degrees Celsius can cause significant changes in organism metabolism and other adverse cellular biology effects   Principal adverse changes can include rendering cell walls less permeable to necessary osmosis  coagulation of cell proteins  and alteration of enzyme metabolism   These cellular level effects can adversely affect mortality and reproduction     A large increase in temperature can lead to the denaturing of life-supporting enzymes by breaking down hydrogen- and disulphide bonds within the quaternary structure of the enzymes  Decreased enzyme activity in aquatic organisms can cause problems such as the inability to break down lipids  which leads to malnutrition  Increased water temperature can also increase the solubility and kinetics of metals  which can increase the uptake of heavy metals by aquatic organisms  This can lead to toxic outcomes for these species  as well as build up of heavy metals in higher trophic levels in the food chain  increasing human exposures via dietary ingestion  22     In limited cases  warm water has little deleterious effect and may even lead to improved function of the receiving aquatic ecosystem  This phenomenon is seen especially in seasonal waters  An extreme case is derived from the aggregational habits of the manatee  which often uses power plant discharge sites during winter  Projections suggest that manatee populations would decline upon the removal of these discharges  23     Releases of unnaturally cold water from reservoirs can dramatically change the fish and macroinvertebrate fauna of rivers  and reduce river productivity  24  In Australia  where many rivers have warmer temperature regimes  native fish species have been eliminated  and macroinvertebrate fauna have been drastically altered  Survival rates of fish have dropped up to 75%  15      When a power plant first opens or shuts down for repair or other causes  fish and other organisms adapted to particular temperature range can be killed by the abrupt change in water temperature  either an increase or decrease  known as \"thermal shock\"  7  208 25  478    Water warming effects  as opposed to water cooling effects  have been the most studied with regard to biogeochemical effects  Much of this research is on the long term effects of nuclear power plants on lakes after a nuclear power plant has been removed  Overall  there is support for thermal pollution leading to an increase in water temperatures  3  4  When power plants are active  short term water temperature increases correlated with electrical needs  with more cooling water release during the winter months  3  Water warming has also been seen to persist in systems for long periods of time  even after plants have been removed  3      When warm water from power plant cooling exports enters systems  it often mixes leading to general increases in water temperature throughout the water body  including deep cooler water  3  Specifically in lakes and similar water bodies  stratification leads to different effects on a seasonal basis  In the summer  thermal pollution has been seen to increase deeper water temperature more dramatically than surface water  though stratification still exists  while in the winter surface water temperatures see a larger increase  3  Stratification is reduced in winter months due to thermal pollution  often eliminating the thermocline  3      A study looking at the effect of a removed nuclear power plant in Lake Stechlin  Germany found a 2 33C increase persisted in surface water during the winter and a 2 04C increase persisted in deep water during the summer  with marginal increases throughout the water column in both winter and summer  3  Stratification and water temperature differences due to thermal pollution seem to correlate with nutrient cycling of phosphorus and nitrogen  as oftentimes water bodies that receive cooling exports will shift toward eutrophication  No clear data has been obtained on this though  as it is difficult to differentiate influences from other industry and agriculture  26  27     Similar to effects seen in aquatic systems due to climatic warming of water in some parts of the world  thermal pollution has also been seen to increase surface temperatures in the summer  3  This can lead surface water temperatures that lead to releases of warm air into the atmosphere  increasing air temperature  3  It therefore can be seen as a contributor to global warming  28  Many ecological effects will be compounded by climate change as well  as water bodies   ambient temperature rises  29     Spacial and climatic factors can impact the severity of water warming due to thermal pollution  High wind speeds tend to increase the impact of thermal pollution  4  Rivers and large bodies of water also tend to loose the effects of thermal pollution as they progress from the source  30  4     Rivers present a unique problem with thermal pollution  As water temperatures are elevated upstream  power plants downstream receive warmer waters  Evidence of this effect has been seen along the Mississippi River  as power plants are forced to use warmer waters as their coolants  31  This reduces the efficiency of the plants and forces the plants to use more water and produce more thermal pollution     Multiscale Green  s function  MSGF   is a generalized and extended version of the classical Green  s function  GF  technique 1  for solving mathematical equations  The main application of the MSGF technique is in modeling of  nanomaterials  2  These materials are very small  of the size of few nanometers   Mathematical modeling of nanomaterials requires special techniques and is now recognized to be an independent branch of science  3   A  mathematical model is needed to calculate the displacements of atoms in a crystal in response to an applied static or time dependent force in order to study the mechanical and physical properties of nanomaterials  One specific requirement of a model for nanomaterials is that the model needs to be multiscale  and provide seamless linking of different length scales  4     Green  s function  GF  was originally formulated by the British mathematical physicist George Green in the year 1828 as a general technique for solution of operator equations  1   It has been extensively used in mathematical Physics  over the last almost two hundred years and applied to a variety of fields  1  5   Reviews of some applications of GFs such as for  many body theory and Laplace equation are available in the Wikipedia   The GF based techniques are used for modeling of various physical processes in materials such as phonons  6  Electronic band structure 7  and elastostatics  5     The MSGF method is a relatively new GF technique for mathematical modeling of nanomaterials  Mathematical  models are used for calculating the response of materials to an applied force in order to simulate their mechanical properties  The MSGF  technique  links different length scales in modeling of nanomaterials  2  8   Nanomaterials are of atomistic dimensions and need to be modeled at the length scales of nanometers   For example  a silicon nanowire  whose width is about five nanometers  contains just 10  12 atoms across its width  Another example is graphene 9  and many new two-dimensional  2D  solids  10  These new materials are ultimate in thinness because they are just one or two atoms thick  Multiscale modeling is needed for such materials because their properties are determined by the discreteness of their atomistic arrangements as well as their overall dimensions  2  4     The MSGF method is multiscale in the sense that it links the response of materials to an applied force at atomistic scales to their response at the macroscopic scales   The response of materials at the macroscopic scales is calculated by using the continuum model of solids  In the continuum model  the discrete atomistic structure of solids is averaged out into a continuum  Properties of nanomaterials are sensitive to their atomistic structure as well as their overall dimensions  They are also sensitive to the macroscopic structure of the host material in which they are embedded  The MSGF method is used to model such composite systems     The MSGF method is also used for analyzing behavior of crystals containing lattice defects such as vacancies  interstitials  or foreign atoms  Study of these lattice defects is of interest as they play a role in materials technology  11  12   Presence of a defect in a lattice displaces the host atoms from their original position or the lattice gets distorted  This is shown in Fig 1 for a 1D lattice as an example  Atomistic scale modeling is needed to calculate this distortion near the defect  13  14  whereas the continuum model is used to calculate the distortion far away from the defect  The MSGF links these two scales seamlessly      The MSGF model of nanomaterials accounts for multiparticles as well as multiscales in materials  8  It is an extension of the lattice statics Greens function  LSGF  method that was originally formulated  at the Atomic Energy Research Establishment Harwell in U K  in 1973  11  15   It is also referred to as Tewary method in the literature 16  17    The LSGF method complements molecular dynamics 18   MD  method for modeling multiparticle systems   The LSGF method is based upon the use of the Born von Karman  BvK  model 6  19   and can be applied to different lattice structures and defects  11  17  20  The MSGF method is an extended version of the LSGF method and has been applied to many nanomaterials and 2D materials 2     At the atomistic scales  a crystal or a crystalline solid is represented by a collection of interacting atoms located at discrete sites on a geometric lattice  19  A perfect crystal consists of a regular and periodic geometric lattice  The perfect lattice has translation symmetry  which means that all the unit cells are identical    In a perfect periodic lattice  which is assumed to be infinite  all atoms are identical  At equilibrium each atom is assumed to be located at its lattice site  The force at any atom due to other atoms just cancels out so the net force at each atom is zero  These conditions break down in a distorted lattice in which atoms get displaced from their positions of equilibrium  15   The lattice distortion may be caused by an externally applied force   The lattice can also be distorted by introducing a defect in the lattice or displacing an atom that disturbs the equilibrium configuration and induces a force on the lattice sites  This is shown in Fig  1   The objective of the mathematical model is to calculate the resulting values of the atomic displacements     The GF in the MSGF method is calculated by minimizing the total energy of the lattice  15  The potential energy of the lattice in the form of an infinite Taylor series in atomic displacements in the harmonic approximation as follows    where L and L label the atoms  a and b denote the Cartesian coordinates  u denotes the atomic displacement  and f and K are the first and second coefficients in the Taylor series  They are defined by 1     and    where the derivatives are evaluated at zero displacements  The negative sign is introduced in the definition of f for convenience  Thus f L  is a 3D vector that denotes the force at the atom L  Its three Cartesian components are denoted by fa L  where a = x  y  or z  Similarly K L L  is a 3x3 matrix  which is called the force- constant matrix between the atoms at L and L   Its 9 elements are denoted by Kab L L  for a  b = x  y  or z     At equilibrium  the energy W is minimum  8   Accordingly  the first derivative of W with respect to each u must be zero  This gives the following relation from Eq   1     It can be shown by direct substitution that the solution of Eq   4  can be written as    where G is defined by the following inversion relation    In Eq   6    m  is the discrete delta function of two discrete variable m and xa0n  Similar to the case of Dirac delta function for continuous variables  it is defined to be 1 if m xa0= xa0n and 0 otherwise  6     Equations  4  6  can be written in the matrix notation as follows     The matrices K and G in the above equations are 3N xa0 xa03N square matrices and u and f are 3N-dimensional column vectors  where N is the total number of atoms in the lattice  The matrix G is the multiparticle GF and is referred to as the lattice statics Greens function  LSGF    15  If G is known  the atomic displacements for all atoms can be calculated by using Eq   8      One of the main objectives of modeling is the calculation of the atomistic displacements u caused by an applied force f   21  The displacements  in principle  are given by Eq   8    However  it involves inversion of the matrix K which is 3N x 3N   For any calculation of practical interest N ~ 10 000 but preferably a million for more realistic simulations  Inversion of such a large matrix is computationally extensive and special techniques are needed for the calculation of us   For regular periodic lattices  LSGF is one such technique  It  consists of calculating G in terms of its Fourier transform  and is similar to the calculation of the phonon GF  6     The LSGF method has now been generalized to include the multiscale effects in the MSGF method  8  The MSGF method is capable of linking length scales seamlessly  This property has been used in developing a hybrid MSGF method  that combines the GF and the MD methods and has been used  for simulating less  symmetric nanoinclusions such as quantum dots in semiconductors  22     For a perfect lattice without defects  the MSGF links directly the atomistic scales in LSGF to the macroscopic scales through the continuum model   A perfect lattice has full translation symmetry so all the atoms are equivalent   In this case any atom can be chosen as the origin and G L L    can be expressed by a single index  L  -L  6    defined as    The asymptotic limit of G L   that satisfies Eq   10   for large values of R L  is given by 8     where x = R L  is the position vector of the atom L  and Gc x  is the continuum Green  s function  CGF   which  is defined in terms of the elastic constants and  used in  modeling of conventional bulk materials at macroscales  5  11  In Eq   11   O 1/xn  is the standard mathematical notation for a term of order 1/xn and higher  The magnitude of Gc x  is O 1/x2   21    The LSGF G 0 L  in this equation reduces smoothly and automatically to the CGF for large enough x as terms O 1/x4  become  gradually small and negligible    This ensures the seamless linkage of the atomistic length scale to the macroscopic continuum scale  8     Equations   8  and  9  along with the limiting relation given by Eq   11    form the basic equations for the MSGF  8   Equation  9  gives the LSGF  which is valid at the atomistic scales and Eq   11   relates it to the CGF  which is valid at the macro continuum scales  This equation also  shows that the LSGF reduces seamlessly to the CGF     If a lattice contains defects  its translation symmetry is broken  Consequently  it is not possible to express G in terms of a single distance variable R L    Hence Eq   10   is not valid anymore and the correspondence between the LSGF and the CGF  needed for their seamless linking breaks down  15   In such cases the MSGF links the lattice and the continuum scales by using the following procedure  15     If p  denotes the change in the matrix K  caused by the defect s   the force constant matrix K* for the defective lattice is written as    As in the case for the perfect lattice in Eq   9   the corresponding defect GF is defined as the inverse of the full K* matrix  Use of Eq   12   then leads to the following Dysons equation for the defect LSGF  15     The MSGF method consists of solving Eq   13  for G* by using the matrix partitioning technique or double Fourier transform  6     Once G* is known  the displacement vector is given by the following GF equation similar to Eq   8      u= G* f                                                                             14     Equation  14   gives the desired solution  that is  the atomic displacements or the lattice distortion caused by the force f  However  it does not show the linkage of the lattice and the continuum multiple scales  because Eqs   10  and  11  are not valid for the defect LSGF G*     The linkage between the lattice and the continuum model in case of lattice with defects is achieved by using an exact transformation described below  8     Using Eq  13   Eq   14  can be written in the following exactly equivalent form     u = Gf  + G p G* f                                                             15     Use of Eq   14  again on the right hand side of Eq   15  gives     u  = G f*                                                                           16     where    f* = f + p  u                                                                       17     Note that Eq   17  defines an effective force f* such that Eqs   14  and  16  are exactly equivalent     Equation  16  expresses  the atomic displacements  u in terms of G  the perfect LSGF even for lattices with defects  The effect of the defects is included exactly in f*  The LSGF  G is independent of f or f* and  reduces to the CGF asymptotically  and smoothly as given in Eq   11     The effective force  f* can be determined in a separate calculation by using an independent method if needed  and the lattice statics or the continuum model can be used for G  This is the basis of  a hybrid model that combines MSGF and MD for simulating a germanium quantum dot in a silicon lattice  22     Equation  16  is the master equation of the MSGF method  2  8   It is truly multiscale  All the discrete atomistic contributions are included in f*  The Green  s function  G can be calculated independently  which   can be fully atomistic  for nanomaterials or  partly or fully continuum for macroscales to account for surfaces and interfaces in material systems as needed  8     Molecular mechanics uses classical mechanics to model molecular systems  The BornOppenheimer approximation is assumed valid and the potential energy of all systems is calculated as a function of the nuclear coordinates using force fields  Molecular mechanics can be used to study molecule systems ranging in size and complexity from small to large biological systems or material assemblies with many thousands to millions of atoms     All-atomistic molecular mechanics methods have the following properties      Variants on this theme are possible  For example  many simulations have historically used a united-atom representation in which each terminal methyl group or intermediate methylene unit was considered one particle  and large protein systems are commonly simulated using a bead model that assigns two to four particles per amino acid     The following functional abstraction  termed an interatomic potential function or force field in chemistry  calculates the molecular system  s potential energy   E  in a given conformation as a sum of individual energy terms                          xa0        E        =                  E                      covalent                          +                  E                      noncovalent                                           displaystyle    E=E_   text covalent  +E_   text noncovalent         n    where the components of the covalent and noncovalent contributions are given by the following summations                          xa0                  E                      covalent                          =                  E                      bond                          +                  E                      angle                          +                  E                      dihedral                                   displaystyle    E_   text covalent  =E_   text bond  +E_   text angle  +E_   text dihedral      n                         xa0                  E                      noncovalent                          =                  E                      electrostatic                          +                  E                      van der Waals                                   displaystyle    E_   text noncovalent  =E_   text electrostatic  +E_   text van der Waals      n    The exact functional form of the potential function  or force field  depends on the particular simulation program being used  Generally the bond and angle terms are modeled as harmonic potentials centered around equilibrium bond-length values derived from experiment or theoretical calculations of electronic structure performed with software which does ab-initio type calculations such as Gaussian  For accurate reproduction of vibrational spectra  the Morse potential can be used instead  at computational cost  The dihedral or torsional terms typically have multiple minima and thus cannot be modeled as harmonic oscillators  though their specific functional form varies with the implementation  This class of terms may include improper dihedral terms  which function as correction factors for out-of-plane deviations  for example  they can be used to keep benzene rings planar  or correct geometry and chirality  of tetrahedral atoms in a united-atom representation      The non-bonded terms are much more computationally costly to calculate in full  since a typical atom is bonded to only a few of its neighbors  but interacts with every other atom in the molecule  Fortunately the van der Waals term falls off rapidly  It is typically modeled using a 612 Lennard-Jones potential  which means that attractive forces fall off with distance as r6 and repulsive forces as r12  where r represents the distance between two atoms  The repulsive part r12 is however unphysical  because repulsion increases exponentially  Description of van der Waals forces by the Lennard-Jones 612 potential introduces inaccuracies  which become significant at short distances  1  Generally a cutoff radius is used to speed up the calculation so that atom pairs which distances are greater than the cutoff have a van der Waals interaction energy of zero     The electrostatic terms are notoriously difficult to calculate well because they do not fall off rapidly with distance  and long-range electrostatic interactions are often important features of the system under study  especially for proteins   The basic functional form is the Coulomb potential  which only falls off as r1  A variety of methods are used to address this problem  the simplest being a cutoff radius similar to that used for the van der Waals terms  However  this introduces a sharp discontinuity between atoms inside and atoms outside the radius  Switching or scaling functions that modulate the apparent electrostatic energy are somewhat more accurate methods that multiply the calculated energy by a smoothly varying scaling factor from 0 to 1 at the outer and inner cutoff radii  Other more sophisticated but computationally intensive methods are particle mesh Ewald  PME  and the multipole algorithm     In addition to the functional form of each energy term  a useful energy function must be assigned parameters for force constants  van der Waals multipliers  and other constant terms  These terms  together with the equilibrium bond  angle  and dihedral values  partial charge values  atomic masses and radii  and energy function definitions  are collectively termed a force field  Parameterization is typically done through agreement with experimental values and theoretical calculations results  Norman L  Allinger  s force field in the last MM4 version calculate for hydrocarbons heats of formation with a rms error of 0 35 kcal/mol  vibrational spectra with a rms error of 24 xa0cm1  rotational barriers with a rms error of 2 2  C-C bond lengths within 0 004  and C-C-C angles within 1  2  Later MM4 versions cover also compounds with heteroatoms such as aliphatic amines  3     Each force field is parameterized to be internally consistent  but the parameters are generally not transferable from one force field to another     The main use of molecular mechanics is in the field of molecular dynamics  This uses the force field to calculate the forces acting on each particle and a suitable integrator to model the dynamics of the particles and predict trajectories  Given enough sampling and subject to the ergodic hypothesis  molecular dynamics trajectories can be used to estimate thermodynamic parameters of a system or probe kinetic properties  such as reaction rates and mechanisms     Another application of molecular mechanics is energy minimization  whereby the force field is used as an optimization criterion  This method uses an appropriate algorithm  e g  steepest descent  to find the molecular structure of a local energy minimum  These minima correspond to stable conformers of the molecule  in the chosen force field  and molecular motion can be modelled as vibrations around and interconversions between these stable conformers  It is thus common to find local energy minimization methods combined with global energy optimization  to find the global energy minimum  and other low energy states   At finite temperature  the molecule spends most of its time in these low-lying states  which thus dominate the molecular properties  Global optimization can be accomplished using simulated annealing  the Metropolis algorithm and other Monte Carlo methods  or using different deterministic methods of discrete or continuous optimization  While the force field represents only the enthalpic component of free energy  and only this component is included during energy minimization   it is possible to include the entropic component through the use of additional methods  such as normal mode analysis     Molecular mechanics potential energy functions have been used to calculate binding constants  4  5  6  7  8  protein folding kinetics  9  protonation equilibria  10  active site coordinates  6  11  and to design binding sites  12     In molecular mechanics  several ways exist to define the environment surrounding a molecule or molecules of interest  A system can be simulated in vacuum  termed a gas-phase simulation  with no surrounding environment  but this is usually undesirable because it introduces artifacts in the molecular geometry  especially in charged molecules  Surface charges that would ordinarily interact with solvent molecules instead interact with each other  producing molecular conformations that are unlikely to be present in any other environment  The best way to solvate a system is to place explicit water molecules in the simulation box with the molecules of interest and treat the water molecules as interacting particles like those in the molecule  A variety of water models exist with increasing levels of complexity  representing water as a simple hard sphere  a united-atom model   as three separate particles with fixed bond angles  or even as four or five separate interaction centers to account for unpaired electrons on the oxygen atom  As water models grow more complex  related simulations grow more computationally intensive  A compromise method has been found in implicit solvation  which replaces the explicitly represented water molecules with a mathematical expression that reproduces the average behavior of water molecules  or other solvents such as lipids   This method is useful to prevent artifacts that arise from vacuum simulations and reproduces bulk solvent properties well  but cannot reproduce situations in which individual water molecules have interesting interactions with the molecules under study     This is a limited list  many more packages are available         Molecular dynamics  MD  is a computer simulation method for analyzing the physical movements of atoms and molecules  The atoms and molecules are allowed to interact for a fixed period of time  giving a view of the dynamic \"evolution\" of the system  In the most common version  the trajectories of atoms and molecules are determined by numerically solving Newton  s equations of motion for a system of interacting particles  where forces between the particles and their potential energies are often calculated using interatomic potentials or molecular mechanics force fields  The method is applied mostly in chemical physics  materials science  and biophysics     Because molecular systems typically consist of a vast number of particles  it is impossible to determine the properties of such complex systems analytically  MD simulation circumvents this problem by using numerical methods  However  long MD simulations are mathematically ill-conditioned  generating cumulative errors in numerical integration that can be minimized with proper selection of algorithms and parameters  but not eliminated entirely     For systems that obey the ergodic hypothesis  the evolution of one molecular dynamics simulation may be used to determine macroscopic thermodynamic properties of the system  the time averages of an ergodic system correspond to microcanonical ensemble averages  MD has also been termed \"statistical mechanics by numbers\" and \"Laplace  s vision of Newtonian mechanics\" of predicting the future by animating nature  s forces 1  and allowing insight into molecular motion on an atomic scale     MD was originally developed in the early 1950s  following the earlier successes with Monte Carlo simulations  which themselves date back to the eighteenth century  in the Buffon  s needle problem for example  but was popularized for statistical mechanics at Los Alamos National Laboratory by Rosenbluth and Metropolis in what is known today as MetropolisHastings algorithm  Interest in the time evolution of N-body systems dates much earlier to the seventeenth century  beginning with Newton  and continued into the following century largely with a focus on celestial mechanics and issues such as the stability of the solar system  Many of the numerical methods used today were developed during this time period  which predates the use of computers  for example  the most common integration algorithm used today  the Verlet integration algorithm  was used as early as 1791 by Jean Baptiste Joseph Delambre  Numerical calculations with these algorithms can be considered to be MD \"by hand \"    As early as 1941  integration of the many-body equations of motion was carried out with analog computers  Some undertook the labor-intensive work of modeling atomic motion by constructing physical models  e g   using macroscopic spheres  The aim was to arrange them in such a way as to replicate the structure of a liquid and use this to examine its behavior  J D  Bernal said  in 1962  \"    I took a number of rubber balls and stuck them together with rods of a selection of different lengths ranging from 2 75 to 4 inches  I tried to do this in the first place as casually as possible  working in my own office  being interrupted every five minutes or so and not remembering what I had done before the interruption \" 2      nFollowing the discovery of microscopic particles and the development of computers  interest expanded beyond the proving ground of gravitational systems to the statistical properties of matter  In an attempt to understand the origin of irreversibility  Fermi proposed in 1953  and published in 1955  3  the use of MANIAC I  also at Los Alamos National Laboratory  to solve the time evolution of the equations of motion for a many-body system subject to several choices of force laws  today  this seminal work is known as the FermiPastaUlamTsingou problem  The time evolution of the energy from the original work is shown in the figure to the right      In 1957  Alder and Wainwright 4  used an IBM 704 computer to simulate perfectly elastic collisions between hard spheres  4  In 1960  in perhaps the first realistic simulation of matter  Gibson et al  simulated radiation damage of solid copper by using a BornMayer type of repulsive interaction along with a cohesive surface force  5  In 1964  Rahman 6   published simulations of liquid argon that used a Lennard-Jones potential  calculations of system properties  such as the coefficient of self-diffusion  compared well with experimental data  6     First used in theoretical physics  the MD method gained popularity in materials science soon afterward  and since the 1970s is also common in biochemistry and biophysics  MD is frequently used to refine 3-dimensional structures of proteins and other macromolecules based on experimental constraints from X-ray crystallography or NMR spectroscopy  In physics  MD is used to examine the dynamics of atomic-level phenomena that cannot be observed directly  such as thin-film growth and ion-subplantation  and also to examine the physical properties of nanotechnological devices that have not or cannot yet be created  In biophysics and structural biology  the method is frequently applied to study the motions of macromolecules such as proteins and nucleic acids  which can be useful for interpreting the results of certain biophysical experiments and for modeling interactions with other molecules  as in ligand docking  In principle MD can be used for ab initio prediction of protein structure by simulating folding of the polypeptide chain from random coil     The results of MD simulations can be tested through comparison to experiments that measure molecular dynamics  of which a popular method is NMR spectroscopy  MD-derived structure predictions can be tested through community-wide experiments in Critical Assessment of protein Structure Prediction  CASP   although the method has historically had limited success in this area  Michael Levitt  who shared the Nobel Prize partly for the application of MD to proteins  wrote in 1999 that CASP participants usually did not use the method due to \"    a central embarrassment of molecular mechanics  namely that energy minimization or molecular dynamics generally leads to a model that is less like the experimental structure \" 7  Improvements in computational resources permitting more and longer MD trajectories  combined with modern improvements in the quality of force field parameters  have yielded some improvements in both structure prediction and homology model refinement  without reaching the point of practical utility in these areas  many identify force field parameters as a key area for further development  8  9  10     MD simulation has been reported for pharmacophore development and drug design  11  For example  Pinto et al  implemented MD simulations of Bcl-Xl complexes to calculate average positions of critical amino acids involved in ligand binding  12  On the other hand  Carlson et al  implemented molecular dynamics simulation to identify compounds that complement the receptor while causing minimal disruption of the conformation and flexibility of the active site  Snapshots of the protein at constant time intervals during the simulation were overlaid to identify conserved binding regions  conserved in at least three out of eleven frames  for pharmacophore development  Spyrakis et al  relied on a workflow of MD simulations  finger prints for ligands and proteins  FLAP  and linear discriminate analysis to identify best ligand protein conformations to act as pharmacophore templates based on retrospective ROC analysis of the resulting pharmacophores  In an attempt to ameliorate structure-based drug discovery modeling  vis-a`-vis the need for many modeled compounds  Hatmal et al proposed a combination of MD simulation and ligand-receptor intermolecular contacts analysis to discern critical intermolecular contacts  binding interactions  from redundant ones in a single ligandprotein complex  Critical contacts can then be converted into pharmacophore models that can be used for virtual screening  13     Limits of the method are related to the parameter sets used  and to the underlying molecular mechanics force fields  One run of an MD simulation optimizes the potential energy  rather than the free energy of the protein dubious   discuss   meaning that all entropic contributions to thermodynamic stability of protein structure are neglected  including the conformational entropy of the polypeptide chain  the main factor that destabilizes protein structure  and hydrophobic effects  the main driving forces of protein folding   14  Another important factor is intramolecular hydrogen bonds  15  which are not explicitly included in modern force fields  but described as Coulomb interactions of atomic point charges  This is a crude approximation because hydrogen bonds have a partially quantum mechanical and chemical nature  Furthermore  electrostatic interactions are usually calculated using the dielectric constant of vacuum  although the surrounding aqueous solution has a much higher dielectric constant  Using the macroscopic dielectric constant at short interatomic distances is questionable  Finally  van der Waals interactions in MD are usually described by Lennard-Jones potentials based on the Fritz London theory that is only applicable in a vacuum  However  all types of van der Waals forces are ultimately of electrostatic origin and therefore depend on dielectric properties of the environment  16  The direct measurement of attraction forces between different materials  as Hamaker constant  shows that \"the interaction between hydrocarbons across water is about 10% of that across vacuum\"  16  The environment-dependence of van der Waals forces is neglected in standard simulations  but can be included by developing polarizable force fields     The design of a molecular dynamics simulation should account for the available computational power  Simulation size  = number of particles   timestep  and total time duration must be selected so that the calculation can finish within a reasonable time period  However  the simulations should be long enough to be relevant to the time scales of the natural processes being studied  To make statistically valid conclusions from the simulations  the time span simulated should match the kinetics of the natural process  Otherwise  it is analogous to making conclusions about how a human walks when only looking at less than one footstep  Most scientific publications about the dynamics of proteins and DNA 17  18  use data from simulations spanning nanoseconds  109 s  to microseconds  106 s   To obtain these simulations  several CPU-days to CPU-years are needed  Parallel algorithms allow the load to be distributed among CPUs  an example is the spatial or force decomposition algorithm  19     During a classical MD simulation  the most CPU intensive task is the evaluation of the potential as a function of the particles   internal coordinates  Within that energy evaluation  the most expensive one is the non-bonded or non-covalent part  In Big O notation  common molecular dynamics simulations scale by                     O                           n                      2                                            displaystyle O n^ 2      if all pair-wise electrostatic and van der Waals interactions must be accounted for explicitly  This computational cost can be reduced by employing electrostatics methods such as particle mesh Ewald summation                       O                 n        log         u2061                 n                                   displaystyle O  log         particleparticle-particlemesh  P3M   or good spherical cutoff methods                       O                 n                          displaystyle O        citation needed     Another factor that impacts total CPU time needed by a simulation is the size of the integration timestep  This is the time length between evaluations of the potential  The timestep must be chosen small enough to avoid discretization errors  i e   smaller than the period related to fastest vibrational frequency in the system   Typical timesteps for classical MD are in the order of 1 xa0femtosecond  1015 s   This value may be extended by using algorithms such as the SHAKE constraint algorithm  which fix the vibrations of the fastest atoms  e g   hydrogens  into place  Multiple time scale methods have also been developed  which allow extended times between updates of slower long-range forces  20  21  22     For simulating molecules in a solvent  a choice should be made between explicit and implicit solvent  Explicit solvent particles  such as the TIP3P  SPC/E and SPC-f water models  must be calculated expensively by the force field  while implicit solvents use a mean-field approach  Using an explicit solvent is computationally expensive  requiring inclusion of roughly ten times more particles in the simulation  But the granularity and viscosity of explicit solvent is essential to reproduce certain properties of the solute molecules  This is especially important to reproduce chemical kinetics     In all kinds of molecular dynamics simulations  the simulation box size must be large enough to avoid boundary condition artifacts  Boundary conditions are often treated by choosing fixed values at the edges  which may cause artifacts   or by employing periodic boundary conditions in which one side of the simulation loops back to the opposite side  mimicking a bulk phase  which may cause artifacts too      In the microcanonical ensemble  the system is isolated from changes in moles  N   volume  V   and energy  E   It corresponds to an adiabatic process with no heat exchange  A microcanonical molecular dynamics trajectory may be seen as an exchange of potential and kinetic energy  with total energy being conserved  For a system of N particles with coordinates                     X                 displaystyle X    and velocities                     V                 displaystyle V     the following pair of first order differential equations may be written in Newton  s notation as    The potential energy function                     U                 X                          displaystyle U X     of the system is a function of the particle coordinates                     X                 displaystyle X     It is referred to simply as the potential in physics  or the force field in chemistry  The first equation comes from Newton  s laws of motion  the force                     F                 displaystyle F    acting on each particle in the system can be calculated as the negative gradient of                     U                 X                          displaystyle U X         For every time step  each particle  s position                     X                 displaystyle X    and velocity                     V                 displaystyle V    may be integrated with a symplectic integrator method such as Verlet integration  The time evolution of                     X                 displaystyle X    and                     V                 displaystyle V    is called a trajectory  Given the initial positions  e g   from theoretical knowledge  and velocities  e g   randomized Gaussian   we can calculate all future  or past  positions and velocities     One frequent source of confusion is the meaning of temperature in MD  Commonly we have experience with macroscopic temperatures  which involve a huge number of particles  But temperature is a statistical quantity  If there is a large enough number of atoms  statistical temperature can be estimated from the instantaneous temperature  which is found by equating the kinetic energy of the system to nkBT/2 where is the number of degrees of freedom of the system     A temperature-related phenomenon arises due to the small number of atoms that are used in MD simulations  For example  consider simulating the growth of a copper film starting with a substrate containing 500 atoms and a deposition energy of 100 eV  In the real world  the 100 eV from the deposited atom would rapidly be transported through and shared among a large number of atoms                                10                      10                                   displaystyle 10^ 10     or more  with no big change in temperature  When there are only 500 atoms  however  the substrate is almost immediately vaporized by the deposition  Something similar happens in biophysical simulations  The temperature of the system in NVE is naturally raised when macromolecules such as proteins undergo exothermic conformational changes and binding     In the canonical ensemble  amount of substance  N   volume  V  and temperature  T  are conserved  It is also sometimes called constant temperature molecular dynamics  CTMD   In NVT  the energy of endothermic and exothermic processes is exchanged with a thermostat     A variety of thermostat algorithms are available to add and remove energy from the boundaries of an MD simulation in a more or less realistic way  approximating the canonical ensemble  Popular methods to control temperature include velocity rescaling  the NosHoover thermostat  NosHoover chains  the Berendsen thermostat  the Andersen thermostat and Langevin dynamics  The Berendsen thermostat might introduce the flying ice cube effect  which leads to unphysical translations and rotations of the simulated system     It is not trivial to obtain a canonical ensemble distribution of conformations and velocities using these algorithms  How this depends on system size  thermostat choice  thermostat parameters  time step and integrator is the subject of many articles in the field     In the isothermalisobaric ensemble  amount of substance  N   pressure  P  and temperature  T  are conserved  In addition to a thermostat  a barostat is needed  It corresponds most closely to laboratory conditions with a flask open to ambient temperature and pressure     In the simulation of biological membranes  isotropic pressure control is not appropriate  For lipid bilayers  pressure control occurs under constant membrane area  NPAT  or constant surface tension \"gamma\"  NPT      The replica exchange method is a generalized ensemble  It was originally created to deal with the slow dynamics of disordered spin systems  It is also called parallel tempering  The replica exchange MD  REMD  formulation 23  tries to overcome the multiple-minima problem by exchanging the temperature of non-interacting replicas of the system running at several temperatures     A molecular dynamics simulation requires the definition of a potential function  or a description of the terms by which the particles in the simulation will interact  In chemistry and biology this is usually referred to as a force field and in materials physics as an interatomic potential  Potentials may be defined at many levels of physical accuracy  those most commonly used in chemistry are based on molecular mechanics and embody a classical mechanics treatment of particle-particle interactions that can reproduce structural and conformational changes but usually cannot reproduce chemical reactions     The reduction from a fully quantum description to a classical potential entails two main approximations  The first one is the BornOppenheimer approximation  which states that the dynamics of electrons are so fast that they can be considered to react instantaneously to the motion of their nuclei  As a consequence  they may be treated separately  The second one treats the nuclei  which are much heavier than electrons  as point particles that follow classical Newtonian dynamics  In classical molecular dynamics  the effect of the electrons is approximated as one potential energy surface  usually representing the ground state     When finer levels of detail are needed  potentials based on quantum mechanics are used  some methods attempt to create hybrid classical/quantum potentials where the bulk of the system is treated classically but a small region is treated as a quantum system  usually undergoing a chemical transformation     Empirical potentials used in chemistry are frequently called force fields  while those used in materials physics are called interatomic potentials     Most force fields in chemistry are empirical and consist of a summation of bonded forces associated with chemical bonds  bond angles  and bond dihedrals  and non-bonded forces associated with van der Waals forces and electrostatic charge  Empirical potentials represent quantum-mechanical effects in a limited way through ad hoc functional approximations  These potentials contain free parameters such as atomic charge  van der Waals parameters reflecting estimates of atomic radius  and equilibrium bond length  angle  and dihedral  these are obtained by fitting against detailed electronic calculations  quantum chemical simulations  or experimental physical properties such as elastic constants  lattice parameters and spectroscopic measurements     Because of the non-local nature of non-bonded interactions  they involve at least weak interactions between all particles in the system  Its calculation is normally the bottleneck in the speed of MD simulations  To lower the computational cost  force fields employ numerical approximations such as shifted cutoff radii  reaction field algorithms  particle mesh Ewald summation  or the newer particleparticle-particlemesh  P3M      Chemistry force fields commonly employ preset bonding arrangements  an exception being ab initio dynamics   and thus are unable to model the process of chemical bond breaking and reactions explicitly  On the other hand  many of the potentials used in physics  such as those based on the bond order formalism can describe several different coordinations of a system and bond breaking  24  25  Examples of such potentials include the Brenner potential 26  for hydrocarbons and its nfurther developments for the C-Si-H 27  and C-O-H 28  systems  The nReaxFF potential 29  can be considered a fully reactive hybrid between bond order potentials and chemistry force fields     The potential functions representing the non-bonded energy are formulated as a sum over interactions between the particles of the system  The simplest choice  employed in many popular force fields  is the \"pair potential\"  in which the total potential energy can be calculated from the sum of energy contributions between pairs of atoms  Therefore  these force fields are also called \"additive force fields\"  An example of such a pair potential is the non-bonded LennardJones potential  also termed the 612 potential   used for calculating van der Waals forces     Another example is the Born  ionic  model of the ionic lattice  The first term in the next equation is Coulomb  s law for a pair of ions  the second term is the short-range repulsion explained by Pauli  s exclusion principle and the final term is the dispersion interaction term  Usually  a simulation only includes the dipolar term  although sometimes the quadrupolar term is also included  30  31  When nl = 6  this potential is also called the CoulombBuckingham potential     In many-body potentials  the potential energy includes the effects of three or more particles interacting with each other  32  In simulations with pairwise potentials  global interactions in the system also exist  but they occur only through pairwise terms  In many-body potentials  the potential energy cannot be found by a sum over pairs of atoms  as these interactions are calculated explicitly as a combination of higher-order terms  In the statistical view  the dependency between the variables cannot in general be expressed using only pairwise products of the degrees of freedom  For example  the Tersoff potential  33  which was originally used to simulate carbon  silicon  and germanium  and has since been used for a wide range of other materials  involves a sum over groups of three atoms  with the angles between the atoms being an important factor in the potential  Other examples are the embedded-atom method  EAM   34  the EDIP  32  and the Tight-Binding Second Moment Approximation  TBSMA  potentials  35  where the electron density of states in the region of an atom is calculated from a sum of contributions from surrounding atoms  and the potential energy contribution is then a function of this sum     Semi-empirical potentials make use of the matrix representation from quantum mechanics  However  the values of the matrix elements are found through empirical formulae that estimate the degree of overlap of specific atomic orbitals  The matrix is then diagonalized to determine the occupancy of the different atomic orbitals  and empirical formulae are used once again to determine the energy contributions of the orbitals     There are a wide variety of semi-empirical potentials  termed tight-binding potentials  which vary according to the atoms being modeled     Most classical force fields implicitly include the effect of polarizability  e g   by scaling up the partial charges obtained from quantum chemical calculations  These partial charges are stationary with respect to the mass of the atom  But molecular dynamics simulations can explicitly model polarizability with the introduction of induced dipoles through different methods  such as Drude particles or fluctuating charges  This allows for a dynamic redistribution of charge between atoms which responds to the local chemical environment     For many years  polarizable MD simulations have been touted as the next generation  For homogenous liquids such as water  increased accuracy has been achieved through the inclusion of polarizability  36  37  38  Some promising results have also been achieved for proteins  39  40  However  it is still uncertain how to best approximate polarizability in a simulation  citation needed     In classical molecular dynamics  one potential energy surface  usually the ground state  is represented in the force field  This is a consequence of the BornOppenheimer approximation  In excited states  chemical reactions or when a more accurate representation is needed  electronic behavior can be obtained from first principles by using a quantum mechanical method  such as density functional theory  This is named Ab Initio Molecular Dynamics  AIMD   Due to the cost of treating the electronic degrees of freedom  the computational cost of these simulations is far higher than classical molecular dynamics  This implies that AIMD is limited to smaller systems and shorter times     Ab initio quantum mechanical and chemical methods may be used to calculate the potential energy of a system on the fly  as needed for conformations in a trajectory  This calculation is usually made in the close neighborhood of the reaction coordinate  Although various approximations may be used  these are based on theoretical considerations  not on empirical fitting  Ab initio calculations produce a vast amount of information that is not available from empirical methods  such as density of electronic states or other electronic properties  A significant advantage of using ab initio methods is the ability to study reactions that involve breaking or formation of covalent bonds  which correspond to multiple electronic states  Moreover  ab initio methods also allow recovering effects beyond the BornOppenheimer approximation using approaches like mixed quantum-classical dynamics     QM  quantum-mechanical  methods are very powerful  However  they are computationally expensive  while the MM  classical or molecular mechanics  methods are fast but suffer from several limits  require extensive parameterization  energy estimates obtained are not very accurate  cannot be used to simulate reactions where covalent bonds are broken/formed  and are limited in their abilities for providing accurate details regarding the chemical environment   A new class of method has emerged that combines the good points of QM  accuracy  and MM  speed  calculations  These methods are termed mixed or hybrid quantum-mechanical and molecular mechanics methods  hybrid QM/MM   41     The most important advantage of hybrid QM/MM method is the speed  The cost of doing classical molecular dynamics  MM  in the most straightforward case scales O n2   where is the number of atoms in the system  This is mainly due to electrostatic interactions term  every particle interacts with every other particle   However  use of cutoff radius  periodic pair-list updates and more recently the variations of the particle-mesh Ewald  s  PME  method has reduced this to between O  to O n2   In other words  if a system with twice as many atoms is simulated then it would take between two and four times as much computing power  On the other hand  the simplest ab initio calculations typically scale O n3  or worse  restricted HartreeFock calculations have been suggested to scale ~O n2 7    To overcome the limit  a small part of the system is treated quantum-mechanically  typically active-site of an enzyme  and the remaining system is treated classically     In more sophisticated implementations  QM/MM methods exist to treat both light nuclei susceptible to quantum effects  such as hydrogens  and electronic states  This allows generating hydrogen wave-functions  similar to electronic wave-functions   This methodology has been useful in investigating phenomena such as hydrogen tunneling  One example where QM/MM methods have provided new discoveries is the calculation of hydride transfer in the enzyme liver alcohol dehydrogenase  In this case  quantum tunneling is important for the hydrogen  as it determines the reaction rate  42     At the other end of the detail scale are coarse-grained and lattice models  Instead of explicitly representing every atom of the system  one uses \"pseudo-atoms\" to represent groups of atoms  MD simulations on very large systems may require such large computer resources that they cannot easily be studied by traditional all-atom methods  Similarly  simulations of processes on long timescales  beyond about 1 microsecond  are prohibitively expensive  because they require so many time steps  In these cases  one can sometimes tackle the problem by using reduced representations  which are also called coarse-grained models  43     Examples for coarse graining  CG  methods are discontinuous molecular dynamics  CG-DMD  44  45  and Go-models  46  Coarse-graining is done sometimes taking larger pseudo-atoms  Such united atom approximations have been used in MD simulations of biological membranes  Implementation of such approach on systems where electrical properties are of interest can be challenging owing to the difficulty of using a proper charge distribution on the pseudo-atoms  47  The aliphatic tails of lipids are represented by a few pseudo-atoms by gathering 2 to 4 methylene groups into each pseudo-atom     The parameterization of these very coarse-grained models must be done empirically  by matching the behavior of the model to appropriate experimental data or all-atom simulations  Ideally  these parameters should account for both enthalpic and entropic contributions to free energy in an implicit way  48  When coarse-graining is done at higher levels  the accuracy of the dynamic description may be less reliable  But very coarse-grained models have been used successfully to examine a wide range of questions in structural biology  liquid crystal organization  and polymer glasses     Examples of applications of coarse-graining     The simplest form of coarse-graining is the united atom  sometimes called extended atom  and was used in most early MD simulations of proteins  lipids  and nucleic acids  For example  instead of treating all four atoms of a CH3 methyl group explicitly  or all three atoms of CH2 methylene group   one represents the whole group with one pseudo-atom  It must  of course  be properly parameterized so that its van der Waals interactions with other groups have the proper distance-dependence  Similar considerations apply to the bonds  angles  and torsions in which the pseudo-atom participates  In this kind of united atom representation  one typically eliminates all explicit hydrogen atoms except those that have the capability to participate in hydrogen bonds  polar hydrogens   An example of this is the CHARMM 19 force-field     The polar hydrogens are usually retained in the model  because proper treatment of hydrogen bonds requires a reasonably accurate description of the directionality and the electrostatic interactions between the donor and acceptor groups  A hydroxyl group  for example  can be both a hydrogen bond donor  and a hydrogen bond acceptor  and it would be impossible to treat this with one OH pseudo-atom  About half the atoms in a protein or nucleic acid are non-polar hydrogens  so the use of united atoms can provide a substantial savings in computer time     In many simulations of a solute-solvent system the main focus is on the behavior of the solute with little interest of the solvent behavior particularly in those solvent molecules residing in regions far from the solute molecule  50  Solvents may influence the dynamic behavior of solutes via random collisions and by imposing a frictional drag on the motion of the solute through the solvent  The use of non-rectangular periodic boundary conditions  stochastic boundaries and solvent shells can all help reduce the number of solvent molecules required and enable a larger proportion of the computing time to be spent instead on simulating the solute  It is also possible to incorporate the effects of a solvent without needing any explicit solvent molecules present  One example of this approach is to use a potential mean force  PMF  which describes how the free energy changes as a particular coordinate is varied  The free energy change described by PMF contains the averaged effects of the solvent     A long range interaction is an interaction in which the spatial interaction falls off no faster than                               r                                  d                                   displaystyle r^ -d     where                     d                 displaystyle d    is the dimensionality of the system  Examples include charge-charge interactions between ions and dipole-dipole interactions between molecules  Modelling these forces presents quite a challenge as they are significant over a distance which may be larger than half the box length with simulations of many thousands of particles  Though one solution would be to significantly increase the size of the box length  this brute force approach is less than ideal as the simulation would become computationally very expensive  Spherically truncating the potential is also out of the question as unrealistic behaviour may be observed when the distance is close to the cut off distance  51     Steered molecular dynamics  SMD  simulations  or force probe simulations  apply forces to a protein in order to manipulate its structure by pulling it along desired degrees of freedom  These experiments can be used to reveal structural changes in a protein at the atomic level  SMD is often used to simulate events such as mechanical unfolding or stretching  52     There are two typical protocols of SMD  one in which pulling velocity is held constant  and one in which applied force is constant  Typically  part of the studied system  e g   an atom in a protein  is restrained by a harmonic potential  Forces are then applied to specific atoms at either a constant velocity or a constant force  Umbrella sampling is used to move the system along the desired reaction coordinate by varying  for example  the forces  distances  and angles manipulated in the simulation  Through umbrella sampling  all of the system  s configurationsboth high-energy and low-energyare adequately sampled  Then  each configuration  s change in free energy can be calculated as the potential of mean force  53  A popular method of computing PMF is through the weighted histogram analysis method  WHAM   which analyzes a series of umbrella sampling simulations  54  55     A lot of important applications of SMD are in the field of drug discovery and biomolecular sciences  For e g  SMD was used to investigate the stability of Alzheimer  s protofibrils  56  to study the protein ligand interaction in cyclin-dependent kinase 5 57  and even to show the effect of electric field on thrombin  protein  and aptamer  nucleotide  complex 58  among many other interesting studies     Molecular dynamics is used in many fields of science     The following biophysical examples illustrate notable efforts to produce simulations of a systems of very large size  a complete virus  or very long simulation times  up to 1 112 milliseconds      Another important application of MD method benefits from its ability of 3-dimensional characterization and analysis of microstructural evolution at atomic scale         Atmospheric physics nAtmospheric dynamics  category     Weather  category  xa0  portal     Climate  category  nClimate change  category     Meteorology is a branch of the atmospheric sciences  which include atmospheric chemistry and atmospheric physics   with a major focus on weather forecasting  The study of meteorology dates back millennia  though significant progress in meteorology did not begin until the 18th century  The 19th century saw modest progress in the field after weather observation networks were formed across broad regions  Prior attempts at prediction of weather depended on historical data  It was not until after the elucidation of the laws of physics and more particularly  the development of the computer  allowing for the automated solution of a great many equations that model the weather  in the latter half of the 20th century that significant breakthroughs in weather forecasting were achieved  An important branch of weather forecasting is marine weather forecasting as it relates to maritime and coastal safety  in which weather effects also include atmospheric interactions with large bodies of water     Meteorological phenomena are observable weather events that are explained by the science of meteorology  Meteorological phenomena are described and quantified by the variables of Earth  s atmosphere  temperature  air pressure  water vapour  mass flow  and the variations and interactions of these variables  and how they change over time  Different spatial scales are used  to describe and predict weather on local  regional  and global levels     Meteorology  climatology  atmospheric physics  and atmospheric chemistry are sub-disciplines of the atmospheric sciences  Meteorology and hydrology compose the interdisciplinary field of hydrometeorology  The interactions between Earth  s atmosphere and its oceans are part of a coupled ocean-atmosphere system  Meteorology has application in many diverse fields such as the military  energy production  transport  agriculture  and construction     The word meteorology is from the Ancient Greek  metros  meteor  and - -logia  - o logy   meaning \"the study of things high in the air \"    The ability to predict rains and floods based on annual cycles was evidently used by humans at least from the time of agricultural settlement if not earlier  Early approaches to predicting weather were based on astrology and were practiced by priests  Cuneiform inscriptions on Babylonian tablets included associations between thunder and rain  The Chaldeans differentiated the 22 and 46 halos  1     Ancient Indian Upanishads contain mentions of clouds and seasons  2  The Samaveda mentions sacrifices to be performed when certain phenomena were noticed  1  Varhamihira  s classical work Brihatsamhita  written about 500 AD  2  provides evidence of weather observation     In 350 BC  Aristotle wrote Meteorology  3  Aristotle is considered the founder of meteorology  4  One of the most impressive achievements described in the Meteorology is the description of what  is now known as the hydrologic cycle  5     The book De Mundo  composed before 250 BC or between 350 and 200 BC  noted  6     The Greek scientist Theophrastus compiled a book on weather forecasting  called the Book of Signs  The work of Theophrastus remained a dominant influence in the study of weather and in weather forecasting for nearly 2 000 years  7  In 25 AD  Pomponius Mela  a geographer for the Roman Empire  formalized the climatic zone system  8  According to Toufic Fahd  around the 9th century  Al-Dinawari wrote the Kitab al-Nabat  Book of Plants   in which he deals with the application of meteorology to agriculture during the Arab Agricultural Revolution  He describes the meteorological character of the sky  the planets and constellations  the sun and moon  the lunar phases indicating seasons and rain  the anwa  heavenly bodies of rain   and atmospheric phenomena such as winds  thunder  lightning  snow  floods  valleys  rivers  lakes  9  10  verification needed     Early attempts at predicting weather were often related to prophecy and divining  and were sometimes based on astrological ideas  Admiral FitzRoy tried to separate scientific approaches from prophetic ones  11     Ptolemy wrote on the atmospheric refraction of light in the context of astronomical observations  12  In 1021  Alhazen showed that atmospheric refraction is also responsible for twilight  he estimated that twilight begins when the sun is 19 degrees below the horizon  and also used a geometric determination based on this to estimate the maximum possible height of the Earth  s atmosphere as 52 000 passim  about 49 miles  or 79 xa0km   13     St  Albert the Great was the first to propose that each drop of falling rain had the form of a small sphere  and that this form meant that the rainbow was produced by light interacting with each raindrop  14  Roger Bacon was the first to calculate the angular size of the rainbow  He stated that a rainbow summit can not appear higher than 42 degrees above the horizon  15  In the late 13th century and early 14th century  Kaml al-Dn al-Fris and Theodoric of Freiberg were the first to give the correct explanations for the primary rainbow phenomenon  Theoderic went further and also explained the secondary rainbow  16  In 1716  Edmund Halley suggested that aurorae are caused by \"magnetic effluvia\" moving along the Earth  s magnetic field lines     In 1441  King Sejong  s son  Prince Munjong of Korea  invented the first standardized rain gauge  17  These were sent throughout the Joseon dynasty of Korea as an official tool to assess land taxes based upon a farmer  s potential harvest  In 1450  Leone Battista Alberti developed a swinging-plate anemometer  and was known as the first anemometer  18  In 1607  Galileo Galilei constructed a thermoscope  In 1611  Johannes Kepler wrote the first scientific treatise on snow crystals  \"Strena Seu de Nive Sexangula  A New Year  s Gift of Hexagonal Snow  \" 19  In 1643  Evangelista Torricelli invented the mercury barometer  18  In 1662  Sir Christopher Wren invented the mechanical  self-emptying  tipping bucket rain gauge  In 1714  Gabriel Fahrenheit created a reliable scale for measuring temperature with a mercury-type thermometer  20  In 1742  Anders Celsius  a Swedish astronomer  proposed the \"centigrade\" temperature scale  the predecessor of the current Celsius scale  21  In 1783  the first hair hygrometer was demonstrated by Horace-Bndict de Saussure  In 18021803  Luke Howard wrote On the Modification of Clouds  in which he assigns cloud types Latin names  22  In 1806  Francis Beaufort introduced his system for classifying wind speeds  23  Near the end of the 19th century the first cloud atlases were published  including the International Cloud Atlas  which has remained in print ever since  The April 1960 launch of the first successful weather satellite  TIROS-1  marked the beginning of the age where weather information became available globally     In 1648  Blaise Pascal rediscovered that atmospheric pressure decreases with height  and deduced that there is a vacuum above the atmosphere  24  In 1738  Daniel Bernoulli published Hydrodynamics  initiating the Kinetic theory of gases and established the basic laws for the theory of gases  25  In 1761  Joseph Black discovered that ice absorbs heat without changing its temperature when melting  In 1772  Black  s student Daniel Rutherford discovered nitrogen  which he called phlogisticated air  and together they developed the phlogiston theory  26  In 1777  Antoine Lavoisier discovered oxygen and developed an explanation for combustion  27  In 1783  in Lavoisier  s essay \"Reflexions sur le phlogistique \" 28  he deprecates the phlogiston theory and proposes a caloric theory  29  30  In 1804  Sir John Leslie observed that a matte black surface radiates heat more effectively than a polished surface  suggesting the importance of black-body radiation  In 1808  John Dalton defended caloric theory in A New System of Chemistry and described how it combines with matter  especially gases  he proposed that the heat capacity of gases varies inversely with atomic weight  In 1824  Sadi Carnot analyzed the efficiency of steam engines using caloric theory  he developed the notion of a reversible process and  in postulating that no such thing exists in nature  laid the foundation for the second law of thermodynamics     In 1494  Christopher Columbus experienced a tropical cyclone  which led to the first written European account of a hurricane  31  In 1686  Edmund Halley presented a systematic study of the trade winds and monsoons and identified solar heating as the cause of atmospheric motions  32  In 1735  an ideal explanation of global circulation through study of the trade winds was written by George Hadley  33  In 1743  when Benjamin Franklin was prevented from seeing a lunar eclipse by a hurricane  he decided that cyclones move in a contrary manner to the winds at their periphery  34  Understanding the kinematics of how exactly the rotation of the Earth affects airflow was partial at first  Gaspard-Gustave Coriolis published a paper in 1835 on the energy yield of machines with rotating parts  such as waterwheels  35  In 1856  William Ferrel proposed the existence of a circulation cell in the mid-latitudes  and the air within deflected by the Coriolis force resulting in the prevailing westerly winds  36  Late in the 19th century  the motion of air masses along isobars was understood to be the result of the large-scale interaction of the pressure gradient force and the deflecting force  By 1912  this deflecting force was named the Coriolis effect  37  Just after World War I  a group of meteorologists in Norway led by Vilhelm Bjerknes developed the Norwegian cyclone model that explains the generation  intensification and ultimate decay  the life cycle  of mid-latitude cyclones  and introduced the idea of fronts  that is  sharply defined boundaries between air masses  38  The group included Carl-Gustaf Rossby  who was the first to explain the large scale atmospheric flow in terms of fluid dynamics   Tor Bergeron  who first determined how rain forms  and Jacob Bjerknes     In the late 16th century and first half of the 17th century a range of meteorological instruments were invented  the thermometer  barometer  hydrometer  as well as wind and rain gauges  In the 1650s natural philosophers started using these instruments to systematically record weather observations  Scientific academies established weather diaries and organised observational networks  39  In 1654  Ferdinando II de Medici established the first weather observing network  that consisted of meteorological stations in Florence  Cutigliano  Vallombrosa  Bologna  Parma  Milan  Innsbruck  Osnabrck  Paris and Warsaw  The collected data were sent to Florence at regular time intervals  40  In the 1660s Robert Hooke of the Royal Society of London sponsored networks of weather observers  Hippocrates   treatise Airs  Waters  and Places had linked weather to disease  Thus early meteorologists attempted to correlate weather patterns with epidemic outbreaks  and the climate with public health  39     During the Age of Enlightenment meteorology tried to rationalise traditional weather lore  including astrological meteorology  But there were also attempts to establish a theoretical understanding of weather phenomena  Edmond Halley and George Hadley tried to explain trade winds  They reasoned that the rising mass of heated equator air is replaced by an inflow of cooler air from high latitudes  A flow of warm air at high altitude from equator to poles in turn established an early picture of circulation  Frustration with the lack of discipline among weather observers  and the poor quality of the instruments  led the early modern nation states to organise large observation networks  Thus by the end of the 18th century  meteorologists had access to large quantities of reliable weather data  39  In 1832  an electromagnetic telegraph was created by Baron Schilling  41  The arrival of the electrical telegraph in 1837 afforded  for the first time  a practical method for quickly gathering surface weather observations from a wide area  42     This data could be used to produce maps of the state of the atmosphere for a region near the Earth  s surface and to study how these states evolved through time  To make frequent weather forecasts based on these data required a reliable network of observations  but it was not until 1849 that the Smithsonian Institution began to establish an observation network across the United States under the leadership of Joseph Henry  43  Similar observation networks were established in Europe at this time  The Reverend William Clement Ley was key in understanding of cirrus clouds and early understandings of Jet Streams  44  Charles Kenneth Mackinnon Douglas  known as   CKM   Douglas read Ley  s papers after his death and carried on the early study of weather systems  45  nNineteenth century researchers in meteorology were drawn from military or medical backgrounds  rather than trained as dedicated scientists  46  In 1854  the United Kingdom government appointed Robert FitzRoy to the new office of Meteorological Statist to the Board of Trade with the task of gathering weather observations at sea  FitzRoy  s office became the United Kingdom Meteorological Office in 1854  the second oldest national meteorological service in the world  the Central Institution for Meteorology and Geodynamics  ZAMG  in Austria was founded in 1851 and is the oldest weather service in the world   The first daily weather forecasts made by FitzRoy  s Office were published in The Times newspaper in 1860  The following year a system was introduced of hoisting storm warning cones at principal ports when a gale was expected     Over the next 50 years  many countries established national meteorological services  The India Meteorological Department  1875  was established to follow tropical cyclone and monsoon  47  The Finnish Meteorological Central Office  1881  was formed from part of Magnetic Observatory of Helsinki University  48  Japan  s Tokyo Meteorological Observatory  the forerunner of the Japan Meteorological Agency  began constructing surface weather maps in 1883  49  The United States Weather Bureau  1890  was established under the United States Department of Agriculture  The Australian Bureau of Meteorology  1906  was established by a Meteorology Act to unify existing state meteorological services  50  51     In 1904  Norwegian scientist Vilhelm Bjerknes first argued in his paper Weather Forecasting as a Problem in Mechanics and Physics that it should be possible to forecast weather from calculations based upon natural laws  52  53     It was not until later in the 20th century that advances in the understanding of atmospheric physics led to the foundation of modern numerical weather prediction  In 1922  Lewis Fry Richardson published \"Weather Prediction By Numerical Process \" 54  after finding notes and derivations he worked on as an ambulance driver in World War I  He described how small terms in the prognostic fluid dynamics equations that govern atmospheric flow could be neglected  and a numerical calculation scheme that could be devised to allow predictions  Richardson envisioned a large auditorium of thousands of people performing the calculations  However  the sheer number of calculations required was too large to complete without electronic computers  and the size of the grid and time steps used in the calculations led to unrealistic results  Though numerical analysis later found that this was due to numerical instability     Starting in the 1950s  numerical forecasts with computers became feasible  55  The first weather forecasts derived this way used barotropic  single-vertical-level  models  and could successfully predict the large-scale movement of midlatitude Rossby waves  that is  the pattern of atmospheric lows and highs  56  In 1959  the UK Meteorological Office received its first computer  a Ferranti Mercury  57     In the 1960s  the chaotic nature of the atmosphere was first observed and mathematically described by Edward Lorenz  founding the field of chaos theory  58  These advances have led to the current use of ensemble forecasting in most major forecasting centers  to take into account uncertainty arising from the chaotic nature of the atmosphere  59  Mathematical models used to predict the long term weather of the Earth  climate models   have been developed that have a resolution today that are as coarse as the older weather prediction models  These climate models are used to investigate long-term climate shifts  such as what effects might be caused by human emission of greenhouse gases     Meteorologists are scientists who study and work in the field of meteorology  60  The American Meteorological Society publishes and continually updates an authoritative electronic Meteorology Glossary  61  Meteorologists work in government agencies  private consulting and research services  industrial enterprises  utilities  radio and television stations  and in education  In the United States  meteorologists held about 10 000 jobs in 2018  62     Although weather forecasts and warnings are the best known products of meteorologists for the public  weather presenters on radio and television are not necessarily professional meteorologists  They are most often reporters with little formal meteorological training  using unregulated titles such as weather specialist or weatherman  The American Meteorological Society and National Weather Association issue \"Seals of Approval\" to weather broadcasters who meet certain requirements but this is not mandatory to be hired by the medias     Each science has its own unique sets of laboratory equipment  In the atmosphere  there are many things or qualities of the atmosphere that can be measured  Rain  which can be observed  or seen anywhere and anytime was one of the first atmospheric qualities measured historically  Also  two other accurately measured qualities are wind and humidity  Neither of these can be seen but can be felt  The devices to measure these three sprang up in the mid-15th century and were respectively the rain gauge  the anemometer  and the hygrometer  Many attempts had been made prior to the 15th century to construct adequate equipment to measure the many atmospheric variables  Many were faulty in some way or were simply not reliable  Even Aristotle noted this in some of his work as the difficulty to measure the air     Sets of surface measurements are important data to meteorologists  They give a snapshot of a variety of weather conditions at one single location and are usually at a weather station  a ship or a weather buoy  The measurements taken at a weather station can include any number of atmospheric observables  Usually  temperature  pressure  wind measurements  and humidity are the variables that are measured by a thermometer  barometer  anemometer  and hygrometer  respectively  63  Professional stations may also include air quality sensors  carbon monoxide  carbon dioxide  methane  ozone  dust  and smoke   ceilometer  cloud ceiling   falling precipitation sensor   flood sensor  lightning sensor  microphone  explosions  sonic booms  thunder   pyranometer/pyrheliometer/spectroradiometer  IR/Vis/UV photodiodes   rain gauge/snow gauge  scintillation counter  background radiation  fallout  radon   seismometer  earthquakes and tremors   transmissometer  visibility   and a GPS clock for data logging  Upper air data are of crucial importance for weather forecasting  The most widely used technique is launches of radiosondes  Supplementing the radiosondes a network of aircraft collection is organized by the World Meteorological Organization     Remote sensing  as used in meteorology  is the concept of collecting data from remote weather events and subsequently producing weather information  The common types of remote sensing are Radar  Lidar  and satellites  or photogrammetry   Each collects data about the atmosphere from a remote location and  usually  stores the data where the instrument is located  Radar and Lidar are not passive because both use EM radiation to illuminate a specific portion of the atmosphere  64   Weather satellites along with more general-purpose Earth-observing satellites circling the earth at various altitudes have become an indispensable tool for studying a wide range of phenomena from forest fires to El Nio     The study of the atmosphere can be divided into distinct areas that depend on both time and spatial scales  At one extreme of this scale is climatology  In the timescales of hours to days  meteorology separates into micro-  meso-  and synoptic scale meteorology  Respectively  the geospatial size of each of these three scales relates directly with the appropriate timescale     Other subclassifications are used to describe the unique  local  or broad effects within those subclasses     Microscale meteorology is the study of atmospheric phenomena on a scale of about 1 kilometre  0 62 xa0mi  or less  Individual thunderstorms  clouds  and local turbulence caused by buildings and other obstacles  such as individual hills  are modeled on this scale  66     Mesoscale meteorology is the study of atmospheric phenomena that has horizontal scales ranging from 1 xa0km to 1000 xa0km and a vertical scale that starts at the Earth  s surface and includes the atmospheric boundary layer  troposphere  tropopause  and the lower section of the stratosphere  Mesoscale timescales last from less than a day to multiple weeks  The events typically of interest are thunderstorms  squall lines  fronts  precipitation bands in tropical and extratropical cyclones  and topographically generated weather systems such as mountain waves and sea and land breezes  67     Synoptic scale meteorology predicts atmospheric changes at scales up to 1000 xa0km and 105 sec  28 days   in time and space  At the synoptic scale  the Coriolis acceleration acting on moving air masses  outside of the tropics  plays a dominant role in predictions  The phenomena typically described by synoptic meteorology include events such as extratropical cyclones  baroclinic troughs and ridges  frontal zones  and to some extent jet streams  All of these are typically given on weather maps for a specific time  The minimum horizontal scale of synoptic phenomena is limited to the spacing between surface observation stations  68     Global scale meteorology is the study of weather patterns related to the transport of heat from the tropics to the poles  Very large scale oscillations are of importance at this scale  These oscillations have time periods typically on the order of months  such as the MaddenJulian oscillation  or years  such as the El NioSouthern Oscillation and the Pacific decadal oscillation  Global scale meteorology pushes into the range of climatology  The traditional definition of climate is pushed into larger timescales and with the understanding of the longer time scale global oscillations  their effect on climate and weather disturbances can be included in the synoptic and mesoscale timescales predictions     Numerical Weather Prediction is a main focus in understanding airsea interaction  tropical meteorology  atmospheric predictability  and tropospheric/stratospheric processes  69  The Naval Research Laboratory in Monterey  California  developed a global atmospheric model called Navy Operational Global Atmospheric Prediction System  NOGAPS   NOGAPS is run operationally at Fleet Numerical Meteorology and Oceanography Center for the United States Military  Many other global atmospheric models are run by national meteorological agencies     Boundary layer meteorology is the study of processes in the air layer directly above Earth  s surface  known as the atmospheric boundary layer  ABL   The effects of the surface xa0 heating  cooling  and friction xa0 cause turbulent mixing within the air layer  Significant movement  of heat  matter  or momentum on time scales of less than a day are caused by turbulent motions  70  Boundary layer meteorology includes the study of all types of surfaceatmosphere boundary  including ocean  lake  urban land and non-urban land for the study of meteorology     Dynamic meteorology generally focuses on the fluid dynamics of the atmosphere  The idea of air parcel is used to define the smallest element of the atmosphere  while ignoring the discrete molecular and chemical nature of the atmosphere  An air parcel is defined as a point in the fluid continuum of the atmosphere  The fundamental laws of fluid dynamics  thermodynamics  and motion are used to study the atmosphere  The physical quantities that characterize the state of the atmosphere are temperature  density  pressure  etc  These variables have unique values in the continuum  71     Weather forecasting is the application of science and technology to predict the state of the atmosphere at a future time and given location  Humans have attempted to predict the weather informally for millennia and formally since at least the 19th century  72  73  Weather forecasts are made by collecting quantitative data about the current state of the atmosphere and using scientific understanding of atmospheric processes to project how the atmosphere will evolve  74     Once an all-human endeavor based mainly upon changes in barometric pressure  current weather conditions  and sky condition  75  76  forecast models are now used to determine future conditions  Human input is still required to pick the best possible forecast model to base the forecast upon  which involves pattern recognition skills  teleconnections  knowledge of model performance  and knowledge of model biases  The chaotic nature of the atmosphere  the massive computational power required to solve the equations that describe the atmosphere  error involved in measuring the initial conditions  and an incomplete understanding of atmospheric processes mean that forecasts become less accurate as the difference in current time and the time for which the forecast is being made  the range of the forecast  increases  The use of ensembles and model consensus help narrow the error and pick the most likely outcome  77  78  79     There are a variety of end uses to weather forecasts  Weather warnings are important forecasts because they are used to protect life and property  80  Forecasts based on temperature and precipitation are important to agriculture  81  82  83  84  and therefore to commodity traders within stock markets  Temperature forecasts are used by utility companies to estimate demand over coming days  85  86  87  On an everyday basis  people use weather forecasts to determine what to wear  Since outdoor activities are severely curtailed by heavy rain  snow  and wind chill  forecasts can be used to plan activities around these events  and to plan ahead and survive them     Aviation meteorology deals with the impact of weather on air traffic management  It is important for air crews to understand the implications of weather on their flight plan as well as their aircraft  as noted by the Aeronautical Information Manual  88     The effects of ice on aircraft are cumulativethrust is reduced  drag increases  lift lessens  and weight increases  The results are an increase in stall speed and a deterioration of aircraft performance  In extreme cases  2 to 3 inches of ice can form on the leading edge of the airfoil in less than 5 minutes  It takes but 1/2 inch of ice to reduce the lifting power of some aircraft by 50 percent and increases the frictional drag by an equal percentage  89     Meteorologists  soil scientists  agricultural hydrologists  and agronomists are people concerned with studying the effects of weather and climate on plant distribution  crop yield  water-use efficiency  phenology of plant and animal development  and the energy balance of managed and natural ecosystems  Conversely  they are interested in the role of vegetation on climate and weather  90     Hydrometeorology is the branch of meteorology that deals with the hydrologic cycle  the water budget  and the rainfall statistics of storms  91  A hydrometeorologist prepares and issues forecasts of accumulating  quantitative  precipitation  heavy rain  heavy snow  and highlights areas with the potential for flash flooding  Typically the range of knowledge that is required overlaps with climatology  mesoscale and synoptic meteorology  and other geosciences  92     The multidisciplinary nature of the branch can result in technical challenges  since tools and solutions from each of the individual disciplines involved may behave slightly differently  be optimized for different hard- and software platforms and use different data formats  There are some initiatives  such as the DRIHM project 93   that are trying to address this issue  94     Nuclear meteorology investigates the distribution of radioactive aerosols and gases in the atmosphere  95     Maritime meteorology deals with air and wave forecasts for ships operating at sea  Organizations such as the Ocean Prediction Center  Honolulu National Weather Service forecast office  United Kingdom Met Office  and JMA prepare high seas forecasts for the world  s oceans     Military meteorology is the research and application of meteorology for military purposes  In the United States  the United States Navy  s Commander  Naval Meteorology and Oceanography Command oversees meteorological efforts for the Navy and Marine Corps while the United States Air Force  s Air Force Weather Agency is responsible for the Air Force and Army     Environmental meteorology mainly analyzes industrial pollution dispersion physically and chemically based on meteorological parameters such as temperature  humidity  wind  and various weather conditions     Meteorology applications in renewable energy includes basic research  \"exploration \" and potential mapping of wind power and solar radiation for wind and solar energy     Please see weather forecasting for weather forecast sites     Temperature is a physical quantity that expresses hot and cold  It is the manifestation of thermal energy  present in all matter  which is the source of the occurrence of heat  a flow of energy  when a body is in contact with another that is colder or hotter     Temperature is measured with a thermometer  Thermometers are calibrated in various temperature scales that historically have used various reference points and thermometric substances for definition  The most common scales are the Celsius scale  formerly called centigrade  denoted as C   the Fahrenheit scale  denoted as F   and the Kelvin scale  denoted as K   the last of which is predominantly used for scientific purposes by conventions of the International System of Units  SI      The lowest theoretical temperature is absolute zero  at which no more thermal energy can be extracted from a body  Experimentally  it can only be approached very closely  100 pK   but not reached  which is recognized in the third law of thermodynamics     Temperature is important in all fields of natural science  including physics  chemistry  Earth science  astronomy  medicine  biology  ecology  material science  metallurgy  mechanical engineering and geography as well as most aspects of daily life     Many physical processes are related to temperature  some of them are given below     Temperature scales differ in two ways  the point chosen as zero degrees and the magnitudes of incremental units or degrees on the scale     The Celsius scale  C  is used for common temperature measurements in most of the world  It is an empirical scale that was developed by historical progress  which led to its zero point 0 xa0C being defined by the freezing point of water  and additional degrees defined so that 100 xa0C was the boiling point of water  both at sea-level atmospheric pressure  Because of the 100-degree interval  it was called a centigrade scale  3  Since the standardization of the kelvin in the International System of Units  it has subsequently been redefined in terms of the equivalent fixing points on the Kelvin scale  and so that a temperature increment of one degree Celsius is the same as an increment of one kelvin  though they differ by an additive offset of approximately 273 15     The United States commonly uses the Fahrenheit scale  on which water freezes at 32 xa0F and boils at 212 xa0F at sea-level atmospheric pressure     At the absolute zero of temperature  no energy can be removed from matter as heat  a fact expressed in the third law of thermodynamics  At this temperature  matter contains no macroscopic thermal energy  but still has quantum-mechanical zero-point energy as predicted by the uncertainty principle  although this does not enter into the definition of absolute temperature  Experimentally  absolute zero can be approached only very closely  it can never be reached  least temperature attained by experiment is 100 pK   citation needed  Theoretically  in a body at absolute zero temperature  all classical motion of its particles has ceased and they are at complete rest in this classical sense  The absolute zero  defined as 0 xa0K  is approximately equal to 273 15 xa0C  or 459 67 xa0F     Referring to the Boltzmann constant  to the MaxwellBoltzmann distribution  and to the Boltzmann statistical mechanical definition of entropy  as distinct from the Gibbs definition  4  for independently moving microscopic particles  disregarding interparticle potential energy   by international agreement  a temperature scale is defined and said to be absolute because it is independent of the characteristics of particular thermometric substances and thermometer mechanisms  Apart from the absolute zero  it does not have a reference temperature  It is known as the Kelvin scale  widely used in science and technology  The kelvin  the word is spelled with a lower-case k  is the unit of temperature in the International System of Units  SI   The temperature of a body in its own state of thermodynamic equilibrium is always positive  relative to the absolute zero     Besides the internationally agreed Kelvin scale  there is also a thermodynamic temperature scale  invented by Lord Kelvin  also with its numerical zero at the absolute zero of temperature  but directly relating to purely macroscopic thermodynamic concepts  including the macroscopic entropy  though microscopically referable to the Gibbs statistical mechanical definition of entropy for the canonical ensemble  that takes interparticle potential energy into account  as well as independent particle motion so that it can account for measurements of temperatures near absolute zero  4  This scale has a reference temperature at the triple point of water  the numerical value of which is defined by measurements using the aforementioned internationally agreed Kelvin scale     Many scientific measurements use the Kelvin temperature scale  unit symbol  K   named in honor of the physicist who first defined it  It is an absolute scale  Its numerical zero point  0 xa0K  is at the absolute zero of temperature  Since May  2019  its degrees have been defined through particle kinetic theory  and statistical mechanics  In the International System of Units  SI   the magnitude of the kelvin is defined through various empirical measurements of the average kinetic energies of microscopic particles  It is numerically evaluated in terms of the Boltzmann constant  the value of which is defined as fixed by international convention  5  6     Since May 2019  the magnitude of the kelvin is defined in relation to microscopic phenomena  characterized in terms of statistical mechanics  Previously  since 1954  the International System of Units defined a scale and unit for the kelvin as a thermodynamic temperature  by using the reliably reproducible temperature of the triple point of water as a second reference point  the first reference point being 0 xa0K at absolute zero  citation needed     Historically  the triple point temperature of water was defined as exactly 273 16 units of the measurement increment  Today it is an empirically measured quantity  The freezing point of water at sea-level atmospheric pressure occurs at approximately 273 15 xa0K = 0 xa0C     There is a variety of kinds of temperature scale  It may be convenient to classify them as empirically and theoretically based  Empirical temperature scales are historically older  while theoretically based scales arose in the middle of the nineteenth century  7  8     Empirically based temperature scales rely directly on measurements of simple macroscopic physical properties of materials  For example  the length of a column of mercury  confined in a glass-walled capillary tube  is dependent largely on temperature and is the basis of the very useful mercury-in-glass thermometer  Such scales are valid only within convenient ranges of temperature  For example  above the boiling point of mercury  a mercury-in-glass thermometer is impracticable  Most materials expand with temperature increase  but some materials  such as water  contract with temperature increase over some specific range  and then they are hardly useful as thermometric materials  A material is of no use as a thermometer near one of its phase-change temperatures  for example  its boiling-point     In spite of these limitations  most generally used practical thermometers are of the empirically based kind  Especially  it was used for calorimetry  which contributed greatly to the discovery of thermodynamics  Nevertheless  empirical thermometry has serious drawbacks when judged as a basis for theoretical physics  Empirically based thermometers  beyond their base as simple direct measurements of ordinary physical properties of thermometric materials  can be re-calibrated  by use of theoretical physical reasoning  and this can extend their range of adequacy     Theoretically based temperature scales are based directly on theoretical arguments  especially those of kinetic theory and thermodynamics  They are more or less ideally realized in practically feasible physical devices and materials  Theoretically based temperature scales are used to provide calibrating standards for practical empirically-based thermometers     In physics  the internationally agreed conventional temperature scale is called the Kelvin scale  It is calibrated through the internationally agreed and prescribed value of the Boltzmann constant  5  6  referring to motions of microscopic particles  such as atoms  molecules  and electrons  constituent in the body whose temperature is to be measured  In contrast with the thermodynamic temperature scale invented by Kelvin  the presently conventional Kelvin temperature is not defined through comparison with the temperature of a reference state of a standard body  nor in terms of macroscopic thermodynamics     Apart from the absolute zero of temperature  the Kelvin temperature of a body in a state of internal thermodynamic equilibrium is defined by measurements of suitably chosen of its physical properties  such as have precisely known theoretical explanations in terms of the Boltzmann constant  citation needed  That constant refers to chosen kinds of motion of microscopic particles in the constitution of the body  In those kinds of motion  the particles move individually  without mutual interaction  Such motions are typically interrupted by inter-particle collisions  but for temperature measurement  the motions are chosen so that  between collisions  the non-interactive segments of their trajectories are known to be accessible to accurate measurement  For this purpose  interparticle potential energy is disregarded     In an ideal gas  and in other theoretically understood bodies  the Kelvin temperature is defined to be proportional to the average kinetic energy of non-interactively moving microscopic particles  which can be measured by suitable techniques  The proportionality constant is a simple multiple of the Boltzmann constant  If molecules  atoms  or electrons  9  10  are emitted from material and their velocities are measured  the spectrum of their velocities often nearly obeys a theoretical law called the MaxwellBoltzmann distribution  which gives a well-founded measurement of temperatures for which the law holds  11  There have not yet been successful experiments of this same kind that directly use the FermiDirac distribution for thermometry  but perhaps that will be achieved in the future  12     The speed of sound in a gas can be calculated theoretically from the molecular character of the gas  from its temperature and pressure  and from the value of Boltzmann  s constant  For a gas of known molecular character and pressure  this provides a relation between temperature and Boltzmann  s constant  Those quantities can be known or measured more precisely than can the thermodynamic variables that define the state of a sample of water at its triple point  Consequently  taking the value of Boltzmann  s constant as a primarily defined reference of exactly defined value  a measurement of the speed of sound can provide a more precise measurement of the temperature of the gas  13     Measurement of the spectrum of electromagnetic radiation from an ideal three-dimensional black body can provide an accurate temperature measurement because the frequency of maximum spectral radiance of black-body radiation is directly proportional to the temperature of the black body  this is known as Wien  s displacement law and has a theoretical explanation in Planck  s law and the BoseEinstein law     Measurement of the spectrum of noise-power produced by an electrical resistor can also provide accurate temperature measurement  The resistor has two terminals and is in effect a one-dimensional body  The Bose-Einstein law for this case indicates that the noise-power is directly proportional to the temperature of the resistor and to the value of its resistance and to the noise bandwidth  In a given frequency band  the noise-power has equal contributions from every frequency and is called Johnson noise  If the value of the resistance is known then the temperature can be found  14  15     Historically  till May 2019  the definition of the Kelvin scale was that invented by Kelvin  based on a ratio of quantities of energy in processes in an ideal Carnot engine  entirely in terms of macroscopic thermodynamics  citation needed  That Carnot engine was to work between two temperatures  that of the body whose temperature was to be measured  and a reference  that of a body at the temperature of the triple point of water  Then the reference temperature  that of the triple point  was defined to be exactly 273 16 xa0K  Since May 2019  that value has not been fixed by definition but is to be measured through microscopic phenomena  involving the Boltzmann constant  as described above  The microscopic statistical mechanical definition does not have a reference temperature     A material on which a macroscopically defined temperature scale may be based is the ideal gas  The pressure exerted by a fixed volume and mass of an ideal gas is directly proportional to its temperature  Some natural gases show so nearly ideal properties over suitable temperature range that they can be used for thermometry  this was important during the development of thermodynamics and is still of practical importance today  16  17  The ideal gas thermometer is  however  not theoretically perfect for thermodynamics  This is because the entropy of an ideal gas at its absolute zero of temperature is not a positive semi-definite quantity  which puts the gas in violation of the third law of thermodynamics  In contrast to real materials  the ideal gas does not liquefy or solidify  no matter how cold it is  Alternatively thinking  the ideal gas law  refers to the limit of infinitely high temperature and zero pressure  these conditions guarantee non-interactive motions of the constituent molecules  18  19  20     The magnitude of the kelvin is now defined in terms of kinetic theory  derived from the value of Boltzmann  s constant     Kinetic theory provides a microscopic account of temperature for some bodies of material  especially gases  based on macroscopic systems   being composed of many microscopic particles  such as molecules and ions of various species  the particles of a species being all alike  It explains macroscopic phenomena through the classical mechanics of the microscopic particles  The equipartition theorem of kinetic theory asserts that each classical degree of freedom of a freely moving particle has an average kinetic energy of kBT/2 where kB denotes Boltzmann  s constant  citation needed  The translational motion of the particle has three degrees of freedom  so that  except at very low temperatures where quantum effects predominate  the average translational kinetic energy of a freely moving particle in a system with temperature T will be 3kBT/2     Molecules  such as oxygen  O2   have more degrees of freedom than single spherical atoms  they undergo rotational and vibrational motions as well as translations  Heating results in an increase of temperature due to an increase in the average translational kinetic energy of the molecules  Heating will also cause  through equipartitioning  the energy associated with vibrational and rotational modes to increase  Thus a diatomic gas will require more energy input to increase its temperature by a certain amount  i e  it will have a greater heat capacity than a monatomic gas     As noted above  the speed of sound in a gas can be calculated from the molecular character of the gas  from its temperature and pressure  and from the value of Boltzmann  s constant  Taking the value of Boltzmann  s constant as a primarily defined reference of exactly defined value  a measurement of the speed of sound can provide a more precise measurement of the temperature of the gas  13     It is possible to measure the average kinetic energy of constituent microscopic particles if they are allowed to escape from the bulk of the system  through a small hole in the containing wall  The spectrum of velocities has to be measured  and the average calculated from that  It is not necessarily the case that the particles that escape and are measured have the same velocity distribution as the particles that remain in the bulk of the system  but sometimes a good sample is possible     Temperature is one of the principal quantities in the study of thermodynamics  Formerly  the magnitude of the kelvin was defined in thermodynamic terms  but nowadays  as mentioned above  it is defined in terms of kinetic theory     The thermodynamic temperature is said to be absolute for two reasons  One is that its formal character is independent of the properties of particular materials  The other reason is that its zero is  in a sense  absolute  in that it indicates absence of microscopic classical motion of the constituent particles of matter  so that they have a limiting specific heat of zero for zero temperature  according to the third law of thermodynamics  Nevertheless  a thermodynamic temperature does in fact have a definite numerical value that has been arbitrarily chosen by tradition and is dependent on the property of particular materials  it is simply less arbitrary than relative \"degrees\" scales such as Celsius and Fahrenheit   Being an absolute scale with one fixed point  zero   there is only one degree of freedom left to arbitrary choice  rather than two as in relative scales  For the Kelvin scale since May 2019  by international convention  the choice has been made to use knowledge of modes of operation of various thermometric devices  relying on microscopic kinetic theories about molecular motion  The numerical scale is settled by a conventional definition of the value of the Boltzmann constant  which relates macroscopic temperature to average microscopic kinetic energy of particles such as molecules  Its numerical value is arbitrary  and an alternate  less widely used absolute temperature scale exists called the Rankine scale  made to be aligned with the Fahrenheit scale as Kelvin is with Celsius     The thermodynamic definition of temperature is due to Kelvin  It is framed in terms of an idealized device called a Carnot engine  imagined to run in a fictive continuous cycle of successive processes that traverse a cycle of states of its working body  The engine takes in a quantity of heat Q1 from a hot reservoir and passes out a lesser quantity of heat Q2 to a cold reservoir  The difference in energy is passed  as thermodynamic work  to a work reservoir  and is considered to be the output of the engine  The cycle is imagined to run so slowly that at each point of the cycle the working body is in a state of thermodynamic equilibrium  The successive processes of the cycle are thus imagined to run reversibly with no entropy production  Then the quantity of entropy taken in from the hot reservoir when the working body is heated is equal to that passed to the cold reservoir when the working body is cooled  Then the absolute or thermodynamic temperatures  T1 and T2   of the reservoirs are defined so that to be such that     xa0     xa0     xa0     xa0     1     The zeroth law of thermodynamics allows this definition to be used to measure the absolute or thermodynamic temperature of an arbitrary body of interest  by making the other heat reservoir have the same temperature as the body of interest     Kelvin  s original work postulating absolute temperature was published in 1848  It was based on the work of Carnot  before the formulation of the first law of thermodynamics  Carnot had no sound understanding of heat and no specific concept of entropy  He wrote of   caloric   and said that all the caloric that passed from the hot reservoir was passed into the cold reservoir  Kelvin wrote in his 1848 paper that his scale was absolute in the sense that it was defined \"independently of the properties of any particular kind of matter\"  His definitive publication  which sets out the definition just stated  was printed in 1853  a paper read in 1851  21  22  23  24     Numerical details were formerly settled by making one of the heat reservoirs a cell at the triple point of water  which was defined to have an absolute temperature of 273 16 K  25  Nowadays  the numerical value is instead obtained from measurement through the microscopic statistical mechanical international definition  as above     In thermodynamic terms  temperature is an intensive variable because it is equal to a differential coefficient of one extensive variable with respect to another  for a given body  It thus has the dimensions of a ratio of two extensive variables  In thermodynamics  two bodies are often considered as connected by contact with a common wall  which has some specific permeability properties  Such specific permeability can be referred to a specific intensive variable  An example is a diathermic wall that is permeable only to heat  the intensive variable for this case is temperature  When the two bodies have been connected through the specifically permeable wall for a very long time  and have settled to a permanent steady state  the relevant intensive variables are equal in the two bodies  for a diathermal wall  this statement is sometimes called the zeroth law of thermodynamics  26  27  28     In particular  when the body is described by stating its internal energy U  an extensive variable  as a function of its entropy S  also an extensive variable  and other state variables V  N  with U = U  S  V  N   then the temperature is equal to the partial derivative of the internal energy with respect to the entropy  27  28  29      xa0     xa0     xa0     xa0     2     Likewise  when the body is described by stating its entropy S as a function of its internal energy U  and other state variables V  N  with S = S  U  V  N   then the reciprocal of the temperature is equal to the partial derivative of the entropy with respect to the internal energy  27  29  30      xa0     xa0     xa0     xa0     3     The above definition  equation  1   of the absolute temperature  is due to Kelvin  It refers to systems closed to the transfer of matter and has a special emphasis on directly experimental procedures  A presentation of thermodynamics by Gibbs starts at a more abstract level and deals with systems open to the transfer of matter  in this development of thermodynamics  the equations  2  and  3  above are actually alternative definitions of temperature  31     Real-world bodies are often not in thermodynamic equilibrium and not homogeneous  For the study by methods of classical irreversible thermodynamics  a body is usually spatially and temporally divided conceptually into   cells   of small size  If classical thermodynamic equilibrium conditions for matter are fulfilled to good approximation in such a   cell    then it is homogeneous and a temperature exists for it  If this is so for every   cell   of the body  then local thermodynamic equilibrium is said to prevail throughout the body  32  33  34  35  36     It makes good sense  for example  to say of the extensive variable U  or of the extensive variable S  that it has a density per unit volume or a quantity per unit mass of the system  but it makes no sense to speak of the density of temperature per unit volume or quantity of temperature per unit mass of the system  On the other hand  it makes no sense to speak of the internal energy at a point  while when local thermodynamic equilibrium prevails  it makes good sense to speak of the temperature at a point  Consequently  the temperature can vary from point to point in a medium that is not in global thermodynamic equilibrium  but in which there is local thermodynamic equilibrium     Thus  when local thermodynamic equilibrium prevails in a body  the temperature can be regarded as a spatially varying local property in that body  and this is because the temperature is an intensive variable     Temperature is a measure of a quality of a state of a material  37   The quality may be regarded as a more abstract entity than any particular temperature scale that measures it  and is called hotness by some writers  38  39  The quality of hotness refers to the state of material only in a particular locality  and in general  apart from bodies held in a steady state of thermodynamic equilibrium  hotness varies from place to place  It is not necessarily the case that a material in a particular place is in a state that is steady and nearly homogeneous enough to allow it to have a well-defined hotness or temperature  Hotness may be represented abstractly as a one-dimensional manifold  Every valid temperature scale has its own one-to-one map into the hotness manifold  40  41     When two systems in thermal contact are at the same temperature no heat transfers between them  When a temperature difference does exist heat flows spontaneously from the warmer system to the colder system until they are in thermal equilibrium  Such heat transfer occurs by conduction or by thermal radiation  42  43  44  45  46  47  48  49     Experimental physicists  for example Galileo and Newton  50  found that there are indefinitely many empirical temperature scales  Nevertheless  the zeroth law of thermodynamics says that they all measure the same quality  This means that for a body in its own state of internal thermodynamic equilibrium  every correctly calibrated thermometer  of whatever kind  that measures the temperature of the body  records one and the same temperature  For a body that is not in its own state of internal thermodynamic equilibrium  different thermometers can record different temperatures  depending respectively on the mechanisms of operation of the thermometers     For experimental physics  hotness means that  when comparing any two given bodies in their respective separate thermodynamic equilibria  any two suitably given empirical thermometers with numerical scale readings will agree as to which is the hotter of the two given bodies  or that they have the same temperature  51  This does not require the two thermometers to have a linear relation between their numerical scale readings  but it does require that the relation between their numerical readings shall be strictly monotonic  52  53  A definite sense of greater hotness can be had  independently of calorimetry  of thermodynamics  and of properties of particular materials  from Wien  s displacement law of thermal radiation  the temperature of a bath of thermal radiation is proportional  by a universal constant  to the frequency of the maximum of its frequency spectrum  this frequency is always positive  but can have values that tend to zero  Thermal radiation is initially defined for a cavity in thermodynamic equilibrium  These physical facts justify a mathematical statement that hotness exists on an ordered one-dimensional manifold  This is a fundamental character of temperature and thermometers for bodies in their own thermodynamic equilibrium  7  40  41  54  55     Except for a system undergoing a first-order phase change such as the melting of ice  as a closed system receives heat  without a change in its volume and without a change in external force fields acting on it  its temperature rises  For a system undergoing such a phase change so slowly that departure from thermodynamic equilibrium can be neglected  its temperature remains constant as the system is supplied with latent heat  Conversely  a loss of heat from a closed system  without phase change  without change of volume  and without a change in external force fields acting on it  decreases its temperature  56     While for bodies in their own thermodynamic equilibrium states  the notion of temperature requires that all empirical thermometers must agree as to which of two bodies is the hotter or that they are at the same temperature  this requirement is not safe for bodies that are in steady states though not in thermodynamic equilibrium  It can then well be that different empirical thermometers disagree about which is hotter  and if this is so  then at least one of the bodies does not have a well-defined absolute thermodynamic temperature  Nevertheless  anyone has given body and any one suitable empirical thermometer can still support notions of empirical  non-absolute  hotness  and temperature  for a suitable range of processes  This is a matter for study in non-equilibrium thermodynamics  citation needed     When a body is not in a steady-state  then the notion of temperature becomes even less safe than for a body in a steady state not in thermodynamic equilibrium  This is also a matter for study in non-equilibrium thermodynamics     For the axiomatic treatment of thermodynamic equilibrium  since the 1930s  it has become customary to refer to a zeroth law of thermodynamics  The customarily stated minimalist version of such a law postulates only that all bodies  which when thermally connected would be in thermal equilibrium  should be said to have the same temperature by definition  but by itself does not establish temperature as a quantity expressed as a real number on a scale  A more physically informative version of such a law views empirical temperature as a chart on a hotness manifold  40  55  57  While the zeroth law permits the definitions of many different empirical scales of temperature  the second law of thermodynamics selects the definition of a single preferred  absolute temperature  unique up to an arbitrary scale factor  whence called the thermodynamic temperature  7  40  58  59  60  61  If internal energy is considered as a function of the volume and entropy of a homogeneous system in thermodynamic equilibrium  thermodynamic absolute temperature appears as the partial derivative of internal energy with respect the entropy at constant volume  Its natural  intrinsic origin or null point is absolute zero at which the entropy of any system is at a minimum  Although this is the lowest absolute temperature described by the model  the third law of thermodynamics postulates that absolute zero cannot be attained by any physical system     When an energy transfer to or from a body is only as heat  the state of the body changes  Depending on the surroundings and the walls separating them from the body  various changes are possible in the body  They include chemical reactions  increase of pressure  increase of temperature and phase change  For each kind of change under specified conditions  the heat capacity is the ratio of the quantity of heat transferred to the magnitude of the change  citation needed     For example  if the change is an increase in temperature at constant volume  with no phase change and no chemical change  then the temperature of the body rises and its pressure increases  The quantity of heat transferred  Q  divided by the observed temperature change  T  is the body  s heat capacity at constant volume     If heat capacity is measured for a well-defined amount of substance  the specific heat is the measure of the heat required to increase the temperature of such a unit quantity by one unit of temperature  For example  raising the temperature of water by one kelvin  equal to one degree Celsius  requires 4186 joules per kilogram  J/kg      Temperature measurement using modern scientific thermometers and temperature scales goes back at least as far as the early 18th century  when Gabriel Fahrenheit adapted a thermometer  switching to mercury  and a scale both developed by Ole Christensen Rmer  Fahrenheit  s scale is still in use in the United States for non-scientific applications     Temperature is measured with thermometers that may be calibrated to a variety of temperature scales  In most of the world  except for Belize  Myanmar  Liberia and the United States   the Celsius scale is used for most temperature measuring purposes  Most scientists measure temperature using the Celsius scale and thermodynamic temperature using the Kelvin scale  which is the Celsius scale offset so that its null point is 0 xa0K = 273 15 xa0C  or absolute zero  Many engineering fields in the US  notably high-tech and US federal specifications  civil and military   also use the Kelvin and Celsius scales  Other engineering fields in the US also rely upon the Rankine scale  a shifted Fahrenheit scale  when working in thermodynamic-related disciplines such as combustion     The basic unit of temperature in the International System of Units  SI  is the Kelvin  It has the symbol K     For everyday applications  it is often convenient to use the Celsius scale  in which 0 xa0C corresponds very closely to the freezing point of water and 100 xa0C is its boiling point at sea level  Because liquid droplets commonly exist in clouds at sub-zero temperatures  0 xa0C is better defined as the melting point of ice  In this scale  a temperature difference of 1 degree Celsius is the same as a 1kelvin increment  but the scale is offset by the temperature at which ice melts  273 15 xa0K      By international agreement  62  until May 2019  the Kelvin and Celsius scales were defined by two fixing points  absolute zero and the triple point of Vienna Standard Mean Ocean Water  which is water specially prepared with a specified blend of hydrogen and oxygen isotopes  Absolute zero was defined as precisely 0 xa0K and 273 15 xa0C  It is the temperature at which all classical translational motion of the particles comprising matter ceases and they are at complete rest in the classical model  Quantum-mechanically  however  zero-point motion remains and has an associated energy  the zero-point energy   Matter is in its ground state  63  and contains no thermal energy  The temperatures 273 16 xa0K and 0 01 xa0C were defined as those of the triple point of water  This definition served the following purposes  it fixed the magnitude of the kelvin as being precisely 1 part in 273 16 parts of the difference between absolute zero and the triple point of water  it established that one kelvin has precisely the same magnitude as one degree on the Celsius scale  and it established the difference between the null points of these scales as being 273 15 xa0K  0 xa0K = 273 15 xa0C and 273 16 xa0K = 0 01 xa0C   Since 2019  there has been a new definition based on the Boltzmann constant  64  but the scales are scarcely changed     In the United States  the Fahrenheit scale is the most widely used  On this scale the freezing point of water corresponds to 32 xa0F and the boiling point to 212 xa0F  The Rankine scale  still used in fields of chemical engineering in the US  is an absolute scale based on the Fahrenheit increment     The following table shows the temperature conversion formulas for conversions to and from the Celsius scale     The field of plasma physics deals with phenomena of electromagnetic nature that involve very high temperatures  It is customary to express temperature as energy in units of electronvolts  eV  or kiloelectronvolts  keV   The energy  which has a different dimension from temperature  is then calculated as the product of the Boltzmann constant and temperature                      E        =                  k                      B                          T                 displaystyle E=k_   text B  T     Then  1 xa0eV corresponds to 11605 xa0K  In the study of QCD matter one routinely encounters temperatures of the order of a few hundred MeV  equivalent to about 1012 xa0K     Historically  there are several scientific approaches to the explanation of temperature  the classical thermodynamic description based on macroscopic empirical variables that can be measured in a laboratory  the kinetic theory of gases which relates the macroscopic description to the probability distribution of the energy of motion of gas particles  and a microscopic explanation based on statistical physics and quantum mechanics  In addition  rigorous and purely mathematical treatments have provided an axiomatic approach to classical thermodynamics and temperature  65  Statistical physics provides a deeper understanding by describing the atomic behavior of matter and derives macroscopic properties from statistical averages of microscopic states  including both classical and quantum states  In the fundamental physical description  using natural units  the temperature may be measured directly in units of energy  However  in the practical systems of measurement for science  technology  and commerce  such as the modern metric system of units  the macroscopic and the microscopic descriptions are interrelated by the Boltzmann constant  a proportionality factor that scales temperature to the microscopic mean kinetic energy     The microscopic description in statistical mechanics is based on a model that analyzes a system into its fundamental particles of matter or into a set of classical or quantum-mechanical oscillators and considers the system as a statistical ensemble of microstates  As a collection of classical material particles  the temperature is a measure of the mean energy of motion  called kinetic energy  of the particles  whether in solids  liquids  gases  or plasmas  The kinetic energy  a concept of classical mechanics  is half the mass of a particle times its speed squared  In this mechanical interpretation of thermal motion  the kinetic energies of material particles may reside in the velocity of the particles of their translational or vibrational motion or in the inertia of their rotational modes  In monatomic perfect gases and  approximately  in most gas  the temperature is a measure of the mean particle kinetic energy  It also determines the probability distribution function of energy  In condensed matter  and particularly in solids  this purely mechanical description is often less useful and the oscillator model provides a better description to account for quantum mechanical phenomena  Temperature determines the statistical occupation of the microstates of the ensemble  The microscopic definition of temperature is only meaningful in the thermodynamic limit  meaning for large ensembles of states or particles  to fulfill the requirements of the statistical model     Kinetic energy is also considered as a component of thermal energy  The thermal energy may be partitioned into independent components attributed to the degrees of freedom of the particles or to the modes of oscillators in a thermodynamic system  In general  the number of these degrees of freedom that are available for the equipartitioning of energy depends on the temperature  i e  the energy region of the interactions under consideration  For solids  the thermal energy is associated primarily with the vibrations of its atoms or molecules about their equilibrium position  In an ideal monatomic gas  the kinetic energy is found exclusively in the purely translational motions of the particles  In other systems  vibrational and rotational motions also contribute degrees of freedom     Maxwell and Boltzmann developed a kinetic theory that yields a fundamental understanding of temperature in gases  66  nThis theory also explains the ideal gas law and the observed heat capacity of monatomic   or   noble    gases  67  68  69     The ideal gas law is based on observed empirical relationships between pressure  p   volume  V   and temperature  T   and was recognized long before the kinetic theory of gases was developed  see Boyle  s and Charles  s laws    The ideal gas law states  70     where is the number of moles of gas and R xa0= xa08 314462618    xa0Jmol1K1 71   is the gas constant     This relationship gives us our first hint that there is an absolute zero on the temperature scale  because it only holds if the temperature is measured on an absolute scale such as Kelvin  s  The ideal gas law allows one to measure temperature on this absolute scale using the gas thermometer   The temperature in kelvins can be defined as the pressure in pascals of one mole of gas in a container of one cubic meter  divided by the gas constant     Although it is not a particularly convenient device  the gas thermometer provides an essential theoretical basis by which all thermometers can be calibrated   As a practical matter  it is not possible to use a gas thermometer to measure absolute zero temperature since the gases tend to condense into a liquid long before the temperature reaches zero  It is possible  however  to extrapolate to absolute zero by using the ideal gas law  as shown in the figure     The kinetic theory assumes that pressure is caused by the force associated with individual atoms striking the walls  and that all energy is translational kinetic energy   Using a sophisticated symmetry argument  72  Boltzmann deduced what is now called the MaxwellBoltzmann probability distribution function for the velocity of particles in an ideal gas   From that probability distribution function  the average kinetic energy  per particle  of a monatomic ideal gas is 68  73     where the Boltzmann constant kB is the ideal gas constant divided by the Avogadro number  and                               v                      rms                          =                                                        v                              2                                                                =                                          v                         v                                               textstyle v_   text rms  =   sqrt    langle v^ 2   rangle   =   sqrt    langle v v  rangle       is the root-mean-square speed  74  This direct proportionality between temperature and mean molecular kinetic energy is a special case of the equipartition theorem  and holds only in the classical limit of a perfect gas  It does not hold exactly for most substances     When two otherwise isolated bodies are connected together by a rigid physical path impermeable to matter  there is the spontaneous transfer of energy as heat from the hotter to the colder of them  Eventually  they reach a state of mutual thermal equilibrium  in which heat transfer has ceased  and the bodies   respective state variables have settled to become unchanging   75  76  77     One statement of the zeroth law of thermodynamics is that if two systems are each in thermal equilibrium with a third system  then they are also in thermal equilibrium with each other  78  79  80     This statement helps to define temperature but it does not  by itself  complete the definition  An empirical temperature is a numerical scale for the hotness of a thermodynamic system  Such hotness may be defined as existing on a one-dimensional manifold  stretching between hot and cold  Sometimes the zeroth law is stated to include the existence of a unique universal hotness manifold  and of numerical scales on it  so as to provide a complete definition of empirical temperature  57  To be suitable for empirical thermometry  a material must have a monotonic relation between hotness and some easily measured state variable  such as pressure or volume  when all other relevant coordinates are fixed  An exceptionally suitable system is the ideal gas  which can provide a temperature scale that matches the absolute Kelvin scale  The Kelvin scale is defined on the basis of the second law of thermodynamics     As an alternative to considering or defining the zeroth law of thermodynamics  it was the historical development in thermodynamics to define temperature in terms of the second law of thermodynamics which deals with entropy  citation needed  The second law states that any process will result in either no change or a net increase in the entropy of the universe  This can be understood in terms of probability     For example  in a series of coin tosses  a perfectly ordered system would be one in which either every toss comes up heads or every toss comes up tails  This means the outcome is always 100% the same result  In contrast  many mixed  disordered  outcomes are possible  and their number increases with each toss  Eventually  the combinations of ~50% heads and ~50% tails dominate  and obtaining an outcome significantly different from 50/50 becomes increasingly unlikely  Thus the system naturally progresses to a state of maximum disorder or entropy     As temperature governs the transfer of heat between two systems and the universe tends to progress toward a maximum of entropy  it is expected that there is some relationship between temperature and entropy  A heat engine is a device for converting thermal energy into mechanical energy  resulting in the performance of work  and analysis of the Carnot heat engine provides the necessary relationships  The work from a heat engine corresponds to the difference between the heat put into the system at high temperature  qH and the heat extracted at the low temperature  qC     The efficiency is the work divided by the heat input      xa0     xa0     xa0     xa0     4     where wcy is the work done per cycle  The efficiency depends only on qC/qH  Because qC and qH correspond to heat transfer at the temperatures TC and TH respectively  qC/qH should be some function of these temperatures      xa0     xa0     xa0     xa0     5     Carnot  s theorem states that all reversible engines operating between the same heat reservoirs are equally efficient  citation needed  Thus  a heat engine operating between T1 and T3 must have the same efficiency as one consisting of two cycles  one between T1 and T2  and the second between T2 and T3  This can only be the case if    which implies    Since the first function is independent of T2  this temperature must cancel on the right side  meaning f T1  T3  is of the form g T1 /g T3   i e  f T1  T3  = f T1  T2 f T2  T3  = g T1 /g T2   g T2 /g T3  = g T1 /g T3    where g is a function of a single temperature  A temperature scale can now be chosen with the property that     xa0     xa0     xa0     xa0     6     Substituting  6  back into  4  gives a relationship for the efficiency in terms of temperature      xa0     xa0     xa0     xa0     7     For TC = 0 xa0K the efficiency is 100% and that efficiency becomes greater than 100% below 0 xa0K  Since an efficiency greater than 100% violates the first law of thermodynamics  this implies that 0 xa0K is the minimum possible temperature  In fact the lowest temperature ever obtained in a macroscopic system was 20 xa0nK  which was achieved in 1995 at NIST  Subtracting the right hand side of  5  from the middle portion and rearranging gives    where the negative sign indicates heat ejected from the system  This relationship suggests the existence of a state function  S  defined by     xa0     xa0     xa0     xa0     8     where the subscript indicates a reversible process  The change of this state function around any cycle is zero  as is necessary for any state function   This function corresponds to the entropy of the system  which was described previously  Rearranging  8  gives a formula for temperature in terms of fictive infinitesimal quasi-reversible elements of entropy and heat      xa0     xa0     xa0     xa0     9     For a system  where entropy S E  is a function of its energy E  the temperature T is given by     xa0     xa0     xa0     xa0     10     i e  the reciprocal of the temperature is the rate of increase of entropy with respect to energy     Statistical mechanics defines temperature based on a system  s fundamental degrees of freedom   Eq  10  is the defining relation of temperature  where the entropy                     S                 displaystyle S    is defined  up to a constant  by the logarithm of the number of microstates of the system in the given macrostate  as specified in the microcanonical ensemble       where                               k                                    B                                               displaystyle k_   mathrm  B       is Boltzmann  s constant and N is the number of microstates     When two systems with different temperatures are put into purely thermal connection  heat will flow from the higher temperature system to the lower temperature one  thermodynamically this is understood by the second law of thermodynamics  The total change in entropy following a transfer of energy                             E                 displaystyle   Delta E    from system 1 to system 2 is     and is thus positive if                               T                      1                          >                  T                      2                                   displaystyle T_ 1 >T_ 2     n    From the point of view of statistical mechanics  the total number of microstates in the combined system 1 + system 2 is                               N                      1                                            N                      2                                   displaystyle N_ 1   cdot N_ 2      the logarithm of which  times Boltzmann  s constant  is the sum of their entropies  thus a flow of heat from high to low temperature  which brings an increase in total entropy  is more likely than any other scenario  normally it is much more likely   as there are more microstates in the resulting macrostate     It is possible to extend the definition of temperature even to systems of few particles  like in a quantum dot  The generalized temperature is obtained by considering time ensembles instead of configuration-space ensembles given in statistical mechanics in the case of thermal and particle exchange between a small system of fermions  N even less than 10  with a single/double-occupancy system  The finite quantum grand canonical ensemble  81  obtained under the hypothesis of ergodicity and orthodicity  82  allows expressing the generalized temperature from the ratio of the average time of occupation                                                     1                                   displaystyle   tau _ 1     and                                                     2                                   displaystyle   tau _ 2     of the single/double-occupancy system  83     where EF is the Fermi energy  This generalized temperature tends to the ordinary temperature when N goes to infinity     On the empirical temperature scales that are not referenced to absolute zero  a negative temperature is one below the zero-point of the scale used  For example  dry ice has a sublimation temperature of 78 5 xa0C which is equivalent to 109 3 xa0F  citation needed  On the absolute Kelvin scale this temperature is 194 6 xa0K  No body can be brought to exactly 0 xa0K  the temperature of the ideally coldest possible body  by any finite practicable process  this is a consequence of the third law of thermodynamics  84  85  86     The international kinetic theory temperature of a body cannot take negative values  The thermodynamic temperature scale  however  is not so constrained     For a body of matter  there can sometimes be conceptually defined  in terms of microscopic degrees of freedom  namely particle spins  a subsystem  with a temperature other than that of the whole body  When the body is in its own state of internal thermodynamic equilibrium  the temperatures of the whole body and of the subsystem must be the same  The two temperatures can differ when  by work through externally imposed force fields  energy can be transferred to and from the subsystem  separately from the rest of the body  then the whole body is not in its own state of internal thermodynamic equilibrium  There is an upper limit of energy such a spin subsystem can attain     Considering the subsystem to be in a temporary state of virtual thermodynamic equilibrium  it is possible to obtain a negative temperature on the thermodynamic scale  Thermodynamic temperature is the inverse of the derivative of the subsystem  s entropy with respect to its internal energy  As the subsystem  s internal energy increases  the entropy increases for some range  but eventually attains a maximum value and then begins to decrease as the highest energy states begin to fill  At the point of maximum entropy  the temperature function shows the behavior of a singularity  because the slope of the entropy function decreases to zero and then turns negative  As the subsystem  s entropy reaches its maximum  its thermodynamic temperature goes to positive infinity  switching to negative infinity as the slope turns negative  Such negative temperatures are hotter than any positive temperature  Over time  when the subsystem is exposed to the rest of the body  which has a positive temperature  energy is transferred as heat from the negative temperature subsystem to the positive temperature system  87  The kinetic theory temperature is not defined for such subsystems         A gene  or genetic  regulatory network  GRN  is a collection of molecular regulators that interact with each other and with other substances in the cell to govern the gene expression levels of mRNA and proteins which  in turn  determine the function of the cell  GRN also play a central role in morphogenesis  the creation of body structures  which in turn is central to evolutionary developmental biology  evo-devo      The regulator can be DNA  RNA  protein and complexes of these  The interaction can be direct or indirect  through transcribed RNA or translated protein   In general  each mRNA molecule goes on to make a specific protein  or set of proteins   In some cases this protein will be structural  and will accumulate at the cell membrane or within the cell to give it particular structural properties  In other cases the protein will be an enzyme  i e   a micro-machine that catalyses a certain reaction  such as the breakdown of a food source or toxin  Some proteins though serve only to activate other genes  and these are the transcription factors that are the main players in regulatory networks or cascades  By binding to the promoter region at the start of other genes they turn them on  initiating the production of another protein  and so on  Some transcription factors are inhibitory  1     In single-celled organisms  regulatory networks respond to the external environment  optimising the cell at a given time for survival in this environment  Thus a yeast cell  finding itself in a sugar solution  will turn on genes to make enzymes that process the sugar to alcohol  2  This process  which we associate with wine-making  is how the yeast cell makes its living  gaining energy to multiply  which under normal circumstances would enhance its survival prospects     In multicellular animals the same principle has been put in the service of gene cascades that control body-shape  3  Each time a cell divides  two cells result which  although they contain the same genome in full  can differ in which genes are turned on and making proteins  Sometimes a   self-sustaining feedback loop   ensures that a cell maintains its identity and passes it on  Less understood is the mechanism of epigenetics by which chromatin modification may provide cellular memory by blocking or allowing transcription  A major feature of multicellular animals is the use of morphogen gradients  which in effect provide a positioning system that tells a cell where in the body it is  and hence what sort of cell to become  A gene that is turned on in one cell may make a product that leaves the cell and diffuses through adjacent cells  entering them and turning on genes only when it is present above a certain threshold level  These cells are thus induced into a new fate  and may even generate other morphogens that signal back to the original cell  Over longer distances morphogens may use the active process of signal transduction  Such signalling controls embryogenesis  the building of a body plan from scratch through a series of sequential steps  They also control and maintain adult bodies through feedback processes  and the loss of such feedback because of a mutation can be responsible for the cell proliferation that is seen in cancer  In parallel with this process of building structure  the gene cascade turns on genes that make structural proteins that give each cell the physical properties it needs     At one level  biological cells can be thought of as \"partially mixed bags\" of biological chemicals  in the discussion of gene regulatory networks  these chemicals are mostly the messenger RNAs  mRNAs  and proteins that arise from gene expression  These mRNA and proteins interact with each other with various degrees of specificity  Some diffuse around the cell  Others are bound to cell membranes  interacting with molecules in the environment  Still others pass through cell membranes and mediate long range signals to other cells in a multi-cellular organism  These molecules and their interactions comprise a gene regulatory network  A typical gene regulatory network looks something like this     The nodes of this network can represent genes  proteins  mRNAs  protein/protein complexes or cellular processes  Nodes that are depicted as lying along vertical lines are associated with the cell/environment interfaces  while the others are free-floating and can diffuse  Edges between nodes represent interactions between the nodes  that can correspond to individual molecular reactions between DNA  mRNA  miRNA  proteins or molecular processes through which the products of one gene affect those of another  though the lack of experimentally obtained information often implies that some reactions are not modeled at such a fine level of detail  These interactions can be inductive  usually represented by arrowheads or the + sign   with an increase in the concentration of one leading to an increase in the other  inhibitory  represented with filled circles  blunt arrows or the minus sign   with an increase in one leading to a decrease in the other  or dual  when depending on the circumstances the regulator can activate or inhibit the target node  The nodes can regulate themselves directly or indirectly  creating feedback loops  which form cyclic chains of dependencies in the topological network  The network structure is an abstraction of the system  s molecular or chemical dynamics  describing the manifold ways in which one substance affects all the others to which it is connected  In practice  such GRNs are inferred from the biological literature on a given system and represent a distillation of the collective knowledge about a set of related biochemical reactions  To speed up the manual curation of GRNs  some recent efforts try to use text mining  curated databases  network inference from massive data  model checking and other information extraction technologies for this purpose  4     Genes can be viewed as nodes in the network  with input being proteins such as transcription factors  and outputs being the level of gene expression  The value of the node depends on a function which depends on the value of its regulators in previous time steps  in the Boolean network described below these are Boolean functions  typically AND  OR  and NOT   These functions have been interpreted as performing a kind of information processing within the cell  which determines cellular behavior  The basic drivers within cells are concentrations of some proteins  which determine both spatial  location within the cell or tissue  and temporal  cell cycle or developmental stage  coordinates of the cell  as a kind of \"cellular memory\"  The gene networks are only beginning to be understood  and it is a next step for biology to attempt to deduce the functions for each gene \"node\"  to help understand the behavior of the system in increasing levels of complexity  from gene to signaling pathway  cell or tissue level  5     Mathematical models of GRNs have been developed to capture the behavior of the system being modeled  and in some cases generate predictions corresponding with experimental observations  In some other cases  models have proven to make accurate novel predictions  which can be tested experimentally  thus suggesting new approaches to explore in an experiment that sometimes wouldn  t be considered in the design of the protocol of an experimental laboratory  Modeling techniques include differential equations  ODEs   Boolean networks  Petri nets  Bayesian networks  graphical Gaussian network models  Stochastic  and Process Calculi  6  Conversely  techniques have been proposed for generating models of GRNs that best explain a set of time series observations  Recently it has been shown that ChIP-seq signal of histone modification are more correlated with transcription factor motifs at promoters in comparison to RNA level  7  Hence it is proposed that time-series histone modification ChIP-seq could provide more reliable inference of gene-regulatory networks in comparison to methods based on expression levels     Gene regulatory networks are generally thought to be made up of a few highly connected nodes  hubs  and many poorly connected nodes nested within a hierarchical regulatory regime  Thus gene regulatory networks approximate a hierarchical scale free network topology  8  This is consistent with the view that most genes have limited pleiotropy and operate within regulatory modules  9  This structure is thought to evolve due to the preferential attachment of duplicated genes to more highly connected genes  8  Recent work has also shown that natural selection tends to favor networks with sparse connectivity  10     There are primarily two ways that networks can evolve  both of which can occur simultaneously  The first is that network topology can be changed by the addition or subtraction of nodes  genes  or parts of the network  modules  may be expressed in different contexts  The Drosophila Hippo signaling pathway provides a good example  The Hippo signaling pathway controls both mitotic growth and post-mitotic cellular differentiation  11  Recently it was found that the network the Hippo signaling pathway operates in differs between these two functions which in turn changes the behavior of the Hippo signaling pathway  This suggests that the Hippo signaling pathway operates as a conserved regulatory module that can be used for multiple functions depending on context  11  Thus  changing network topology can allow a conserved module to serve multiple functions and alter the final output of the network  The second way networks can evolve is by changing the strength of interactions between nodes  such as how strongly a transcription factor may bind to a cis-regulatory element  Such variation in strength of network edges has been shown to underlie between species variation in vulva cell fate patterning of Caenorhabditis worms  12     Another widely cited characteristic of gene regulatory network is their abundance of certain repetitive sub-networks known as network motifs  Network motifs can be regarded as repetitive topological patterns when dividing a big network into small blocks  Previous analysis found several types of motifs that appeared more often in gene regulatory networks than in randomly generated networks  13  14  15  As an example  one such motif is called feed-forward loops  which consist three nodes  This motif is the most abundant among all possible motifs made up of three nodes  as is shown in the gene regulatory networks of fly  nematode  and human  15     The enriched motifs have been proposed to follow convergent evolution  suggesting they are \"optimal designs\" for certain regulatory purposes  16  For example  modeling shows that feed-forward loops are able to coordinate the change in node A  in terms of concentration and activity  and the expression dynamics of node C  creating different input-output behaviors  17  18  The galactose utilization system of E  coli contains a feed-forward loop which accelerates the activation of galactose utilization operon galETK  potentially facilitating the metabolic transition to galactose when glucose is depleted  19  The feed-forward loop in the arabinose utilization systems of E coli delays the activation of arabinose catabolism operon and transporters  potentially avoiding unnecessary metabolic transition due to temporary fluctuations in upstream signaling pathways  20  Similarly in the Wnt signaling pathway of Xenopus  the feed-forward loop acts as a fold-change detector that responses to the fold change  rather than the absolute change  in the level of -catenin  potentially increasing the resistance to fluctuations in -catenin levels  21  Following the convergent evolution hypothesis  the enrichment of feed-forward loops would be an adaptation for fast response and noise resistance  A recent research found that yeast grown in an environment of constant glucose developed mutations in glucose signaling pathways and growth regulation pathway  suggesting regulatory components responding to environmental changes are dispensable under constant environment  22     On the other hand  some researchers hypothesize that the enrichment of network motifs is non-adaptive  23  In other words  gene regulatory networks can evolve to a similar structure without the specific selection on the proposed input-output behavior  Support for this hypothesis often comes from computational simulations  For example  fluctuations in the abundance of feed-forward loops in a model that simulates the evolution of gene regulatory networks by randomly rewiring nodes may suggest that the enrichment of feed-forward loops is a side-effect of evolution  24  In another model of gene regulator networks evolution  the ratio of the frequencies of gene duplication and gene deletion show great influence on network topology  certain ratios lead to the enrichment of feed-forward loops and create networks that show features of hierarchical scale free networks  De novo evolution of coherent type 1 feed-forward loops has been demonstrated computationally in response to selection for their hypothesized function of filtering out a short spurious signal  supporting adaptive evolution  but for non-idealized noise  a dynamics-based system of feed-forward regulation with different topology was instead favored  25     Regulatory networks allow bacteria to adapt to almost every environmental niche on earth  26  27  A network of interactions among diverse types of molecules including DNA  RNA  proteins and metabolites  is utilised by the bacteria to achieve regulation of gene expression  In bacteria  the principal function of regulatory networks is to control the response to environmental changes  for example nutritional status and environmental stress  28  A complex organization of networks permits the microorganism to coordinate and integrate multiple environmental signals  26     It is common to model such a network with a set of coupled ordinary differential equations  ODEs  or SDEs  describing the reaction kinetics of the constituent parts  Suppose that our regulatory network has                     N                 displaystyle N    nodes  and let                               S                      1                                   t                                    S                      2                                   t                                                     S                      N                                   t                          displaystyle S_ 1  t  S_ 2  t    ldots  S_ N  t     represent the concentrations of the                     N                 displaystyle N    corresponding substances at time                     t                 displaystyle t     Then the temporal evolution of the system can be described approximately by    where the functions                               f                      j                                   displaystyle f_ j     express the dependence of                               S                      j                                   displaystyle S_ j     on the concentrations of other substances present in the cell  The functions                               f                      j                                   displaystyle f_ j     are ultimately derived from basic principles of chemical kinetics or simple expressions derived from these e g  MichaelisMenten enzymatic kinetics  Hence  the functional forms of the                               f                      j                                   displaystyle f_ j     are usually chosen as low-order polynomials or Hill functions that serve as an ansatz for the real molecular dynamics  Such models are then studied using the mathematics of nonlinear dynamics  System-specific information  like reaction rate constants and sensitivities  are encoded as constant parameters  29     By solving for the fixed point of the system     for all                     j                 displaystyle j     one obtains  possibly several  concentration profiles of proteins and mRNAs that are theoretically sustainable  though not necessarily stable   Steady states of kinetic equations thus correspond to potential cell types  and oscillatory solutions to the above equation to naturally cyclic cell types  Mathematical stability of these attractors can usually be characterized by the sign of higher derivatives at critical points  and then correspond to biochemical stability of the concentration profile  Critical points and bifurcations in the equations correspond to critical cell states in which small state or parameter perturbations could switch the system between one of several stable differentiation fates  Trajectories correspond to the unfolding of biological pathways and transients of the equations to short-term biological events  For a more mathematical discussion  see the articles on nonlinearity  dynamical systems  bifurcation theory  and chaos theory     The following example illustrates how a Boolean network can model a GRN together with its gene products  the outputs  and the substances from the environment that affect it  the inputs   Stuart Kauffman was amongst the first biologists to use the metaphor of Boolean networks to model genetic regulatory networks  30  31     The validity of the model can be tested by comparing simulation results with time series observations  A partial validation of a Boolean network model can also come from testing the predicted existence of a yet unknown regulatory connection between two particular transcription factors that each are nodes of the model  32     Continuous network models of GRNs are an extension of the boolean networks described above  Nodes still represent genes and connections between them regulatory influences on gene expression  Genes in biological systems display a continuous range of activity levels and it has been argued that using a continuous representation captures several properties of gene regulatory networks not present in the Boolean model  33  Formally most of these approaches are similar to an artificial neural network  as inputs to a node are summed up and the result serves as input to a sigmoid function  e g   34  but proteins do often control gene expression in a synergistic  i e  non-linear  way  35  However  there is now a continuous network model 36  that allows grouping of inputs to a node thus realizing another level of regulation  This model is formally closer to a higher order recurrent neural network  The same model has also been used to mimic the evolution of cellular differentiation 37  and even multicellular morphogenesis  38     Recent experimental results 39  40  have demonstrated that gene expression is a stochastic process  Thus  many authors are now using the stochastic formalism  after the work by Arkin et al  41  Works on single gene expression 42  and small synthetic genetic networks  43  44  such as the genetic toggle switch of Tim Gardner and Jim Collins  provided additional experimental data on the phenotypic variability and the stochastic nature of gene expression  The first versions of stochastic models of gene expression involved only instantaneous reactions and were driven by the Gillespie algorithm  45     Since some processes  such as gene transcription  involve many reactions and could not be correctly modeled as an instantaneous reaction in a single step  it was proposed to model these reactions as single step multiple delayed reactions in order to account for the time it takes for the entire process to be complete  46     From here  a set of reactions were proposed 47  that allow generating GRNs  These are then simulated using a modified version of the Gillespie algorithm  that can simulate multiple time delayed reactions  chemical reactions where each of the products is provided a time delay that determines when will it be released in the system as a \"finished product\"      For example  basic transcription of a gene can be represented by the following single-step reaction  RNAP is the RNA polymerase  RBS is the RNA ribosome binding site  and Pro xa0i is the promoter region of gene i      Furthermore  there seems to be a trade-off between the noise in gene expression  the speed with which genes can switch  and the metabolic cost associated their functioning  More specifically  for any given level of metabolic cost  there is an optimal trade-off between noise and processing speed and increasing the metabolic cost leads to better speed-noise trade-offs  48  49  50     A recent work proposed a simulator  SGNSim  Stochastic Gene Networks Simulator   51  that can model GRNs where transcription and translation are modeled as multiple time delayed events and its dynamics is driven by a stochastic simulation algorithm  SSA  able to deal with multiple time delayed events  nThe time delays can be drawn from several distributions and the reaction rates from complex nfunctions or from physical parameters  SGNSim can generate ensembles of GRNs within a set of user-defined parameters  such as topology  It can also be used to model specific GRNs and systems of chemical reactions  Genetic perturbations such as gene deletions  gene over-expression  insertions  frame shift mutations can also be modeled as well    \"The GRN is created from a graph with the desired topology  imposing in-degree and out-degree distributions  Gene promoter activities are affected by other genes expression products that act as inputs  in the form of monomers or combined into multimers and set as direct or indirect  Next  each direct input is assigned to an operator site and different transcription factors can be allowed  or not  to compete for the same operator site  while indirect inputs are given a target  Finally  a function is assigned to each gene  defining the gene s response to a combination of transcription factors  promoter state   The transfer functions  that is  how genes respond to a combination of inputs  can be assigned to each combination of promoter states as desired  n\"   In other recent work  multiscale models of gene regulatory networks have been developed that focus on synthetic biology applications  Simulations have been used that model all biomolecular interactions in transcription  translation  regulation  and induction of gene regulatory networks  guiding the design of synthetic systems  52     Other work has focused on predicting the gene expression levels in a gene regulatory network  The approaches used to model gene regulatory networks have been constrained to be interpretable and  as a result  are generally simplified versions of the network  For example  Boolean networks have been used due to their simplicity and ability to handle noisy data but lose data information by having a binary representation of the genes  Also  artificial neural networks omit using a hidden layer so that they can be interpreted  losing the ability to model higher order correlations in the data  Using a model that is not constrained to be interpretable  a more accurate model can be produced  Being able to predict gene expressions more accurately provides a way to explore how drugs affect a system of genes as well as for finding which genes are interrelated in a process  This has been encouraged by the DREAM competition 53  which promotes a competition for the best prediction algorithms  54  Some other recent work has used artificial neural networks with a hidden layer  55     There are three classes of multiple sclerosis  relapsing-remitting  RRMS   primary progressive  PPMS  and secondary progressive  SPMS   Gene regulatory network  GRN  plays a vital role to understand the disease mechanism across these three different multiple sclerosis classes  56     Biochemistry or biological chemistry  is the study of chemical processes within and relating to living organisms  1   A sub-discipline of both chemistry and biology  biochemistry may be divided into three fields  structural biology  enzymology and metabolism   Over the last decades of the 20th century  biochemistry has become successful at explaining living processes through these three disciplines   Almost all areas of the life sciences are being uncovered and developed through biochemical methodology and research  2   Biochemistry focuses on understanding the chemical basis which allows biological molecules to give rise to the processes that occur within living cells and between cells  3  in turn relating greatly to the understanding of tissues and organs  as well as organism structure and function  4  Biochemistry is closely related to molecular biology  which is the study of the molecular mechanisms of biological phenomena  5     Much of biochemistry deals with the structures  functions  and interactions of biological macromolecules  such as proteins  nucleic acids  carbohydrates  and lipids   They provide the structure of cells and perform many of the functions associated with life  6   The chemistry of the cell also depends upon the reactions of small molecules and ions   These can be inorganic  for example  water and metal ions  or organic  for example  the amino acids  which are used to synthesize proteins   7   The mechanisms used by cells to harness energy from their environment via chemical reactions are known as metabolism   The findings of biochemistry are applied primarily in medicine  nutrition and agriculture   In medicine  biochemists investigate the causes and cures of diseases  8   Nutrition studies how to maintain health and wellness and also the effects of nutritional deficiencies  9   In agriculture  biochemists investigate soil and fertilizers  Improving crop cultivation  crop storage  and pest control are also goals     At its most comprehensive definition  biochemistry can be seen as a study of the components and composition of living things and how they come together to become life  In this sense  the history of biochemistry may therefore go back as far as the ancient Greeks  10  However  biochemistry as a specific scientific discipline began sometime in the 19th century  or a little earlier  depending on which aspect of biochemistry is being focused on  Some argued that the beginning of biochemistry may have been the discovery of the first enzyme  diastase  now called amylase   in 1833 by Anselme Payen  11  while others considered Eduard Buchner  s first demonstration of a complex biochemical process alcoholic fermentation in cell-free extracts in 1897 to be the birth of biochemistry  12  13  14   Some might also point as its beginning to the influential 1842 work by Justus von Liebig  Animal chemistry  or  Organic chemistry in its applications to physiology and pathology  which presented a chemical theory of metabolism  10  or even earlier to the 18th century studies on fermentation and respiration by Antoine Lavoisier  15  16  Many other pioneers in the field who helped to uncover the layers of complexity of biochemistry have been proclaimed founders of modern biochemistry  Emil Fischer  who studied the chemistry of proteins  17  and F  Gowland Hopkins  who studied enzymes and the dynamic nature of biochemistry  represent two examples of early biochemists  18     The term \"biochemistry\" itself  is derived from a combination of biology and chemistry  In 1877  Felix Hoppe-Seyler used the term  biochemie in German  as a synonym for physiological chemistry in the foreword to the first issue of Zeitschrift fr Physiologische Chemie  Journal of Physiological Chemistry  where he argued for the setting up of institutes dedicated to this field of study  19  20  The German chemist Carl Neuberg however is often cited to have coined the word in 1903  21  22  23  while some credited it to Franz Hofmeister  24     It was once generally believed that life and its materials had some essential property or substance  often referred to as the \"vital principle\"  distinct from any found in non-living matter  and it was thought that only living beings could produce the molecules of life  26  In 1828  Friedrich Whler published a paper on his serendipitous urea synthesis from potassium cyanate and ammonium sulfate  some regarded that as a direct overthrow of vitalism and the establishment of organic chemistry  27   28   However  the Whler synthesis has sparked controversy as some reject the death of vitalism at his hands  29   Since then  biochemistry has advanced  especially since the mid-20th century  with the development of new techniques such as chromatography  X-ray diffraction  dual polarisation interferometry  NMR spectroscopy  radioisotopic labeling  electron microscopy and molecular dynamics simulations  These techniques allowed for the discovery and detailed analysis of many molecules and metabolic pathways of the cell  such as glycolysis and the Krebs cycle  citric acid cycle   and led to an understanding of biochemistry on a molecular level     Another significant historic event in biochemistry is the discovery of the gene  and its role in the transfer of information in the cell  In the 1950s  James D  Watson  Francis Crick  Rosalind Franklin and Maurice Wilkins were instrumental in solving DNA structure and suggesting its relationship with the genetic transfer of information  30  In 1958  George Beadle and Edward Tatum received the Nobel Prize for work in fungi showing that one gene produces one enzyme  31  In 1988  Colin Pitchfork was the first person convicted of murder with DNA evidence  which led to the growth of forensic science  32  More recently  Andrew Z  Fire and Craig C  Mello received the 2006 Nobel Prize for discovering the role of RNA interference  RNAi   in the silencing of gene expression  33     Around two dozen chemical elements are essential to various kinds of biological life  Most rare elements on Earth are not needed by life  exceptions being  selenium and iodine   34  while a few common ones  aluminum and titanium  are not used  Most organisms share element needs  but there are a few differences between plants and animals  For example  ocean algae use bromine  but land plants and animals seem to need none  All animals require sodium  but some plants do not  Plants need boron and silicon  but animals may not  or may need ultra-small amounts      Just six elementscarbon  hydrogen  nitrogen  oxygen  calcium and phosphorusmake up almost 99% of the mass of living cells  including those in the human body  see composition of the human body for a complete list   In addition to the six major elements that compose most of the human body  humans require smaller amounts of possibly 18 more  35     The 4 main classes of molecules in bio-chemistry  often called biomolecules  are carbohydrates  lipids  proteins  and nucleic acids  36  Many biological molecules are polymers  in this terminology  monomers are relatively small macromolecules that are linked together to create large macromolecules known as polymers  When monomers are linked together to synthesize a biological polymer  they undergo a process called dehydration synthesis  Different macromolecules can assemble in larger complexes  often needed for biological activity     Two of the main functions of carbohydrates are energy storage and providing structure  One of the common sugars known as glucose is carbohydrate  but not all carbohydrates are sugars  There are more carbohydrates on Earth than any other known type of biomolecule  they are used to store energy and genetic information  as well as play important roles in cell to cell interactions and communications     The simplest type of carbohydrate is a monosaccharide  which among other properties contains carbon  hydrogen  and oxygen  mostly in a ratio of 1 2 1  generalized formula CnH2nOn  where is at least 3   Glucose   C6H12O6  is one of the most important carbohydrates  others include fructose  C6H12O6   the sugar commonly associated with the sweet taste of fruits  37  a  and deoxyribose  C5H10O4   a component of DNA   A monosaccharide can switch between acyclic  open-chain  form and a cyclic form   The open-chain form can be turned into a ring of carbon atoms bridged by an oxygen atom created from the carbonyl group of one end and the hydroxyl group of another  The cyclic molecule has a hemiacetal or hemiketal group  depending on whether the linear form was an aldose or a ketose  38     In these cyclic forms  the ring usually has 5 or 6 atoms   These forms are called furanoses and pyranoses  respectivelyby analogy with furan and pyran  the simplest compounds with the same carbon-oxygen ring  although they lack the carbon-carbon double bonds of these two molecules    For example  the aldohexose glucose may form a hemiacetal linkage between the hydroxyl on carbon 1 and the oxygen on carbon 4  yielding a molecule with a 5-membered ring  called glucofuranose   The same reaction can take place between carbons 1 and 5 to form a molecule with a 6-membered ring  called glucopyranose  Cyclic forms with a 7-atom ring called heptoses are rare     Two monosaccharides can be joined together by a glycosidic or ether bond into a disaccharide through a dehydration reaction during which a molecule of water is released  The reverse reaction in which the glycosidic bond of a disaccharide is broken into two monosaccharides is termed hydrolysis   The best-known disaccharide is sucrose or ordinary sugar  which consists of a glucose molecule and a fructose molecule joined together  Another important disaccharide is lactose found in milk  consisting of a glucose molecule and a galactose molecule  Lactose may be hydrolysed by lactase  and deficiency in this enzyme results in lactose intolerance     When a few  around three to six  monosaccharides are joined  it is called an oligosaccharide  oligo- meaning \"few\"   These molecules tend to be used as markers and signals  as well as having some other uses  39  Many monosaccharides joined together form a polysaccharide  They can be joined together in one long linear chain  or they may be branched  Two of the most common polysaccharides are cellulose and glycogen  both consisting of repeating glucose monomers    Cellulose is an important structural component of plant  s cell walls and glycogen is used as a form of energy storage in animals     Sugar can be characterized by having reducing or non-reducing ends  A reducing end of a carbohydrate is a carbon atom that can be in equilibrium with the open-chain aldehyde  aldose  or keto form  ketose   If the joining of monomers takes place at such a carbon atom  the free hydroxy group of the pyranose or furanose form is exchanged with an OH-side-chain of another sugar  yielding a full acetal  This prevents opening of the chain to the aldehyde or keto form and renders the modified residue non-reducing  Lactose contains a reducing end at its glucose moiety  whereas the galactose moiety forms a full acetal with the C4-OH group of glucose  Saccharose does not have a reducing end because of full acetal formation between the aldehyde carbon of glucose  C1  and the keto carbon of fructose  C2      Lipids comprise a diverse range of molecules and to some extent is a catchall for relatively water-insoluble or nonpolar compounds of biological origin  including waxes  fatty acids  fatty-acid derived phospholipids  sphingolipids  glycolipids  and terpenoids  e g   retinoids and steroids   Some lipids are linear  open-chain aliphatic molecules  while others have ring structures  Some are aromatic  with a cyclic  ring  and planar  flat  structure  while others are not  Some are flexible  while others are rigid     Lipids are usually made from one molecule of glycerol combined with other molecules  In triglycerides  the main group of bulk lipids  there is one molecule of glycerol and three fatty acids  Fatty acids are considered the monomer in that case  and may be saturated  no double bonds in the carbon chain  or unsaturated  one or more double bonds in the carbon chain      Most lipids have some polar character in addition to being largely nonpolar  In general  the bulk of their structure is nonpolar or hydrophobic  \"water-fearing\"   meaning that it does not interact well with polar solvents like water  Another part of their structure is polar or hydrophilic  \"water-loving\"  and will tend to associate with polar solvents like water  This makes them amphiphilic molecules  having both hydrophobic and hydrophilic portions   In the case of cholesterol  the polar group is a mere OH  hydroxyl or alcohol   In the case of phospholipids  the polar groups are considerably larger and more polar  as described below     Lipids are an integral part of our daily diet  Most oils and milk products that we use for cooking and eating like butter  cheese  ghee etc   are composed of fats  Vegetable oils are rich in various polyunsaturated fatty acids  PUFA   Lipid-containing foods undergo digestion within the body and are broken into fatty acids and glycerol  which are the final degradation products of fats and lipids  Lipids  especially phospholipids  are also used in various pharmaceutical products  either as co-solubilisers  e g   in parenteral infusions  or else as drug carrier components  e g   in a liposome or transfersome      Proteins are very large moleculesmacro-biopolymersmade from monomers called amino acids  An amino acid consists of an alpha carbon atom attached to an amino group  NH2  a carboxylic acid group  COOH  although these exist as NH3+ and COO under physiologic conditions   a simple hydrogen atom  and a side chain commonly denoted as \"R\"  The side chain  \"R\" is different for each amino acid of which there are 20 standard ones   It is this \"R\" group that made each amino acid different  and the properties of the side-chains greatly influence the overall three-dimensional conformation of a protein  Some amino acids have functions by themselves or in a modified form  for instance  glutamate functions as an important neurotransmitter  Amino acids can be joined via a peptide bond  In this dehydration synthesis  a water molecule is removed and the peptide bond connects the nitrogen of one amino acid  s amino group to the carbon of the other  s carboxylic acid group  The resulting molecule is called a dipeptide  and short stretches of amino acids  usually  fewer than thirty  are called peptides or polypeptides  Longer stretches merit the title proteins  As an example  the important blood serum protein albumin contains 585 amino acid residues  42     Proteins can have structural and/or functional roles  For instance  movements of the proteins actin and myosin ultimately are responsible for the contraction of skeletal muscle  One property many proteins have is that they specifically bind to a certain molecule or class of moleculesthey may be extremely selective in what they bind  Antibodies are an example of proteins that attach to one specific type of molecule  Antibodies are composed of heavy and light chains  Two heavy chains would be linked to two light chains through disulfide linkages between their amino acids  Antibodies are specific through variation based on differences in the N-terminal domain  43     The enzyme-linked immunosorbent assay  ELISA   which uses antibodies  is one of the most sensitive tests modern medicine uses to detect various biomolecules  Probably the most important proteins  however  are the enzymes  Virtually every reaction in a living cell requires an enzyme to lower the activation energy of the reaction  12  These molecules recognize specific reactant molecules called substrates  they then catalyze the reaction between them  By lowering the activation energy  the enzyme speeds up that reaction by a rate of 1011 or more  12  a reaction that would normally take over 3 000 years to complete spontaneously might take less than a second with an enzyme  44  The enzyme itself is not used up in the process and is free to catalyze the same reaction with a new set of substrates  Using various modifiers  the activity of the enzyme can be regulated  enabling control of the biochemistry of the cell as a whole  12     The structure of proteins is traditionally described in a hierarchy of four levels  The primary structure of a protein consists of its linear sequence of amino acids  for instance  \"alanine-glycine-tryptophan-serine-glutamate-asparagine-glycine-lysine-\"  Secondary structure is concerned with local morphology  morphology being the study of structure   Some combinations of amino acids will tend to curl up in a coil called an -helix or into a sheet called a -sheet  some -helixes can be seen in the hemoglobin schematic above  Tertiary structure is the entire three-dimensional shape of the protein  This shape is determined by the sequence of amino acids  In fact  a single change can change the entire structure  The alpha chain of hemoglobin contains 146 amino acid residues  substitution of the glutamate residue at position 6 with a valine residue changes the behavior of hemoglobin so much that it results in sickle-cell disease  Finally  quaternary structure is concerned with the structure of a protein with multiple peptide subunits  like hemoglobin with its four subunits  Not all proteins have more than one subunit  45     Ingested proteins are usually broken up into single amino acids or dipeptides in the small intestine and then absorbed  They can then be joined to form new proteins  Intermediate products of glycolysis  the citric acid cycle  and the pentose phosphate pathway can be used to form all twenty amino acids  and most bacteria and plants possess all the necessary enzymes to synthesize them  Humans and other mammals  however  can synthesize only half of them  They cannot synthesize isoleucine  leucine  lysine  methionine  phenylalanine  threonine  tryptophan  and valine  Because they must be ingested  these are the essential amino acids  Mammals do possess the enzymes to synthesize alanine  asparagine  aspartate  cysteine  glutamate  glutamine  glycine  proline  serine  and tyrosine  the nonessential amino acids  While they can synthesize arginine and histidine  they cannot produce it in sufficient amounts for young  growing animals  and so these are often considered essential amino acids     If the amino group is removed from an amino acid  it leaves behind a carbon skeleton called an -keto acid  Enzymes called transaminases can easily transfer the amino group from one amino acid  making it an -keto acid  to another -keto acid  making it an amino acid   This is important in the biosynthesis of amino acids  as for many of the pathways  intermediates from other biochemical pathways are converted to the -keto acid skeleton  and then an amino group is added  often via transamination  The amino acids may then be linked together to form a protein     A similar process is used to break down proteins  It is first hydrolyzed into its component amino acids  Free ammonia  NH3   existing as the ammonium ion  NH4+  in blood  is toxic to life forms  A suitable method for excreting it must therefore exist  Different tactics have evolved in different animals  depending on the animals   needs  Unicellular organisms simply release the ammonia into the environment  Likewise  bony fish can release the ammonia into the water where it is quickly diluted  In general  mammals convert the ammonia into urea  via the urea cycle     In order to determine whether two proteins are related  or in other words to decide whether they are homologous or not  scientists use sequence-comparison methods  Methods like sequence alignments and structural alignments are powerful tools that help scientists identify homologies between related molecules  The relevance of finding homologies among proteins goes beyond forming an evolutionary pattern of protein families  By finding how similar two protein sequences are  we acquire knowledge about their structure and therefore their function     Nucleic acids  so-called because of their prevalence in cellular nuclei  is the generic name of the family of biopolymers  They are complex  high-molecular-weight biochemical macromolecules that can convey genetic information in all living cells and viruses  2  The monomers are called nucleotides  and each consists of three components  a nitrogenous heterocyclic base  either a purine or a pyrimidine   a pentose sugar  and a phosphate group  46     The most common nucleic acids are deoxyribonucleic acid  DNA  and ribonucleic acid  RNA   The phosphate group and the sugar of each nucleotide bond with each other to form the backbone of the nucleic acid  while the sequence of nitrogenous bases stores the information  The most common nitrogenous bases are adenine  cytosine  guanine  thymine  and uracil  The nitrogenous bases of each strand of a nucleic acid will form hydrogen bonds with certain other nitrogenous bases in a complementary strand of nucleic acid  similar to a zipper   Adenine binds with thymine and uracil  thymine binds only with adenine  and cytosine and guanine can bind only with one another  Adenine and Thymine & Adenine and Uracil contains two hydrogen Bonds  while Hydrogen Bonds formed between cytosine and guanine are three in number     Aside from the genetic material of the cell  nucleic acids often play a role as second messengers  as well as forming the base molecule for adenosine triphosphate  ATP   the primary energy-carrier molecule found in all living organisms  Also  the nitrogenous bases possible in the two nucleic acids are different  adenine  cytosine  and guanine occur in both RNA and DNA  while thymine occurs only in DNA and uracil occurs in RNA     Glucose is an energy source in most life forms  For instance  polysaccharides are broken down into their monomers by enzymes  glycogen phosphorylase removes glucose residues from glycogen  a polysaccharide   Disaccharides like lactose or sucrose are cleaved into their two component monosaccharides     Glucose is mainly metabolized by a very important ten-step pathway called glycolysis  the net result of which is to break down one molecule of glucose into two molecules of pyruvate  This also produces a net two molecules of ATP  the energy currency of cells  along with two reducing equivalents of converting NAD+  nicotinamide adenine dinucleotide  oxidized form  to NADH  nicotinamide adenine dinucleotide  reduced form   This does not require oxygen  if no oxygen is available  or the cell cannot use oxygen   the NAD is restored by converting the pyruvate to lactate  lactic acid   e g   in humans  or to ethanol plus carbon dioxide  e g   in yeast   Other monosaccharides like galactose and fructose can be converted into intermediates of the glycolytic pathway  47     In aerobic cells with sufficient oxygen  as in most human cells  the pyruvate is further metabolized  It is irreversibly converted to acetyl-CoA  giving off one carbon atom as the waste product carbon dioxide  generating another reducing equivalent as NADH  The two molecules acetyl-CoA  from one molecule of glucose  then enter the citric acid cycle  producing two molecules of ATP  six more NADH molecules and two reduced  ubi quinones  via FADH2 as enzyme-bound cofactor   and releasing the remaining carbon atoms as carbon dioxide  The produced NADH and quinol molecules then feed into the enzyme complexes of the respiratory chain  an electron transport system transferring the electrons ultimately to oxygen and conserving the released energy in the form of a proton gradient over a membrane  inner mitochondrial membrane in eukaryotes   Thus  oxygen is reduced to water and the original electron acceptors NAD+ and quinone are regenerated  This is why humans breathe in oxygen and breathe out carbon dioxide  The energy released from transferring the electrons from high-energy states in NADH and quinol is conserved first as proton gradient and converted to ATP via ATP synthase  This generates an additional 28 molecules of ATP  24 from the 8 NADH + 4 from the 2 quinols   totaling to 32 molecules of ATP conserved per degraded glucose  two from glycolysis + two from the citrate cycle   48  It is clear that using oxygen to completely oxidize glucose provides an organism with far more energy than any oxygen-independent metabolic feature  and this is thought to be the reason why complex life appeared only after Earth  s atmosphere accumulated large amounts of oxygen     In vertebrates  vigorously contracting skeletal muscles  during weightlifting or sprinting  for example  do not receive enough oxygen to meet the energy demand  and so they shift to anaerobic metabolism  converting glucose to lactate   nThe combination of glucose from noncarbohydrates origin  such as fat and proteins  This only happens when glycogen supplies in the liver are worn out  The pathway is a crucial reversal of glycolysis from pyruvate to glucose and can utilize many sources like amino acids  glycerol and Krebs Cycle  Large scale protein and fat catabolism usually occur when those suffer from starvation or certain endocrine disorders  49  The liver regenerates the glucose  using a process called gluconeogenesis  This process is not quite the opposite of glycolysis  and actually requires three times the amount of energy gained from glycolysis  six molecules of ATP are used  compared to the two gained in glycolysis   Analogous to the above reactions  the glucose produced can then undergo glycolysis in tissues that need energy  be stored as glycogen  or starch in plants   or be converted to other monosaccharides or joined into di- or oligosaccharides  The combined pathways of glycolysis during exercise  lactate  s crossing via the bloodstream to the liver  subsequent gluconeogenesis and release of glucose into the bloodstream is called the Cori cycle  50     Researchers in biochemistry use specific techniques native to biochemistry  but increasingly combine these with techniques and ideas developed in the fields of genetics  molecular biology  and biophysics  There is not a defined line between these disciplines  Biochemistry studies the chemistry required for biological activity of molecules  molecular biology studies their biological activity  genetics studies their heredity  which happens to be carried by their genome  This is shown in the following schematic that depicts one possible view of the relationships between the fields     Extremophiles are microorganisms that live in extreme conditions  some of which may provide some exceptions or variations on some of the natural laws cited above  For example  in July 2019  a scientific study of Kidd Mine in Canada discovered sulfur-breathing organisms that live 7900 feet below the surface and absorb sulfur instead of oxygen to facilitate cellular respiration  These organisms are also remarkable due to eating rocks such as pyrite as their regular food source  51  52  53     The DNA polymerase of the thermophile bacteria Thermus aquaticus  extracted in 1968 and named Taq polymerase  is a biochemical DNA replicator resistant to relative high temperatures  50-80 xa0C   which has allowed molecular biologists to ease complications in the PCR  Polymerase Chain Reaction  method     a  ^  Fructose is not the only sugar found in fruits  Glucose and sucrose are also found in varying quantities in various fruits  and sometimes exceed the fructose present  For example  32% of the edible portion of a date is glucose  compared with 24% fructose and 8% sucrose  However  peaches contain more sucrose  6 66%  than they do fructose  0 93%  or glucose  1 47%   54      n    Genetic drift  allelic drift or the Sewall Wright effect  1  is the change in the frequency of an existing gene variant  allele  in a population due to random sampling of organisms  2  The alleles in the offspring are a sample of those in the parents  and chance has a role in determining whether a given individual survives and reproduces  A population  s allele frequency is the fraction of the copies of one gene that share a particular form  3     Genetic drift may cause gene variants to disappear completely and thereby reduce genetic variation  4  It can also cause initially rare alleles to become much more frequent and even fixed     When there are few copies of an allele  the effect of genetic drift is larger  and when there are many copies the effect is smaller  In the middle of the 20th century  vigorous debates occurred over the relative importance of natural selection versus neutral processes  including genetic drift  Ronald Fisher  who explained natural selection using Mendelian genetics  5  held the view that genetic drift plays at the most a minor role in evolution  and this remained the dominant view for several decades  In 1968  population geneticist Motoo Kimura rekindled the debate with his neutral theory of molecular evolution  which claims that most instances where a genetic change spreads across a population  although not necessarily changes in phenotypes  are caused by genetic drift acting on neutral mutations  6  7     The process of genetic drift can be illustrated using 20 marbles in a jar to represent 20 organisms in a population  8  Consider this jar of marbles as the starting population  Half of the marbles in the jar are red and half are blue  with each colour corresponding to a different allele of one gene in the population  In each new generation the organisms reproduce at random  To represent this reproduction  randomly select a marble from the original jar and deposit a new marble with the same colour into a new jar  This is the \"offspring\" of the original marble  meaning that the original marble remains in its jar  Repeat this process until there are 20 new marbles in the second jar  The second jar will now contain 20 \"offspring\"  or marbles of various colours  Unless the second jar contains exactly 10 red marbles and 10 blue marbles  a random shift has occurred in the allele frequencies     If this process is repeated a number of times  the numbers of red and blue marbles picked each generation will fluctuate  Sometimes a jar will have more red marbles than its \"parent\" jar and sometimes more blue  This fluctuation is analogous to genetic drift xa0 a change in the population  s allele frequency resulting from a random variation in the distribution of alleles from one generation to the next     It is even possible that in any one generation no marbles of a particular colour are chosen  meaning they have no offspring  In this example  if no red marbles are selected  the jar representing the new generation contains only blue offspring  If this happens  the red allele has been lost permanently in the population  while the remaining blue allele has become fixed  all future generations are entirely blue  In small populations  fixation can occur in just a few generations     The mechanisms of genetic drift can be illustrated with a simplified example  Consider a very large colony of bacteria isolated in a drop of solution  The bacteria are genetically identical except for a single gene with two alleles labeled A and B  A and B are neutral alleles meaning that they do not affect the bacteria  s ability to survive and reproduce  all bacteria in this colony are equally likely to survive and reproduce  Suppose that half the bacteria have allele A and the other half have allele B  Thus A and B each have allele frequency 1/2     The drop of solution then shrinks until it has only enough food to sustain four bacteria  All other bacteria die without reproducing  Among the four who survive  there are sixteen possible combinations for the A and B alleles      A-A-A-A    B-A-A-A    A-B-A-A    B-B-A-A     A-A-B-A    B-A-B-A    A-B-B-A    B-B-B-A    A-A-A-B    B-A-A-B    A-B-A-B    B-B-A-B    A-A-B-B    B-A-B-B    A-B-B-B    B-B-B-B      Since all bacteria in the original solution are equally likely to survive when the solution shrinks  the four survivors are a random sample from the original colony  The probability that each of the four survivors has a given allele is 1/2  and so the probability that any particular allele combination occurs when the solution shrinks is     The original population size is so large that the sampling effectively happens with replacement   In other words  each of the sixteen possible allele combinations is equally likely to occur  with probability 1/16     Counting the combinations with the same number of A and B  we get the following table     As shown in the table  the total number of combinations that have the same number of A alleles as of B alleles is six  and the probability of this combination is 6/16  The total number of other combinations is ten  so the probability of unequal number of A and B alleles is 10/16  Thus  although the original colony began with an equal number of A and B alleles  it is very possible that the number of alleles in the remaining population of four members will not be equal  Equal numbers is actually less likely than unequal numbers  In the latter case  genetic drift has occurred because the population  s allele frequencies have changed due to random sampling  In this example the population contracted to just four random survivors  a phenomenon known as population bottleneck     The probabilities for the number of copies of allele A  or B  that survive  given in the last column of the above table  can be calculated directly from the binomial distribution where the \"success\" probability  probability of a given allele being present  is 1/2  i e   the probability that there are k copies of A  or B  alleles in the combination  is given by    where n=4 is the number of surviving bacteria     Mathematical models of genetic drift can be designed using either branching processes or a diffusion equation describing changes in allele frequency in an idealised population  9     Consider a gene with two alleles  A or B  In diploid populations consisting of N individuals there are 2N copies of each gene  An individual can have two copies of the same allele or two different alleles  We can call the frequency of one allele p and the frequency of the other q  The WrightFisher model  named after Sewall Wright and Ronald Fisher  assumes that generations do not overlap  for example  annual plants have exactly one generation per year  and that each copy of the gene found in the new generation is drawn independently at random from all copies of the gene in the old generation  The formula to calculate the probability of obtaining k copies of an allele that had frequency p in the last generation is then 10  11     where the symbol \"!\" signifies the factorial function  This expression can also be formulated using the binomial coefficient     The Moran model assumes overlapping generations  At each time step  one individual is chosen to reproduce and one individual is chosen to die  So in each timestep  the number of copies of a given allele can go up by one  go down by one  or can stay the same  This means that the transition matrix is tridiagonal  which means that mathematical solutions are easier for the Moran model than for the WrightFisher model  On the other hand  computer simulations are usually easier to perform using the WrightFisher model  because fewer time steps need to be calculated  In the Moran model  it takes N timesteps to get through one generation  where N is the effective population size  In the WrightFisher model  it takes just one  12     In practice  the Moran and WrightFisher models give qualitatively similar results  but genetic drift runs twice as fast in the Moran model     If the variance in the number of offspring is much greater than that given by the binomial distribution assumed by the WrightFisher model  then given the same overall speed of genetic drift  the variance effective population size   genetic drift is a less powerful force compared to selection  13  Even for the same variance  if higher moments of the offspring number distribution exceed those of the binomial distribution then again the force of genetic drift is substantially weakened  14     Random changes in allele frequencies can also be caused by effects other than sampling error  for example random changes in selection pressure  15     One important alternative source of stochasticity  perhaps more important than genetic drift  is genetic draft  16  Genetic draft is the effect on a locus by selection on linked loci  The mathematical properties of genetic draft are different from those of genetic drift  17  The direction of the random change in allele frequency is autocorrelated across generations  2     The HardyWeinberg principle states that within sufficiently large populations  the allele frequencies remain constant from one generation to the next unless the equilibrium is disturbed by migration  genetic mutations  or selection  18     However  in finite populations  no new alleles are gained from the random sampling of alleles passed to the next generation  but the sampling can cause an existing allele to disappear  Because random sampling can remove  but not replace  an allele  and because random declines or increases in allele frequency influence expected allele distributions for the next generation  genetic drift drives a population towards genetic uniformity over time  When an allele reaches a frequency of 1  100%  it is said to be \"fixed\" in the population and when an allele reaches a frequency of 0  0%  it is lost  Smaller populations achieve fixation faster  whereas in the limit of an infinite population  fixation is not achieved  Once an allele becomes fixed  genetic drift comes to a halt  and the allele frequency cannot change unless a new allele is introduced in the population via mutation or gene flow  Thus even while genetic drift is a random  directionless process  it acts to eliminate genetic variation over time  19     Assuming genetic drift is the only evolutionary force acting on an allele  after t generations in many replicated populations  starting with allele frequencies of p and q  the variance in allele frequency across those populations is    Assuming genetic drift is the only evolutionary force acting on an allele  at any given time the probability that an allele will eventually become fixed in the population is simply its frequency in the population at that time  21  For example  if the frequency p for allele A is 75% and the frequency q for allele B is 25%  then given unlimited time the probability A will ultimately become fixed in the population is 75% and the probability that B will become fixed is 25%     The expected number of generations for fixation to occur is proportional to the population size  such that fixation is predicted to occur much more rapidly in smaller populations  22  Normally the effective population size  which is smaller than the total population  is used to determine these probabilities  The effective population  Ne  takes into account factors such as the level of inbreeding  the stage of the lifecycle in which the population is the smallest  and the fact that some neutral genes are genetically linked to others that are under selection  13  The effective population size may not be the same for every gene in the same population  23     One forward-looking formula used for approximating the expected time before a neutral allele becomes fixed through genetic drift  according to the WrightFisher model  is    where T is the number of generations  Ne is the effective population size  and p is the initial frequency for the given allele  The result is the number of generations expected to pass before fixation occurs for a given allele in a population with given size  Ne  and allele frequency  p   24     The expected time for the neutral allele to be lost through genetic drift can be calculated as 10     When a mutation appears only once in a population large enough for the initial frequency to be negligible  the formulas can be simplified to 25     for average number of generations expected before fixation of a neutral mutation  and    for the average number of generations expected before the loss of a neutral mutation  26     The formulae above apply to an allele that is already present in a population  and which is subject to neither mutation nor natural selection  If an allele is lost by mutation much more often than it is gained by mutation  then mutation  as well as drift  may influence the time to loss  If the allele prone to mutational loss begins as fixed in the population  and is lost by mutation at rate m per replication  then the expected time in generations until its loss in a haploid population is given by    where                                      displaystyle   gamma     is Euler  s constant  27  The first approximation represents the waiting time until the first mutant destined for loss  with loss then occurring relatively rapidly by genetic drift  taking time Ne xa0 xa01/m  The second approximation represents the time needed for deterministic loss by mutation accumulation  In both cases  the time to fixation is dominated by mutation via the term 1/m  and is less affected by the effective population size     In natural populations  genetic drift and natural selection do not act in isolation  both phenomena are always at play  together with mutation and migration  Neutral evolution is the product of both mutation and drift  not of drift alone  Similarly  even when selection overwhelms genetic drift  it can only act on variation that mutation provides     While natural selection has a direction  guiding evolution towards heritable adaptations to the current environment  genetic drift has no direction and is guided only by the mathematics of chance  28  As a result  drift acts upon the genotypic frequencies within a population without regard to their phenotypic effects  In contrast  selection favors the spread of alleles whose phenotypic effects increase survival and/or reproduction of their carriers  lowers the frequencies of alleles that cause unfavorable traits  and ignores those that are neutral  29     The law of large numbers predicts that when the absolute number of copies of the allele is small  e g   in small populations   the magnitude of drift on allele frequencies per generation is larger  The magnitude of drift is large enough to overwhelm selection at any allele frequency when the selection coefficient is less than 1 divided by the effective population size  Non-adaptive evolution resulting from the product of mutation and genetic drift is therefore considered to be a consequential mechanism of evolutionary change primarily within small  isolated populations  30  The mathematics of genetic drift depend on the effective population size  but it is not clear how this is related to the actual number of individuals in a population  16  Genetic linkage to other genes that are under selection can reduce the effective population size experienced by a neutral allele  With a higher recombination rate  linkage decreases and with it this local effect on effective population size  31  32  This effect is visible in molecular data as a correlation between local recombination rate and genetic diversity  33  and negative correlation between gene density and diversity at noncoding DNA regions  34  Stochasticity associated with linkage to other genes that are under selection is not the same as sampling error  and is sometimes known as genetic draft in order to distinguish it from genetic drift  16     When the allele frequency is very small  drift can also overpower selection even in large populations  For example  while disadvantageous mutations are usually eliminated quickly in large populations  new advantageous mutations are almost as vulnerable to loss through genetic drift as are neutral mutations  Not until the allele frequency for the advantageous mutation reaches a certain threshold will genetic drift have no effect  29     A population bottleneck is when a population contracts to a significantly smaller size over a short period of time due to some random environmental event  In a true population bottleneck  the odds for survival of any member of the population are purely random  and are not improved by any particular inherent genetic advantage  The bottleneck can result in radical changes in allele frequencies  completely independent of selection  35     The impact of a population bottleneck can be sustained  even when the bottleneck is caused by a one-time event such as a natural catastrophe  nAn interesting example of a bottleneck causing unusual genetic distribution is the relatively high proportion of individuals with total rod cell color blindness  achromatopsia  on Pingelap atoll in Micronesia  After a bottleneck  inbreeding increases  This increases the damage done by recessive deleterious mutations  in a process known as inbreeding depression  The worst of these mutations are selected against  leading to the loss of other alleles that are genetically linked to them  in a process of background selection  2  For recessive harmful mutations  this selection can be enhanced as a consequence of the bottleneck  due to genetic purging  This leads to a further loss of genetic diversity  In addition  a sustained reduction in population size increases the likelihood of further allele fluctuations from drift in generations to come     A population  s genetic variation can be greatly reduced by a bottleneck  and even beneficial adaptations may be permanently eliminated  36  The loss of variation leaves the surviving population vulnerable to any new selection pressures such as disease  climatic change or shift in the available food source  because adapting in response to environmental changes requires sufficient genetic variation in the population for natural selection to take place  37  38     There have been many known cases of population bottleneck in the recent past  Prior to the arrival of Europeans  North American prairies were habitat for millions of greater prairie chickens  In Illinois alone  their numbers plummeted from about 100 million birds in 1900 to about 50 birds in the 1990s  The declines in population resulted from hunting and habitat destruction  but a consequence has been a loss of most of the species   genetic diversity  DNA analysis comparing birds from the mid century to birds in the 1990s documents a steep decline in the genetic variation in just the latter few decades  Currently the greater prairie chicken is experiencing low reproductive success  39     However  the genetic loss caused by bottleneck and genetic drift can increase fitness  as in Ehrlichia  40     Over-hunting also caused a severe population bottleneck in the northern elephant seal in the 19th century  Their resulting decline in genetic variation can be deduced by comparing it to that of the southern elephant seal  which were not so aggressively hunted  41     The founder effect is a special case of a population bottleneck  occurring when a small group in a population splinters off from the original population and forms a new one  The random sample of alleles in the just formed new colony is expected to grossly misrepresent the original population in at least some respects  42  It is even possible that the number of alleles for some genes in the original population is larger than the number of gene copies in the founders  making complete representation impossible  When a newly formed colony is small  its founders can strongly affect the population  s genetic make-up far into the future     A well-documented example is found in the Amish migration to Pennsylvania in 1744  Two members of the new colony shared the recessive allele for EllisVan Creveld syndrome  Members of the colony and their descendants tend to be religious isolates and remain relatively insular  As a result of many generations of inbreeding  EllisVan Creveld syndrome is now much more prevalent among the Amish than in the general population  29  43     The difference in gene frequencies between the original population and colony may also trigger the two groups to diverge significantly over the course of many generations  As the difference  or genetic distance  increases  the two separated populations may become distinct  both genetically and phenetically  although not only genetic drift but also natural selection  gene flow  and mutation contribute to this divergence  This potential for relatively rapid changes in the colony  s gene frequency led most scientists to consider the founder effect  and by extension  genetic drift  a significant driving force in the evolution of new species  Sewall Wright was the first to attach this significance to random drift and small  newly isolated populations with his shifting balance theory of speciation  44  Following after Wright  Ernst Mayr created many persuasive models to show that the decline in genetic variation and small population size following the founder effect were critically important for new species to develop  45  However  there is much less support for this view today since the hypothesis has been tested repeatedly through experimental research and the results have been equivocal at best  46     The role of random chance in evolution was first outlined by Arend L  Hagedoorn and A  C  Hagedoorn-Vorstheuvel La Brand in 1921  47  They highlighted that random survival plays a key role in the loss of variation from populations  Fisher  1922  responded to this with the first  albeit marginally incorrect  mathematical treatment of the   Hagedoorn effect    48  Notably  he expected that many natural populations were too large  an N ~10 000  for the effects of drift to be substantial and thought drift would have an insignificant effect on the evolutionary process  The corrected mathematical treatment and term \"genetic drift\" was later coined by a founder of population genetics  Sewall Wright  His first use of the term \"drift\" was in 1929  49  though at the time he was using it in the sense of a directed process of change  or natural selection  Random drift by means of sampling error came to be known as the \"SewallWright effect \" though he was never entirely comfortable to see his name given to it  Wright referred to all changes in allele frequency as either \"steady drift\"  e g   selection  or \"random drift\"  e g   sampling error   50  \"Drift\" came to be adopted as a technical term in the stochastic sense exclusively  51  Today it is usually defined still more narrowly  in terms of sampling error  52  although this narrow definition is not universal  53  54  Wright wrote that the \"restriction of \"random drift\" or even \"drift\" to only one component  the effects of accidents of sampling  tends to lead to confusion \" 50  Sewall Wright considered the process of random genetic drift by means of sampling error equivalent to that by means of inbreeding  but later work has shown them to be distinct  55     In the early days of the modern evolutionary synthesis  scientists were beginning to blend the new science of population genetics with Charles Darwin  s theory of natural selection  Within this framework  Wright focused on the effects of inbreeding on small relatively isolated populations  He introduced the concept of an adaptive landscape in which phenomena such as cross breeding and genetic drift in small populations could push them away from adaptive peaks  which in turn allow natural selection to push them towards new adaptive peaks  56  Wright thought smaller populations were more suited for natural selection because \"inbreeding was sufficiently intense to create new interaction systems through random drift but not intense enough to cause random nonadaptive fixation of genes \" 57     Wright  s views on the role of genetic drift in the evolutionary scheme were controversial almost from the very beginning  One of the most vociferous and influential critics was colleague Ronald Fisher  Fisher conceded genetic drift played some role in evolution  but an insignificant one  Fisher has been accused of misunderstanding Wright  s views because in his criticisms Fisher seemed to argue Wright had rejected selection almost entirely  To Fisher  viewing the process of evolution as a long  steady  adaptive progression was the only way to explain the ever-increasing complexity from simpler forms  But the debates have continued between the \"gradualists\" and those who lean more toward the Wright model of evolution where selection and drift together play an important role  58     In 1968  Motoo Kimura rekindled the debate with his neutral theory of molecular evolution  which claims that most of the genetic changes are caused by genetic drift acting on neutral mutations  6  7     The role of genetic drift by means of sampling error in evolution has been criticized by John H  Gillespie 59  and William B  Provine  who argue that selection on linked sites is a more important stochastic force         Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur  or how likely it is that a proposition is true   The probability of an event is a number between 0 and 1  where  roughly speaking  0 indicates impossibility of the event and 1 indicates certainty  note 1  1  2  The higher the probability of an event  the more likely it is that the event will occur  A simple example is the tossing of a fair  unbiased  coin  Since the coin is fair  the two outcomes  \"heads\" and \"tails\"  are both equally probable  the probability of \"heads\" equals the probability of \"tails\"  and since no other outcomes are possible  the probability of either \"heads\" or \"tails\" is 1/2  which could also be written as 0 5 or 50%      These concepts have been given an axiomatic mathematical formalization in probability theory  which is used widely in areas of study such as statistics  mathematics  science  finance  gambling  artificial intelligence  machine learning  computer science  game theory  and philosophy to  for example  draw inferences about the expected frequency of events  Probability theory is also used to describe the underlying mechanics and regularities of complex systems  3     Experiment  An operation which can produce some well-defined outcomes  is called an Experiment     Example  When we toss a coin  we know that either head or tail shows up  So  the operation of tossing a coin may be said to have two well-defined outcomes  namely   a  heads showing up  and  b  tails showing up     Random Experiment  When we roll a die we are well aware of the fact that any of the numerals 1 2 3 4 5  or 6 may appear on the upper face but we cannot say that which exact number will show up     Such an experiment in which all possible outcomes are known and the exact outcome cannot be predicted in advance  is called a Random Experiment     Sample Space  All the possible outcomes of an experiment as an whole  form the Sample Space     Example  When we roll a die we can get any outcome from 1 to 6  All the possible numbers which can appear on the upper face form the Sample Space denoted by S   Hence  the Sample Space of a dice roll is S= 1 2 3 4 5 6     Outcome   Any possible result out of the Sample Space S for a Random Experiment is called an Outcome     Example  When we roll a die  we might obtain 3 or when we toss a coin  we might obtain heads     Event  Any subset of the Sample Space S is called an Event  denoted by E   When an outcome which belongs to the subset E takes place  it is said that an Event has occurred  Whereas  when an outcome which does not belong to subset E takes place  the Event has not occurred    \"Example  Consider the experiment of throwing a die  Over here the Sample Space S= 1 2 3 4 5 6   Let E denote the event of  a number appearing less than 4   Thus the Event E= 1 2 3   If the number 1 appears  we say that Event E has occurred  Similarly  if the outcomes are 2 or 3  we can say Event E has occurred since these outcomes belong to subset E \"   Trial  By a trial  we mean performing a random experiment     Example   i  Tossing a fair coin   ii  rolling an unbiased die 4      When dealing with experiments that are random and well-defined in a purely theoretical setting  like tossing a fair coin   probabilities can be numerically described by the number of desired outcomes  divided by the total number of all outcomes  For example  tossing a fair coin twice will yield \"head-head\"  \"head-tail\"  \"tail-head\"  and \"tail-tail\" outcomes  The probability of getting an outcome of \"head-head\" is 1 out of 4 outcomes  or  in numerical terms  1/4  0 25 or 25%  However  when it comes to practical application  there are two major competing categories of probability interpretations  whose adherents hold different views about the fundamental nature of probability     The word probability derives from the Latin probabilitas  which can also mean \"probity\"  a measure of the authority of a witness in a legal case in Europe  and often correlated with the witness  s nobility  In a sense  this differs much from the modern meaning of probability  which in contrast is a measure of the weight of empirical evidence  and is arrived at from inductive reasoning and statistical inference  10     The scientific study of probability is a modern development of mathematics  Gambling shows that there has been an interest in quantifying the ideas of probability for millennia  but exact mathematical descriptions arose much later  There are reasons for the slow development of the mathematics of probability  Whereas games of chance provided the impetus for the mathematical study of probability  fundamental issues  note 2  are still obscured by the superstitions of gamblers  11     According to Richard Jeffrey  \"Before the middle of the seventeenth century  the term   probable    Latin probabilis  meant approvable  and was applied in that sense  univocally  to opinion and to action  A probable action or opinion was one such as sensible people would undertake or hold  in the circumstances \" 12  However  in legal contexts especially    probable   could also apply to propositions for which there was good evidence  13     The sixteenth-century Italian polymath Gerolamo Cardano demonstrated the efficacy of defining odds as the ratio of favourable to unfavourable outcomes  which implies that the probability of an event is given by the ratio of favourable outcomes to the total number of possible outcomes 14    nAside from the elementary work by Cardano  the doctrine of probabilities dates to the correspondence of Pierre de Fermat and Blaise Pascal  1654   Christiaan Huygens  1657  gave the earliest known scientific treatment of the subject  15  Jakob Bernoulli  s Ars Conjectandi  posthumous  1713  and Abraham de Moivre  s Doctrine of Chances  1718  treated the subject as a branch of mathematics  16  See Ian Hacking  s The Emergence of Probability 10  and James Franklin  s The Science of Conjecture 17  for histories of the early development of the very concept of mathematical probability     The theory of errors may be traced back to Roger Cotes  s Opera Miscellanea  posthumous  1722   but a memoir prepared by Thomas Simpson in 1755  printed 1756  first applied the theory to the discussion of errors of observation  18  The reprint  1757  of this memoir lays down the axioms that positive and negative errors are equally probable  and that certain assignable limits define the range of all errors  Simpson also discusses continuous errors and describes a probability curve     The first two laws of error that were proposed both originated with Pierre-Simon Laplace  The first law was published in 1774  and stated that the frequency of an error could be expressed as an exponential function of the numerical magnitude of the errordisregarding sign  The second law of error was proposed in 1778 by Laplace  and stated that the frequency of the error is an exponential function of the square of the error  19  The second law of error is called the normal distribution or the Gauss law  \"It is difficult historically to attribute that law to Gauss  who in spite of his well-known precocity had probably not made this discovery before he was two years old \" 19     Daniel Bernoulli  1778  introduced the principle of the maximum product of the probabilities of a system of concurrent errors     Adrien-Marie Legendre  1805  developed the method of least squares  and introduced it in his Nouvelles mthodes pour la dtermination des orbites des comtes  New Methods for Determining the Orbits of Comets   20  In ignorance of Legendre  s contribution  an Irish-American writer  Robert Adrain  editor of \"The Analyst\"  1808   first deduced the law of facility of error     where                     h                 displaystyle h    is a constant depending on precision of observation  and                     c                 displaystyle c    is a scale factor ensuring that the area under the curve equals 1  He gave two proofs  the second being essentially the same as John Herschel  s  1850   citation needed  Gauss gave the first proof that seems to have been known in Europe  the third after Adrain  s  in 1809  Further proofs were given by Laplace  1810  1812   Gauss  1823   James Ivory  1825  1826   Hagen  1837   Friedrich Bessel  1838   W F  Donkin  1844  1856   and Morgan Crofton  1870   Other contributors were Ellis  1844   De Morgan  1864   Glaisher  1872   and Giovanni Schiaparelli  1875   Peters  s  1856  formula clarification needed  for r  the probable error of a single observation  is well known     In the nineteenth century  authors on the general theory included Laplace  Sylvestre Lacroix  1816   Littrow  1833   Adolphe Quetelet  1853   Richard Dedekind  1860   Helmert  1872   Hermann Laurent  1873   Liagre  Didion and Karl Pearson  Augustus De Morgan and George Boole improved the exposition of the theory     In 1906  Andrey Markov introduced 21  the notion of Markov chains  which played an important role in stochastic processes theory and its applications  The modern theory of probability based on the measure theory was developed by Andrey Kolmogorov in 1931  22     On the geometric side  contributors to The Educational Times were influential  Miller  Crofton  McColl  Wolstenholme  Watson  and Artemas Martin   23  See integral geometry for more info     Like other theories  the theory of probability is a representation of its concepts in formal termsthat is  in terms that can be considered separately from their meaning  These formal terms are manipulated by the rules of mathematics and logic  and any results are interpreted or translated back into the problem domain     There have been at least two successful attempts to formalize probability  namely the Kolmogorov formulation and the Cox formulation  In Kolmogorov  s formulation  see also probability space   sets are interpreted as events and probability as a measure on a class of sets  In Cox  s theorem  probability is taken as a primitive  i e   not further analyzed   and the emphasis is on constructing a consistent assignment of probability values to propositions  In both cases  the laws of probability are the same  except for technical details     There are other methods for quantifying uncertainty  such as the DempsterShafer theory or possibility theory  but those are essentially different and not compatible with the usually-understood laws of probability     Probability theory is applied in everyday life in risk assessment and modeling  The insurance industry and markets use actuarial science to determine pricing and make trading decisions  Governments apply probabilistic methods in environmental regulation  entitlement analysis  and financial regulation     An example of the use of probability theory in equity trading is the effect of the perceived probability of any widespread Middle East conflict on oil prices  which have ripple effects in the economy as a whole  An assessment by a commodity trader that a war is more likely can send that commodity  s prices up or down  and signals other traders of that opinion  Accordingly  the probabilities are neither assessed independently nor necessarily rationally  The theory of behavioral finance emerged to describe the effect of such groupthink on pricing  on policy  and on peace and conflict  24     In addition to financial assessment  probability can be used to analyze trends in biology  e g   disease spread  as well as ecology  e g   biological Punnett squares   As with finance  risk assessment can be used as a statistical tool to calculate the likelihood of undesirable events occurring  and can assist with implementing protocols to avoid encountering such circumstances  Probability is used to design games of chance so that casinos can make a guaranteed profit  yet provide payouts to players that are frequent enough to encourage continued play  25     Another significant application of probability theory in everyday life is reliability  Many consumer products  such as automobiles and consumer electronics  use reliability theory in product design to reduce the probability of failure  Failure probability may influence a manufacturer  s decisions on a product  s warranty  26     The cache language model and other statistical language models that are used in natural language processing are also examples of applications of probability theory      For a list of mathematical logic notation used in this article see Notation in Probability and Statistics and/or List of Logic Symbols      Consider an experiment that can produce a number of results  The collection of all possible results is called the sample space of the experiment  sometimes denoted as                                      displaystyle   Omega      27  The power set of the sample space is formed by considering all different collections of possible results  For example  rolling a die can produce six possible results  One collection of possible results gives an odd number on the die  Thus  the subset  1 3 5  is an element of the power set of the sample space of dice rolls  These collections are called \"events\"  In this case   1 3 5  is the event that the die falls on some odd number  If the results that actually occur fall in a given event  the event is said to have occurred     A probability is a way of assigning every event a value between zero and one  with the requirement that the event made up of all possible results  in our example  the event  1 2 3 4 5 6   is assigned a value of one  To qualify as a probability  the assignment of values must satisfy the requirement that for any collection of mutually exclusive events  events with no common results  such as the events  1 6    3   and  2 4    the probability that at least one of the events will occur is given by the sum of the probabilities of all the individual events  28     The probability of an event A is written as                     P                 A                          displaystyle P A      27  29                      p                 A                          displaystyle p A      or                               Pr                         A                          displaystyle    text Pr   A      30  This mathematical definition of probability can extend to infinite sample spaces  and even uncountable sample spaces  using the concept of a measure     The opposite or complement of an event A is the event  not A   that is  the event of A not occurring   often denoted as                               A                                             A                      c                                   displaystyle A   A^ c      27                                            A                                                         A                                                                 A                 displaystyle    overline  A   A^   complement     neg A     or                                               A                 displaystyle    sim  A     its probability is given by P not A  = 1  P A   31  As an example  the chance of not rolling a six on a six-sided die is 1   chance of rolling a six                      =        1                                                    1              6                                      =                                            5              6                                               displaystyle =1-   tfrac  1  6  =   tfrac  5  6       For a more comprehensive treatment  see Complementary event     If two events A and B occur on a single performance of an experiment  this is called the intersection or joint probability of A and B  denoted as                     P                 A                B                          displaystyle P A  cap B      27     If two events  A and B are independent then the joint probability is 29     For example  if two coins are flipped  then the chance of both being heads is                                                         1              2                                                                                  1              2                                      =                                            1              4                                               displaystyle    tfrac  1  2    times    tfrac  1  2  =   tfrac  1  4       32     If either event A or event B can occur but never both simultaneously  then they are called mutually exclusive events     If two events are mutually exclusive  then the probability of both occurring is denoted as                     P                 A                B                          displaystyle P A  cap B     and    If two events are mutually exclusive  then the probability of either occurring is denoted as                     P                 A                B                          displaystyle P A  cup B     and    For example  the chance of rolling a 1 or 2 on a six-sided die is                     P                 1                               xa0or xa0                          2                 =        P                 1                 +        P                 2                 =                                            1              6                                      +                                            1              6                                      =                                            1              3                                                        displaystyle P 1   mbox  or   2 =P 1 +P 2 =   tfrac  1  6  +   tfrac  1  6  =   tfrac  1  3       n    If the events are not mutually exclusive then    For example  when drawing a single card at random from a regular deck of cards  the chance of getting a heart or a face card  J Q K   or one that is both  is                                                         13              52                                      +                                            12              52                                                                                  3              52                                      =                                            11              26                                               displaystyle    tfrac  13  52  +   tfrac  12  52  -   tfrac  3  52  =   tfrac  11  26       since among the 52 cards of a deck  13 are hearts  12 are face cards  and 3 are both  here the possibilities included in the \"3 that are both\" are included in each of the \"13 hearts\" and the \"12 face cards\"  but should only be counted once     Conditional probability is the probability of some event A  given the occurrence of some other event B  nConditional probability is written                     P                 A                B                          displaystyle P A  mid B      27  and is read \"the probability of A  given B\"  It is defined by 33     If                     P                 B                 =        0                 displaystyle P B =0    then                     P                 A                B                          displaystyle P A  mid B     is formally undefined by this expression  In this case                     A                 displaystyle A    and                     B                 displaystyle B    are independent  since                     P                 A                B                 =        P                 A                 P                 B                 =        0                 displaystyle P A  cap B =P A P B =0     However  it is possible to define a conditional probability for some zero-probability events using a -algebra of such events  such as those arising from a continuous random variable   citation needed     For example  in a bag of 2 red balls and 2 blue balls  4 balls in total   the probability of taking a red ball is                     1                  /                2                 displaystyle 1/2     however  when taking a second ball  the probability of it being either a red ball or a blue ball depends on the ball previously taken  For example  if a red ball was taken  then the probability of picking a red ball again would be                     1                  /                3                 displaystyle 1/3     since only 1 red and 2 blue balls would have been remaining  And if a blue ball was taken previously  the probability of taking a red ball will be                     2                  /                3                 displaystyle 2/3        In probability theory and applications  Bayes   rule relates the odds of event                               A                      1                                   displaystyle A_ 1     to event                               A                      2                                   displaystyle A_ 2      before  prior to  and after  posterior to  conditioning on another event                     B                 displaystyle B     The odds on                               A                      1                                   displaystyle A_ 1     to event                               A                      2                                   displaystyle A_ 2     is simply the ratio of the probabilities of the two events  When arbitrarily many events                     A                 displaystyle A    are of interest  not just two  the rule can be rephrased as posterior is proportional to prior times likelihood                      P                 A                  |                B                         P                 A                 P                 B                  |                A                          displaystyle P A|B   propto P A P B|A     where the proportionality symbol means that the left hand side is proportional to  i e   equals a constant times  the right hand side as                     A                 displaystyle A    varies  for fixed or given                     B                 displaystyle B     Lee  2012  Bertsch McGrayne  2012   In this form it goes back to Laplace  1774  and to Cournot  1843   see Fienberg  2005   See Inverse probability and Bayes   rule     In a deterministic universe  based on Newtonian concepts  there would be no probability if all conditions were known  Laplace  s demon    but there are situations in which sensitivity to initial conditions exceeds our ability to measure them  i e  know them    In the case of a roulette wheel  if the force of the hand and the period of that force are known  the number on which the ball will stop would be a certainty  though as a practical matter  this would likely be true only of a roulette wheel that had not been exactly levelled  as Thomas A  Bass   Newtonian Casino revealed    This also assumes knowledge of inertia and friction of the wheel  weight  smoothness and roundness of the ball  variations in hand speed during the turning and so forth  A probabilistic description can thus be more useful than Newtonian mechanics for analyzing the pattern of outcomes of repeated rolls of a roulette wheel  Physicists face the same situation in kinetic theory of gases  where the system  while deterministic in principle  is so complex  with the number of molecules typically the order of magnitude of the Avogadro constant 6 021023  that only a statistical description of its properties is feasible     Probability theory is required to describe quantum phenomena  34  A revolutionary discovery of early 20th century physics was the random character of all physical processes that occur at sub-atomic scales and are governed by the laws of quantum mechanics  The objective wave function evolves deterministically but  according to the Copenhagen interpretation  it deals with probabilities of observing  the outcome being explained by a wave function collapse when an observation is made  However  the loss of determinism for the sake of instrumentalism did not meet with universal approval  Albert Einstein famously remarked in a letter to Max Born  \"I am convinced that God does not play dice\"  35  Like Einstein  Erwin Schrdinger  who discovered the wave function  believed quantum mechanics is a statistical approximation of an underlying deterministic reality  36  In some modern interpretations of the statistical mechanics of measurement  quantum decoherence is invoked to account for the appearance of subjectively probabilistic experimental outcomes         Ecology  from Greek    \"house\" and -  \"study of\"  A  is the study of the relationships between living organisms  including humans  and their physical environment  1    2   Ecology considers organisms at the individual  population  community  ecosystems  and biosphere level  Ecology overlaps with the closely related sciences of biogeography  evolutionary biology  genetics  ethology and  natural history  Ecology is a branch of biology  and it is not synonymous with environmentalism     Among other things  ecology is the study of     Ecology has practical applications in conservation biology  wetland management  natural resource management  agroecology  agriculture  forestry  agroforestry  fisheries   city planning  urban ecology   community health  economics  basic and applied science  and human social interaction  human ecology       The word \"ecology\"  \"kologie\"  was coined in 1866 by the German scientist Ernst Haeckel  and it became a rigorous science in the late 19th century  Evolutionary concepts relating to adaptation and natural selection are cornerstones of modern ecological theory     Ecosystems are dynamically interacting systems of organisms  the communities they make up  and the non-living components of their environment  Ecosystem processes  such as primary production  nutrient cycling  and niche construction  regulate the flux of energy and matter through an environment  Ecosystems have biophysical feedback mechanisms that moderate processes acting on living  biotic  and non-living  abiotic  components of the planet  Ecosystems sustain life-supporting functions and provide ecosystem services like biomass production  food  fuel  fiber  and medicine   the regulation of climate  global biogeochemical cycles  water filtration  soil formation  erosion control  flood protection  and many other natural features of scientific  historical  economic  or intrinsic value     The scope of ecology contains a wide array of interacting levels of organization spanning micro-level  e g   cells  to a planetary scale  e g   biosphere  phenomena  Ecosystems  for example  contain abiotic resources and interacting life forms  i e   individual organisms that aggregate into populations which aggregate into distinct ecological communities   Ecosystems are dynamic  they do not always follow a linear successional path  but they are always changing  sometimes rapidly and sometimes so slowly that it can take thousands of years for ecological processes to bring about certain successional stages of a forest  An ecosystem  s area can vary greatly  from tiny to vast  A single tree is of little consequence to the classification of a forest ecosystem  but critically relevant to organisms living in and on it  1  Several generations of an aphid population can exist over the lifespan of a single leaf  Each of those aphids  in turn  support diverse bacterial communities  2  The nature of connections in ecological communities cannot be explained by knowing the details of each species in isolation  because the emergent pattern is neither revealed nor predicted until the ecosystem is studied as an integrated whole  3  Some ecological principles  however  do exhibit collective properties where the sum of the components explain the properties of the whole  such as birth rates of a population being equal to the sum of individual births over a designated time frame  4     The main subdisciplines of ecology  population  or community  ecology and ecosystem ecology  exhibit a difference not only of scale but also of two contrasting paradigms in the field  The former focuses on organisms   distribution and abundance  while the latter focuses on materials and energy fluxes  5     O  Neill et al   1986  6  76    The scale of ecological dynamics can operate like a closed system  such as aphids migrating on a single tree  while at the same time remain open with regard to broader scale influences  such as atmosphere or climate  Hence  ecologists classify ecosystems hierarchically by analyzing data collected from finer scale units  such as vegetation associations  climate  and soil types  and integrate this information to identify emergent patterns of uniform organization and processes that operate on local to regional  landscape  and chronological scales     To structure the study of ecology into a conceptually manageable framework  the biological world is organized into a nested hierarchy  ranging in scale from genes  to cells  to tissues  to organs  to organisms  to species  to populations  to communities  to ecosystems  to biomes  and up to the level of the biosphere  7  This framework forms a panarchy 8  and exhibits non-linear behaviors  this means that \"effect and cause are disproportionate  so that small changes to critical variables  such as the number of nitrogen fixers  can lead to disproportionate  perhaps irreversible  changes in the system properties \" 9  14    Noss & Carpenter  1994  10  5    Biodiversity  an abbreviation of \"biological diversity\"  describes the diversity of life from genes to ecosystems and spans every level of biological organization  The term has several interpretations  and there are many ways to index  measure  characterize  and represent its complex organization  11  12  13  Biodiversity includes species diversity  ecosystem diversity  and genetic diversity and scientists are interested in the way that this diversity affects the complex ecological processes operating at and among these respective levels  12  14  15  Biodiversity plays an important role in ecosystem services which by definition maintain and improve human quality of life  13  16  17  Conservation priorities and management techniques require different approaches and considerations to address the full ecological scope of biodiversity  Natural capital that supports populations is critical for maintaining ecosystem services 18  19  and species migration  e g   riverine fish runs and avian insect control  has been implicated as one mechanism by which those service losses are experienced  20  An understanding of biodiversity has practical applications for species and ecosystem-level conservation planners as they make management recommendations to consulting firms  governments  and industry  21     The habitat of a species describes the environment over which a species is known to occur and the type of community that is formed as a result  23  More specifically  \"habitats can be defined as regions in environmental space that are composed of multiple dimensions  each representing a biotic or abiotic environmental variable  that is  any component or characteristic of the environment related directly  e g  forage biomass and quality  or indirectly  e g  elevation  to the use of a location by the animal \" 24  745 For example  a habitat might be an aquatic or terrestrial environment that can be further categorized as a montane or alpine ecosystem  Habitat shifts provide important evidence of competition in nature where one population changes relative to the habitats that most other individuals of the species occupy  For example  one population of a species of tropical lizard  Tropidurus hispidus  has a flattened body relative to the main populations that live in open savanna  The population that lives in an isolated rock outcrop hides in crevasses where its flattened body offers a selective advantage  Habitat shifts also occur in the developmental life history of amphibians  and in insects that transition from aquatic to terrestrial habitats  Biotope and habitat are sometimes used interchangeably  but the former applies to a community  s environment  whereas the latter applies to a species   environment  23  25  26     Definitions of the niche date back to 1917  29  but G  Evelyn Hutchinson made conceptual advances in 1957 30  31  by introducing a widely adopted definition   \"the set of biotic and abiotic conditions in which a species is able to persist and maintain stable population sizes \" 29  519 The ecological niche is a central concept in the ecology of organisms and is sub-divided into the fundamental and the realized niche  The fundamental niche is the set of environmental conditions under which a species is able to persist  The realized niche is the set of environmental plus ecological conditions under which a species persists  29  31  32  The Hutchinsonian niche is defined more technically as a \"Euclidean hyperspace whose dimensions are defined as environmental variables and whose size is a function of the number of values that the environmental values may assume for which an organism has positive fitness \" 33  71    Biogeographical patterns and range distributions are explained or predicted through knowledge of a species   traits and niche requirements  34  Species have functional traits that are uniquely adapted to the ecological niche  A trait is a measurable property  phenotype  or characteristic of an organism that may influence its survival  Genes play an important role in the interplay of development and environmental expression of traits  35  Resident species evolve traits that are fitted to the selection pressures of their local environment  This tends to afford them a competitive advantage and discourages similarly adapted species from having an overlapping geographic range  The competitive exclusion principle states that two species cannot coexist indefinitely by living off the same limiting resource  one will always out-compete the other  When similarly adapted species overlap geographically  closer inspection reveals subtle ecological differences in their habitat or dietary requirements  36  Some models and empirical studies  however  suggest that disturbances can stabilize the co-evolution and shared niche occupancy of similar species inhabiting species-rich communities  37  The habitat plus the niche is called the ecotope  which is defined as the full range of environmental and biological variables affecting an entire species  23     Organisms are subject to environmental pressures  but they also modify their habitats  The regulatory feedback between organisms and their environment can affect conditions from local  e g   a beaver pond  to global scales  over time and even after death  such as decaying logs or silica skeleton deposits from marine organisms  38  The process and concept of ecosystem engineering are related to niche construction  but the former relates only to the physical modifications of the habitat whereas the latter also considers the evolutionary implications of physical changes to the environment and the feedback this causes on the process of natural selection  Ecosystem engineers are defined as  \"organisms that directly or indirectly modulate the availability of resources to other species  by causing physical state changes in biotic or abiotic materials  In so doing they modify  maintain and create habitats \" 39  373    The ecosystem engineering concept has stimulated a new appreciation for the influence that organisms have on the ecosystem and evolutionary process  The term \"niche construction\" is more often used in reference to the under-appreciated feedback mechanisms of natural selection imparting forces on the abiotic niche  27  40  An example of natural selection through ecosystem engineering occurs in the nests of social insects  including ants  bees  wasps  and termites  There is an emergent homeostasis or homeorhesis in the structure of the nest that regulates  maintains and defends the physiology of the entire colony  Termite mounds  for example  maintain a constant internal temperature through the design of air-conditioning chimneys  The structure of the nests themselves is subject to the forces of natural selection  Moreover  a nest can survive over successive generations  so that progeny inherit both genetic material and a legacy niche that was constructed before their time  4  27  28     Biomes are larger units of organization that categorize regions of the Earth  s ecosystems  mainly according to the structure and composition of vegetation  41  There are different methods to define the continental boundaries of biomes dominated by different functional types of vegetative communities that are limited in distribution by climate  precipitation  weather and other environmental variables  Biomes include tropical rainforest  temperate broadleaf and mixed forest  temperate deciduous forest  taiga  tundra  hot desert  and polar desert  42  Other researchers have recently categorized other biomes  such as the human and oceanic microbiomes  To a microbe  the human body is a habitat and a landscape  43  Microbiomes were discovered largely through advances in molecular genetics  which have revealed a hidden richness of microbial diversity on the planet  The oceanic microbiome plays a significant role in the ecological biogeochemistry of the planet  s oceans  44     The largest scale of ecological organization is the biosphere  the total sum of ecosystems on the planet  Ecological relationships regulate the flux of energy  nutrients  and climate all the way up to the planetary scale  For example  the dynamic history of the planetary atmosphere  s CO2 and O2 composition has been affected by the biogenic flux of gases coming from respiration and photosynthesis  with levels fluctuating over time in relation to the ecology and evolution of plants and animals  45  Ecological theory has also been used to explain self-emergent regulatory phenomena at the planetary scale  for example  the Gaia hypothesis is an example of holism applied in ecological theory  46  The Gaia hypothesis states that there is an emergent feedback loop generated by the metabolism of living organisms that maintains the core temperature of the Earth and atmospheric conditions within a narrow self-regulating range of tolerance  47     Population ecology studies the dynamics of species populations and how these populations interact with the wider environment  4  A population consists of individuals of the same species that live  interact  and migrate through the same niche and habitat  48     A primary law of population ecology is the Malthusian growth model 49  which states  \"a population will grow  or decline  exponentially as long as the environment experienced by all individuals in the population remains constant \" 49  18 Simplified population models usually start with four variables  death  birth  immigration  and emigration     An example of an introductory population model describes a closed population  such as on an island  where immigration and emigration does not take place  Hypotheses are evaluated with reference to a null hypothesis which states that random processes create the observed data  In these island models  the rate of population change is described by     where N is the total number of individuals in the population  b and d are the per capita rates of birth and death respectively  and r is the per capita rate of population change  49  50     Using these modeling techniques  Malthus   population principle of growth was later transformed into a model known as the logistic equation by Pierre Verhulst     where N t  is the number of individuals measured as biomass density as a function of time  t  r is the maximum per-capita rate of change commonly known as the intrinsic rate of growth  and                                      displaystyle   alpha     is the crowding coefficient  which represents the reduction in population growth rate per individual added  The formula states that the rate of change in population size                                d                N                 t                           /                          d                t                 displaystyle   mathrm  d  N t /  mathrm  d  t     will grow to approach equilibrium  where                                d                N                 t                           /                          d                t        =        0                 displaystyle   mathrm  d  N t /  mathrm  d  t=0      when the rates of increase and crowding are balanced                      r                  /                                 displaystyle r/  alpha      A common  analogous model fixes the equilibrium                      r                  /                                 displaystyle r/  alpha     as K  which is known as the \"carrying capacity \"    Population ecology builds upon these introductory models to further understand demographic processes in real study populations  Commonly used types of data include life history  fecundity  and survivorship  and these are analysed using mathematical techniques such as matrix algebra  The information is used for managing wildlife stocks and setting harvest quotas  50  51  In cases where basic models are insufficient  ecologists may adopt different kinds of statistical methods  such as the Akaike information criterion  52  or use models that can become mathematically complex as \"several competing hypotheses are simultaneously confronted with the data \" 53     The concept of metapopulations was defined in 1969 54  as \"a population of populations which go extinct locally and recolonize\"  55  105 Metapopulation ecology is another statistical approach that is often used in conservation research  56  Metapopulation models simplify the landscape into patches of varying levels of quality  57  and metapopulations are linked by the migratory behaviours of organisms  Animal migration is set apart from other kinds of movement because it involves the seasonal departure and return of individuals from a habitat  58  Migration is also a population-level phenomenon  as with the migration routes followed by plants as they occupied northern post-glacial environments  Plant ecologists use pollen records that accumulate and stratify in wetlands to reconstruct the timing of plant migration and dispersal relative to historic and contemporary climates  These migration routes involved an expansion of the range as plant populations expanded from one area to another  There is a larger taxonomy of movement  such as commuting  foraging  territorial behaviour  stasis  and ranging  Dispersal is usually distinguished from migration because it involves the one-way permanent movement of individuals from their birth population into another population  59  60     In metapopulation terminology  migrating individuals are classed as emigrants  when they leave a region  or immigrants  when they enter a region   and sites are classed either as sources or sinks  A site is a generic term that refers to places where ecologists sample populations  such as ponds or defined sampling areas in a forest  Source patches are productive sites that generate a seasonal supply of juveniles that migrate to other patch locations  Sink patches are unproductive sites that only receive migrants  the population at the site will disappear unless rescued by an adjacent source patch or environmental conditions become more favourable  Metapopulation models examine patch dynamics over time to answer potential questions about spatial and demographic ecology  The ecology of metapopulations is a dynamic process of extinction and colonization  Small patches of lower quality  i e   sinks  are maintained or rescued by a seasonal influx of new immigrants  A dynamic metapopulation structure evolves from year to year  where some patches are sinks in dry years and are sources when conditions are more favourable  Ecologists use a mixture of computer models and field studies to explain metapopulation structure  61  62     Johnson & Stinchcomb  2007  63  250    Community ecology is the study of the interactions among a collection of species that inhabit the same geographic area  Community ecologists study the determinants of patterns and processes for two or more interacting species  Research in community ecology might measure species diversity in grasslands in relation to soil fertility  It might also include the analysis of predator-prey dynamics  competition among similar plant species  or mutualistic interactions between crabs and corals     Tansley  1935  64  299    Ecosystems may be habitats within biomes that form an integrated whole and a dynamically responsive system having both physical and biological complexes  Ecosystem ecology is the science of determining the fluxes of materials  e g  carbon  phosphorus  between different pools  e g   tree biomass  soil organic material   Ecosystem ecologists attempt to determine the underlying causes of these fluxes  Research in ecosystem ecology might measure primary production  g C/m^2  in a wetland in relation to decomposition and consumption rates  g C/m^2/y   This requires an understanding of the community connections between plants  i e   primary producers  and the decomposers  e g   fungi and bacteria   65     The underlying concept of an ecosystem can be traced back to 1864 in the published work of George Perkins Marsh  \"Man and Nature\"   66  67  Within an ecosystem  organisms are linked to the physical and biological components of their environment to which they are adapted  64  Ecosystems are complex adaptive systems where the interaction of life processes form self-organizing patterns across different scales of time and space  68  Ecosystems are broadly categorized as terrestrial  freshwater  atmospheric  or marine  Differences stem from the nature of the unique physical environments that shapes the biodiversity within each  A more recent addition to ecosystem ecology are technoecosystems  which are affected by or primarily the result of human activity  4     A food web is the archetypal ecological network  Plants capture solar energy and use it to synthesize simple sugars during photosynthesis  As plants grow  they accumulate nutrients and are eaten by grazing herbivores  and the energy is transferred through a chain of organisms by consumption  The simplified linear feeding pathways that move from a basal trophic species to a top consumer is called the food chain  The larger interlocking pattern of food chains in an ecological community creates a complex food web  Food webs are a type of concept map or a heuristic device that is used to illustrate and study pathways of energy and material flows  6  69  70     Food webs are often limited relative to the real world  Complete empirical measurements are generally restricted to a specific habitat  such as a cave or a pond  and principles gleaned from food web microcosm studies are extrapolated to larger systems  71  Feeding relations require extensive investigations into the gut contents of organisms  which can be difficult to decipher  or stable isotopes can be used to trace the flow of nutrient diets and energy through a food web  72  Despite these limitations  food webs remain a valuable tool in understanding community ecosystems  73     Food webs exhibit principles of ecological emergence through the nature of trophic relationships  some species have many weak feeding links  e g   omnivores  while some are more specialized with fewer stronger feeding links  e g   primary predators   Theoretical and empirical studies identify non-random emergent patterns of few strong and many weak linkages that explain how ecological communities remain stable over time  74  Food webs are composed of subgroups where members in a community are linked by strong interactions  and the weak interactions occur between these subgroups  This increases food web stability  75  Step by step lines or relations are drawn until a web of life is illustrated  70  76  77  78     A trophic level  from Greek troph    troph  meaning \"food\" or \"feeding\"  is \"a group of organisms acquiring a considerable majority of its energy from the lower adjacent level  according to ecological pyramids  nearer the abiotic source \" 79  383 Links in food webs primarily connect feeding relations or trophism among species  Biodiversity within ecosystems can be organized into trophic pyramids  in which the vertical dimension represents feeding relations that become further removed from the base of the food chain up toward top predators  and the horizontal dimension represents the abundance or biomass at each level  80  When the relative abundance or biomass of each species is sorted into its respective trophic level  they naturally sort into a   pyramid of numbers    81     Species are broadly categorized as autotrophs  or primary producers   heterotrophs  or consumers   and Detritivores  or decomposers   Autotrophs are organisms that produce their own food  production is greater than respiration  by photosynthesis or chemosynthesis  Heterotrophs are organisms that must feed on others for nourishment and energy  respiration exceeds production   4  Heterotrophs can be further sub-divided into different functional groups  including primary consumers  strict herbivores   secondary consumers  carnivorous predators that feed exclusively on herbivores   and tertiary consumers  predators that feed on a mix of herbivores and predators   82  Omnivores do not fit neatly into a functional category because they eat both plant and animal tissues  It has been suggested that omnivores have a greater functional influence as predators because compared to herbivores  they are relatively inefficient at grazing  83     Trophic levels are part of the holistic or complex systems view of ecosystems  84  85  Each trophic level contains unrelated species that are grouped together because they share common ecological functions  giving a macroscopic view of the system  86  While the notion of trophic levels provides insight into energy flow and top-down control within food webs  it is troubled by the prevalence of omnivory in real ecosystems  This has led some ecologists to \"reiterate that the notion that species clearly aggregate into discrete  homogeneous trophic levels is fiction \" 87  815 Nonetheless  recent studies have shown that real trophic levels do exist  but \"above the herbivore trophic level  food webs are better characterized as a tangled web of omnivores \" 88  612    A keystone species is a species that is connected to a disproportionately large number of other species in the food-web  Keystone species have lower levels of biomass in the trophic pyramid relative to the importance of their role  The many connections that a keystone species holds means that it maintains the organization and structure of entire communities  The loss of a keystone species results in a range of dramatic cascading effects that alters trophic dynamics  other food web connections  and can cause the extinction of other species  89  90     Sea otters  Enhydra lutris  are commonly cited as an example of a keystone species because they limit the density of sea urchins that feed on kelp  If sea otters are removed from the system  the urchins graze until the kelp beds disappear  and this has a dramatic effect on community structure  91  Hunting of sea otters  for example  is thought to have led indirectly to the extinction of the Steller  s sea cow  Hydrodamalis gigas   92  While the keystone species concept has been used extensively as a conservation tool  it has been criticized for being poorly defined from an operational stance  It is difficult to experimentally determine what species may hold a keystone role in each ecosystem  Furthermore  food web theory suggests that keystone species may not be common  so it is unclear how generally the keystone species model can be applied  91  93     Complexity is understood as a large computational effort needed to piece together numerous interacting parts exceeding the iterative memory capacity of the human mind  Global patterns of biological diversity are complex  This biocomplexity stems from the interplay among ecological processes that operate and influence patterns at different scales that grade into each other  such as transitional areas or ecotones spanning landscapes  Complexity stems from the interplay among levels of biological organization as energy  and matter is integrated into larger units that superimpose onto the smaller parts  \"What were wholes on one level become parts on a higher one \" 94  209 Small scale patterns do not necessarily explain large scale phenomena  otherwise captured in the expression  coined by Aristotle    the sum is greater than the parts    95  96  E     \"Complexity in ecology is of at least six distinct types  spatial  temporal  structural  process  behavioral  and geometric \" 97  3 From these principles  ecologists have identified emergent and self-organizing phenomena that operate at different environmental scales of influence  ranging from molecular to planetary  and these require different explanations at each integrative level  47  98  Ecological complexity relates to the dynamic resilience of ecosystems that transition to multiple shifting steady-states directed by random fluctuations of history  8  99  Long-term ecological studies provide important track records to better understand the complexity and resilience of ecosystems over longer temporal and broader spatial scales  These studies are managed by the International Long Term Ecological Network  LTER   100  The longest experiment in existence is the Park Grass Experiment  which was initiated in 1856  101  Another example is the Hubbard Brook study  which has been in operation since 1960  102     Holism remains a critical part of the theoretical foundation in contemporary ecological studies  Holism addresses the biological organization of life that self-organizes into layers of emergent whole systems that function according to non-reducible properties  This means that higher-order patterns of a whole functional system  such as an ecosystem  cannot be predicted or understood by a simple summation of the parts  103  \"New properties emerge because the components interact  not because the basic nature of the components is changed \" 4  8    Ecological studies are necessarily holistic as opposed to reductionistic  35  98  104  Holism has three scientific meanings or uses that identify with ecology  1  the mechanistic complexity of ecosystems  2  the practical description of patterns in quantitative reductionist terms where correlations may be identified but nothing is understood about the causal relations without reference to the whole system  which leads to 3  a metaphysical hierarchy whereby the causal relations of larger systems are understood without reference to the smaller parts  Scientific holism differs from mysticism that has appropriated the same term  An example of metaphysical holism is identified in the trend of increased exterior thickness in shells of different species  The reason for a thickness increase can be understood through reference to principles of natural selection via predation without the need to reference or understand the biomolecular properties of the exterior shells  105     Ecology and evolutionary biology are considered sister disciplines of the life sciences  Natural selection  life history  development  adaptation  populations  and inheritance are examples of concepts that thread equally into ecological and evolutionary theory  Morphological  behavioural  and genetic traits  for example  can be mapped onto evolutionary trees to study the historical development of a species in relation to their functions and roles in different ecological circumstances  In this framework  the analytical tools of ecologists and evolutionists overlap as they organize  classify  and investigate life through common systematic principles  such as phylogenetics or the Linnaean system of taxonomy  106  The two disciplines often appear together  such as in the title of the journal Trends in Ecology and Evolution  107  There is no sharp boundary separating ecology from evolution  and they differ more in their areas of applied focus  Both disciplines discover and explain emergent and unique properties and processes operating across different spatial or temporal scales of organization  35  47  While the boundary between ecology and evolution is not always clear  ecologists study the abiotic and biotic factors that influence evolutionary processes  108  109  and evolution can be rapid  occurring on ecological timescales as short as one generation  110     All organisms can exhibit behaviours  Even plants express complex behaviour  including memory and communication  112  Behavioural ecology is the study of an organism  s behaviour in its environment and its ecological and evolutionary implications  Ethology is the study of observable movement or behaviour in animals  This could include investigations of motile sperm of plants  mobile phytoplankton  zooplankton swimming toward the female egg  the cultivation of fungi by weevils  the mating dance of a salamander  or social gatherings of amoeba  113  114  115  116  117     Adaptation is the central unifying concept in behavioural ecology  118  Behaviours can be recorded as traits and inherited in much the same way that eye and hair colour can  Behaviours can evolve by means of natural selection as adaptive traits conferring functional utilities that increases reproductive fitness  119  120     Predator-prey interactions are an introductory concept into food-web studies as well as behavioural ecology  122  Prey species can exhibit different kinds of behavioural adaptations to predators  such as avoid  flee  or defend  Many prey species are faced with multiple predators that differ in the degree of danger posed  To be adapted to their environment and face predatory threats  organisms must balance their energy budgets as they invest in different aspects of their life history  such as growth  feeding  mating  socializing  or modifying their habitat  Hypotheses posited in behavioural ecology are generally based on adaptive principles of conservation  optimization  or efficiency  32  108  123  For example  \" t he threat-sensitive predator avoidance hypothesis predicts that prey should assess the degree of threat posed by different predators and match their behaviour according to current levels of risk\" 124  or \" t he optimal flight initiation distance occurs where expected postencounter fitness is maximized  which depends on the prey  s initial fitness  benefits obtainable by not fleeing  energetic escape costs  and expected fitness loss due to predation risk \" 125     Elaborate sexual displays and posturing are encountered in the behavioural ecology of animals  The birds-of-paradise  for example  sing and display elaborate ornaments during courtship  These displays serve a dual purpose of signalling healthy or well-adapted individuals and desirable genes  The displays are driven by sexual selection as an advertisement of quality of traits among suitors  126     Cognitive ecology integrates theory and observations from evolutionary ecology and neurobiology  primarily cognitive science  in order to understand the effect that animal interaction with their habitat has on their cognitive systems and how those systems restrict behavior within an ecological and evolutionary framework  127  \"Until recently  however  cognitive scientists have not paid sufficient attention to the fundamental fact that cognitive traits evolved under particular natural settings  With consideration of the selection pressure on cognition  cognitive ecology can contribute intellectual coherence to the multidisciplinary study of cognition \" 128  129  As a study involving the   coupling   or interactions between organism and environment  cognitive ecology is closely related to enactivism  127  a field based upon the view that \"   we must see the organism and environment as bound together in reciprocal specification and selection   \"  130     Social-ecological behaviours are notable in the social insects  slime moulds  social spiders  human society  and naked mole-rats where eusocialism has evolved  Social behaviours include reciprocally beneficial behaviours among kin and nest mates 115  120  131  and evolve from kin and group selection  Kin selection explains altruism through genetic relationships  whereby an altruistic behaviour leading to death is rewarded by the survival of genetic copies distributed among surviving relatives  The social insects  including ants  bees  and wasps are most famously studied for this type of relationship because the male drones are clones that share the same genetic make-up as every other male in the colony  120  In contrast  group selectionists find examples of altruism among non-genetic relatives and explain this through selection acting on the group  whereby  it becomes selectively advantageous for groups if their members express altruistic behaviours to one another  Groups with predominantly altruistic members survive better than groups with predominantly selfish members  120  132     Ecological interactions can be classified broadly into a host and an associate relationship  A host is any entity that harbours another that is called the associate  133  Relationships within a species that are mutually or reciprocally beneficial are called mutualisms  Examples of mutualism include fungus-growing ants employing agricultural symbiosis  bacteria living in the guts of insects and other organisms  the fig wasp and yucca moth pollination complex  lichens with fungi and photosynthetic algae  and corals with photosynthetic algae  134  135  If there is a physical connection between host and associate  the relationship is called symbiosis  Approximately 60% of all plants  for example  have a symbiotic relationship with arbuscular mycorrhizal fungi living in their roots forming an exchange network of carbohydrates for mineral nutrients  136     Indirect mutualisms occur where the organisms live apart  For example  trees living in the equatorial regions of the planet supply oxygen into the atmosphere that sustains species living in distant polar regions of the planet  This relationship is called commensalism because many others receive the benefits of clean air at no cost or harm to trees supplying the oxygen  4  137  If the associate benefits while the host suffers  the relationship is called parasitism  Although parasites impose a cost to their host  e g   via damage to their reproductive organs or propagules  denying the services of a beneficial partner   their net effect on host fitness is not necessarily negative and  thus  becomes difficult to forecast  138  139  Co-evolution is also driven by competition among species or among members of the same species under the banner of reciprocal antagonism  such as grasses competing for growth space  The Red Queen Hypothesis  for example  posits that parasites track down and specialize on the locally common genetic defense systems of its host that drives the evolution of sexual reproduction to diversify the genetic constituency of populations responding to the antagonistic pressure  140  141     Biogeography  an amalgamation of biology and geography  is the comparative study of the geographic distribution of organisms and the corresponding evolution of their traits in space and time  142  The Journal of Biogeography was established in 1974  143  Biogeography and ecology share many of their disciplinary roots  For example  the theory of island biogeography  published by the Robert MacArthur and Edward O  Wilson in 1967 144  is considered one of the fundamentals of ecological theory  145     Biogeography has a long history in the natural sciences concerning the spatial distribution of plants and animals  Ecology and evolution provide the explanatory context for biogeographical studies  142  Biogeographical patterns result from ecological processes that influence range distributions  such as migration and dispersal  145  and from historical processes that split populations or species into different areas  The biogeographic processes that result in the natural splitting of species explain much of the modern distribution of the Earth  s biota  The splitting of lineages in a species is called vicariance biogeography and it is a sub-discipline of biogeography  146  There are also practical applications in the field of biogeography concerning ecological systems and processes  For example  the range and distribution of biodiversity and invasive species responding to climate change is a serious concern and active area of research in the context of global warming  147  148     A population ecology concept is r/K selection theory  D  one of the first predictive models in ecology used to explain life-history evolution  The premise behind the r/K selection model is that natural selection pressures change according to population density  For example  when an island is first colonized  density of individuals is low  The initial increase in population size is not limited by competition  leaving an abundance of available resources for rapid population growth  These early phases of population growth experience density-independent forces of natural selection  which is called r-selection  As the population becomes more crowded  it approaches the island  s carrying capacity  thus forcing individuals to compete more heavily for fewer available resources  Under crowded conditions  the population experiences density-dependent forces of natural selection  called K-selection  149     In the r/K-selection model  the first variable r is the intrinsic rate of natural increase in population size and the second variable K is the carrying capacity of a population  32  Different species evolve different life-history strategies spanning a continuum between these two selective forces  An r-selected species is one that has high birth rates  low levels of parental investment  and high rates of mortality before individuals reach maturity  Evolution favours high rates of fecundity in r-selected species  Many kinds of insects and invasive species exhibit r-selected characteristics  In contrast  a K-selected species has low rates of fecundity  high levels of parental investment in the young  and low rates of mortality as individuals mature  Humans and elephants are examples of species exhibiting K-selected characteristics  including longevity and efficiency in the conversion of more resources into fewer offspring  144  150     The important relationship between ecology and genetic inheritance predates modern techniques for molecular analysis  Molecular ecological research became more feasible with the development of rapid and accessible genetic technologies  such as the polymerase chain reaction  PCR   The rise of molecular technologies and the influx of research questions into this new ecological field resulted in the publication Molecular Ecology in 1992  151  Molecular ecology uses various analytical techniques to study genes in an evolutionary and ecological context  In 1994  John Avise also played a leading role in this area of science with the publication of his book  Molecular Markers  Natural History and Evolution  152  Newer technologies opened a wave of genetic analysis into organisms once difficult to study from an ecological or evolutionary standpoint  such as bacteria  fungi  and nematodes  Molecular ecology engendered a new research paradigm for investigating ecological questions considered otherwise intractable  Molecular investigations revealed previously obscured details in the tiny intricacies of nature and improved resolution into probing questions about behavioural and biogeographical ecology  152  For example  molecular ecology revealed promiscuous sexual behaviour and multiple male partners in tree swallows previously thought to be socially monogamous  153  In a biogeographical context  the marriage between genetics  ecology  and evolution resulted in a new sub-discipline called phylogeography  154     Rachel Carson  \"Silent Spring\" 155     Ecology is as much a biological science as it is a human science  4  Human ecology is an interdisciplinary investigation into the ecology of our species  \"Human ecology may be defined   1  from a bioecological standpoint as the study of man as the ecological dominant in plant and animal communities and systems   2  from a bioecological standpoint as simply another animal affecting and being affected by his physical environment  and  3  as a human being  somehow different from animal life in general  interacting with physical and modified environments in a distinctive and creative way  A truly interdisciplinary human ecology will most likely address itself to all three \" 156  3 The term was formally introduced in 1921  but many sociologists  geographers  psychologists  and other disciplines were interested in human relations to natural systems centuries prior  especially in the late 19th century  156  157     The ecological complexities human beings are facing through the technological transformation of the planetary biome has brought on the Anthropocene  The unique set of circumstances has generated the need for a new unifying science called coupled human and natural systems that builds upon  but moves beyond the field of human ecology  103  Ecosystems tie into human societies through the critical and all-encompassing life-supporting functions they sustain  In recognition of these functions and the incapability of traditional economic valuation methods to see the value in ecosystems  there has been a surge of interest in social-natural capital  which provides the means to put a value on the stock and use of information and materials stemming from ecosystem goods and services  Ecosystems produce  regulate  maintain  and supply services of critical necessity and beneficial to human health  cognitive and physiological   economies  and they even provide an information or reference function as a living library giving opportunities for science and cognitive development in children engaged in the complexity of the natural world  Ecosystems relate importantly to human ecology as they are the ultimate base foundation of global economics as every commodity  and the capacity for exchange ultimately stems from the ecosystems on Earth  103  158  159  160     Grumbine  1994  161  27    Ecology is an employed science of restoration  repairing disturbed sites through human intervention  in natural resource management  and in environmental impact assessments  Edward O  Wilson predicted in 1992 that the 21st century \"will be the era of restoration in ecology\"  162  Ecological science has boomed in the industrial investment of restoring ecosystems and their processes in abandoned sites after disturbance  Natural resource managers  in forestry  for example  employ ecologists to develop  adapt  and implement ecosystem based methods into the planning  operation  and restoration phases of land-use  Ecological science is used in the methods of sustainable harvesting  disease  and fire outbreak management  in fisheries stock management  for integrating land-use with protected areas and communities  and conservation in complex geo-political landscapes  21  161  163  164     The environment of ecosystems includes both physical parameters and biotic attributes  It is dynamically interlinked and contains resources for organisms at any time throughout their life cycle  4  165  Like ecology  the term environment has different conceptual meanings and overlaps with the concept of nature  Environment \"includes the physical world  the social world of human relations and the built world of human creation \" 166  62 The physical environment is external to the level of biological organization under investigation  including abiotic factors such as temperature  radiation  light  chemistry  climate and geology  The biotic environment includes genes  cells  organisms  members of the same species  conspecifics  and other species that share a habitat  167     The distinction between external and internal environments  however  is an abstraction parsing life and environment into units or facts that are inseparable in reality  There is an interpenetration of cause and effect between the environment and life  The laws of thermodynamics  for example  apply to ecology by means of its physical state  With an understanding of metabolic and thermodynamic principles  a complete accounting of energy and material flow can be traced through an ecosystem  In this way  the environmental and ecological relations are studied through reference to conceptually manageable and isolated material parts  After the effective environmental components are understood through reference to their causes  however  they conceptually link back together as an integrated whole  or holocoenotic system as it was once called  This is known as the dialectical approach to ecology  The dialectical approach examines the parts but integrates the organism and the environment into a dynamic whole  or umwelt   Change in one ecological or environmental factor can concurrently affect the dynamic state of an entire ecosystem  35  168     Ecosystems are regularly confronted with natural environmental variations and disturbances over time and geographic space  A disturbance is any process that removes biomass from a community  such as a fire  flood  drought  or predation  169  Disturbances occur over vastly different ranges in terms of magnitudes as well as distances and time periods  170  and are both the cause and product of natural fluctuations in death rates  species assemblages  and biomass densities within an ecological community  These disturbances create places of renewal where new directions emerge from the patchwork of natural experimentation and opportunity  169  171  172  Ecological resilience is a cornerstone theory in ecosystem management  Biodiversity fuels the resilience of ecosystems acting as a kind of regenerative insurance  172     Ernest et al  173  991    The Earth was formed approximately 4 5 xa0billion years ago  174  As it cooled and a crust and oceans formed  its atmosphere transformed from being dominated by hydrogen to one composed mostly of methane and ammonia  Over the next billion years  the metabolic activity of life transformed the atmosphere into a mixture of carbon dioxide  nitrogen  and water vapor  These gases changed the way that light from the sun hit the Earth  s surface and greenhouse effects trapped heat  There were untapped sources of free energy within the mixture of reducing and oxidizing gasses that set the stage for primitive ecosystems to evolve and  in turn  the atmosphere also evolved  175     Throughout history  the Earth  s atmosphere and biogeochemical cycles have been in a dynamic equilibrium with planetary ecosystems  The history is characterized by periods of significant transformation followed by millions of years of stability  176  The evolution of the earliest organisms  likely anaerobic methanogen microbes  started the process by converting atmospheric hydrogen into methane  4H2 + CO2  CH4 + 2H2O   Anoxygenic photosynthesis reduced hydrogen concentrations and increased atmospheric methane  by converting hydrogen sulfide into water or other sulfur compounds  for example  2H2S + CO2 + hv  CH2O + H2O + 2S   Early forms of fermentation also increased levels of atmospheric methane  The transition to an oxygen-dominant atmosphere  the Great Oxidation  did not begin until approximately 2 42 3 xa0billion years ago  but photosynthetic processes started 0 3 to 1 xa0billion years prior  176  177      nThe biology of life operates within a certain range of temperatures  Heat is a form of energy that regulates temperature  Heat affects growth rates  activity  behaviour  and primary production  Temperature is largely dependent on the incidence of solar radiation  The latitudinal and longitudinal spatial variation of temperature greatly affects climates and consequently the distribution of biodiversity and levels of primary production in different ecosystems or biomes across the planet  Heat and temperature relate importantly to metabolic activity  Poikilotherms  for example  have a body temperature that is largely regulated and dependent on the temperature of the external environment  In contrast  homeotherms regulate their internal body temperature by expending metabolic energy  108  109  168     There is a relationship between light  primary production  and ecological energy budgets  Sunlight is the primary input of energy into the planet  s ecosystems  Light is composed of electromagnetic energy of different wavelengths  Radiant energy from the sun generates heat  provides photons of light measured as active energy in the chemical reactions of life  and also acts as a catalyst for genetic mutation  108  109  168  Plants  algae  and some bacteria absorb light and assimilate the energy through photosynthesis  Organisms capable of assimilating energy by photosynthesis or through inorganic fixation of H2S are autotrophs  Autotrophsresponsible for primary productionassimilate light energy which becomes metabolically stored as potential energy in the form of biochemical enthalpic bonds  108  109  168     Cronk & Fennessy  2001  178  29    Diffusion of carbon dioxide and oxygen is approximately 10 000 times slower in water than in air  When soils are flooded  they quickly lose oxygen  becoming hypoxic  an environment with O2 concentration below 2 xa0mg/liter  and eventually completely anoxic where anaerobic bacteria thrive among the roots  Water also influences the intensity and spectral composition of light as it reflects off the water surface and submerged particles  178  Aquatic plants exhibit a wide variety of morphological and physiological adaptations that allow them to survive  compete  and diversify in these environments  For example  their roots and stems contain large air spaces  aerenchyma  that regulate the efficient transportation of gases  for example  CO2 and O2  used in respiration and photosynthesis  Salt water plants  halophytes  have additional specialized adaptations  such as the development of special organs for shedding salt and osmoregulating their internal salt  NaCl  concentrations  to live in estuarine  brackish  or oceanic environments  Anaerobic soil microorganisms in aquatic environments use nitrate  manganese ions  ferric ions  sulfate  carbon dioxide  and some organic compounds  other microorganisms are facultative anaerobes and use oxygen during respiration when the soil becomes drier  The activity of soil microorganisms and the chemistry of the water reduces the oxidation-reduction potentials of the water  Carbon dioxide  for example  is reduced to methane  CH4  by methanogenic bacteria  178  The physiology of fish is also specially adapted to compensate for environmental salt levels through osmoregulation  Their gills form electrochemical gradients that mediate salt excretion in salt water and uptake in fresh water  179     The shape and energy of the land are significantly affected by gravitational forces  On a large scale  the distribution of gravitational forces on the earth is uneven and influences the shape and movement of tectonic plates as well as influencing geomorphic processes such as orogeny and erosion  These forces govern many of the geophysical properties and distributions of ecological biomes across the Earth  On the organismal scale  gravitational forces provide directional cues for plant and fungal growth  gravitropism   orientation cues for animal migrations  and influence the biomechanics and size of animals  108  Ecological traits  such as allocation of biomass in trees during growth are subject to mechanical failure as gravitational forces influence the position and structure of branches and leaves  180  The cardiovascular systems of animals are functionally adapted to overcome the pressure and gravitational forces that change according to the features of organisms  e g   height  size  shape   their behaviour  e g   diving  running  flying   and the habitat occupied  e g   water  hot deserts  cold tundra   181     Climatic and osmotic pressure places physiological constraints on organisms  especially those that fly and respire at high altitudes  or dive to deep ocean depths  182  These constraints influence vertical limits of ecosystems in the biosphere  as organisms are physiologically sensitive and adapted to atmospheric and osmotic water pressure differences  108  For example  oxygen levels decrease with decreasing pressure and are a limiting factor for life at higher altitudes  183  Water transportation by plants is another important ecophysiological process affected by osmotic pressure gradients  184  185  186  Water pressure in the depths of oceans requires that organisms adapt to these conditions  For example  diving animals such as whales  dolphins  and seals are specially adapted to deal with changes in sound due to water pressure differences  187  Differences between hagfish species provide another example of adaptation to deep-sea pressure through specialized protein adaptations  188     Turbulent forces in air and water affect the environment and ecosystem distribution  form  and dynamics  On a planetary scale  ecosystems are affected by circulation patterns in the global trade winds  Wind power and the turbulent forces it creates can influence heat  nutrient  and biochemical profiles of ecosystems  108  For example  wind running over the surface of a lake creates turbulence  mixing the water column and influencing the environmental profile to create thermally layered zones  affecting how fish  algae  and other parts of the aquatic ecosystem are structured  191  192  Wind speed and turbulence also influence evapotranspiration rates and energy budgets in plants and animals  178  193  Wind speed  temperature and moisture content can vary as winds travel across different land features and elevations  For example  the westerlies come into contact with the coastal and interior mountains of western North America to produce a rain shadow on the leeward side of the mountain  The air expands and moisture condenses as the winds increase in elevation  this is called orographic lift and can cause precipitation  This environmental process produces spatial divisions in biodiversity  as species adapted to wetter conditions are range-restricted to the coastal mountain valleys and unable to migrate across the xeric ecosystems  e g   of the Columbia Basin in western North America  to intermix with sister lineages that are segregated to the interior mountain systems  194  195     Plants convert carbon dioxide into biomass and emit oxygen into the atmosphere  By approximately 350 million years ago  the end of the Devonian period   photosynthesis had brought the concentration of atmospheric oxygen above 17%  which allowed combustion to occur  196  Fire releases CO2 and converts fuel into ash and tar  Fire is a significant ecological parameter that raises many issues pertaining to its control and suppression  197  While the issue of fire in relation to ecology and plants has been recognized for a long time  198  Charles Cooper brought attention to the issue of forest fires in relation to the ecology of forest fire suppression and management in the 1960s  199  200     Native North Americans were among the first to influence fire regimes by controlling their spread near their homes or by lighting fires to stimulate the production of herbaceous foods and basketry materials  201  Fire creates a heterogeneous ecosystem age and canopy structure  and the altered soil nutrient supply and cleared canopy structure opens new ecological niches for seedling establishment  202  203  Most ecosystems are adapted to natural fire cycles  Plants  for example  are equipped with a variety of adaptations to deal with forest fires  Some species  e g   Pinus halepensis  cannot germinate until after their seeds have lived through a fire or been exposed to certain compounds from smoke  Environmentally triggered germination of seeds is called serotiny  204  205  Fire plays a major role in the persistence and resilience of ecosystems  171     Soil is the living top layer of mineral and organic dirt that covers the surface of the planet  It is the chief organizing centre of most ecosystem functions  and it is of critical importance in agricultural science and ecology  The decomposition of dead organic matter  for example  leaves on the forest floor   results in soils containing minerals and nutrients that feed into plant production  The whole of the planet  s soil ecosystems is called the pedosphere where a large biomass of the Earth  s biodiversity organizes into trophic levels  Invertebrates that feed and shred larger leaves  for example  create smaller bits for smaller organisms in the feeding chain  Collectively  these organisms are the detritivores that regulate soil formation  206  207  Tree roots  fungi  bacteria  worms  ants  beetles  centipedes  spiders  mammals  birds  reptiles  amphibians  and other less familiar creatures all work to create the trophic web of life in soil ecosystems  Soils form composite phenotypes where inorganic matter is enveloped into the physiology of a whole community  As organisms feed and migrate through soils they physically displace materials  an ecological process called bioturbation  This aerates soils and stimulates heterotrophic growth and production  Soil microorganisms are influenced by and are fed back into the trophic dynamics of the ecosystem  No single axis of causality can be discerned to segregate the biological from geomorphological systems in soils  208  209  Paleoecological studies of soils places the origin for bioturbation to a time before the Cambrian period  Other events  such as the evolution of trees and the colonization of land in the Devonian period played a significant role in the early development of ecological trophism in soils  207  210  211     Ecologists study and measure nutrient budgets to understand how these materials are regulated  flow  and recycled through the environment  108  109  168  This research has led to an understanding that there is global feedback between ecosystems and the physical parameters of this planet  including minerals  soil  pH  ions  water  and atmospheric gases  Six major elements  hydrogen  carbon  nitrogen  oxygen  sulfur  and phosphorus  H  C  N  O  S  and P  form the constitution of all biological macromolecules and feed into the Earth  s geochemical processes  From the smallest scale of biology  the combined effect of billions upon billions of ecological processes amplify and ultimately regulate the biogeochemical cycles of the Earth  Understanding the relations and cycles mediated between these elements and their ecological pathways has significant bearing toward understanding global biogeochemistry  212     The ecology of global carbon budgets gives one example of the linkage between biodiversity and biogeochemistry  It is estimated that the Earth  s oceans hold 40 000 gigatonnes  Gt  of carbon  that vegetation and soil hold 2070 Gt  and that fossil fuel emissions are 6 3 Gt carbon per year  213  There have been major restructurings in these global carbon budgets during the Earth  s history  regulated to a large extent by the ecology of the land  For example  through the early-mid Eocene volcanic outgassing  the oxidation of methane stored in wetlands  and seafloor gases increased atmospheric CO2  carbon dioxide  concentrations to levels as high as 3500 xa0ppm  214     In the Oligocene  from twenty-five to thirty-two million years ago  there was another significant restructuring of the global carbon cycle as grasses evolved a new mechanism of photosynthesis  C4 photosynthesis  and expanded their ranges  This new pathway evolved in response to the drop in atmospheric CO2 concentrations below 550 ppm  215  The relative abundance and distribution of biodiversity alters the dynamics between organisms and their environment such that ecosystems can be both cause and effect in relation to climate change  Human-driven modifications to the planet  s ecosystems  e g   disturbance  biodiversity loss  agriculture  contributes to rising atmospheric greenhouse gas levels  Transformation of the global carbon cycle in the next century is projected to raise planetary temperatures  lead to more extreme fluctuations in weather  alter species distributions  and increase extinction rates  The effect of global warming is already being registered in melting glaciers  melting mountain ice caps  and rising sea levels  Consequently  species distributions are changing along waterfronts and in continental areas where migration patterns and breeding grounds are tracking the prevailing shifts in climate  Large sections of permafrost are also melting to create a new mosaic of flooded areas having increased rates of soil decomposition activity that raises methane  CH4  emissions  There is concern over increases in atmospheric methane in the context of the global carbon cycle  because methane is a greenhouse gas that is 23 times more effective at absorbing long-wave radiation than CO2 on a 100-year time scale  216  Hence  there is a relationship between global warming  decomposition and respiration in soils and wetlands producing significant climate feedbacks and globally altered biogeochemical cycles  103  217  218  219  220  221     Ernst Haeckel  1866  222  140  B     Ecology has a complex origin  due in large part to its interdisciplinary nature  223  Ancient Greek philosophers such as Hippocrates and Aristotle were among the first to record observations on natural history  However  they viewed life in terms of essentialism  where species were conceptualized as static unchanging things while varieties were seen as aberrations of an idealized type  This contrasts against the modern understanding of ecological theory where varieties are viewed as the real phenomena of interest and having a role in the origins of adaptations by means of natural selection  4  224  225  Early conceptions of ecology  such as a balance and regulation in nature can be traced to Herodotus  died c  425 BC   who described one of the earliest accounts of mutualism in his observation of \"natural dentistry\"  Basking Nile crocodiles  he noted  would open their mouths to give sandpipers safe access to pluck leeches out  giving nutrition to the sandpiper and oral hygiene for the crocodile  223  Aristotle was an early influence on the philosophical development of ecology  He and his student Theophrastus made extensive observations on plant and animal migrations  biogeography  physiology  and their behavior  giving an early analogue to the modern concept of an ecological niche  226  227     Stephen Forbes  1887  228          Ecological concepts such as food chains  population regulation  and productivity were first developed in the 1700s  through the published works of microscopist Antoni van Leeuwenhoek  16321723  and botanist Richard Bradley  1688?1732   4  Biogeographer Alexander von Humboldt  17691859  was an early pioneer in ecological thinking and was among the first to recognize ecological gradients  where species are replaced or altered in form along environmental gradients  such as a cline forming along a rise in elevation  Humboldt drew inspiration from Isaac Newton  as he developed a form of \"terrestrial physics\"  In Newtonian fashion  he brought a scientific exactitude for measurement into natural history and even alluded to concepts that are the foundation of a modern ecological law on species-to-area relationships  229  230  231  Natural historians  such as Humboldt  James Hutton  and Jean-Baptiste Lamarck  among others  laid the foundations of the modern ecological sciences  232  The term \"ecology\"  German  Oekologie  kologie  was coined by Ernst Haeckel in his book Generelle Morphologie der Organismen  1866   233  Haeckel was a zoologist  artist  writer  and later in life a professor of comparative anatomy  222  234     Opinions differ on who was the founder of modern ecological theory  Some mark Haeckel  s definition as the beginning  235  others say it was Eugenius Warming with the writing of Oecology of Plants  An Introduction to the Study of Plant Communities  1895   236  or Carl Linnaeus   principles on the economy of nature that matured in the early 18th century  237  238  Linnaeus founded an early branch of ecology that he called the economy of nature  237  His works influenced Charles Darwin  who adopted Linnaeus   phrase on the economy or polity of nature in The Origin of Species  222  Linnaeus was the first to frame the balance of nature as a testable hypothesis  Haeckel  who admired Darwin  s work  defined ecology in reference to the economy of nature  which has led some to question whether ecology and the economy of nature are synonymous  238     From Aristotle until Darwin  the natural world was predominantly considered static and unchanging  Prior to The Origin of Species  there was little appreciation or understanding of the dynamic and reciprocal relations between organisms  their adaptations  and the environment  224  An exception is the 1789 publication Natural History of Selborne by Gilbert White  17201793   considered by some to be one of the earliest texts on ecology  241  While Charles Darwin is mainly noted for his treatise on evolution  242  he was one of the founders of soil ecology  243  and he made note of the first ecological experiment in The Origin of Species  239  Evolutionary theory changed the way that researchers approached the ecological sciences  244     Modern ecology is a young science that first attracted substantial scientific attention toward the end of the 19th century  around the same time that evolutionary studies were gaining scientific interest   The scientist Ellen Swallow Richards may have first introduced the term \"oekology\"  which eventually morphed into home economics  in the U S  as early 1892  245     In the early 20th century  ecology transitioned from a more descriptive form of natural history to a more analytical form of scientific natural history  229  232  Frederic Clements published the first American ecology book in 1905  246  presenting the idea of plant communities as a superorganism  This publication launched a debate between ecological holism and individualism that lasted until the 1970s  Clements   superorganism concept proposed that ecosystems progress through regular and determined stages of seral development that are analogous to the developmental stages of an organism  The Clementsian paradigm was challenged by Henry Gleason  247  who stated that ecological communities develop from the unique and coincidental association of individual organisms  This perceptual shift placed the focus back onto the life histories of individual organisms and how this relates to the development of community associations  248     The Clementsian superorganism theory was an overextended application of an idealistic form of holism  35  105  The term \"holism\" was coined in 1926 by Jan Christiaan Smuts  a South African general and polarizing historical figure who was inspired by Clements   superorganism concept  249  C  Around the same time  Charles Elton pioneered the concept of food chains in his classical book Animal Ecology  81  Elton 81  defined ecological relations using concepts of food chains  food cycles  and food size  and described numerical relations among different functional groups and their relative abundance  Elton  s   food cycle   was replaced by   food web   in a subsequent ecological text  250  Alfred J  Lotka brought in many theoretical concepts applying thermodynamic principles to ecology     In 1942  Raymond Lindeman wrote a landmark paper on the trophic dynamics of ecology  which was published posthumously after initially being rejected for its theoretical emphasis  Trophic dynamics became the foundation for much of the work to follow on energy and material flow through ecosystems  Robert MacArthur advanced mathematical theory  predictions  and tests in ecology in the 1950s  which inspired a resurgent school of theoretical mathematical ecologists  232  251  252  Ecology also has developed through contributions from other nations  including Russia  s Vladimir Vernadsky and his founding of the biosphere concept in the 1920s 253  and Japan  s Kinji Imanishi and his concepts of harmony in nature and habitat segregation in the 1950s  254  Scientific recognition of contributions to ecology from non-English-speaking cultures is hampered by language and translation barriers  253     Rachel Carson  1962  255  48    Ecology surged in popular and scientific interest during the 19601970s environmental movement  There are strong historical and scientific ties between ecology  environmental management  and protection  232  The historical emphasis and poetic naturalistic writings advocating the protection of wild places by notable ecologists in the history of conservation biology  such as Aldo Leopold and Arthur Tansley  have been seen as far removed from urban centres where  it is claimed  the concentration of pollution and environmental degradation is located  232  256  Palamar  2008  256  notes an overshadowing by mainstream environmentalism of pioneering women in the early 1900s who fought for urban health ecology  then called euthenics  245  and brought about changes in environmental legislation  Women such as Ellen Swallow Richards and Julia Lathrop  among others  were precursors to the more popularized environmental movements after the 1950s     In 1962  marine biologist and ecologist Rachel Carson  s book Silent Spring helped to mobilize the environmental movement by alerting the public to toxic pesticides  such as DDT  bioaccumulating in the environment  Carson used ecological science to link the release of environmental toxins to human and ecosystem health  Since then  ecologists have worked to bridge their understanding of the degradation of the planet  s ecosystems with environmental politics  law  restoration  and natural resources management  21  232  256  257         Monte Carlo methods  or Monte Carlo experiments  are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results  The underlying concept is to use randomness to solve problems that might be deterministic in principle  They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches  Monte Carlo methods are mainly used in three problem classes  1  optimization  numerical integration  and generating draws from a probability distribution     In physics-related problems  Monte Carlo methods are useful for simulating systems with many coupled degrees of freedom  such as fluids  disordered materials  strongly coupled solids  and cellular structures  see cellular Potts model  interacting particle systems  McKeanVlasov processes  kinetic models of gases      Other examples include modeling phenomena with significant uncertainty in inputs such as the calculation of risk in business and  in mathematics  evaluation of multidimensional definite integrals with complicated boundary conditions   In application to systems engineering problems  space  oil exploration  aircraft design  etc    Monte Carlobased predictions of failure  cost overruns and schedule overruns are routinely better than human intuition or alternative \"soft\" methods  2     In principle  Monte Carlo methods can be used to solve any problem having a probabilistic interpretation  By the law of large numbers  integrals described by the expected value of some random variable can be approximated by taking the empirical mean  a k a  the sample mean  of independent samples of the variable  When the probability distribution of the variable is parametrized  mathematicians often use a Markov chain Monte Carlo  MCMC  sampler  3  4  5  The central idea is to design a judicious Markov chain model with a prescribed stationary probability distribution  That is  in the limit  the samples being generated by the MCMC method will be samples from the desired  target  distribution  6  7  By the ergodic theorem  the stationary distribution is approximated by the empirical measures of the random states of the MCMC sampler     In other problems  the objective is generating draws from a sequence of probability distributions satisfying a nonlinear evolution equation  These flows of probability distributions can always be interpreted as the distributions of the random states of a Markov process whose transition probabilities depend on the distributions of the current random states  see McKeanVlasov processes  nonlinear filtering equation   8  9  In other instances we are given a flow of probability distributions with an increasing level of sampling complexity  path spaces models with an increasing time horizon  BoltzmannGibbs measures associated with decreasing temperature parameters  and many others   These models can also be seen as the evolution of the law of the random states of a nonlinear Markov chain  9  10  A natural way to simulate these sophisticated nonlinear Markov processes is to sample multiple copies of the process  replacing in the evolution equation the unknown distributions of the random states by the sampled empirical measures  In contrast with traditional Monte Carlo and MCMC methodologies  these mean field particle techniques rely on sequential interacting samples  The terminology mean field reflects the fact that each of the samples  a k a  particles  individuals  walkers  agents  creatures  or phenotypes  interacts with the empirical measures of the process  When the size of the system tends to infinity  these random empirical measures converge to the deterministic distribution of the random states of the nonlinear Markov chain  so that the statistical interaction between particles vanishes     Despite its conceptual and algorithmic simplicity  the computational cost associated with a Monte Carlo simulation can be staggeringly high  In general the method requires many samples to get a good approximation  which may incurs an arbitrarily large total runtime if the processing time of a single sample is high  11  Although this is a severe limitation in very complex problems  the embarrassingly parallel nature of the algorithm allows this large cost to be reduced  perhaps to a feasible level  through parallel computing strategies in local processors  clusters  cloud computing  GPU  FPGA etc  12  13  14  15     Monte Carlo methods vary  but tend to follow a particular pattern     For example  consider a quadrant  circular sector  inscribed in a unit square  Given that the ratio of their areas is /4  the value of  can be approximated using a Monte Carlo method  16     In this procedure the domain of inputs is the square that circumscribes the quadrant   We generate random inputs by scattering grains over the square then perform a computation on each input  test whether it falls within the quadrant   Aggregating the results yields our final result  the approximation of      There are two important considerations     Uses of Monte Carlo methods require large amounts of random numbers  and it was their use that spurred the development of pseudorandom number generators citation needed   which were far quicker to use than the tables of random numbers that had been previously used for statistical sampling     Before the Monte Carlo method was developed  simulations tested a previously understood deterministic problem  and statistical sampling was used to estimate uncertainties in the simulations  Monte Carlo simulations invert this approach  solving deterministic problems using probabilistic metaheuristics  see simulated annealing      An early variant of the Monte Carlo method was devised to solve the Buffon  s needle problem  in which  can be estimated by dropping needles on a floor made of parallel equidistant strips  In the 1930s  Enrico Fermi first experimented with the Monte Carlo method while studying neutron diffusion  but he did not publish this work  17      In the late 1940s  Stanislaw Ulam invented the modern version of the Markov Chain Monte Carlo method while he was working on nuclear weapons projects at the Los Alamos National Laboratory  Immediately after Ulam  s breakthrough  John von Neumann understood its importance  Von Neumann programmed the ENIAC computer to perform Monte Carlo calculations  In 1946  nuclear weapons physicists at Los Alamos were investigating neutron diffusion in fissionable material  17  Despite having most of the necessary data  such as the average distance a neutron would travel in a substance before it collided with an atomic nucleus and how much energy the neutron was likely to give off following a collision  the Los Alamos physicists were unable to solve the problem using conventional  deterministic mathematical methods  Ulam proposed using random experiments  He recounts his inspiration as follows     The first thoughts and attempts I made to practice  the Monte Carlo Method  were suggested by a question which occurred to me in 1946 as I was convalescing from an illness and playing solitaires  The question was what are the chances that a Canfield solitaire laid out with 52 cards will come out successfully? After spending a lot of time trying to estimate them by pure combinatorial calculations  I wondered whether a more practical method than \"abstract thinking\" might not be to lay it out say one hundred times and simply observe and count the number of successful plays  This was already possible to envisage with the beginning of the new era of fast computers  and I immediately thought of problems of neutron diffusion and other questions of mathematical physics  and more generally how to change processes described by certain differential equations into an equivalent form interpretable as a succession of random operations  Later  in 1946   I described the idea to John von Neumann  and we began to plan actual calculations  18     Being secret  the work of von Neumann and Ulam required a code name  19  A colleague of von Neumann and Ulam  Nicholas Metropolis  suggested using the name Monte Carlo  which refers to the Monte Carlo Casino in Monaco where Ulam  s uncle would borrow money from relatives to gamble  17  Using lists of \"truly random\" random numbers was extremely slow  but von Neumann developed a way to calculate pseudorandom numbers  using the middle-square method  Though this method has been criticized as crude  von Neumann was aware of this  he justified it as being faster than any other method at his disposal  and also noted that when it went awry it did so obviously  unlike methods that could be subtly incorrect  20     Monte Carlo methods were central to the simulations required for the Manhattan Project  though severely limited by the computational tools at the time  In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb  and became popularized in the fields of physics  physical chemistry  and operations research  The Rand Corporation and the U S  Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time  and they began to find a wide application in many different fields     The theory of more sophisticated mean field type particle Monte Carlo methods had certainly started by the mid-1960s  with the work of Henry P  McKean Jr  on Markov interpretations of a class of nonlinear parabolic partial differential equations arising in fluid mechanics  21  22  We also quote an earlier pioneering article by Theodore E  Harris and Herman Kahn  published in 1951  using mean field genetic-type Monte Carlo methods for estimating particle transmission energies  23  Mean field genetic type Monte Carlo methodologies are also used as heuristic natural search algorithms  a k a  metaheuristic  in evolutionary computing  The origins of these mean field computational techniques can be traced to 1950 and 1954 with the work of Alan Turing on genetic type mutation-selection learning machines 24  and the articles by Nils Aall Barricelli at the Institute for Advanced Study in Princeton  New Jersey  25  26     Quantum Monte Carlo  and more specifically diffusion Monte Carlo methods can also be interpreted as a mean field particle Monte Carlo approximation of FeynmanKac path integrals  27  28  29  30  31  32  33  The origins of Quantum Monte Carlo methods are often attributed to Enrico Fermi and Robert Richtmyer who developed in 1948 a mean field particle interpretation of neutron-chain reactions  34  but the first heuristic-like and genetic type particle algorithm  a k a  Resampled or Reconfiguration Monte Carlo methods  for estimating ground state energies of quantum systems  in reduced matrix models  is due to Jack H  Hetherington in 1984 33  In molecular chemistry  the use of genetic heuristic-like particle methodologies  a k a  pruning and enrichment strategies  can be traced back to 1955 with the seminal work of Marshall N  Rosenbluth and Arianna W  Rosenbluth  35     The use of Sequential Monte Carlo in advanced signal processing and Bayesian inference is more recent  It was in 1993  that Gordon et al   published in their seminal work 36  the first application of a Monte Carlo resampling algorithm in Bayesian statistical inference  The authors named their algorithm   the bootstrap filter    and demonstrated that compared to other filtering methods  their bootstrap algorithm does not require any assumption about that state-space or the noise of the system  We also quote another pioneering article in this field of Genshiro Kitagawa on a related \"Monte Carlo filter\"  37  and the ones by Pierre Del Moral 38  and Himilcon Carvalho  Pierre Del Moral  Andr Monin and Grard Salut 39  on particle filters published in the mid-1990s  Particle filters were also developed in signal processing in 19891992 by P  Del Moral  J  C  Noyer  G  Rigal  and G  Salut in the LAAS-CNRS in a series of restricted and classified research reports with STCAN  Service Technique des Constructions et Armes Navales   the IT company DIGILOG  and the LAAS-CNRS  the Laboratory for Analysis and Architecture of Systems  on radar/sonar and GPS signal processing problems  40  41  42  43  44  45  These Sequential Monte Carlo methodologies can be interpreted as an acceptance-rejection sampler equipped with an interacting recycling mechanism     From 1950 to 1996  all the publications on Sequential Monte Carlo methodologies  including the pruning and resample Monte Carlo methods introduced in computational physics and molecular chemistry  present natural and heuristic-like algorithms applied to different situations without a single proof of their consistency  nor a discussion on the bias of the estimates and on genealogical and ancestral tree based algorithms  The mathematical foundations and the first rigorous analysis of these particle algorithms were written by Pierre Del Moral in 1996  38  46     Branching type particle methodologies with varying population sizes were also developed in the end of the 1990s by Dan Crisan  Jessica Gaines and Terry Lyons  47  48  49  and by Dan Crisan  Pierre Del Moral and Terry Lyons  50  Further developments in this field were developed in 2000 by P  Del Moral  A  Guionnet and L  Miclo  28  51  52     There is no consensus on how Monte Carlo should be defined  For example  Ripley 53  defines most probabilistic modeling as stochastic simulation  with Monte Carlo being reserved for Monte Carlo integration and Monte Carlo statistical tests  Sawilowsky 54  distinguishes between a simulation  a Monte Carlo method  and a Monte Carlo simulation  a simulation is a fictitious representation of reality  a Monte Carlo method is a technique that can be used to solve a mathematical or statistical problem  and a Monte Carlo simulation uses repeated sampling to obtain the statistical properties of some phenomenon  or behavior   Examples     Kalos and Whitlock 55  point out that such distinctions are not always easy to maintain  For example  the emission of radiation from atoms is a natural stochastic process  It can be simulated directly  or its average behavior can be described by stochastic equations that can themselves be solved using Monte Carlo methods  \"Indeed  the same computer code can be viewed simultaneously as a   natural simulation   or as a solution of the equations by natural sampling \"    The main idea behind this method is that the results are computed based on repeated random sampling and statistical analysis  The Monte Carlo simulation is  in fact  random experimentations  in the case that  the results of these experiments are not well known  nMonte Carlo simulations are typically characterized by many unknown parameters  many of which are difficult to obtain experimentally  56  Monte Carlo simulation methods do not always require truly random numbers to be useful  although  for some applications such as primality testing  unpredictability is vital   57  Many of the most useful techniques use deterministic  pseudorandom sequences  making it easy to test and re-run simulations  The only quality usually necessary to make good simulations is for the pseudo-random sequence to appear \"random enough\" in a certain sense     What this means depends on the application  but typically they should pass a series of statistical tests  Testing that the numbers are uniformly distributed or follow another desired distribution when a large enough number of elements of the sequence are considered is one of the simplest and most common ones  Weak correlations between successive samples are also often desirable/necessary     Sawilowsky lists the characteristics of a high-quality Monte Carlo simulation  54     Pseudo-random number sampling algorithms are used to transform uniformly distributed pseudo-random numbers into numbers that are distributed according to a given probability distribution     Low-discrepancy sequences are often used instead of random sampling from a space as they ensure even coverage and normally have a faster order of convergence than Monte Carlo simulations using random or pseudorandom sequences  Methods based on their use are called quasi-Monte Carlo methods     In an effort to assess the impact of random number quality on Monte Carlo simulation outcomes  astrophysical researchers tested cryptographically-secure pseudorandom numbers generated via Intel  s RDRAND instruction set  as compared to those derived from algorithms  like the Mersenne Twister  in Monte Carlo simulations of radio flares from brown dwarfs   RDRAND is the closest pseudorandom number generator to a true random number generator  No statistically significant difference was found between models generated with typical pseudorandom number generators and RDRAND for trials consisting of the generation of 107 random numbers  58     There are ways of using probabilities that are definitely not Monte Carlo simulations  for example  deterministic modeling using single-point estimates  Each uncertain variable within a model is assigned a \"best guess\" estimate   Scenarios  such as best  worst  or most likely case  for each input variable are chosen and the results recorded  59     By contrast  Monte Carlo simulations sample from a probability distribution for each variable to produce hundreds or thousands of possible outcomes  The results are analyzed to get probabilities of different outcomes occurring  60  For example  a comparison of a spreadsheet cost construction model run using traditional \"what if\" scenarios  and then running the comparison again with Monte Carlo simulation and triangular probability distributions shows that the Monte Carlo analysis has a narrower range than the \"what if\" analysis  example  needed   This is because the \"what if\" analysis gives equal weight to all scenarios  see quantifying uncertainty in corporate finance   while the Monte Carlo method hardly samples in the very low probability regions  The samples in such regions are called \"rare events\"     Monte Carlo methods are especially useful for simulating phenomena with significant uncertainty in inputs and systems with many coupled degrees of freedom  Areas of application include     Finite element xa0 Boundary element  nLattice Boltzmann xa0 Riemann solver nDissipative particle dynamics nSmoothed particle hydrodynamics    Monte Carlo methods are very important in computational physics  physical chemistry  and related applied fields  and have diverse applications from complicated quantum chromodynamics calculations to designing heat shields and aerodynamic forms as well as in modeling radiation transport for radiation dosimetry calculations  61  62  63   In statistical physics Monte Carlo molecular modeling is an alternative to computational molecular dynamics  and Monte Carlo methods are used to compute statistical field theories of simple particle and polymer systems  35  64  Quantum Monte Carlo methods solve the many-body problem for quantum systems  8  9  27  In radiation materials science  the binary collision approximation for simulating ion implantation is usually based on a Monte Carlo approach to select the next colliding atom  65  In experimental particle physics  Monte Carlo methods are used for designing detectors  understanding their behavior and comparing experimental data to theory  In astrophysics  they are used in such diverse manners as to model both galaxy evolution 66  and microwave radiation transmission through a rough planetary surface  67  Monte Carlo methods are also used in the ensemble models that form the basis of modern weather forecasting     Monte Carlo methods are widely used in engineering for sensitivity analysis and quantitative probabilistic analysis in process design  The need arises from the interactive  co-linear and non-linear behavior of typical process simulations  For example     The Intergovernmental Panel on Climate Change relies on Monte Carlo methods in probability density function analysis of radiative forcing     Probability density function  PDF  of ERF due to total GHG  aerosol forcing and total anthropogenic forcing  The GHG consists of WMGHG  ozone and stratospheric water vapour  The PDFs are generated based on uncertainties provided in Table 8 6  The combination of the individual RF agents to derive total forcing over the Industrial Era are done by Monte Carlo simulations and based on the method in Boucher and Haywood  2001   PDF of the ERF from surface albedo changes and combined contrails and contrail-induced cirrus are included in the total anthropogenic forcing  but not shown as a separate PDF  We currently do not have ERF estimates for some forcing mechanisms  ozone  land use  solar  etc  77     Monte Carlo methods are used in various fields of computational biology  for example for Bayesian inference in phylogeny  or for studying biological systems such as genomes  proteins  78  or membranes  79  nThe systems can be studied in the coarse-grained or ab initio frameworks depending on the desired accuracy   nComputer simulations allow us to monitor the local environment of a particular molecule to see if some chemical reaction is happening for instance  In cases where it is not feasible to conduct a physical experiment  thought experiments can be conducted  for instance  breaking bonds  introducing impurities at specific sites  changing the local/global structure  or introducing external fields      Path tracing  occasionally referred to as Monte Carlo ray tracing  renders a 3D scene by randomly tracing samples of possible light paths  Repeated sampling of any given pixel will eventually cause the average of the samples to converge on the correct solution of the rendering equation  making it one of the most physically accurate 3D graphics rendering methods in existence     The standards for Monte Carlo experiments in statistics were set by Sawilowsky  80  In applied statistics  Monte Carlo methods may be used for at least four purposes     Monte Carlo methods are also a compromise between approximate randomization and permutation tests  An approximate randomization test is based on a specified subset of all permutations  which entails potentially enormous housekeeping of which permutations have been considered   The Monte Carlo approach is based on a specified number of randomly drawn permutations  exchanging a minor loss in precision if a permutation is drawn twiceor more frequentlyfor the efficiency of not having to track which permutations have already been selected          Monte Carlo methods have been developed into a technique called Monte-Carlo tree search that is useful for searching for the best move in a game  Possible moves are organized in a search tree and many random simulations are used to estimate the long-term potential of each move  A black box simulator represents the opponent  s moves  84     The Monte Carlo tree search  MCTS  method has four steps  85     The net effect  over the course of many simulated games  is that the value of a node representing a move will go up or down  hopefully corresponding to whether or not that node represents a good move     Monte Carlo Tree Search has been used successfully to play games such as Go  86  Tantrix  87  Battleship  88  Havannah  89  and Arimaa  90     Monte Carlo methods are also efficient in solving coupled integral differential equations of radiation fields and energy transport  and thus these methods have been used in global illumination computations that produce photo-realistic images of virtual 3D models  with applications in video games  architecture  design  computer generated films  and cinematic special effects  91     The US Coast Guard utilizes Monte Carlo methods within its computer modeling software SAROPS in order to calculate the probable locations of vessels during search and rescue operations  Each simulation can generate as many as ten thousand data points that are randomly distributed based upon provided variables  92  Search patterns are then generated based upon extrapolations of these data in order to optimize the probability of containment  POC  and the probability of detection  POD   which together will equal an overall probability of success  POS   Ultimately this serves as a practical application of probability distribution in order to provide the swiftest and most expedient method of rescue  saving both lives and resources  93     Monte Carlo simulation is commonly used to evaluate the risk and uncertainty that would affect the outcome of different decision options  Monte Carlo simulation allows the business risk analyst to incorporate the total effects of uncertainty in variables like sales volume  commodity and labour prices  interest and exchange rates  as well as the effect of distinct risk events like the cancellation of a contract or the change of a tax law     Monte Carlo methods in finance are often used to evaluate investments in projects at a business unit or corporate level  or other financial valuations  They can be used to model project schedules  where simulations aggregate estimates for worst-case  best-case  and most likely durations for each task to determine outcomes for the overall project  1  Monte Carlo methods are also used in option pricing  default risk analysis  94  95  96  Additionally  they can be used to estimate the financial impact of medical interventions  97     A Monte Carlo approach was used for evaluating the potential value of a proposed program to help female petitioners in Wisconsin be successful in their applications for harassment and domestic abuse restraining orders   It was proposed to help women succeed in their petitions by providing them with greater advocacy thereby potentially reducing the risk of rape and physical assault   However  there were many variables in play that could not be estimated perfectly  including the effectiveness of restraining orders  the success rate of petitioners both with and without advocacy  and many others   The study ran trials that varied these variables to come up with an overall estimate of the success level of the proposed program as a whole  98     In general  the Monte Carlo methods are used in mathematics to solve various problems by generating suitable random numbers  see also Random number generation  and observing that fraction of the numbers that obeys some property or properties  The method is useful for obtaining numerical solutions to problems too complicated to solve analytically   The most common application of the Monte Carlo method is Monte Carlo integration     Deterministic numerical integration algorithms work well in a small number of dimensions  but encounter two problems when the functions have many variables  First  the number of function evaluations needed increases rapidly with the number of dimensions  For example  if 10 evaluations provide adequate accuracy in one dimension  then 10100 points are needed for 100 dimensionsfar too many to be computed  This is called the curse of dimensionality  Second  the boundary of a multidimensional region may be very complicated  so it may not be feasible to reduce the problem to an iterated integral  99  100 dimensions is by no means unusual  since in many physical problems  a \"dimension\" is equivalent to a degree of freedom     Monte Carlo methods provide a way out of this exponential increase in computation time  As long as the function in question is reasonably well-behaved  it can be estimated by randomly selecting points in 100-dimensional space  and taking some kind of average of the function values at these points  By the central limit theorem  this method displays                               1                      /                                              N                                               displaystyle   scriptstyle 1/   sqrt  N      convergencei e   quadrupling the number of sampled points halves the error  regardless of the number of dimensions  99     A refinement of this method  known as importance sampling in statistics  involves sampling the points randomly  but more frequently where the integrand is large  To do this precisely one would have to already know the integral  but one can approximate the integral by an integral of a similar function or use adaptive routines such as stratified sampling  recursive stratified sampling  adaptive umbrella sampling 100  101  or the VEGAS algorithm     A similar approach  the quasi-Monte Carlo method  uses low-discrepancy sequences  These sequences \"fill\" the area better and sample the most important points more frequently  so quasi-Monte Carlo methods can often converge on the integral more quickly     Another class of methods for sampling points in a volume is to simulate random walks over it  Markov chain Monte Carlo   Such methods include the MetropolisHastings algorithm  Gibbs sampling  Wang and Landau algorithm  and interacting type MCMC methodologies such as the sequential Monte Carlo samplers  102     Another powerful and very popular application for random numbers in numerical simulation is in numerical optimization  The problem is to minimize  or maximize  functions of some vector that often has many dimensions  Many problems can be phrased in this way  for example  a computer chess program could be seen as trying to find the set of  say  10 moves that produces the best evaluation function at the end  In the traveling salesman problem the goal is to minimize distance traveled  There are also applications to engineering design  such as multidisciplinary design optimization  It has been applied with quasi-one-dimensional models to solve particle dynamics problems by efficiently exploring large configuration space  Reference 103  is a comprehensive review of many issues related to simulation and optimization     The traveling salesman problem is what is called a conventional optimization problem  That is  all the facts  distances between each destination point  needed to determine the optimal path to follow are known with certainty and the goal is to run through the possible travel choices to come up with the one with the lowest total distance  However  let  s assume that instead of wanting to minimize the total distance traveled to visit each desired destination  we wanted to minimize the total time needed to reach each destination  This goes beyond conventional optimization since travel time is inherently uncertain  traffic jams  time of day  etc    As a result  to determine our optimal path we would want to use simulation - optimization to first understand the range of potential times it could take to go from one point to another  represented by a probability distribution in this case rather than a specific distance  and then optimize our travel decisions to identify the best path to follow taking that uncertainty into account     Probabilistic formulation of inverse problems leads to the definition of a probability distribution in the model space  This probability distribution combines prior information with new information obtained by measuring some observable parameters  data   nAs  in the general case  the theory linking data with model parameters is nonlinear  the posterior probability in the model space may not be easy to describe  it may be multimodal  some moments may not be defined  etc       When analyzing an inverse problem  obtaining a maximum likelihood model is usually not sufficient  as we normally also wish to have information on the resolution power of the data  In the general case we may have many model parameters  and an inspection of the marginal probability densities of interest may be impractical  or even useless  But it is possible to pseudorandomly generate a large collection of models according to the posterior probability distribution and to analyze and display the models in such a way that information on the relative likelihoods of model properties is conveyed to the spectator  This can be accomplished by means of an efficient Monte Carlo method  even in cases where no explicit formula for the a priori distribution is available     The best-known importance sampling method  the Metropolis algorithm  can be generalized  and this gives a method that allows analysis of  possibly highly nonlinear  inverse problems with complex a priori information and data with an arbitrary noise distribution  104  105     Popular exposition of the Monte Carlo Method was conducted by McCracken  106  Method  s general philosophy was discussed by Elishakoff 107  and Grne-Yanoff and Weirich  108         Continuum mechanics is a branch of mechanics that deals with the mechanical behavior of materials modeled as a continuous mass rather than as discrete particles  The French mathematician Augustin-Louis Cauchy was the first to formulate such models in the 19th century     Modeling an object as a continuum assumes that the substance of the object completely fills the space it occupies  Modeling objects in this way ignores the fact that matter is made of atoms  and so is not continuous  however  on length scales much greater than that of inter-atomic distances  such models are highly accurate  Fundamental physical laws such as the conservation of mass  the conservation of momentum  and the conservation of energy may be applied to such models to derive differential equations describing the behavior of such objects  and some information about the material under investigation is added through constitutive relations     Continuum mechanics deals with physical properties of solids and fluids which are independent of any particular coordinate system in which they are observed  These physical properties are then represented by tensors  which are mathematical objects that have the required property of being independent of coordinate system  These tensors can be expressed in coordinate systems for computational convenience     Materials  such as solids  liquids and gases  are composed of molecules separated by space  On a microscopic scale  materials have cracks and discontinuities  However  certain physical phenomena can be modeled assuming the materials exist as a continuum  meaning the matter in the body is continuously distributed and fills the entire region of space it occupies  A continuum is a body that can be continually sub-divided into infinitesimal elements with properties being those of the bulk material     The validity of the continuum assumption may be verified by a theoretical analysis  in which either some clear periodicity is identified or statistical homogeneity and ergodicity of the microstructure exists  More specifically  the continuum hypothesis/assumption hinges on the concepts of a representative elementary volume and separation of scales based on the HillMandel condition  This condition provides a link between an experimentalist  s and a theoretician  s viewpoint on constitutive equations  linear and nonlinear elastic/inelastic or coupled fields  as well as a way of spatial and statistical averaging of the microstructure  1  page xa0needed     When the separation of scales does not hold  or when one wants to establish a continuum of a finer resolution than that of the representative volume element  RVE  size  one employs a statistical volume element  SVE   which  in turn  leads to random continuum fields  The latter then provide a micromechanics basis for stochastic finite elements  SFE   The levels of SVE and RVE link continuum mechanics to statistical mechanics  The RVE may be assessed only in a limited way via experimental testing  when the constitutive response becomes spatially homogeneous     Specifically for fluids  the Knudsen number is used to assess to what extent the approximation of continuity can be made     Consider car traffic on a highway  with just one lane for simplicity  nSomewhat surprisingly  and in a tribute to its effectiveness  continuum mechanics effectively models the movement of cars via a partial differential equation  PDE  for the density of cars  nThe familiarity of this situation empowers us to understand a little of the continuum-discrete dichotomy underlying continuum modelling in general     To start modelling define that                      x                 displaystyle x    measures distance  in km  along the highway                      t                 displaystyle t    is time  in minutes                                        x                 t                          displaystyle   rho  x t     is the density of cars on the highway  in cars/km in the lane   and                     u                 x                 t                          displaystyle u x t     is the flow velocity  average velocity  of those cars   at   position                     x                 displaystyle x        Cars do not appear and disappear  nConsider any group of cars  from the particular car at the back of the group located at                     x        =        a                 t                          displaystyle x=a t     to the particular car at the front located at                     x        =        b                 t                          displaystyle x=b t      nThe total number of cars in this group                     N        =                                        a                         t                                             b                         t                                                        x                 t                         d        x                 textstyle N=  int _ a t  ^ b t    rho  x t    dx     nSince cars are conserved  if there is  overtaking  then the   car at the front / back   may become a different car                      d        N                  /                d        t        =        0                 displaystyle dN/dt=0     nBut via the Leibniz integral rule    This integral being zero holds for all groups  that is  for all intervals                              a                 b                          displaystyle  a b      nThe only way an integral can be zero for all intervals is if the integrand is zero for all                     x                 displaystyle x     nConsequently  conservation derives the first order nonlinear conservation PDE    for all positions on the highway     This conservation PDE applies not only to car traffic but also to fluids  solids  crowds  animals  plants  bushfires  financial traders  and so on     The previous PDE is one equation with two unknowns  so another equation is needed to form a well-posed problem  Such an extra equation is typically needed in continuum mechanics and typically comes from experiments  For car traffic it is well established that cars typically travel at a speed depending upon density                      u        =        V                                           displaystyle u=V   rho      for some experimentally determined function                     V                 displaystyle V    that is a decreasing function of density  For example  experiments in the Lincoln Tunnel found that a good fit  except at low density  is obtained by                     u        =        V                                  =        27 5        ln         u2061                 142                  /                                          displaystyle u=V   rho  =27 5  ln 142/  rho       km/hr for density in cars/km   2  page xa0needed     Thus the basic continuum model for car traffic is the PDE    for the car density                                      x                 t                          displaystyle   rho  x t     on the highway     An additional area of continuum mechanics comprises elastomeric foams  which exhibit a curious hyperbolic stress-strain relationship   The elastomer is a true continuum  but a homogeneous distribution of voids gives it unusual properties  3     Continuum mechanics models begin by assigning a region in three-dimensional Euclidean space to the material body                                           B                                   displaystyle    mathcal  B      being modeled  The points within this region are called particles or material points  Different configurations or states of the body correspond to different regions in Euclidean space  The region corresponding to the body  s configuration at time                     t                 displaystyle t    is labeled                                                     t                                                         B                                            displaystyle   kappa _ t     mathcal  B           A particular particle within the body in a particular configuration is characterized by a position vector     where                                           e                                i                                   displaystyle   mathbf  e  _ i     are the coordinate vectors in some frame of reference chosen for the problem  See figure 1   This vector can be expressed as a function of the particle position                               X                         displaystyle   mathbf  X      in some reference configuration  for example the configuration at the initial time  so that    This function needs to have various properties so that the model makes physical sense                                                      t                                                             displaystyle   kappa _ t    cdot      needs to be     For the mathematical formulation of the model                                                      t                                                             displaystyle   kappa _ t    cdot      is also assumed to be twice continuously differentiable  so that differential equations describing the motion may be formulated     Continuum mechanics deals with deformable bodies  as opposed to rigid bodies  A solid is a deformable body that possesses shear strength  sc  a solid can support shear forces  forces parallel to the material surface on which they act   Fluids  on the other hand  do not sustain shear forces  For the study of the mechanical behavior of solids and fluids these are assumed to be continuous bodies  which means that the matter fills the entire region of space it occupies  despite the fact that matter is made of atoms  has voids  and is discrete  Therefore  when continuum mechanics refers to a point or particle in a continuous body it does not describe a point in the interatomic space or an atomic particle  rather an idealized part of the body occupying that point     Following the classical dynamics of Newton and Euler  the motion of a material body is produced by the action of externally applied forces which are assumed to be of two kinds  surface forces                                           F                                C                                   displaystyle   mathbf  F  _ C     and body forces                                           F                                B                                   displaystyle   mathbf  F  _ B      4  Thus  the total force                                           F                                   displaystyle    mathcal  F      applied to a body or to a portion of the body can be expressed as     Surface forces or contact forces  expressed as force per unit area  can act either on the bounding surface of the body  as a result of mechanical contact with other bodies  or on imaginary internal surfaces that bound portions of the body  as a result of the mechanical interaction between the parts of the body to either side of the surface  Euler-Cauchy  s stress principle   When a body is acted upon by external contact forces  internal contact forces are then transmitted from point to point inside the body to balance their action  according to Newton  s third law of motion of conservation of linear momentum and angular momentum  for continuous bodies these laws are called the Euler  s equations of motion   The internal contact forces are related to the body  s deformation through constitutive equations  The internal contact forces may be mathematically described by how they relate to the motion of the body  independent of the body  s material makeup  5  full citation needed     The distribution of internal contact forces throughout the volume of the body is assumed to be continuous  Therefore  there exists a contact force density or Cauchy traction field 6  full citation needed                                T                                   n                                   x                         t                          displaystyle   mathbf  T     mathbf  n     mathbf  x   t     that represents this distribution in a particular configuration of the body at a given time                     t                                 displaystyle t     !     It is not a vector field because it depends not only on the position                               x                         displaystyle   mathbf  x      of a particular material point  but also on the local orientation of the surface element as defined by its normal vector                               n                         displaystyle   mathbf  n       7  page xa0needed     Any differential area                     d        S                                 displaystyle dS     !    with normal vector                               n                         displaystyle   mathbf  n      of a given internal surface area                     S                                 displaystyle S     !     bounding a portion of the body  experiences a contact force                     d                              F                                C                                                   displaystyle d  mathbf  F  _ C      !    arising from the contact between both portions of the body on each side of                     S                                 displaystyle S     !     and it is given by    where                                           T                                                           n                                                            displaystyle   mathbf  T  ^    mathbf  n        is the surface traction  8  full citation needed  also called stress vector  9  full citation needed  traction  10  page xa0needed  or traction vector  11  full citation needed  The stress vector is a frame-indifferent vector  see Euler-Cauchy  s stress principle      The total contact force on the particular internal surface                     S                                 displaystyle S     !    is then expressed as the sum  surface integral  of the contact forces on all differential surfaces                     d        S                                 displaystyle dS     !        In continuum mechanics a body is considered stress-free if the only forces present are those inter-atomic forces  ionic  metallic  and van der Waals forces  required to hold the body together and to keep its shape in the absence of all external influences  including gravitational attraction  11  full citation needed  12  full citation needed  Stresses generated during manufacture of the body to a specific configuration are also excluded when considering stresses in a body  Therefore  the stresses considered in continuum mechanics are only those produced by deformation of the body  sc  only relative changes in stress are considered  not the absolute values of stress     Body forces are forces originating from sources outside of the body 13  full citation needed  that act on the volume  or mass  of the body  Saying that body forces are due to outside sources implies that the interaction between different parts of the body  internal forces  are manifested through the contact forces alone  8  full citation needed  These forces arise from the presence of the body in force fields  e g  gravitational field  gravitational forces  or electromagnetic field  electromagnetic forces   or from inertial forces when bodies are in motion  As the mass of a continuous body is assumed to be continuously distributed  any force originating from the mass is also continuously distributed  Thus  body forces are specified by vector fields which are assumed to be continuous over the entire volume of the body  14  full citation needed  i e  acting on every point in it  Body forces are represented by a body force density                               b                                   x                         t                          displaystyle   mathbf  b     mathbf  x   t      per unit of mass   which is a frame-indifferent vector field     In the case of gravitational forces  the intensity of the force depends on  or is proportional to  the mass density                                                                  x                         t                                          displaystyle   mathbf    rho      mathbf  x   t      !    of the material  and it is specified in terms of force per unit mass                                b                      i                                                   displaystyle b_ i      !     or per unit volume                                p                      i                                                   displaystyle p_ i      !      These two specifications are related through the material density by the equation                                       b                      i                          =                  p                      i                                                   displaystyle   rho b_ i =p_ i      !     Similarly  the intensity of electromagnetic forces depends upon the strength  electric charge  of the electromagnetic field     The total body force applied to a continuous body is expressed as    Body forces and contact forces acting on the body lead to corresponding moments of force  torques  relative to a given point  Thus  the total applied torque                                           M                                   displaystyle    mathcal  M      about the origin is given by    In certain situations  not commonly considered in the analysis of the mechanical behavior of materials  it becomes necessary to include two other types of forces  these are couple stresses note 1  note 2   surface couples  13  full citation needed  contact torques  14  full citation needed  and body moments  Couple stresses are moments per unit area applied on a surface  Body moments  or body couples  are moments per unit volume or per unit mass applied to the volume of the body  Both are important in the analysis of stress for a polarized dielectric solid under the action of an electric field  materials where the molecular structure is taken into consideration  e g  bones   solids under the action of an external magnetic field  and the dislocation theory of metals  9  full citation needed  10  page xa0needed  13  full citation needed     Materials that exhibit body couples and couple stresses in addition to moments produced exclusively by forces are called polar materials  10  page xa0needed  14  full citation needed  Non-polar materials are then those materials with only moments of forces  In the classical branches of continuum mechanics the development of the theory of stresses is based on non-polar materials     Thus  the sum of all applied forces and torques  with respect to the origin of the coordinate system  in the body can be given by    A change in the configuration of a continuum body results in a displacement  The displacement of a body has two components  a rigid-body displacement and a deformation  A rigid-body displacement consists of a simultaneous translation and rotation of the body without changing its shape or size  Deformation implies the change in shape and/or size of the body from an initial or undeformed configuration                                                     0                                                         B                                            displaystyle   kappa _ 0     mathcal  B       to a current or deformed configuration                                                     t                                                         B                                            displaystyle   kappa _ t     mathcal  B        Figure 2      The motion of a continuum body is a continuous time sequence of displacements  Thus  the material body will occupy different configurations at different times so that a particle occupies a series of points in space which describe a path line     There is continuity during motion or deformation of a continuum body in the sense that     It is convenient to identify a reference configuration or initial condition which all subsequent configurations are referenced from  The reference configuration need not be one that the body will ever occupy  Often  the configuration at                     t        =        0                 displaystyle t=0    is considered the reference configuration                                                      0                                                         B                                            displaystyle   kappa _ 0     mathcal  B        The components                               X                      i                                   displaystyle X_ i     of the position vector                               X                         displaystyle   mathbf  X      of a particle  taken with respect to the reference configuration  are called the material or reference coordinates     When analyzing the motion or deformation of solids  or the flow of fluids  it is necessary to describe the sequence or evolution of configurations throughout time  One description for motion is made in terms of the material or referential coordinates  called material description or Lagrangian description     In the Lagrangian description the position and physical properties of the particles are described in terms of the material or referential coordinates and time  In this case the reference configuration is the configuration at                     t        =        0                 displaystyle t=0     An observer standing in the frame of reference observes the changes in the position and physical properties as the material body moves in space as time progresses  The results obtained are independent of the choice of initial time and reference configuration                                                      0                                                         B                                            displaystyle   kappa _ 0     mathcal  B        This description is normally used in solid mechanics     In the Lagrangian description  the motion of a continuum body is expressed by the mapping function                                                                displaystyle   chi    cdot       Figure 2      which is a mapping of the initial configuration                                                     0                                                         B                                            displaystyle   kappa _ 0     mathcal  B       onto the current configuration                                                     t                                                         B                                            displaystyle   kappa _ t     mathcal  B        giving a geometrical correspondence between them  i e  giving the position vector                               x                =                  x                      i                                                e                                i                                   displaystyle   mathbf  x  =x_ i   mathbf  e  _ i     that a particle                     X                 displaystyle X     with a position vector                               X                         displaystyle   mathbf  X      in the undeformed or reference configuration                                                     0                                                         B                                            displaystyle   kappa _ 0     mathcal  B        will occupy in the current or deformed configuration                                                     t                                                         B                                            displaystyle   kappa _ t     mathcal  B       at time                     t                 displaystyle t     The components                               x                      i                                   displaystyle x_ i     are called the spatial coordinates     Physical and kinematic properties                               P                      i            j                                               displaystyle P_ ij  ldots       i e  thermodynamic properties and flow velocity  which describe or characterize features of the material body  are expressed as continuous functions of position and time  i e                                P                      i            j                                      =                  P                      i            j                                                         X                         t                          displaystyle P_ ij  ldots  =P_ ij  ldots     mathbf  X   t         The material derivative of any property                               P                      i            j                                               displaystyle P_ ij  ldots      of a continuum  which may be a scalar  vector  or tensor  is the time rate of change of that property for a specific group of particles of the moving continuum body  The material derivative is also known as the substantial derivative  or comoving derivative  or convective derivative  It can be thought as the rate at which the property changes when measured by an observer traveling with that group of particles     In the Lagrangian description  the material derivative of                               P                      i            j                                               displaystyle P_ ij  ldots      is simply the partial derivative with respect to time  and the position vector                               X                         displaystyle   mathbf  X      is held constant as it does not change with time  Thus  we have    The instantaneous position                               x                         displaystyle   mathbf  x      is a property of a particle  and its material derivative is the instantaneous flow velocity                               v                         displaystyle   mathbf  v      of the particle  Therefore  the flow velocity field of the continuum is given by    Similarly  the acceleration field is given by    Continuity in the Lagrangian description is expressed by the spatial and temporal continuity of the mapping from the reference configuration to the current configuration of the material points  All physical quantities characterizing the continuum are described this way  In this sense  the function                                                                displaystyle   chi    cdot      and                               P                      i            j                                                                         displaystyle P_ ij  ldots     cdot      are single-valued and continuous  with continuous derivatives with respect to space and time to whatever order is required  usually to the second or third     Continuity allows for the inverse of                                                                displaystyle   chi    cdot      to trace backwards where the particle currently located at                               x                         displaystyle   mathbf  x      was located in the initial or referenced configuration                                                     0                                                         B                                            displaystyle   kappa _ 0     mathcal  B        In this case the description of motion is made in terms of the spatial coordinates  in which case is called the spatial description or Eulerian description  i e  the current configuration is taken as the reference configuration     The Eulerian description  introduced by d  Alembert  focuses on the current configuration                                                     t                                                         B                                            displaystyle   kappa _ t     mathcal  B        giving attention to what is occurring at a fixed point in space as time progresses  instead of giving attention to individual particles as they move through space and time  This approach is conveniently applied in the study of fluid flow where the kinematic property of greatest interest is the rate at which change is taking place rather than the shape of the body of fluid at a reference time  17     Mathematically  the motion of a continuum using the Eulerian description is expressed by the mapping function    which provides a tracing of the particle which now occupies the position                               x                         displaystyle   mathbf  x      in the current configuration                                                     t                                                         B                                            displaystyle   kappa _ t     mathcal  B       to its original position                               X                         displaystyle   mathbf  X      in the initial configuration                                                     0                                                         B                                            displaystyle   kappa _ 0     mathcal  B           A necessary and sufficient condition for this inverse function to exist is that the determinant of the Jacobian Matrix  often referred to simply as the Jacobian  should be different from zero  Thus     In the Eulerian description  the physical properties                               P                      i            j                                               displaystyle P_ ij  ldots      are expressed as    where the functional form of                               P                      i            j                                               displaystyle P_ ij  ldots      in the Lagrangian description is not the same as the form of                               p                      i            j                                               displaystyle p_ ij  ldots      in the Eulerian description     The material derivative of                               p                      i            j                                                         x                         t                          displaystyle p_ ij  ldots     mathbf  x   t      using the chain rule  is then    The first term on the right-hand side of this equation gives the local rate of change of the property                               p                      i            j                                                         x                         t                          displaystyle p_ ij  ldots     mathbf  x   t     occurring at position                               x                         displaystyle   mathbf  x       The second term of the right-hand side is the convective rate of change and expresses the contribution of the particle changing position in space  motion      Continuity in the Eulerian description is expressed by the spatial and temporal continuity and continuous differentiability of the flow velocity field  All physical quantities are defined this way at each instant of time  in the current configuration  as a function of the vector position                               x                         displaystyle   mathbf  x          The vector joining the positions of a particle                     P                 displaystyle P    in the undeformed configuration and deformed configuration is called the displacement vector                               u                                   X                         t                 =                  u                      i                                                e                                i                                   displaystyle   mathbf  u     mathbf  X   t =u_ i   mathbf  e  _ i      in the Lagrangian description  or                               U                                   x                         t                 =                  U                      J                                                E                                J                                   displaystyle   mathbf  U     mathbf  x   t =U_ J   mathbf  E  _ J      in the Eulerian description     A displacement field is a vector field of all displacement vectors for all particles in the body  which relates the deformed configuration with the undeformed configuration  It is convenient to do the analysis of deformation or motion of a continuum body in terms of the displacement field   In general  the displacement field is expressed in terms of the material coordinates as    or in terms of the spatial coordinates as    where                                                     J            i                                   displaystyle   alpha _ Ji     are the direction cosines between the material and spatial coordinate systems with unit vectors                                           E                                J                                   displaystyle   mathbf  E  _ J     and                                           e                                i                                   displaystyle   mathbf  e  _ i      respectively  Thus    and the relationship between                               u                      i                                   displaystyle u_ i     and                               U                      J                                   displaystyle U_ J     is then given by    Knowing that    then    It is common to superimpose the coordinate systems for the undeformed and deformed configurations  which results in                               b                =        0                 displaystyle   mathbf  b  =0      and the direction cosines become Kronecker deltas  i e     Thus  we have    or in terms of the spatial coordinates as    Continuum mechanics deals with the behavior of materials that can be approximated as continuous for certain length and time scales   The equations that govern the mechanics of such materials include the balance laws for mass  momentum  and energy  Kinematic relations and constitutive equations are needed to complete the system of governing equations   Physical restrictions on the form of the constitutive relations can be applied by requiring that the second law of thermodynamics be satisfied under all conditions   In the continuum mechanics of solids  the second law of thermodynamics is satisfied if the ClausiusDuhem form of the entropy inequality is satisfied     The balance laws express the idea that the rate of change of a quantity  mass  momentum  energy  in a volume must arise from three causes     Let                                      displaystyle   Omega     be the body  an open subset of Euclidean space  and let                                                displaystyle   partial   Omega     be its surface  the boundary of                                      displaystyle   Omega            Let the motion of material points in the body be described by the map    where                               X                         displaystyle   mathbf  X      is the position of a point in the initial configuration and                               x                         displaystyle   mathbf  x      is the location of the same point in the deformed configuration     The deformation gradient is given by    Let                     f                           x                         t                          displaystyle f   mathbf  x   t     be a physical quantity that is flowing through the body   Let                     g                           x                         t                          displaystyle g   mathbf  x   t     be sources on the surface of the body and let                     h                           x                         t                          displaystyle h   mathbf  x   t     be sources inside the body   Let                               n                                   x                         t                          displaystyle   mathbf  n     mathbf  x   t     be the outward unit normal to the surface                                              displaystyle   partial   Omega       Let                               v                                   x                         t                          displaystyle   mathbf  v     mathbf  x   t     be the flow velocity of the physical particles that carry the physical quantity that is flowing   Also  let the speed at which the bounding surface                                              displaystyle   partial   Omega     is moving be                               u                      n                                   displaystyle u_ n      in the direction                               n                         displaystyle   mathbf  n           Then  balance laws can be expressed in the general form    The functions                     f                           x                         t                          displaystyle f   mathbf  x   t                          g                           x                         t                          displaystyle g   mathbf  x   t      and                     h                           x                         t                          displaystyle h   mathbf  x   t     can be scalar valued  vector valued  or tensor valued - depending on the physical quantity that the balance equation deals with   If there are internal boundaries in the body  jump discontinuities also need to be specified in the balance laws     If we take the Eulerian point of view  it can be shown that the balance laws of mass  momentum  and energy for a solid can be written as  assuming the source term is zero for the mass and angular momentum equations     In the above equations                                                x                         t                          displaystyle   rho    mathbf  x   t     is the mass density  current                                                                                                                          displaystyle    dot    rho       is the material time derivative of                                      displaystyle   rho                                    v                                   x                         t                          displaystyle   mathbf  v     mathbf  x   t     is the particle velocity                                                                          v                                                                           displaystyle    dot    mathbf  v        is the material time derivative of                               v                         displaystyle   mathbf  v                                                                        x                         t                          displaystyle    boldsymbol    sigma      mathbf  x   t     is the Cauchy stress tensor                                b                                   x                         t                          displaystyle   mathbf  b     mathbf  x   t     is the body force density                      e                           x                         t                          displaystyle e   mathbf  x   t     is the internal energy per unit mass                                                          e                                                             displaystyle    dot  e      is the material time derivative of                     e                 displaystyle e                                   q                                   x                         t                          displaystyle   mathbf  q     mathbf  x   t     is the heat flux vector  and                     s                           x                         t                          displaystyle s   mathbf  x   t     is an energy source per unit mass     With respect to the reference configuration  the Lagrangian point of view   the balance laws can be written as    In the above                                P                         displaystyle    boldsymbol  P      is the first Piola-Kirchhoff stress tensor  and                                                       0                                   displaystyle   rho _ 0     is the mass density in the reference configuration   The first Piola-Kirchhoff stress tensor is related to the Cauchy stress tensor by    We can alternatively define the nominal stress tensor                               N                         displaystyle    boldsymbol  N      which is the transpose of the first Piola-Kirchhoff stress tensor such that    Then the balance laws become    The operators in the above equations are defined as such that    where                               v                         displaystyle   mathbf  v      is a vector field                                S                         displaystyle    boldsymbol  S      is a second-order tensor field  and                                           e                                i                                   displaystyle   mathbf  e  _ i     are the components of an orthonormal basis in the current configuration   Also     where                               v                         displaystyle   mathbf  v      is a vector field                                S                         displaystyle    boldsymbol  S      is a second-order tensor field  and                                           E                                i                                   displaystyle   mathbf  E  _ i     are the components of an orthonormal basis in the reference configuration     The inner product is defined as    The ClausiusDuhem inequality can be used to express the second law of thermodynamics for elastic-plastic materials   This inequality is a statement concerning the irreversibility of natural processes  especially when energy dissipation is involved     Just like in the balance laws in the previous section  we assume that there is a flux of a quantity  a source of the quantity  and an internal density of the quantity per unit mass   The quantity of interest in this case is the entropy   Thus  we assume that there is an entropy flux  an entropy source  an internal mass density                                      displaystyle   rho     and an internal specific entropy  i e  entropy per unit mass                                       displaystyle   eta     in the region of interest     Let                                      displaystyle   Omega     be such a region and let                                              displaystyle   partial   Omega     be its boundary   Then the second law of thermodynamics states that the rate of increase of                                       displaystyle   eta     in this region is greater than or equal to the sum of that supplied to                                      displaystyle   Omega      as a flux or from internal sources  and the change of the internal entropy density                                              displaystyle   rho   eta     due to material flowing in and out of the region     Let                                              displaystyle   partial   Omega     move with a flow velocity                               u                      n                                   displaystyle u_ n     and let particles inside                                      displaystyle   Omega     have velocities                               v                         displaystyle   mathbf  v        Let                               n                         displaystyle   mathbf  n      be the unit outward normal to the surface                                              displaystyle   partial   Omega       Let                                      displaystyle   rho     be the density of matter in the region                                                          q                                                             displaystyle    bar  q      be the entropy flux at the surface  and                     r                 displaystyle r    be the entropy source per unit mass    nThen the entropy inequality may be written as    The scalar entropy flux can be related to the vector flux at the surface by the relation                                                         q                                                    =                                                             x                                           n                         displaystyle    bar  q  =-   boldsymbol    psi      mathbf  x     cdot   mathbf  n        Under the assumption  of incrementally isothermal conditions  we have    where                               q                         displaystyle   mathbf  q      is the heat flux vector                      s                 displaystyle s    is an energy source per unit mass  and                     T                 displaystyle T    is the absolute temperature of a material point at                               x                         displaystyle   mathbf  x      at time                     t                 displaystyle t        We then have the ClausiusDuhem inequality in integral form     We can show that the entropy inequality may be written in differential form as    In terms of the Cauchy stress and the internal energy  the ClausiusDuhem inequality may be written as     Stochastic  from Greek    stkhos  xa0  aim  guess   1   refers to the property of being well described by a random probability distribution  1  Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves  these two terms are often used synonymously  Furthermore  in probability theory  the formal concept of a stochastic process is also referred to as a random process  2  3  4  5  6     Stochasticity is used in many different fields  including the natural sciences such as biology  7  chemistry  8  ecology  9  neuroscience  10  and physics  11  as well as technology and engineering fields such as image processing  signal processing  12  information theory  13  computer science  14  cryptography  15  and telecommunications  16  It is also used in finance  due to seemingly random changes in financial markets 17  18  19  as well as in medicine  linguistics   music  media  colour theory   botany  manufacturing  and geomorphology  Stochastic modeling is also used in social science     The word stochastic in English was originally used as an adjective with the definition \"pertaining to conjecturing\"  and stemming from a Greek word meaning \"to aim at a mark   guess\"  and the Oxford English Dictionary gives the year 1662 as its earliest occurrence  1  In his work on probability Ars Conjectandi  originally published in Latin in 1713  Jakob Bernoulli  used the phrase \"Ars Conjectandi sive Stochastice\"  which has been translated to  \"the art of conjecturing or stochastics\"  20  This phrase was used  with reference to Bernoulli  by Ladislaus Bortkiewicz  21  who in 1917 wrote in German the word stochastik with a sense meaning random   The term stochastic process first appeared in English in a 1934 paper by  Joseph Doob  1  For the term and a specific mathematical definition  Doob cited another 1934 paper  where the term stochastischer Proze  was used in German by Aleksandr Khinchin  22  23  though the German term had been used earlier in 1931 by Andrey Kolmogorov  24     In the early 1930s  Aleksandr Khinchin gave the first mathematical definition of a stochastic process as a family of random variables indexed by the real line  25  22  a  Further fundamental work on probability theory and stochastic processes was done by Khinchin as well as other mathematicians such as Andrey Kolmogorov  Joseph Doob  William Feller  Maurice Frchet  Paul Lvy  Wolfgang Doeblin  and Harald Cramr  27  28  Decades later Cramr referred to the 1930s as the \"heroic period of mathematical probability theory\"  28     In mathematics  the theory of stochastic processes is considered to be an important contribution to  probability theory   29  and continues to be an active topic of research for  both theoretical reasons and applications  30  31  32     The word stochastic is used to describe other terms and objects in mathematics  Examples include a stochastic matrix  which describes a stochastic process known as a Markov process  and stochastic calculus  which involves differential equations and  integrals based on stochastic processes such as the Wiener process  also called the Brownian motion process     One of the simplest continuous-time stochastic processes is Brownian motion  This was first observed by botanist Robert Brown while looking through a microscope at pollen grains in water     The Monte Carlo method is a stochastic method popularized by physics researchers Stanisaw Ulam  Enrico Fermi  John von Neumann  and Nicholas Metropolis  33  The use of randomness and the repetitive nature of the process are analogous to the activities conducted at a casino  nMethods of simulation and statistical sampling generally did the opposite  using simulation to test a previously understood deterministic problem  Though examples of an \"inverted\" approach do exist historically  they were not considered a general method until the popularity of the Monte Carlo method spread     Perhaps the most famous early use was by Enrico Fermi in 1930  when he used a random method to calculate the properties of the newly discovered neutron  Monte Carlo methods were central to the simulations required for the Manhattan Project  though they were severely limited by the computational tools of the time  Therefore  it was only after electronic computers were first built  from 1945 on  that Monte Carlo methods began to be studied in depth  In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb  and became popularized in the fields of physics  physical chemistry  and operations research  The RAND Corporation and the U S  Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time  and they began to find a wide application in many different fields     Uses of Monte Carlo methods require large amounts of random numbers  and it was their use that spurred the development of pseudorandom number generators  which were far quicker to use than the tables of random numbers which had been previously used for statistical sampling     Stochastic resonance  In biological systems  introducing stochastic \"noise\" has been found to help improve the signal strength of the internal feedback loops for balance and other vestibular communication  34  It has been found to help diabetic and stroke patients with balance control  35  Many biochemical events also lend themselves to stochastic analysis  Gene expression  for example  has a stochastic component through the molecular collisionsas during binding and unbinding of RNA polymerase to a gene promotervia the solution  s Brownian motion     Simonton  2003  Psych Bulletin  argues that creativity in science  of scientists  is a constrained stochastic behaviour such that new theories in all sciences are  at least in part  the product of a stochastic process     Stochastic ray tracing is the application of Monte Carlo simulation to the computer graphics ray tracing algorithm  \"Distributed ray tracing samples the integrand at many randomly chosen points and averages the results to obtain a better approximation  It is essentially an application of the Monte Carlo method to 3D computer graphics  and for this reason is also called Stochastic ray tracing \" citation needed     Stochastic forensics analyzes computer crime by viewing computers as stochastic processes     In artificial intelligence  stochastic programs work by using probabilistic methods to solve problems  as in simulated annealing  stochastic neural networks  stochastic optimization  genetic algorithms  and genetic programming  A problem itself may be stochastic as well  as in planning under uncertainty     The financial markets use stochastic models to represent the seemingly random behaviour of assets such as stocks  commodities  relative currency prices  i e   the price of one currency compared to that of another  such as the price of US Dollar compared to that of the Euro   and interest rates  These models are then used by quantitative analysts to value options on stock prices  bond prices  and on interest rates  see Markov models  Moreover  it is at the heart of the insurance industry     The formation of river meanders has been analyzed as a stochastic process    Non-deterministic approaches in language studies are largely inspired by the work of Ferdinand de Saussure  for example  in functionalist linguistic theory  which argues that competence is based on performance  36  37  This distinction in functional theories of grammar should be carefully distinguished from the langue and parole distinction  To the extent that linguistic knowledge is constituted by experience with language  grammar is argued to be probabilistic and variable rather than fixed and absolute   This conception of grammar as probabilistic and variable follows from the idea that one  s competence changes in accordance with one  s experience with language  Though this conception has been contested  38  it has also provided the foundation for modern statistical natural language processing 39  and for theories of language learning and change  40     Manufacturing processes are assumed to be stochastic processes   This assumption is largely valid for either continuous or batch manufacturing processes   Testing and monitoring of the process is recorded using a process control chart which plots a given process control parameter over time   Typically a dozen or many more parameters will be tracked simultaneously   Statistical models are used to define limit lines which define when corrective actions must be taken to bring the process back to its intended operational window     This same approach is used in the service industry where parameters are replaced by processes related to service level agreements     The marketing and the changing movement of audience tastes and preferences  as well as the solicitation of and the scientific appeal of certain film and television debuts  i e   their opening weekends  word-of-mouth  top-of-mind knowledge among surveyed groups  star name recognition and other elements of social media outreach and advertising   are determined in part by stochastic modeling  A recent attempt at repeat business analysis was done by Japanese scholars citation needed  and is part of the Cinematic Contagion Systems patented by Geneva Media Holdings  and such modeling has been used in data collection from the time of the original Nielsen ratings to modern studio and television test audiences     Stochastic effect  or \"chance effect\" is one classification of radiation effects that refers to the random  statistical nature of the damage  In contrast to the deterministic effect  severity is independent of dose  Only the probability of an effect increases with dose     In music  mathematical processes based on probability can generate stochastic elements     Stochastic processes may be used in music to compose a fixed piece or may be produced in performance  Stochastic music was pioneered by Iannis Xenakis  who coined the term stochastic music  Specific examples of mathematics  statistics  and physics applied to music composition are the use of the statistical mechanics of gases in Pithoprakta  statistical distribution of points on a plane in Diamorphoses  minimal constraints in Achorripsis  the normal distribution in ST/10 and Atres  Markov chains in Analogiques  game theory in Duel and Stratgie  group theory in Nomos Alpha  for Siegfried Palm   set theory in Herma and Eonta  41  and Brownian motion in N  Shima  citation needed  Xenakis frequently used computers to produce his scores  such as the ST series including Morsima-Amorsima and Atres  and founded CEMAMu  Earlier  John Cage and others had composed aleatoric or indeterminate music  which is created by chance processes but does not have the strict mathematical basis  Cage  s Music of Changes  for example  uses a system of charts based on the I-Ching   Lejaren Hiller and Leonard Issacson used generative grammars and Markov chains in their 1957 Illiac Suite  Modern electronic music production techniques make these processes relatively simple to implement  and many hardware devices such as synthesizers and drum machines incorporate randomization features  Generative music techniques are therefore readily accessible to composers  performers  and producers     Stochastic social science theory is similar to systems theory in that events are interactions of systems  although with a marked emphasis on unconscious processes   The event creates its own conditions of possibility  rendering it unpredictable if simply for the number of variables involved   Stochastic social science theory can be seen as an elaboration of a kind of   third axis   in which to situate human behavior alongside the traditional   nature vs  nurture   opposition   See Julia Kristeva on her usage of the   semiotic    Luce Irigaray on reverse Heideggerian epistemology  and Pierre Bourdieu on polythetic space for examples of stochastic social science theory  citation needed     The term \"Stochastic Terrorism\" has fallen into frequent use 42  with regard to lone wolf terrorism  The terms \"Scripted Violence\" and \"Stochastic Terrorism\" are linked in a \"cause <> effect\" relationship  \"Scripted Violence\" rhetoric can result in an act of \"Stochastic Terrorism \" The phrase \"scripted violence\" has been used in social science since at least 2002  43     Author David Neiwert  who wrote the book Alt-America  told Salon interviewer Chauncey Devega     Scripted violence is where a person who has a national platform describes the kind of violence that they want to be carried out  He identifies the targets and leaves it up to the listeners to carry out this violence  It is a form of terrorism  It is an act and a social phenomenon where there is an agreement to inflict massive violence on a whole segment of society  Again  this violence is led by people in high-profile positions in the media and the government  Theyre the ones who do the scripting  and it is ordinary people who carry it out     Think of it like Charles Manson and his followers  Manson wrote the script  he didnt commit any of those murders  He just had his followers carry them out  44     When color reproductions are made  the image is separated into its component colors by taking multiple photographs filtered for each color  One resultant film or plate represents each of the cyan  magenta  yellow  and black data  Color printing is a binary system  where ink is either present or not present  so all color separations to be printed must be translated into dots at some stage of the work-flow  Traditional line screens which are amplitude modulated had problems with moir but were used until stochastic screening became available  A stochastic  or frequency modulated  dot pattern creates a sharper image      nChemical kinetics  also known as reaction kinetics  is the branch of physical chemistry that is concerned with understanding the rates of chemical reactions  It is to be contrasted with thermodynamics  which deals with the direction in which a process occurs but in itself tells nothing about its rate  Chemical kinetics includes investigations of how experimental conditions influence the speed of a chemical reaction and yield information about the reaction  s mechanism and transition states  as well as the construction of mathematical models that also can describe the characteristics of a chemical reaction     In 1864  Peter Waage and Cato Guldberg pioneered the development of chemical kinetics by formulating the law of mass action  which states that the speed of a chemical reaction is proportional to the quantity of the reacting substances  1  2  3     Van   t Hoff studied chemical dynamics and in 1884 published his famous \"tudes de dynamique chimique\"  4  In 1901 he was awarded by the first Nobel Prize in Chemistry \"in recognition of the extraordinary services he has rendered by the discovery of the laws of chemical dynamics and osmotic pressure in solutions\"  5  After van   t Hoff  chemical kinetics deals with the experimental determination of reaction rates from which rate laws and rate constants are derived  Relatively simple rate laws exist for zero order reactions  for which reaction rates are independent of concentration   first order reactions  and second order reactions  and can be derived for others  Elementary reactions follow the law of mass action  but the rate law of stepwise reactions has to be derived by combining the rate laws of the various elementary steps  and can become rather complex  In consecutive reactions  the rate-determining step often determines the kinetics  In consecutive first order reactions  a steady state approximation can simplify the rate law  The activation energy for a reaction is experimentally determined through the Arrhenius equation and the Eyring equation  The main factors that influence the reaction rate include  the physical state of the reactants  the concentrations of the reactants  the temperature at which the reaction occurs  and whether or not any catalysts are present in the reaction     Gorban and Yablonsky have suggested that the history of chemical dynamics can be divided into three eras  6  The first is the van   t Hoff wave searching for the general laws of chemical reactions and relating kinetics to thermodynamics  The second may be called the Semenov--Hinshelwood wave with emphasis on reaction mechanisms  especially for chain reactions  The third is associated with Aris and the detailed mathematical description of chemical reaction networks     The reaction rate varies depending upon what substances are reacting  Acid/base reactions  the formation of salts  and ion exchange are usually fast reactions  When covalent bond formation takes place between the molecules and when large molecules are formed  the reactions tend to be slower     The nature and strength of bonds in reactant molecules greatly influence the rate of their transformation into products     The physical state  solid  liquid  or gas  of a reactant is also an important factor of the rate of change  When reactants are in the same phase  as in aqueous solution  thermal motion brings them into contact  However  when they are in separate phases  the reaction is limited to the interface between the reactants  Reaction can occur only at their area of contact  in the case of a liquid and a gas  at the surface of the liquid  Vigorous shaking and stirring may be needed to bring the reaction to completion  This means that the more finely divided a solid or liquid reactant the greater its surface area per unit volume and the more contact it with the other reactant  thus the faster the reaction  To make an analogy  for example  when one starts a fire  one uses wood chips and small branches  one does not start with large logs right away  In organic chemistry  on water reactions are the exception to the rule that homogeneous reactions take place faster than heterogeneous reactions   are those reactions in which solute and solvent not mix properly     In a solid  only those particles that are at the surface can be involved in a reaction  Crushing a solid into smaller parts means that more particles are present at the surface  and the frequency of collisions between these and reactant particles increases  and so reaction occurs more rapidly  For example  Sherbet  powder  is a mixture of very fine powder of malic acid  a weak organic acid  and sodium hydrogen carbonate  On contact with the saliva in the mouth  these chemicals quickly dissolve and react  releasing carbon dioxide and providing for the fizzy sensation  Also  fireworks manufacturers modify the surface area of solid reactants to control the rate at which the fuels in fireworks are oxidised  using this to create diverse effects  For example  finely divided aluminium confined in a shell explodes violently  If larger pieces of aluminium are used  the reaction is slower and sparks are seen as pieces of burning metal are ejected     The reactions are due to collisions of reactant species  The frequency with which the molecules or ions collide depends upon their concentrations  The more crowded the molecules are  the more likely they are to collide and react with one another  Thus  an increase in the concentrations of the reactants will usually result in the corresponding increase in the reaction rate  while a decrease in the concentrations will usually have a reverse effect  For example  combustion will occur more rapidly in pure oxygen than in air  21% oxygen      The rate equation shows the detailed dependence of the reaction rate on the concentrations of reactants and other species present  The mathematical forms depend on the reaction mechanism  The actual rate equation for a given reaction is determined experimentally and provides information about the reaction mechanism  The mathematical expression of the rate equation is often given by     Here                     k                 displaystyle k    is the reaction rate constant                                c                      i                                   displaystyle c_ i     is the molar concentration of reactant i and                               m                      i                                   displaystyle m_ i     is the partial order of reaction for this reactant  The partial order for a reactant can only be determined experimentally and is often not indicated by its stoichiometric coefficient     Temperature usually has a major effect on the rate of a chemical reaction  Molecules at a higher temperature have more thermal energy  Although collision frequency is greater at higher temperatures  this alone contributes only a very small proportion to the increase in rate of reaction  Much more important is the fact that the proportion of reactant molecules with sufficient energy to react  energy greater than activation energy  E xa0> xa0Ea  is significantly higher and is explained in detail by the MaxwellBoltzmann distribution of molecular energies     The effect of temperature on the reaction rate constant usually obeys the Arrhenius equation                     k        =        A                  e                                                E                                                a                                                                    /                                     R            T                                                displaystyle k=Ae^ -E_   rm  a  / RT       where A is the pre-exponential factor or A-factor  Ea is the activation energy  R is the molar gas constant and T is the absolute temperature  7     At a given temperature  the chemical rate of a reaction depends on the value of the A-factor  the magnitude of the activation energy  and the concentrations of the reactants  Usually  rapid reactions require relatively small activation energies     The   rule of thumb   that the rate of chemical reactions doubles for every 10 xa0C temperature rise is a common misconception  This may have been generalized from the special case of biological systems  where the   temperature coefficient  is often between 1 5 and 2 5     The kinetics of rapid reactions can be studied with the temperature jump method  This involves using a sharp rise in temperature and observing the relaxation time of the return to equilibrium  A particularly useful form of temperature jump apparatus is a shock tube  which can rapidly increase a gas  s temperature by more than 1000 degrees     A catalyst is a substance that alters the rate of a chemical reaction but it remains chemically unchanged afterwards  The catalyst increases the rate of the reaction by providing a new reaction mechanism to occur with in a lower activation energy  In autocatalysis a reaction product is itself a catalyst for that reaction leading to positive feedback  Proteins that act as catalysts in biochemical reactions are called enzymes  MichaelisMenten kinetics describe the rate of enzyme mediated reactions  A catalyst does not affect the position of the equilibrium  as the catalyst speeds up the backward and forward reactions equally     In certain organic molecules  specific substituents can have an influence on reaction rate in neighbouring group participation  citation needed     Increasing the pressure in a gaseous reaction will increase the number of collisions between reactants  increasing the rate of reaction  This is because the activity of a gas is directly proportional to the partial pressure of the gas  This is similar to the effect of increasing the concentration of a solution     In addition to this straightforward mass-action effect  the rate coefficients themselves can change due to pressure  The rate coefficients and products of many high-temperature gas-phase reactions change if an inert gas is added to the mixture  variations on this effect are called fall-off and chemical activation  These phenomena are due to exothermic or endothermic reactions occurring faster than heat transfer  causing the reacting molecules to have non-thermal energy distributions  non-Boltzmann distribution   Increasing the pressure increases the heat transfer rate between the reacting molecules and the rest of the system  reducing this effect     Condensed-phase rate coefficients can also be affected by pressure  although rather high pressures are required for a measurable effect because ions and molecules are not very compressible  This effect is often studied using diamond anvils     A reaction  s kinetics can also be studied with a pressure jump approach  This involves making fast changes in pressure and observing the relaxation time of the return to equilibrium     The activation energy for a chemical reaction can be provided when one reactant molecule absorbs light of suitable wavelength and is promoted to an excited state  The study of reactions initiated by light is photochemistry  one prominent example being photosynthesis     The experimental determination of reaction rates involves measuring how the concentrations of reactants or products change over time  For example  the concentration of a reactant can be measured by spectrophotometry at a wavelength where no other reactant or product in the system absorbs light     For reactions which take at least several minutes  it is possible to start the observations after the reactants have been mixed at the temperature of interest     For faster reactions  the time required to mix the reactants and bring them to a specified temperature may be comparable or longer than the half-life of the reaction  8  Special methods to start fast reactions without slow mixing step include    While chemical kinetics is concerned with the rate of a chemical reaction  thermodynamics determines the extent to which reactions occur  In a reversible reaction  chemical equilibrium is reached when the rates of the forward and reverse reactions are equal  the principle of dynamic equilibrium   and the concentrations of the reactants and products no longer change  This is demonstrated by  for example  the HaberBosch process for combining nitrogen and hydrogen to produce ammonia  Chemical clock reactions such as the BelousovZhabotinsky reaction demonstrate that component concentrations can oscillate for a long time before finally attaining the equilibrium     In general terms  the free energy change  G  of a reaction determines whether a chemical change will take place  but kinetics describes how fast the reaction is  A reaction can be very exothermic and have a very positive entropy change but will not happen in practice if the reaction is too slow  If a reactant can produce two products  the thermodynamically most stable one will form in general  except in special circumstances when the reaction is said to be under kinetic reaction control  The CurtinHammett principle applies when determining the product ratio for two reactants interconverting rapidly  each going to a distinct product  It is possible to make predictions about reaction rate constants for a reaction from free-energy relationships     The kinetic isotope effect is the difference in the rate of a chemical reaction when an atom in one of the reactants is replaced by one of its isotopes     Chemical kinetics provides information on residence time and heat transfer in a chemical reactor in chemical engineering and the molar mass distribution in polymer chemistry  It is also provides information in corrosion engineering     The mathematical models that describe chemical reaction kinetics provide chemists and chemical engineers with tools to better understand and describe chemical processes such as food decomposition  microorganism growth  stratospheric ozone decomposition  and the chemistry of biological systems  These models can also be used in the design or modification of chemical reactors to optimize product yield  more efficiently separate products  and eliminate environmentally harmful by-products  When performing catalytic cracking of heavy hydrocarbons into gasoline and light gas  for example  kinetic models can be used to find the temperature and pressure at which the highest yield of heavy hydrocarbons into gasoline will occur     Chemical Kinetics is frequently validated and explored through modeling in specialized packages as a function of ordinary differential equation-solving  ODE-solving  and curve-fitting  17     In some cases  equations are unsolvable analytically  but can be solved using numerical methods if data values are given  There are two different ways to do this  by either using software programmes or mathematical methods such as the Euler method  Examples of software for chemical kinetics are i  Tenua  a Java app which simulates chemical reactions numerically and allows comparison of the simulation to real data  ii  Python coding for calculations and estimates and iii  the Kintecus software compiler to model  regress  fit and optimize reactions     -Numerical integration  for a 1st order reaction A->B    The differential equation of the reactant A is     It can also be expressed as     To solve the differential equations with Euler and Runge-Kutta methods we need to have the initial values     At any point                               y                          =        f                 y                 x                                          displaystyle y  =f y x   qquad   qquad     is the same as     We can approximate the differentials as discrete increases     The unknown part of the equation is y x+x   which can be found if we have the data of initial values     In this method  an initial condition is required  y=y0 at x=x0  The proposal is to find the value of y when x=x0 + h  where h is a given constant     It can be shown analytically that the ordinat at that moment to the curve through  x0  y0  is given by the third-order Runge-Kutta formula     In first-order odinary equations  the Runge-Kutta method uses a mathematical model that represents the relationship between the temperature and the rate of reaction  It is worth it to calculate the rate of reaction at different temperatures for different concentrations  The equation obtained is                      d        r                  /                d        t        =        R                  /                T        +        r                          H                                                          /                R                  T                      2                                   displaystyle dr/dt=R/T+r  Delta H^   circ  /RT^ 2        \"In an equilibrioum reaction with directed and inverse rate constants  it s easier to transform from A to B rather than B to A  n\"   As for probability computations  at each time it choose a random number to be compared with a threshold to know if the reaction runs from A to B or the other way around     Roadway air dispersion modeling is the study of air pollutant transport from a roadway or other linear emitter  Computer models  are required to conduct this analysis  because of the complex variables involved  including vehicle emissions  vehicle speed  meteorology  and terrain geometry   Line source dispersion has been studied since at least the 1960s  when the regulatory framework in the United States began requiring quantitative analysis of the air pollution consequences of major roadway and airport projects   By the early 1970s this subset of atmospheric dispersion models were being applied to real world cases of highway planning  even including some controversial court cases     The basic concept of the roadway air dispersion model is to calculate air pollutant levels in the vicinity of a highway or arterial roadway by considering them as line sources   The model takes into account source characteristics such as  traffic volume  vehicle speeds  truck mix  and fleet emission controls  in addition  the roadway geometry  surrounding terrain and local meteorology are addressed   For example  many air quality standards require that certain near worst case meteorological conditions be applied     The calculations are sufficiently complex that a computer model is essential to arrive at authoritative results  although workbook type manuals have been developed as screening techniques   In some cases where results must be refereed  such as legal cases   model validation may be needed with field test data in the local setting  this step is not usually warranted  because the best models have been extensively validated over a wide spectrum of input data variables     The product of the calculations is usually a set of isopleths or mapped contour lines either in plan view or cross sectional view   Typically these might be stated as concentrations of carbon monoxide  total reactive hydrocarbons  oxides of nitrogen  particulate or benzene   The air quality scientist  can run the model successively to study techniques of reducing adverse air pollutant concentrations  for example  by redesigning roadway geometry  altering speed controls or limiting certain types of trucks    The model is frequently utilized in an Environmental Impact Statement involving a major new roadway or land use change which will induce new vehicular traffic     The logical building block for this theory was the use of the Gaussian air pollutant dispersion equation for point sources  1  2    One of the early point source air pollutant plume dispersion equations was derived by Bosanquet and Pearson 3  in 1936   Their equation did not include the effect of ground reflection of the pollutant plume   Sir Graham Sutton derived a point source air pollutant plume dispersion equation in 1947 4  which included the assumption of Gaussian distribution for the vertical and crosswind dispersion of the plume and also addressed the effect of ground reflection of the plume   Further advances were made by G  A  Briggs 5  in model refinement and validation and by D B  Turner 1   for his user-friendly workbook that included screening calculations which do not require a computer     In seeing the need to develop a line source model to approach the study of roadway air pollution  nMichael Hogan and Richard Venti developed a closed form solution to integrating the point source equation in a series of publications  6  7     While the ESL mathematical model  was  completed for a line source by 1970   model refinement resulted in a strip source  emulating the horizontal extent of the roadway surface   This theory would be the precursor of area source dispersion models   But their focus was roadway simulation  so they proceeded with the development of a computer model by adding to the team Leda Patmore  a computer programmer in the field of atmospheric physics and satellite trajectory calculations   A working computer model was produced by late 1970  then the model was calibrated with carbon monoxide field measurements targeting from traffic on U S  Route 101 in Sunnyvale  California     The ESL model received endorsement from the U S  Environmental Protection Agency  EPA  in the form of a major grant to validate the model using actual roadway tests of tracer gas sulfur hexafluoride dispersion   That gas was chosen since it does not occur naturally or in vehicular emissions and provides a unique tracer for such dispersion studies   Part of the Environmental Protection Agencys motives may have been to bring the model into public domain   After a successful validation through the EPA research  the model was soon put to use in a variety of settings to forecast air pollution levels in the vicinity of roadways   The ESL group applied the model to the U S  Route 101 bypass  project in Cloverdale  California  the extension of Interstate 66 through Arlington  Virginia  the widening of the New Jersey Turnpike through Raritan and East Brunswick  New Jersey  and several transportation projects in Boston for the Boston Transportation Planning Review     By the early 1970s at least two other research groups were known to be actively developing some type to roadway air dispersion model  the Environmental Research and Technology group of Lexington  Massachusetts and Caltrans headquarters in Sacramento  California   The Caline model of Caltrans borrowed some of the technology from the ESL Inc  group  since Caltrans funded some of the early model application work in Cloverdale and other locations and was given rights to use parts of their model     The resulting solution for an infinite line source is                                  xa0        =                                        0                                                                                q                                                                                           u                  c                  d                                      x                                          2                                                                                                                                                      c                  o                  s                                     xa0                                                                                                                      exp             u2061                                                            y                                      2                                                                    2                                      c                                          2                                                                            x                                          2                                                                                                                     d        x                 displaystyle   chi    =  int _ 0 ^   infty     frac  q    pi   left ucdx^ 2   right   left cos  alpha      right     left   exp    frac  y^ 2   2c^ 2 x^ 2     right dx    n    where     x is the distance from the observer to the roadway    y is the height of the observer    u is the mean wind speed     is the angle of tilt of the line source relative to the reference frame    c and d are the standard deviation of horizontal and vertical  wind directions  measured in radians  respectively     This equation was integrated into a closed form solution using the error function  erf   and variations in geometry can be performed to include the full infinite line  line segment  elevated line  or arc made from segments   In any case one can calculate three-dimensional contours of resulting air pollutant concentrations and use the mathematical model to study alternative roadway designs  various assumptions of worst case meteorology or varying traffic conditions  for example  variations in truck mix  fleet emission controls  or vehicle speed      The ESL research group also  extended their model by  introducing the area source concept of a vertical strip to simulate the mixing zone on the highway produced by vehicle turbulence   This model too was validated in 1971 and showed good correlation with field test data     There were several early applications of the model in somewhat dramatic cases    In 1971 the Arlington Coalition on Transportation  ACT   was the plaintiff in an action against the Virginia Highway Commission over the extension of Interstate 66 through Arlington  Virginia  having filed a suit in the federal district court   The ESL model was used to produce calculations of air quality in the vicinity of the proposed highway   ACT won this case after a decision by the U S  Fourth Circuit Court of Appeals  The court paid special attention to the plaintiff  s expert calculations and testimony projecting that air quality levels would violate Federal ambient air quality standards as set forth in the Clean Air Act     A second contentious case took place in East Brunswick  New Jersey where the New Jersey Turnpike Authority planned a major widening of the Turnpike   Again the roadway air dispersion model was employed to predict levels of air pollution for residences  schools and parks  near the Turnpike   After an initial hearing in Superior Court where the ESL model results were set forth  the judge ordered the Turnpike Authority to negotiate with the plaintiff  Concerned Citizens of East Brunswick and develop air quality mitigation for the adverse effects   The Turnpike Authority hired ERT as its expert  and the two research teams negotiated a settlement to this case using the newly created roadway air dispersion models     The CALINE3 model is a steady-state Gaussian dispersion model designed to determine air pollution concentrations at receptor locations downwind of  highways located in relatively uncomplicated terrain  CALINE3 is incorporated into the more elaborate CAL3QHC and CAL3QHCR models  CALINE3 is in widespread use due to its user friendly nature and promotion in governmental circles  but it falls short of analyzing the complexity of cases addressed by the original Hogan-Venti model  CAL3QHC and CAL3QHCR models are available in the Fortran programming language   They have options to model either particulate matter or carbon monoxide  and include algorithms to simulate queued traffic at signalized intersections  1      In addition  several more recent models have been developed that employ non-steady state Lagrangian puff algorithms   The HYROAD dispersion model has been developed through the National Cooperative Highway Research Program  s Project 25-06  incorporating ROADWAY-2 model puff and steady-state plume algorithms  Rao et al   2002  dead link      The TRAQSIM model was developed in 2004 as part of a Ph D dissertation with support by the U S  Department of Transportation  s Volpe National Transportation Systems Center Air Quality Facility  The model incorporates dynamic vehicle behavior with a non-steady state Gaussian puff algorithm  Unlike HYROAD  TRAQSIM combines traffic simulation  second-by-second modal emissions  and Gaussian puff dispersion into a fully integrated system  a true simulation  that models individual vehicles as discrete moving sources   TRAQSIM was developed as a next generation model to be the successor to the current CALINE3 and CAL3QHC regulatory models   The next step in the development of TRAQSIM is to incorporate methods to model the dispersion of particulate matter  PM  and hazardous air pollutants  HAPs      Several models have been developed that handle complex urban meteorology resulting from urban canyons and highway configurations  The earliest such model development  1968-1970  was by the Air Pollution Control Office of the U S  EPA in conjunction with New York City  8  The model was successfully applied to the Spadina Expressway in Toronto by Jack Fensterstock of the New York City Department of Air Resources   9  10  Other examples include the Turner-Fairbank Highway Research Center  s 11  Canyon Plume Box model  12  now in version 3  CPB-3   the National Environmental Research Institute of Denmark  s Operational Street Pollution Model  OSPM   and the MICRO-CALGRID model  which includes photochemistry  allowing for both primary and secondary species to be modeled  Cornell University  s CTAG model  which resolves vehicle-induced turbulence  VIT   road-induced turbulence  RIT   13  chemical transformation and aerosol dynamics of air pollutants using turbulence reacting flow models  The CTAG model has also been applied to characterize highway-building environments and study effects of vegetation barriers on near-road air pollution     Recent health literature indicating that residents near major roads face elevated rates of several adverse health outcomes has prompted legal dispute over the responsibility of transportation agencies to use roadway air dispersion models to characterize the impacts of new and expanded roadways  bus terminals  truck stops  and other sources     Recently  the Sierra Club of Nevada sued the Nevada Department of Transportation and the Federal Highway Administration over its failure to assess the impact of the expansion of U S  Route 95 in Las Vegas on neighborhood air quality  2   The Sierra Club asserted that a supplemental Environmental Impact Statement should be issued to address emissions of hazardous air pollutants and particulate matter from new motor vehicle traffic   The plaintiffs asserted that modeling tools were available  including the Environmental Protection Agency  s MOBILE6 2 model  the CALINE3 dispersion model  and other relevant models   The defendants won in the U S  District Court under Judge Philip Pro  who ruled that the transportation agencies had acted in a manner that was not \"arbitrary and capricious \" despite the agencies   technical arguments regarding the lack of available modeling tools being contradicted by a number of peer-reviewed studies published in scientific journals  e g  Korenstein and Piazza  Journal of Environmental Health  2002   On appeal to the U S  Ninth Circuit  the Appeals Court stayed new construction on the highway pending the court  s final decision   The Sierra Club and the defendants settled out of court  setting up a research program on the air quality impacts of U S  Route 95 on nearby schools     A number of other high-profile cases have prompted environmental groups to call for dispersion modeling to be used to assess the air quality impacts of new transportation projects on nearby communities  but to date state transportation agencies and the Federal Highways Administration has claimed that no tools are available  despite models and guidance available through EPA  s Support Center for Regulatory Air Models  SCRAM   3     Among the more contentious of cases  the Detroit Intermodal Freight Terminal and Detroit River International Crossing  Michigan  USA   and the expansion of Interstate 70 East in Denver  Colorado  USA      In all of these cases  community-based organizations have asserted that modeling tools are available  but transportation planning agencies have asserted that too much uncertainty exists in all of the steps   A major concern for community-based organizations has been  transportation agencies   unwillingness to define the level of uncertainty that they are willing to tolerate in air quality analyses  how that compares to the Environmental Protection Agency  s guideline on air quality models  which addresses uncertainty and accuracy in model use  4     Roadway noise is the collective sound energy emanating from motor vehicles  It consists chiefly of road surface  tire  engine/transmission  aerodynamic  and braking elements  Noise of rolling tires driving on pavement is found to be the biggest contributor of highway noise and increases with higher vehicle speeds  1  2     In developed and developing countries  roadway noise contributes a proportionately large share of the total societal noise pollution  In the U S   it contributes more to environmental noise exposure 3  than any other noise source     Roadway noise began to be measured in a widespread manner in the 1960s  as computer modeling of this phenomenon began to become meaningful   After passage of the National Environmental Policy Act and Noise Control Act  4  the demand for detailed analysis soared  and decision makers began to look to acoustical scientists for answers regarding the planning of new roadways and the design of noise mitigation     Partial bans on motor vehicles from urban areas have been shown to have minimal impacts upon reducing sound levels  as would become clear from later modeling studies   for example  the partial ban in Gothenburg  Sweden resulted in minuscule reduction of sound levels  5     Regulation in the EU and Japan of tire and power-train noise has only sought to reduce noise by approx 3 xa0dB  and will only slowly take effect because a few older noisier vehicles can dominate the soundscape     Small reductions in vehicle noise occurred in the 1970s as states and provinces enforced unmuffled vehicle ordinances     The vehicle fleet noise has not changed very much over the last three decades  however  if the trend in hybrid vehicle use continues  substantial noise reduction will occur  especially in the regime of traffic flow below 35 miles per hour  Hybrid vehicles are so quiet at low speeds that they create a pedestrian safety issue when reversing or maneuvering when parking etc   but not when travelling forward   6  and so are typically fitted with electric vehicle warning sounds     Traffic operations noise is affected significantly by vehicle speeds  since sound energy roughly doubles for each increment of ten miles an hour in vehicle velocity  an exception to this rule occurs at very low speeds where braking and acceleration noise dominate over aerodynamic noise       Trucks contribute a disproportionate amount of noise not only because of their large engines  but also the height of the diesel stack and the aerodynamic drag citation needed   Significant interior noise is usually present inside moving motor vehicles  in fact  passengers are generally not aware that these levels are high  because experience has led motorists to expect levels commonly exceeding 65 dBA     Roadway surface types contribute to different noise levels  Of the common types of surfaces in modern cities  there is a 4 xa0dB citation needed  difference between the loudest and the softest  chip seal type and grooved roads being the loudest citation needed   and concrete surfaces without spacers being the quietest  and asphaltic surfaces being about average     Rubberized asphalt  which uses recycled old tires  is much quieter and is already widely used   nExperimental Porous Elastic Road Surfaces  PERS  might cut road noise in half   PERS is made by adding ground up tires to asphalt paving material  7     Studies have shown that cutting longitudinal grooves in the pavement reduces noise  8  9     Tire types can cause 10 xa0dB A  variations in noise  based on a 2001 sample of 100 commercially available tires  As of 2001  there was no correlation between grip and noise  Quieter tires may have slightly lower rolling resistance  10   nTire labeling for noise  grip  and rolling resistance has been widely introduced in Europe  with noisy tires being taxed     Roadway geometrics and surrounding terrain are interrelated  since the propagation of sound is sensitive to the overall geometry and must consider diffraction  bending of sound waves around obstacles   reflection  ground wave attenuation  spreading loss and refraction   A simple discussion indicates that sound will be diminished when the path of sound is blocked by terrain  or will be enhanced if the roadway is elevated so as to broadcast  however  the complexities of variable interaction are so great  that there are many exceptions to this simple argument     Micrometeorology is significant in that sound waves can be refracted by wind gradients or thermoclines  effectively dismissing the effect of some noise barriers or terrain intervention  2     Geometry of area structures is an important input  since the presence of buildings or walls can block sound under certain circumstances  but reflective properties can augment sound energy at other locations     At a macro level  ongoing research is required for national and worldwide responses to road noise pollution - issues include road surface choices  the regulation and taxing of noisy designs  and the ongoing inspecting of individual vehicles     At the micro level of managing particular roads  because of the complexity of the variables discussed above  it is necessary to create a computer model that can analyze sound levels in the vicinity of roadways   The first meaningful models arose in the late 1960s and early 1970s addressing the noise line source  e g  roadway    Two of the leading research teams were BBN in Boston and ESL of Sunnyvale  California   Both of these groups developed complex mathematical models to allow the study of alternate roadway designs  traffic operations and noise mitigation strategies in an arbitrary setting   Later model alterations have come into widespread use among state departments of transportation and city planners  but the accuracy of early models has had little change in 40 years     Generally the models trace sound ray bundles and calculate spreading loss along with ray bundle divergence  or convergence  from refractive phenomena   Diffraction is usually addressed by establishing secondary emitters at any points of topographic or anthropomorphic sharpness  such as noise barriers or building surfaces    Meteorology can be addressed in a statistical manner allowing for actual wind rose and wind speed statistics  along with thermocline data   Recent models have also attempted to predict levels of local air pollution based on an analysis of specific frequencies that are related to tire and engine noise  11     An interesting early case where two of the leading models were pitted against each other involved a proposed widening of the New Jersey Turnpike from six to twelve lanes   The BBN 12  and ESL 13   models were on opposing sides of a matter decided in New Jersey Superior Court   This case in the early 1970s was one of the first U S  examples of acoustical scientists playing a role in the design of a major highway   The models allowed the court to understand the effects of roadway geometry  width in this case   vehicle speeds  proposed noise barriers  residential setback and pavement types  The outcome was a compromise that involved substantial mitigation of noise pollution impacts     Another early case involved the proposed extension of Interstate 66 through Arlington  Virginia   The plaintiff  Arlington Coalition on Transportation sued the Virginia Department of Transportation on the grounds of air quality  noise and neighborhood disruption   To analyze roadway noise  the ESL model was used by the plaintiff  who won this case partially due to the credibility of the computer model   The matter was revisited a decade later and a greatly reduced highway design with transit element and extensive noise mitigation was agreed to     Later cases have occurred in every state  both in contentious actions and in routine highway planning and design  The public as well as governmental agencies have become aware of the value of acoustical science to provide useful insights to the roadway design process    \"Even without regulation  there are strong individual economic pressures for quieter vehicles  because owners and employers see quieter vehicles as more luxurious and less stressful  The tighter regulatory requirements of the EU and Japan encourage quieter design even in unregulated countries  because most car manufacturers aspire to international sales  On the other hand  individual owners of motorbikes   boom-box  cars  with very loud music systems   and  muscle-cars  may prefer their vehicle to be louder  at least at idling or low speeds   and such noise  often from modified exhaust systems  can only be controlled by on-going inspection and sanctions  n\"  \"Several studies have concluded that reducing traffic noise pollution is low-cost or cost-effective  Such studies include consideration of the reduced value of noise-affected real-estate  the costs of supporting a dispersed population  trying to get away from all the noise   and the increased healthcare costs statistically attributable to a noisier environment  n\"   European technology began to emulate the United States treatment of roadway noise by the 1980s  although the national requirements of noise studies generally remain less stringent than the U S   In developing countries  noise pollution from motor vehicles represents a significant impact  but technologies are not as advanced as in Western nations   For example  a recent paper from Iran illustrates a level of technology that the United States encountered in the 1960s  14   The European Union has recently proposed a set of vehicle tire requirements  similar to those introduced in the U S  in the 1970s  15     In Mumbai  India  excessive honking and road noise is seen as a significant nuisance  The local police launched an experimental program in 2020 to link the time-length of red lights to an ambient noise sensor  increasing red light times if ambient noise from traffic exceeds limits  This acts as a deterrent to use of the horn  16     General         Numerical climate models use quantitative methods to simulate the interactions of the important drivers of climate  including atmosphere  oceans  land surface and ice  They are used for a variety of purposes from study of the dynamics of the climate system to projections of future climate   Climate models may also be qualitative  i e  not numerical  models and also narratives  largely descriptive  of possible futures  1     Quantitative climate models take account of incoming energy from the sun as short wave electromagnetic radiation  chiefly visible and short-wave  near  infrared  as well as outgoing long wave  far  infrared electromagnetic  Any imbalance results in a change in temperature     Quantitative models vary in complexity     Box models are simplified versions of complex systems  reducing them to boxes  or reservoirs  linked by fluxes  The boxes are assumed to be mixed homogeneously  Within a given box  the concentration of any chemical species is therefore uniform  However  the abundance of a species within a given box may vary as a function of time due to the input to  or loss from  the box or due to the production  consumption or decay of this species within the box     Simple box models  i e  box model with a small number of boxes whose properties  e g  their volume  do not change with time  are often useful to derive analytical formulas describing the dynamics and steady-state abundance of a species  More complex box models are usually solved using numerical techniques     Box models are used extensively to model environmental systems or ecosystems and in studies of ocean circulation and the carbon cycle  2  nThey are instances of a multi-compartment model     A very simple model of the radiative equilibrium of the Earth is    where    and    The constant r2 can be factored out  giving    Solving for the temperature     This yields an apparent effective average earth temperature of 288 xa0K  15 xa0C  59 xa0F   5  This is because the above equation represents the effective radiative temperature of the Earth  including the clouds and atmosphere       This very simple model is quite instructive  For example  it easily determines the effect on average earth temperature of changes in solar constant or change of albedo or effective earth emissivity     The average emissivity of the earth is readily estimated from available data  The emissivities of terrestrial surfaces are all in the range of 0 96 to 0 99 6  7   except for some small desert areas which may be as low as 0 7   Clouds  however  which cover about half of the earth  s surface  have an average emissivity of about 0 5 8   which must be reduced by the fourth power of the ratio of cloud absolute temperature to average earth absolute temperature  and an average cloud temperature of about 258 xa0K  15 xa0C  5 xa0F   9  Taking all this properly into account results in an effective earth emissivity of about 0 64  earth average temperature 285 xa0K  12 xa0C  53 xa0F       This simple model readily determines the effect of changes in solar output or change of earth albedo or effective earth emissivity on average earth temperature  It says nothing  however about what might cause these things to change  Zero-dimensional models do not address the temperature distribution on the earth or the factors that move energy about the earth     The zero-dimensional model above  using the solar constant and given average earth temperature  determines the effective earth emissivity of long wave radiation emitted to space  This can be refined in the vertical to a one-dimensional radiative-convective model  which considers two processes of energy transport     The radiative-convective models have advantages over the simple model  they can determine the effects of varying greenhouse gas concentrations on effective emissivity and therefore the surface temperature  But added parameters are needed to determine local emissivity and albedo and address the factors that move energy about the earth     Effect of ice-albedo feedback on global sensitivity in a one-dimensional radiative-convective climate model  10  11  12     The zero-dimensional model may be expanded to consider the energy transported horizontally in the atmosphere  This kind of model may well be zonally averaged  This model has the advantage of allowing a rational dependence of local albedo and emissivity on temperature  the poles can be allowed to be icy and the equator warm  but the lack of true dynamics means that horizontal transports have to be specified  13     Depending on the nature of questions asked and the pertinent time scales  there are  on the one extreme   conceptual  more inductive models  and  on the other extreme  general circulation models operating at the highest spatial and temporal resolution currently feasible  Models of intermediate complexity bridge the gap  One example is the Climber-3 model  Its atmosphere is a 2 5-dimensional statistical-dynamical model with 7 5  22 5 resolution and time step of half a day   the ocean is MOM-3  Modular Ocean Model  with a 3 75  3 75 grid and 24 vertical levels  14     General Circulation Models  GCMs  discretise the equations for fluid motion and energy transfer and integrate these over time  Unlike simpler models  GCMs divide the atmosphere and/or oceans into grids of discrete \"cells\"  which represent computational units  Unlike simpler models which make mixing assumptions  processes internal to a cellsuch as convectionthat occur on scales too small to be resolved directly are parameterised at the cell level  while other functions govern the interface between cells     Atmospheric GCMs  AGCMs  model the atmosphere and impose sea surface temperatures as boundary conditions  Coupled atmosphere-ocean GCMs  AOGCMs  e g  HadCM3  EdGCM  GFDL CM2 X  ARPEGE-Climat  15  combine the two models  The first general circulation climate model that combined both oceanic and atmospheric processes was developed in the late 1960s at the NOAA Geophysical Fluid Dynamics Laboratory 16  AOGCMs represent the pinnacle of complexity in climate models and internalise as many processes as possible  However  they are still under development and uncertainties remain   They may be coupled to models of other processes  such as the carbon cycle  so as to better model feedback effects  Such integrated multi-system models are sometimes referred to as either \"earth system models\" or \"global climate models \"    There are three major types of institution where climate models are developed  implemented and used     The World Climate Research Programme  WCRP   hosted by the World Meteorological Organization  WMO   coordinates research activities on climate modelling worldwide     A 2012 U S  National Research Council report discussed how the large and diverse U S  climate modeling enterprise could evolve to become more unified  17  Efficiencies could be gained by developing a common software infrastructure shared by all U S  climate researchers  and holding an annual climate modeling forum  the report found  18     In physics and engineering  fluid dynamics  is a subdiscipline of fluid mechanics that describes the flow of fluidsliquids and gases   It has several subdisciplines  including aerodynamics  the study of air and other gases in motion  and hydrodynamics  the study of liquids in motion    Fluid dynamics has a wide range of applications  including calculating forces and moments on aircraft  determining the mass flow rate of petroleum through pipelines  predicting weather patterns  understanding nebulae in interstellar space and modelling fission weapon detonation     Fluid dynamics offers a systematic structurewhich underlies these practical disciplinesthat embraces empirical and semi-empirical laws derived from flow measurement and used to solve practical problems  The solution to a fluid dynamics problem typically involves the calculation of various properties of the fluid  such as flow velocity  pressure  density  and temperature  as functions of space and time     Before the twentieth century  hydrodynamics was synonymous with fluid dynamics   This is still reflected in names of some fluid dynamics topics  like magnetohydrodynamics and hydrodynamic stability  both of which can also be applied to gases  1     The foundational axioms of fluid dynamics are the conservation laws  specifically  conservation of mass   conservation of linear momentum  and conservation of energy  also known as First Law of Thermodynamics   These are based on classical mechanics and are modified in quantum mechanics and general relativity   They are expressed using the Reynolds transport theorem     In addition to the above  fluids are assumed to obey the continuum assumption  Fluids are composed of molecules that collide with one another and solid objects  However  the continuum assumption assumes that fluids are continuous  rather than discrete  Consequently  it is assumed that properties such as density  pressure  temperature  and flow velocity are well-defined at infinitesimally small points in space and vary continuously from one point to another  The fact that the fluid is made up of discrete molecules is ignored     For fluids that are sufficiently dense to be a continuum  do not contain ionized species  and have flow velocities small in relation to the speed of light  the momentum equations for Newtonian fluids are the NavierStokes equationswhich is a non-linear set of differential equations that describes the flow of a fluid whose stress depends linearly on flow velocity gradients and pressure  The unsimplified equations do not have a general closed-form solution  so they are primarily of use in computational fluid dynamics   The equations can be simplified in a number of ways  all of which make them easier to solve   Some of the simplifications allow some simple fluid dynamics problems to be solved in closed form  citation needed     In addition to the mass  momentum  and energy conservation equations  a thermodynamic equation of state that gives the pressure as a function of other thermodynamic variables is required to completely describe the problem  An example of this would be the perfect gas equation of state     where p is pressure   is density  T the absolute temperature  while Ru is the gas constant and M is molar mass for a particular gas     Three conservation laws are used to solve fluid dynamics problems  and may be written in integral or differential form  The conservation laws may be applied to a region of the flow called a control volume   A control volume is a discrete volume in space through which fluid is assumed to flow   The integral formulations of the conservation laws are used to describe the change of mass  momentum  or energy within the control volume   Differential formulations of the conservation laws apply Stokes   theorem to yield an expression which may be interpreted as the integral form of the law applied to an infinitesimally small volume  at a point  within the flow     All fluids are compressible to an extent  that is  changes in pressure or temperature cause changes in density  However  in many situations the changes in pressure and temperature are sufficiently small that the changes in density are negligible  In this case the flow can be modelled as an incompressible flow  Otherwise the more general compressible flow equations must be used     Mathematically  incompressibility is expressed by saying that the density  of a fluid parcel does not change as it moves in the flow field  that is     where D/Dt is the material derivative  which is the sum of local and convective derivatives  This additional constraint simplifies the governing equations  especially in the case when the fluid has a uniform density     For flow of gases  to determine whether to use compressible or incompressible fluid dynamics  the Mach number of the flow is evaluated  As a rough guide  compressible effects can be ignored at Mach numbers below approximately 0 3   For liquids  whether the incompressible assumption is valid depends on the fluid properties  specifically the critical pressure and temperature of the fluid  and the flow conditions  how close to the critical pressure the actual flow pressure becomes    Acoustic problems always require allowing compressibility  since sound waves are compression waves involving changes in pressure and density of the medium through which they propagate     All fluids are viscous  meaning that they exert some resistance to deformation  neighbouring parcels of fluid moving at different velocities exert viscous forces on each other  The velocity gradient is referred to as a strain rate  it has dimensions T1  Isaac Newton showed that for many familiar fluids such as water and air  the stress due to these viscous forces is linearly related to the strain rate  Such fluids are called Newtonian fluids  The coefficient of proportionality is called the fluid  s viscosity  for Newtonian fluids  it is a fluid property that is independent of the strain rate     Non-Newtonian fluids have a more complicated  non-linear stress-strain behaviour  The sub-discipline of rheology describes the stress-strain behaviours of such fluids  which include emulsions and slurries  some viscoelastic materials such as blood and some polymers  and sticky liquids such as latex  honey and lubricants  5     The dynamic of fluid parcels is described with the help of Newton  s second law  An accelerating parcel of fluid is subject to inertial effects     The Reynolds number is a dimensionless quantity which characterises the magnitude of inertial effects compared to the magnitude of viscous effects  A low Reynolds number  Re  1  indicates that viscous forces are very strong compared to inertial forces  In such cases  inertial forces are sometimes neglected  this flow regime is called Stokes or creeping flow     In contrast  high Reynolds numbers  Re  1  indicate that the inertial effects have more effect on the velocity field than the viscous  friction  effects  In high Reynolds number flows  the flow is often modeled as an inviscid flow  an approximation in which viscosity is completely neglected  Eliminating viscosity allows the NavierStokes equations to be simplified into the Euler equations  The integration of the Euler equations along a streamline in an inviscid flow yields Bernoulli  s equation  When  in addition to being inviscid  the flow is irrotational everywhere  Bernoulli  s equation can completely describe the flow everywhere  Such flows are called potential flows  because the velocity field may be expressed as the gradient of a potential energy expression     This idea can work fairly well when the Reynolds number is high  However  problems such as those involving solid boundaries may require that the viscosity be included  Viscosity cannot be neglected near solid boundaries because the no-slip condition generates a thin region of large strain rate  the boundary layer  in which viscosity effects dominate and which thus generates vorticity  Therefore  to calculate net forces on bodies  such as wings   viscous flow equations must be used  inviscid flow theory fails to predict drag forces  a limitation known as the d  Alembert  s paradox     A commonly used citation needed  model  especially in computational fluid dynamics  is to use two flow models  the Euler equations away from the body  and boundary layer equations in a region close to the body  The two solutions can then be matched with each other  using the method of matched asymptotic expansions     A flow that is not a function of time is called steady flow  Steady-state flow refers to the condition where the fluid properties at a point in the system do not change over time  Time dependent flow is known as unsteady  also called transient 7    Whether a particular flow is steady or unsteady  can depend on the chosen frame of reference  For instance  laminar flow over a sphere is steady in the frame of reference that is stationary with respect to the sphere  In a frame of reference that is stationary with respect to a background flow  the flow is unsteady     Turbulent flows are unsteady by definition  A turbulent flow can  however  be statistically stationary  The random velocity field U x  t  is statistically stationary if all statistics are invariant under a shift in time  8  75 This roughly means that all statistical properties are constant in time  Often  the mean field is the object of interest  and this is constant too in a statistically stationary flow     Steady flows are often more tractable than otherwise similar unsteady flows  The governing equations of a steady problem have one dimension fewer  time  than the governing equations of the same problem without taking advantage of the steadiness of the flow field     Turbulence is flow characterized by recirculation  eddies  and apparent randomness   Flow in which turbulence is not exhibited is called laminar  The presence of eddies or recirculation alone does not necessarily indicate turbulent flowthese phenomena may be present in laminar flow as well   Mathematically  turbulent flow is often represented via a Reynolds decomposition  in which the flow is broken down into the sum of an average component and a perturbation component     It is believed that turbulent flows can be described well through the use of the NavierStokes equations  Direct numerical simulation  DNS   based on the NavierStokes equations  makes it possible to simulate turbulent flows at moderate Reynolds numbers  Restrictions depend on the power of the computer used and the efficiency of the solution algorithm  The results of DNS have been found to agree well with experimental data for some flows  9     Most flows of interest have Reynolds numbers much too high for DNS to be a viable option  8  344 given the state of computational power for the next few decades   Any flight vehicle large enough to carry a human  L > 3 xa0m   moving faster than 20 xa0m/s  72 xa0km/h  45 xa0mph  is well beyond the limit of DNS simulation  Re = 4 xa0million    Transport aircraft wings  such as on an Airbus A300 or Boeing 747  have Reynolds numbers of 40 million  based on the wing chord dimension    Solving these real-life flow problems requires turbulence models for the foreseeable future  Reynolds-averaged NavierStokes equations  RANS  combined with turbulence modelling provides a model of the effects of the turbulent flow  Such a modelling mainly provides the additional momentum transfer by the Reynolds stresses  although the turbulence also enhances the heat and mass transfer  Another promising methodology is large eddy simulation  LES   especially in the guise of detached eddy simulation  DES which is a combination of RANS turbulence modelling and large eddy simulation     There are a large number of other possible approximations to fluid dynamic problems  Some of the more commonly used are listed below     While many flows  such as flow of water through a pipe  occur at low Mach numbers  subsonic flows   many flows of practical interest in aerodynamics or in turbomachines occur at high fractions of M = 1  transonic flows  or in excess of it  supersonic or even hypersonic flows   New phenomena occur at these regimes such as instabilities in transonic flow  shock waves for supersonic flow  or non-equilibrium chemical behaviour due to ionization in hypersonic flows  In practice  each of those flow regimes is treated separately     Reactive flows are flows that are chemically reactive  which finds its applications in many areas  including combustion  IC engine   propulsion devices  rockets  jet engines  and so on   detonations  fire and safety hazards  and astrophysics  In addition to conservation of mass  momentum and energy  conservation of individual species  for example  mass fraction of methane in methane combustion  need to be derived  where the production/depletion rate of any species are obtained by simultaneously solving the equations of chemical kinetics     Magnetohydrodynamics is the multidisciplinary study of the flow of electrically conducting fluids in electromagnetic fields  Examples of such fluids include plasmas  liquid metals  and salt water  The fluid flow equations are solved simultaneously with Maxwell  s equations of electromagnetism     Relativistic fluid dynamics studies the macroscopic and microscopic fluid motion at large velocities comparable to the velocity of light  11  This branch of fluid dynamics accounts for the relativistic effects both from the special theory of relativity and the general theory of relativity  The governing equations are derived in Riemannian geometry for Minkowski spacetime     The concept of pressure is central to the study of both fluid statics and fluid dynamics   A pressure can be identified for every point in a body of fluid  regardless of whether the fluid is in motion or not   Pressure can be measured using an aneroid  Bourdon tube  mercury column  or various other methods     Some of the terminology that is necessary in the study of fluid dynamics is not found in other similar areas of study   In particular  some of the terminology used in fluid dynamics is not used in fluid statics     The concepts of total pressure and dynamic pressure arise from Bernoulli  s equation and are significant in the study of all fluid flows    These two pressures are not pressures in the usual sensethey cannot be measured using an aneroid  Bourdon tube or mercury column    To avoid potential ambiguity when referring to pressure in fluid dynamics  many authors use the term static pressure to distinguish it from total pressure and dynamic pressure   Static pressure is identical to pressure and can be identified for every point in a fluid flow field     A point in a fluid flow where the flow has come to rest  that is to say  speed is equal to zero adjacent to some solid body immersed in the fluid flow  is of special significance   It is of such importance that it is given a special namea stagnation point   The static pressure at the stagnation point is of special significance and is given its own namestagnation pressure   In incompressible flows  the stagnation pressure at a stagnation point is equal to the total pressure throughout the flow field     In a compressible fluid  it is convenient to define the total conditions  also called stagnation conditions  for all thermodynamic state properties  such as total temperature  total enthalpy  total speed of sound    These total flow conditions are a function of the fluid velocity and have different values in frames of reference with different motion     To avoid potential ambiguity when referring to the properties of the fluid associated with the state of the fluid rather than its motion  the prefix \"static\" is commonly used  such as static temperature and static enthalpy    Where there is no prefix  the fluid property is the static condition  so \"density\" and \"static density\" mean the same thing    The static conditions are independent of the frame of reference     Because the total flow conditions are defined by isentropically bringing the fluid to rest  there is no need to distinguish between total entropy and static entropy as they are always equal by definition   As such  entropy is most commonly referred to as simply \"entropy\"     In mathematics  a differential equation is an equation that relates one or more functions and their derivatives  1  In applications  the functions generally represent physical quantities  the derivatives represent their rates of change  and the differential equation defines a relationship between the two  Such relations are common  therefore  differential equations play a prominent role in many disciplines including engineering  physics  economics  and biology     Mainly the study of differential equations consists of the study of their solutions  the set of functions that satisfy each equation   and of the properties of their solutions  Only the simplest differential equations are solvable by explicit formulas  however  many properties of solutions of a given differential equation may be determined without computing them exactly     Often when a closed-form expression for the solutions is not available  solutions may be approximated numerically using computers  The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations  while many numerical methods have been developed to determine solutions with a given degree of accuracy     Differential equations first came into existence with the invention of calculus by Newton and Leibniz  In Chapter 2 of his 1671 work Methodus fluxionum et Serierum Infinitarum  2  Isaac Newton listed three kinds of differential equations     In all these cases  y is an unknown function of x  or of x1 and x2   and f is a given function     He solves these examples and others using infinite series and discusses the non-uniqueness of solutions     Jacob Bernoulli proposed the Bernoulli differential equation in 1695  3  This is an ordinary differential equation of the form    for which the following year Leibniz obtained solutions by simplifying it  4     Historically  the problem of a vibrating string such as that of a musical instrument was studied by Jean le Rond d  Alembert  Leonhard Euler  Daniel Bernoulli  and Joseph-Louis Lagrange  5  6  7  8  In 1746  dAlembert discovered the one-dimensional wave equation  and within ten years Euler discovered the three-dimensional wave equation  9     The EulerLagrange equation was developed in the 1750s by Euler and Lagrange in connection with their studies of the tautochrone problem  This is the problem of determining a curve on which a weighted particle will fall to a fixed point in a fixed amount of time  independent of the starting point  Lagrange solved this problem in 1755 and sent the solution to Euler  Both further developed Lagrange  s method and applied it to mechanics  which led to the formulation of Lagrangian mechanics     In 1822  Fourier published his work on heat flow in Thorie analytique de la chaleur  The Analytic Theory of Heat   10  in which he based his reasoning on Newton  s law of cooling  namely  that the flow of heat between two adjacent molecules is proportional to the extremely small difference of their temperatures  Contained in this book was Fourier  s proposal of his heat equation for conductive diffusion of heat  This partial differential equation is now taught to every student of mathematical physics     In classical mechanics  the motion of a body is described by its position and velocity as the time value varies  Newton  s laws allow these variables to be expressed dynamically  given the position  velocity  acceleration and various forces acting on the body  as a differential equation for the unknown position of the body as a function of time     In some cases  this differential equation  called an equation of motion  may be solved explicitly    \"An example of modeling a real-world problem using differential equations is the determination of the velocity of a ball falling through the air  considering only gravity and air resistance  The ball s acceleration towards the ground is the acceleration due to gravity minus the deceleration due to air resistance  Gravity is considered constant  and air resistance may be modeled as proportional to the ball s velocity  This means that the ball s acceleration  which is a derivative of its velocity  depends on the velocity  and the velocity depends on time   Finding the velocity as a function of time involves solving a differential equation and verifying its validity  n\"   Differential equations can be divided into several types  Apart from describing the properties of the equation itself  these classes of differential equations can help inform the choice of approach to a solution  Commonly used distinctions include whether the equation is ordinary or partial  linear or non-linear  and homogeneous or heterogeneous  This list is far from exhaustive  there are many other properties and subclasses of differential equations which can be very useful in specific contexts     An ordinary differential equation  ODE  is an equation containing an unknown function of one real or complex variable x  its derivatives  and some given functions of x  The unknown function is generally represented by a variable  often denoted y   which  therefore  depends on x  Thus x is often called the independent variable of the equation  The term \"ordinary\" is used in contrast with the term partial differential equation  which may be with respect to more than one independent variable     Linear differential equations are the differential equations that are linear in the unknown function and its derivatives  Their theory is well developed  and in many cases one may express their solutions in terms of integrals     Most ODEs that are encountered in physics are linear  Therefore  most special functions may be defined as solutions of linear differential equations  see Holonomic function      As  in general  the solutions of a differential equation cannot be expressed by a closed-form expression  numerical methods are commonly used for solving differential equations on a computer     A partial differential equation  PDE  is a differential equation that contains unknown multivariable functions and their partial derivatives   This is in contrast to ordinary differential equations  which deal with functions of a single variable and their derivatives   PDEs are used to formulate problems involving functions of several variables  and are either solved in closed form  or used to create a relevant computer model     PDEs can be used to describe a wide variety of phenomena in nature such as sound  heat  electrostatics  electrodynamics  fluid flow  elasticity  or quantum mechanics  These seemingly distinct physical phenomena can be formalized similarly in terms of PDEs  Just as ordinary differential equations often model one-dimensional dynamical systems  partial differential equations often model multidimensional systems  Stochastic partial differential equations generalize partial differential equations for modeling randomness     A non-linear differential equation is a differential equation that is not a linear equation in the unknown function and its derivatives  the linearity or non-linearity in the arguments of the function are not considered here   There are very few methods of solving nonlinear differential equations exactly  those that are known typically depend on the equation having particular symmetries  Nonlinear differential equations can exhibit very complicated behaviour over extended time intervals  characteristic of chaos  Even the fundamental questions of existence  uniqueness  and extendability of solutions for nonlinear differential equations  and well-posedness of initial and boundary value problems for nonlinear PDEs are hard problems and their resolution in special cases is considered to be a significant advance in the mathematical theory  cf  NavierStokes existence and smoothness   However  if the differential equation is a correctly formulated representation of a meaningful physical process  then one expects it to have a solution  11     Linear differential equations frequently appear as approximations to nonlinear equations  These approximations are only valid under restricted conditions  For example  the harmonic oscillator equation is an approximation to the nonlinear pendulum equation that is valid for small amplitude oscillations  see below      Differential equations are described by their order  determined by the term with the highest derivatives  An equation containing only first derivatives is a first-order differential equation  an equation containing the second derivative is a second-order differential equation  and so on  12  13  Differential equations that describe natural phenomena almost always have only first and second order derivatives in them  but there are some exceptions  such as the thin film equation  which is a fourth order partial differential equation     In the first group of examples u is an unknown function of x  and c and  are constants that are supposed to be known  Two broad classifications of both ordinary and partial differential equations consist of distinguishing between linear and nonlinear differential equations  and between homogeneous differential equations and heterogeneous ones     In the next group of examples  the unknown function u depends on two variables x and t or x and y     Solving differential equations is not like solving algebraic equations  Not only are their solutions often unclear  but whether solutions are unique or exist at all are also notable subjects of interest     For first order initial value problems  the Peano existence theorem gives one set of circumstances in which a solution exists  Given any point                              a                 b                          displaystyle  a b     in the xy-plane  define some rectangular region                     Z                 displaystyle Z     such that                     Z        =                 l                 m                                  n                 p                          displaystyle Z= l m   times  p     and                              a                 b                          displaystyle  a b     is in the interior of                     Z                 displaystyle Z     If we are given a differential equation                                                         d              y                                      d              x                                      =        g                 x                 y                          displaystyle    frac  dy  dx  =g x y     and the condition that                     y        =        b                 displaystyle y=b    when                     x        =        a                 displaystyle x=a     then there is locally a solution to this problem if                     g                 x                 y                          displaystyle g x y     and                                                                       g                                                    x                                               displaystyle    frac    partial g    partial x      are both continuous on                     Z                 displaystyle Z     This solution exists on some interval with its center at                     a                 displaystyle a     The solution may not be unique   See Ordinary differential equation for other results      However  this only helps us with first order initial value problems  Suppose we had a linear initial value problem of the nth order     such that    For any nonzero                               f                      n                                   x                          displaystyle f_ n  x      if                                        f                      0                                             f                      1                                                             displaystyle    f_ 0  f_ 1    ldots        and                     g                 displaystyle g    are continuous on some interval containing                               x                      0                                   displaystyle x_ 0                          y                 displaystyle y    is unique and exists  14     The theory of differential equations is closely related to the theory of difference equations  in which the coordinates assume only discrete values  and the relationship involves values of the unknown function or functions and values at nearby coordinates  Many methods to compute numerical solutions of differential equations or study the properties of differential equations involve the approximation of the solution of a differential equation by the solution of a corresponding difference equation     The study of differential equations is a wide field in pure and applied mathematics  physics  and engineering  All of these disciplines are concerned with the properties of differential equations of various types  Pure mathematics focuses on the existence and uniqueness of solutions  while applied mathematics emphasizes the rigorous justification of the methods for approximating solutions  Differential equations play an important role in modeling virtually every physical  technical  or biological process  from celestial motion  to bridge design  to interactions between neurons  Differential equations such as those used to solve real-life problems may not necessarily be directly solvable  i e  do not have closed form solutions  Instead  solutions can be approximated using numerical methods     Many fundamental laws of physics and chemistry can be formulated as differential equations  In biology and economics  differential equations are used to model the behavior of complex systems  The mathematical theory of differential equations first developed together with the sciences where the equations had originated and where the results found application  However  diverse problems  sometimes originating in quite distinct scientific fields  may give rise to identical differential equations  Whenever this happens  mathematical theory behind the equations can be viewed as a unifying principle behind diverse phenomena  As an example  consider the propagation of light and sound in the atmosphere  and of waves on the surface of a pond  All of them may be described by the same second-order partial differential equation  the wave equation  which allows us to think of light and sound as forms of waves  much like familiar waves in the water  Conduction of heat  the theory of which was developed by Joseph Fourier  is governed by another second-order partial differential equation  the heat equation  It turns out that many diffusion processes  while seemingly different  are described by the same equation  the BlackScholes equation in finance is  for instance  related to the heat equation     The number of differential equations that have received a name  in various scientific areas is a witness of the importance of the topic  See List of named differential equations     Some CAS softwares can solve differential equations  These CAS softwares and their commands are worth mentioning     Test and Training Enabling Architecture  TENA  is an architecture designed to bring interoperability to United States Department of Defense test and training systems  TENA is designed to promote integrated testing and simulation-based acquisition through the use of a large-scale  distributed  real-time synthetic environment  which integrates testing  ntraining  simulation  and high-performance computing technologies  distributed across many facilities  using a common architecture  1     The purpose of TENA is to provide the architecture and the software implementation necessary to 2     TENA recognizes five basic categories of software     JDAT displays TENA data on Joint Windows based Warfare Assessment Mode  JWinWAM   3     The TENA software was developed for use by the United States Government with unlimited rights  The software is provided freely for the purpose of promoting interoperability among United States Department of Defense systems  There are no International Traffic in Arms Regulations  ITAR  or export restrictions in using the TENA middleware and related TENA products at an international site  although any restrictions for user provided object models  software  or documents are the responsibility of the author s  of those products   nUse of the TENA software  source code and binary code  by individuals is permitted only upon acceptance of license  4     TENA official website     nThe High Level Architecture  HLA  is a standard for distributed simulation  used when building a simulation for a larger purpose by combining  federating  several simulations  1  The standard was developed in the 90s under the leadership of the US Department of Defense 2  and was later transitioned to become an open international IEEE standard  It is a recommended standard within NATO through STANAG 4603  3  Today the HLA is used in a number of domains including defense and security and civilian applications     The purpose of HLA is to enable interoperability and reuse  Key properties of HLA are     HLA forms the basis for developing standardized and extendable FOMs in different communities  for example in aerospace and defense     The architecture specifies the following components     Together the above components form a Federation     The HLA standard consists of three parts     HLA was initiated in the early 1990s when Dr  Anita K  Jones  the Director of Defense Research and Engineering within the US Department of Defense  gave the Defense Modeling and Simulation Office  DMSO  the task of assuring interoperability and reusability of defense models and simulations  1  In 1995 DMSO formulated a vision for modeling and simulation and established a modeling and simulation masterplan  which included the High Level Architecture     Two protocols for M&S interoperability already existed  Distributed Interactive Simulation  DIS   focusing on real-time platform level simulation with a fixed object model  and Aggregate Level Simulation Protocol  ALSP  focusing on simulation of aggregate with time management  ownership management and flexible object models  called confederation models  The purpose of HLA was to provide one unified standard that would meet the simulation interoperability requirements of all US DoD components  2     The development of HLA was based on four prototypical federations  the Platform Prototype Federation  the Joint Training Protofederation  the Analysis Protofederation and the Engineering Prototype Federation  The HLA specification was prototyped and refined  until HLA 1 3 was finally released  To facilitate usage outside of the defense community  HLA was then transitioned into an IEEE standard  maintained by Simulation Interoperability Standards Organization  SISO   To facilitate the migration for DIS users  a Federation Object Model corresponding to the fixed object model of DIS was also developed as the Real-time Platform Reference FOM  RPR FOM      The following HLA versions exist     HLA 1 3 was published in March 1998 by DMSO  It consists of       The US DoD also published interpretations for HLA 1 3     HLA IEEE 1516-2000 was published in 2000 by IEEE  It consists of     Major improvements in IEEE 1516-2000 included an XML-based FOM with detailed data type specifications  as well as an improved DDM design     The IEEE 1516-2000 standard was also complemented by a recommended development process as well as a recommended VV&A process     It was soon found that the 1516-2000 standard had APIs that were slightly different for each RTI implementation  SISO produced a standard with alternate  dynamic link compatible  DLC  C++ and Java APIs     The DLC APIs were later merged into the main standard     The IEEE 1516-2010 standard was published in August 2010 by IEEE and is commonly known as HLA Evolved  7  It consists of     Major improvements in IEEE 1516-2010 include Modular FOMs  8  incorporation of the DLC APIs in C++ and Java  a Web Services API 9  and Fault Tolerance  10     Machine-readable parts of this version of HLA  such as XML Schemas  C++  Java and WSDL APIs as well as FOM/SOM samples can be downloaded from the IEEE 1516 download area of the IEEE web site  The full standards texts are available at no cost to SISO members or can be purchased from the IEEE shop     The development of a new version of HLA started in January 2016 by SISO and is currently ongoing     The HLA standard consists of three parts     The RTI services are defined in the HLA Interface Specification  They are grouped into seven service groups  In addition to these services  the Management Object Model  MOM  provides services that makes it possible to inspect and adjust the state of the federation programmatically     Most RTIs consist of a Central RTI Component  CRC   which is an executable and Local RTI Components  LRCs   which are libraries that are used by the federates  Services are provided through a C++ or Java API and also using Web services  In the C++ and Java APIs  services are invoked using calls to an instance of the RTI Ambassador class  The RTI delivers information to a federate using callbacks  which are delivered using calls to an instance of the Federate Ambassador class  In the Web Services API  defined using WSDL  calls are made  and callbacks are fetched  by the federate using Web Services requests and responses     The service group descriptions below focus on key services  Exceptions and advisories are not included     The purpose of Federation Management services  described in chapter 4 of the HLA Interface Specification  5  is to manage Federation Executions as well as federation-wide operations such as Synchronization Points and Save/Restore     One set of Federation Management services manages the connection to the RTI  the federation execution and the set of joined federates  Key services are     Another set of services relates to synchronization points  These are federation-wide events where all  or selected federates are required to complete an operation  such as initializing a scenario  before the execution can continue  Key services are     Yet another set of service relates to saving and restoring a federation execution  A save operation requires both the RTI and each federate to perform a save of their internal state  A restore operation requires both the RTI and each federate to perform a restore of their internal state  Key services are     Save     Restore     The purpose of Declaration Management services  described in chapter 5 of the HLA Interface Specification  5  is to enable federates to declare what information they wish to publish  send  and subscribe to  receive  based on object and interaction classes in the FOM  The RTI uses this information to route updates and interactions to subscribing federates  For an object class  publishing and subscribing are performed for a specific set of attributes  For interaction classes  the entire interaction  including all parameters  is published and subscribed  Key services are     The purpose of Object Management services  described in chapter 6 of the HLA Interface Specification  5  is to enable federates to share information about object instances and to exchange interactions     Object instance names can be reserved or be automatically generated  Federates can register object instances of specified object classes  that are then discovered by subscribing federates  Attributes of these object instances can be updated  These updates will be reflected to subscribing federates  Interactions can be sent  These interactions will be delivered to subscribing federates  Key services are     Objects     Attributes     Interactions     The purpose of Ownership Management services  described in chapter 7 of the HLA Interface Specification  5  is to dynamically manage what federate that simulates what aspect of an object instance  In HLA  only one federate is allowed to update a given attribute of a given object instance  That federate is considered the owner of the attribute  A federate that registers a new object instance will automatically be the owner of all attributes it publishes  In some cases  attributes of an object instance may become unowned  i e  not owned by any federate     Ownership management provides services for transferring ownership of one or several attributes at runtime  which can include a federate Divesting the attribute and another federate Acquiring the attribute  There are two main patterns  pull that are initiated by the acquiring federate and push that are initiated by the divesting federate     Key services for initiating pull ownership are     Key services for initiating push ownership are     All object instances have a predefined attribute called HLAPrivilegeToDeleteObject  Only the owner of this attribute for an object instance is allowed to delete the object instance  Ownership of this attribute can be transferred at runtime using the above operations     The information exchange in an HLA federation takes place in real-time with immediate  Receive Order  RO  delivery of messages  unless HLA Time Management has been enabled  The purpose of HLA Time Management  described in chapter 8 of the HLA Interface Specification  5  is to guarantee causality and a correct and consistent exchange of time stamped messages  updates and interactions  in Time Stamp Order  TSO   no matter if the federation executes in real-time  faster-than-real-time  slower-than-real-time or as-fast-as-possible     Some important concepts in HLA Time Management are     Logical time  A time axis in HLA  starting at zero  Logical time is used for Time Management timestamps and operations  The logical time axis can be mapped to the scenario time of the federation  An example of such a mapping is to let zero represent the scenario time 8 00 of the 1-Jan-1066 and let an increment by one represent one second of the scenario     Lookahead  A time interval specifying the lowest time in the future for which a federate will produce messages  For a federate with a fixed time step  this is usually the length of the time step     Granted  A federate is granted  allowed to advance  to a particular logical time by the RTI  when all time-stamped messages up to that time have been delivered  The federate can then safely start calculating messages with a timestamp in the future  This timestamp may not be earlier than the granted time plus the federates lookahead     Advancing  When a federate has finished producing data for the granted time plus the lookahead  it may request to be advanced to a later time  which also means that it promises not to produce any more messages with a time stamp less than the requested time plus the lookahead  The federate is now in the advancing state     Time Regulating  A federate that sends time stamped events is considered Time Regulating since the time advance by other federates may be regulated by this     Time Constrained  A federate that receives time managed events is considered Time Constrained since the reception of time stamped messages  constrains its time advance     The main principles of HLA Time Management are that     Example of Lookahead  granted and advancing     If at least one federate in the federation performs pacing  i e   correlates their time advance requests with a real time clock  the federation may run in real time or scaled real time  Without pacing  the federation will run as fast as possible  which is used for example in Monte Carlo simulation     Key services include     For event driven simulation it is also possible for a federate to request to be advanced to the next event using the following service     Another important concept is Greatest Available Logical Time  GALT   The greatest time that each federate can be granted to  depends on the time that other federates have been granted to as well as their lookahead  The GALT for a federate specifies how far a federate can be granted  without having to wait for other federates to be granted  This is particularly interesting for a federate that joins late into a time managed federation     Key services for GALT are     More advanced services include     The purpose of DDM  described in chapter 9 of the HLA Interface Specification  5  is to increase scalability of federations by performing additional filtering of subscribed data beyond class and attribute subscriptions  11  Filtering can be based on continuous values  like latitude and longitude  or discreet values  like car brand      Key concepts of DDM are     Dimension  a named interval  0   used for filtering  with values starting with 0 and ending with an upper bound  Data in the simulation domain is mapped to one or more dimensions  For example  dimensions for geographical filtering could be LatitudeDimension and LongitudeDimension  A dimension for filtering based on car brand could be CarBrandDimension     Normalization function  a function that maps input values to integer values to be used in a dimension  An example is that a normalization function for the LatitudeDimension could map a latitude value ranging from -90 0 to +90 0 to an integer in the range 0  179  A normalization function for the CarBrandDimension could map a set of car brands Kia  Ford  BMW and Peugeot to an integer in the range 0  3     Range  an interval on a dimension  specified by a lower bound  inclusive  and an upper bound  exclusive      Region  a set of ranges  each one relating to a particular dimension  In the above example  a region could consist of the range  3  5  for LatitudeDimension  55  65  for LongitudeDimension and  0  1  for the CarBrandDimension  At runtime Region Realizations  objects  are instantiated to represent regions  The ranges of a region can be modified over time     Region overlap  two regions overlap if  for all dimensions that they have in common  their ranges overlap     At runtime  a federate can provide Regions when subscribing to object class attributes and interactions  Regions are also used when sending attribute updates and interactions  When DDM is used  attribute updates and interactions will only be delivered if there is a region overlap     Key services for Regions are     Key services for exchanging attribute updates with DDM are     Key services for exchanging interactions with DDM are     The HLA Support Services  described in chapter 10 of the HLA Interface Specification  5  provide a number of supporting services  These include     The purpose of the Management Object Model  described in chapter 11 of the HLA Interface Specification  5  is to provide services for managing a federation  This is performed using the MOM object and interaction classes  MOM objects are defined in a special FOM module called the MIM  that is automatically loaded by the RTI  Key MOM features include     The OMT is a template used for describing Federation Object Models  FOMs  and Simulation Object Models  SOMs   FOMs and SOMs can be represented in a tabular format or using XML  The latter format is used when a FOM is loaded into the RTI     In earlier versions of HLA  FOMs were monolithic  but the current version of the standard supports modular FOMs  i e  several modules  covering different aspects of the information exchange  can be provided to the RTI     A number of predefined classes  datatypes  dimensions and transportation types are provided in the standard  These are provided in the HLAstandardMIM xml FOM module  Predefined concepts are prefixed with HLA  for example HLAobjectRoot and HLAunicodeString     There are three different XML Schemas for the OMT     The purpose of the identification table is to provide meta-data about the model  to facilitate reuse of the FOM/SOM or federates      The following fields are specified     The purpose of the object class structure table is to specify the class hierarchy  subclass/superclass  of the object classes that are used to instantiate objects in an HLA federation  Object class attributes are inherited from superclasses to subclasses based on this hierarchy  The root of the object class tree is known as HLAobjectRoot  An example of a fully qualified name of an object class is HLAobjectRoot Car ElectricCar    The following fields are specified for an object class in the hierarchy     The purpose of the attribute table is to specify the attributes that are available for a given object class  Since attributes are inherited  an object class will have the union of all attributes that are locally defined on the object class or specified on any direct or indirect superclass      The following fields are specified for an attribute    The purpose of the interaction class structure table is to specify the class hierarchy  subclass/superclass  of the interaction classes that are used to exchange interactions in an HLA federation  Interaction class parameters are inherited from superclasses to subclasses based on this hierarchy  The root of the interaction class tree is known as HLAinteractionRoot  An example of a fully qualified name of an interaction class is HLAinteractionRoot CarCommand Start     The following fields are specified for an interaction class in the hierarchy     The purpose of the parameter table is to specify the parameters that are available for a given interaction class  Since parameters are inherited  an interaction class will have the union of all parameters that are locally defined on the interaction class or specified on any direct or indirect superclass     The purpose of the dimensions table is to specify the DDM dimensions  used for attributes and interaction classes     The purpose of the time representation table is to specify the datatypes used by the Time Management services     A user-supplied tag can be supplied when calling certain HLA services  The purpose of the user-supplied tag table is to specify the datatypes of these tags     The purpose of the synchronization table is to specify the synchronisation points used in a federation     The purpose of the transportation type table is to specify the available transportation types  There are two predefined transportation types  HLAreliable and HLAbestEffort     The purpose of the update rate table is to specify the available maximum update rates     The runtime behaviour of the RTI can be controlled using a number of predefined switches  The purpose of the switches table is to provide initial values for these switches  Some of the switches can alsobe updated at runtime     The purpose of the datatype tables is to provide specifications of the datatypes used for attributes  parameters  dimensions  time representation  user supplied tag and synchronization points  There are six categories of datatypes  with a separate tabular format for each of them     The purpose of the basic data representation table is to provide binary representations for use in other tables  A number of predefined basic datatypes are provided in the HLA standard  HLAinteger16BE  HLAinteger32BE  HLAinteger64BE  HLAfloat32BE  HLAfloat64BE  HLAoctetPairBE  HLAinteger16LE  HLAinteger32LE  HLAinteger64LE  HLAfloat32LE  HLAfloat64LE  HLAoctetPairLE and HLAoctet  The set of basic datatypes is usually not extended with user defined basic datatypes     The purpose of the simple datatypes table is to describe simple scalar data items  A number of predefined simple datatypes are provided in the HLA standard  HLAASCIIchar  HLAunicodeChar  HLAbyte  HLAinteger64time and HLAfloat64time  It is common to include user defined simple datatypes in a FOM     The purpose of the enumerated datatypes table is to describe data elements that can take on a finite discrete set of values  One predefined enumerated datatype is provided in the standard  HLAboolean  It is common to include user defined enumerated datatypes in a FOM     The purpose of the enumerated datatypes table is to describe arrays of data elements  simple  enumerated  arrays  fixed records or variant records   A number of predefined simple datatypes are provided in the HLA standard  HLAASCIIstring  HLAunicodeString  HLAopaqueData and HLAtoken  It is common to include user defined array datatypes in a FOM     The purpose of the fixed record datatypes table is to describe records with a fixed set of data elements  simple  enumerated  arrays  fixed records or variant records   It is common to include user defined simple datatypes in a FOM  No predefined simple datatypes are provided in the HLA standard     The purpose of the notes the table is to provide annotations and additional descriptions of items in other tables     The HLA rules describe the responsibilities of federations and the federates that join  12     The IEEE 1516 standard has been revised under the SISO HLA-Evolved Product Development Group and was approved 25-Mar-2010 by the IEEE Standards Activities Board  The revised IEEE 15162010 standard includes current DoD standard interpretations and the EDLC API  an extended version of the SISO DLC API  Other major improvements include     In order to ensure the proper interaction between simulations  a way of testing federate conformance is defined  This involves ensuring that every class and interaction listed in the SOM for a particular federate is used according to the usage described  \"PublishSubscribe\"  \"Publish\"  \"Subscribe\" or \"None\"     HLA  in both the current IEEE 1516 version and its ancestor \"1 3\" version  is the subject of the NATO standardization agreement  STANAG 4603  for modeling and simulation  Modeling And Simulation Architecture Standards For Technical Interoperability  High Level Architecture  HLA   13     The Base Object Model  BOM   SISO-STD-003-2006 is a related standard by SISO to provide better reuse and composability for HLA simulations  It provides a way to specify conceptual models and how to map them to an HLA FOM  14     In regards to the Distributed Modeling and Simulation  DM&S  industry the most often used alternative to the HLA  for real-time simulation of military platforms  is Distributed Interactive Simulation  DIS   IEEE 1278 1-2012  a simulation protocol  Most HLA RTI vendors also feature DIS in their products   As for middleware applications that most closely match HLA features  such as nthe publish and subscribe feature  P&S  see Data Distribution Service  DDS  which shares many of the same characteristics but having an open on-the-wire protocol for system interoperability  15     HLA is a Message-oriented middleware that defines as a set of services  provided by a C++ or Java API  There is no standardized on-the-wire protocol  Participants in a federation must use RTI libraries from the same provider and usually also of the same version  which in some cases is perceived as a drawback  16  Most current tools also provide interconnectivity through sockets     Distributed Interactive Simulation  DIS  is an IEEE standard for conducting real-time platform-level wargaming across multiple host computers and is used worldwide  especially by military organizations but also by other agencies such as those involved in space exploration and medicine     The standard was developed over a series of \"DIS Workshops\" at the Interactive Networked Simulation for Training symposium  held by the University of Central Florida  s Institute for Simulation and Training  IST   The standard itself is very closely patterned after the original SIMNET distributed interactive simulation protocol  developed by Bolt  Beranek and Newman  BBN  for Defense Advanced Research Project Agency  DARPA  in the early through late 1980s  BBN introduced the concept of dead reckoning to efficiently transmit the state of battle field entities     In the early 1990s  IST was contracted by the United States Defense Advanced Research Project Agency to undertake research in support of the US Army Simulator Network  SimNet  program  Funding and research interest for DIS standards development decreased following the proposal and promulgation of its successor  the High Level Architecture  simulation   HLA  in 1996  HLA was produced by the merger of the DIS protocol with the Aggregate Level Simulation Protocol  ALSP  designed by MITRE     There was a NATO standardisation agreement  STANAG 4482  Standardised Information Technology Protocols for Distributed Interactive Simulation  DIS   adopted in 1995  on DIS for modelling and simulation interoperability  This was retired in favour of HLA in 1998 and officially cancelled in 2010 by the NATO Standardization Agency  NSA      DIS is defined under IEEE Standard 1278     In addition to the IEEE standards  the Simulation Interoperability Standards Organization  SISO  maintains and publishes an \"enumerations and bit encoded fields\" document yearly  This document is referenced by the IEEE standards and used by DIS  TENA and HLA federations   Both PDF and XML versions are available     SISO  a sponsor committee of the IEEE  promulgates improvements in DIS  Major changes occurred in the DIS 7 update to IEEE 1278 1 1  to make DIS more extensible  efficient and to support the simulation of more real world capabilities  2     Simulation state information is encoded in formatted messages  known as protocol data units  PDUs  and exchanged between hosts using existing transport layer protocols  including multicast  though broadcast User Datagram Protocol is also supported  There are several versions of the DIS application protocol  not only including the formal standards  but also drafts submitted during the standards balloting process     The current version  DIS 7  defines 72 different PDU 3  types  arranged into 13 families  Frequently used PDU types are listed below for each family   PDU and family names shown in italics are found in DIS 7     The RPR FOM is a Federation Object Model  FOM  for the High-Level Architecture designed to organize the PDUs of DIS into an HLA object class and interaction class hierarchy  It has been developed as the SISO standard SISO-STD-001  4  The purpose is to support transition of legacy DIS systems to the HLA  to enhance a priori interoperability among RPR FOM users and to support newly developed federates with similar requirements  The most recent version is RPR FOM version 2 0 5  that corresponds to DIS version 6     The Aggregate Level Simulation Protocol  ALSP  is a protocol and supporting software that enables simulations to interoperate with one another   Replaced by the High Level Architecture  simulation   HLA   it was used by the US military to link analytic and training simulations     ALSP consists of     In 1990  the Defense Advanced Research Projects Agency  DARPA  employed The MITRE Corporation to study the application of distributed interactive simulation principles employed in SIMNET to aggregate-level constructive training simulations   Based on prototype efforts  a community-based experiment was conducted in 1991 to extend SIMNET to link the US Army  s Corps Battle Simulation  CBS  and the US Air Force  s Air Warfare Simulation  AWSIM    The success of the prototype and users   recognition of the value of this technology to the training community led to development of production software   The first ALSP confederation  providing air-ground interactions between CBS and AWSIM  supported three major exercises in 1992     By 1995  ALSP had transitioned to a multi-Service program with simulations representing the US Army  CBS   the US Air Force  AWSIM   the US Navy  RESA   the US Marine Corps  MTWS   electronic warfare  JECEWSI   logistics  CSSTSS   and intelligence  TACSIM    The program had also transitioned from DARPA  s research and development emphasis to mainstream management by the US Army  s Program Executive Office for Simulation  Training  and Instrumentation  PEO STRI     ALSP developed and demonstrated key aspects of distributed simulation  many of which were applied in the development of HLA      In 1989  the Warrior Preparation Center  WPC  in Einsiedlerhof  Germany hosted the computerized military exercise ACE-89  The Defense Advanced Research Projects Agency  DARPA  used ACE-89 as a technology insertion opportunity by funding deployment of the Defense Simulation Internet  DSI   Its packetized video teleconferencing brought general officers of NATO nations face-to-face during a military exercise for the first time  this was well received  But the software application of DSI  distribution of Ground Warfare Simulation  GRWSIM   was less successful  The GRWSIM simulation was unreliable and its distributed database was inconsistent  degrading the effectiveness of the exercise     DARPA was funding development of a distributed tank trainer system called SIMNET where individual  computerized  tank-crew trainers were connected over local area networks and the DSI to cooperate in a single  virtual battlefield  The success of SIMNET  the disappointment of ACE-89  and the desire to combine existing combat simulations prompted DARPA to initiate research that lead to ALSP     DARPA sponsored the design of a general interface between large  existing  aggregate-level combat simulations  Aggregate-level combat simulations use Lanchestrian models of combat rather than individual physical weapon models and are typically used for high-level training  Despite representational differences  several principles of SIMNET applied to aggregate-level simulations     The ALSP challenge had requirements beyond those of SIMNET     A conceptual framework is an organizing structure of concepts that facilitates simulation model development  2  Common conceptual frameworks include  event scheduling  activity scanning and process interaction     The ALSP conceptual framework is object-based where a model is composed of objects that are characterized by attributes to which values are assigned  Object classes are organized hierarchically in much the same manner as with object-oriented programming languages   ALSP supports a confederation of simulations that coordinate using a common model     To design a mechanism that permits existing simulations to interact  two strategies are possible   1  define an infrastructure that translates between the representations in each simulation  or  2  define a common representational scheme and require all simulations to map to that scheme     The first strategy requires few perturbations to existing simulations  interaction is facilitated entirely through the interconnection infrastructure  However  this solution does not scale well  Because of an underlying requirement for scalability  the ALSP design adopted the second strategy  ALSP prescribes that each simulation maps between the representational scheme of the confederation and its own representational scheme  This mapping represents one of the three ways in which a simulation must be altered to participate in an ALSP confederation  The remaining modifications are     In stand-alone simulations  objects come into  and go out of  existence with the passage of simulation time and the disposition of these objects is solely the purview of the simulation  When acting within a confederation  the simulation-object relationship is more complicated     The simulation-object ownership property is dynamic  i e  during its lifetime an object may be owned by more than one simulation  In fact  for any value of simulation time  several simulations may own different attributes of a given object  By convention  a simulation owns an object if it owns the \"identifying\" attribute of the object  Owning an object  s attribute means that a simulation is responsible for calculating and reporting changes to the value of the attribute  Objects not owned by a particular simulation but within the area of perception for the simulation are known as ghosts  Ghosts are local copies of objects owned by other simulations     When a simulation creates an object  it reports this fact to the confederation to let other simulations create ghosts  Likewise  when a simulation deletes an object  it reports this fact to enable ghost deletion  Whenever a simulation takes an action between one of its objects and a ghost  the simulation must report this to the confederation  In the parlance of ALSP  this is an interaction  nThese fundamental concepts provide the basis for the remainder of the presentation  The term confederation model describes the object hierarchy  attributes and interactions supported by a confederation     The object-based conceptual framework adopted by ALSP defines classes of information that must be distributed  The ALSP Infrastructure Software  AIS  provides data distribution and process coordination  Principal components of AIS are the ALSP Common Module  ACM  and the ALSP Broadcast Emulator  ABE      The ALSP Common Module  ACM  provides a common interface for all simulations and contains the essential functionality for ALSP  One ACM instance exists for each simulation in a confederation   ACM services require time management and object management  they include     Joining and departing a confederation is an integral part of time management process  When a simulation joins a confederation  all other ACMs in the confederation create input message queues for the new simulation  Conversely  when a simulation departs a confederation the other ACMs delete input message queues for that simulation     ALSP time management facilities support discrete event simulation using either asynchronous  next-event  or synchronous  time-stepped  time advance mechanisms  3  The mechanism to support next-event simulations is    The mechanism to support time-stepped simulation is     AIS includes a deadlock avoidance mechanism using null messages  The mechanism requires that the processes have exploitable lookahead characteristics     The ACM administers attribute database and filter information  The attribute database maintains objects known to the simulation  either owned or ghosted  and attributes of those objects that the simulation currently owns  For any object class  attributes may be members of    Information flow across the network can be further restricted through filters  Filtering provides discrimination by  1  object class   2  attribute value or range  and  3  geographic location  Filters also define the interactions relevant to a simulation     The ownership and filtering information maintained by the ACM provide the information necessary to coordinate the transfer of attribute ownership between simulations     An ALSP Broadcast Emulator  ABE  facilitates the distribution of ALSP information  It receives a message on one of its communications paths and retransmits the message on all of its remaining communications paths  This permits configurations where all ALSP components are local to one another  on the same computer or on a local area network   It also permits configurations where sets of ACMs communicate with their own local ABE with inter-ABE communication over wide area networks     The ALSP communication scheme consists of  1  an inter-component communications model that defines the transport layer interface that connects ALSP components   2  a layered protocol for simulation-to-simulation communication  object management  and time management   3  a message filtering scheme to define the information of interest to a simulation  and  4  a mechanism for intelligent message distribution     AIS employs a persistent connection communications model 4  to provide the inter-component communications  The transport layer interface used to provide inter-component communications was dictated by simulation requirements and the transport layer interfaces on AIS-supporting operating systems  local VMS platforms used shared mailboxes  non-local VMS platforms used either Transparent DECnet or TCP/IP  and UNIX-like platforms use TCP/IP    \"The ALSP protocol is based on a set of orthogonal issues that comprise ALSP s problem space  simulation-to-simulation communication  object management  and time management  These issues are addressed by a layered protocol that has at the top a simulation protocol with underlying simulation/ACM  object management  time management  and event distribution protocols  n\"   The simulation protocol is the main level of the ALSP protocol  It consists of four message types     The simulation protocol is text-based  It is defined by an LALR  1  context-free grammar  The semantics of the protocol are confederation-dependent  where the set of classes  class attributes  interactions  and interaction parameters are variable  Therefore  the syntactical representation of the simulation protocol may be defined without a priori knowledge of the semantics of the objects and interactions of any particular confederation     The simulation/ACM connection protocol provides services for managing the connection between a simulation and its ACM and a method of information exchange between a simulation and its ACM  Two services control distribution of simulation protocol messages  events and dispatches  Event messages are time-stamped and delivered in a temporally-consistent order  Dispatch messages are delivered as soon as possible  without regard for simulation time  Additional protocol messages provide connection state  filter registration  attribute lock control  confederation save control  object resource control  and time control services     The object management protocol is a peer-level protocol that sits below the simulation protocol and provides object management services  ACMs solely use it for object attribute creation  acquisition  release  and verification  of the consistency of the distributed object database   These services allow AIS to manage distributed object ownership     Distributed object ownership presumes that no single simulation must own all objects in a confederation  but many simulations require knowledge of some objects  A simulation uses simulation protocol update messages to discover objects owned by other simulations  If this simulation is interested in the objects  it can ghost them  track their locations and state  and model interactions to them from owned objects     Locks implement attribute ownership  A primary function of the object management protocol is to ensure that a simulation only updates attributes for which it has acquired a lock  The object manager in the ACM manages the objects and object attributes of the owned and ghosted objects known to the ACM  Services provided by the simulation/ACM protocol are used by the simulations to interact with the ACM  s attribute locking mechanism  The coordination of status  request  acquisition  and release of object attributes  between ACMs  uses the object management protocol  nEach attribute of each object known to a given ACM has a status that assumes one of three values    \"From the ACM s perspective  objects come into existence through the registration process performed by its simulation or through the discovery of objects registered by other simulations  The initial state attribute locks for registered objects and discovered objects is as follows  n\"  \"The time management protocol is also a peer-level protocol that sits below the simulation protocol  It provides time management services for synchronizing simulation time among ACMs  The protocol provides services for the distributed coordination of a simulation s entrance into the confederation  time progression  and confederation saves  n\"   The join/resign services and time synchronization mechanisms are described in Section earlier  The save mechanism provides fault tolerance  Coordination is required to produce a consistent snapshot of all ACMs  translators and simulations for a particular value of simulation time     The ACM uses simulation message filtering to evaluates the content of a message received from the confederation  The ACM delivers messages to its simulation that are of interest  and pass filtering criteria and discards those that are not of interest  The ACM filters two types of messages  update messages and interaction messages    \"Update messages  The ACM evaluates update messages based on the simulation s update message filtering criteria that the simulation provides  As discussed in earlier  when an ACM receives an update message there are four possible outcomes   1  the ACM discards the message   2  the ACM sends the simulation a create message   3  the ACM sends the simulation the update message  or  4  the ACM sends the simulation a delete message  n\"   Interaction messages  An ACM may discard interaction messages because of the kind parameter  The kind parameter has a hierarchical structure similar to the object class structure  The simulation informs its ACM of the interaction kinds that should pass or fail the interaction filter     To minimize message traffic between components in an ALSP confederation  AIS employs a form of intelligent message routing that uses the Event Distribution Protocol  EDP   5  The EDP allows ACMs to inform the other AIS components about the update and interaction filters registered by their simulations  nIn the case of update messages  distribution of this information allows ACMs to only distribute data on classes  and attributes of classes  that are of interest to the confederation  The ABE also use this information to send only information that is of interest to the components it serves  For interaction messages  the process is similar  except that the kind parameter in the interaction message determines where the message is sent     Dynamic simulation  or dynamic system simulation  is the use of a computer program to model the time-varying behavior of a dynamical system  The systems are typically described by ordinary differential equations or partial differential equations  A simulation run solves the state-equation system to find the behavior of the state variables over a specified period of time  The equation is solved through numerical integration methods to produce the transient behavior of the state variables  Simulation of dynamic systems predicts the values of model-system state variables  as they are determined the past state values  This relationship is found by creating a model of the system  1     Simulation models are commonly obtained from discrete-time approximations of continuous-time mathematical models  2  nAs mathematical models incorporate real-world constraints  like gear backlash and rebound from a hard stop  equations become nonlinear  This requires numerical methods to solve the equations  3  nA numerical simulation is done by stepping through a time interval and calculating the integral of the derivatives through numerical integration   nSome methods use a fixed step through the interval  and others use an adaptive step that can shrink or grow automatically to maintain an acceptable error tolerance  Some methods can use different time steps in different parts of the simulation model     There are two types of system models to be simulated  difference-equation models  and differential-equation models  Classical physics is usually based on differential equation models  This is why most old simulation programs are simply differential equation solvers and delegate solving difference-equations to procedural program segments Some dynamic systems are modeled with differential equations that can only be presented in an implicit form  These differential-algebraic-equation systems require special mathematical methods for simulation  4     Some complex systems behavior can be quite sensitive to initial conditions  which could lead to large errors from the correct values  To avoid these possible errors  a rigorous approach can be applied  where an algorithm is found which can compute the value up to any desired precision  For example  the constant e is a computable number because there is an algorithm that is able to produce the constant up to any given precision  5     The first applications of computer simulations for dynamic systems was in the aerospace industry  6  Commercial uses of dynamic simulation are many and range from nuclear power  steam turbines  6 degrees of freedom vehicle modeling  electric motors  econometric models  biological systems  robot arms  mass-spring-damper systems  hydraulic systems  and drug dose migration through the human body to name a few  These models can often be run in real time to give a virtual response close to the actual system  This is useful in process control and mechatronic systems for tuning the automatic control systems before they are connected to the real system  or for human training before they control the real system  nSimulation is also used in computer games and animation and can be accelerated by using a physics engine  the technology used in many powerful computer graphics software programs  like 3ds Max  Maya  Lightwave  and many others to simulate physical characteristics  In computer animation  things like hair  cloth  liquid  fire  and particles can be easily modeled  while the human animator animates simpler objects  Computer-based dynamic animation was first used at a very simple level in the 1989 Pixar short film Knick Knack to move the fake snow in the snowglobe and pebbles in a fish tank     This animation was made with a software system dynamics  with a 3D modeler  The calculated values are associated with parameters of the rod and crank  In this example the crank is driving  we vary both the speed of rotation  its radius  and the length of the rod  the piston follows          n    Hartmann  Stephan   n 2005  nThe World as a Process  Simulations in the Natural and Social Sciences  n     Preprint    n   n n          Simulation techniques  especially those implemented on a computer  are frequently employed in natural as well as in social sciences with considerable success  There is mounting evidence that the \"model-building era\"  J  Niehans  that dominated the theoretical activities of the sciences for a long time is about to be succeeded or at least lastingly supplemented by the \"simulation era\"  But what exactly are models? What is a simulation and what is the difference and the relation between a model and a simulation? These are some of the questions addressed in this article  I maintain that the most significant feature of a simulation is that it allows scientists to imitate one process by another process  \"Process\" here refers solely to a temporal sequence of states of a system  Given the observation that processes are dealt with by all sorts of scientists  it is apparent that simulations prove to be a powerful interdisciplinarily acknowledged tool  Accordingly  simulations are best suited to investigate the various research strategies in different sciences more carefully  To this end  I focus on the function of simulations in the research process  Finally  a somewhat detailed case-study from nuclear physics is presented which  in my view  illustrates elements of a typical simulation in physics     This site is hosted by the University Library System of the University of Pittsburgh as part of its D-Scribe Digital Publishing Program    Philsci Archive is powered by EPrints 3 which is developed by the School of Electronics and Computer Science at the University of Southampton  More information and software credits               Atom                  RSS 1 0                  RSS 2 0          Physical cosmology is a branch of cosmology concerned with the study of cosmological models   A cosmological model  or simply cosmology  provides a description of the largest-scale structures and dynamics of the universe and allows study of fundamental questions about its origin  structure  evolution  and ultimate fate  1  Cosmology as a science originated with the Copernican principle  which implies that celestial bodies obey identical physical laws to those on Earth  and Newtonian mechanics  which first allowed those physical laws to be understood  Physical cosmology  as it is now understood  began with the development in 1915 of Albert Einstein  s general theory of relativity  followed by major observational discoveries in the 1920s  first  Edwin Hubble discovered that the universe contains a huge number of external galaxies beyond the Milky Way  then  work by Vesto Slipher and others showed that the universe is expanding  These advances made it possible to speculate about the origin of the universe  and allowed the establishment of the Big Bang theory  by Georges Lematre  as the leading cosmological model  A few researchers still advocate a handful of alternative cosmologies  2  however  most cosmologists agree that the Big Bang theory best explains the observations     Dramatic advances in observational cosmology since the 1990s  including the cosmic microwave background  distant supernovae and galaxy redshift surveys  have led to the development of a standard model of cosmology  This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood  but the model gives detailed predictions that are in excellent agreement with many diverse observations  3     Cosmology draws heavily on the work of many disparate areas of research in theoretical and applied physics  Areas relevant to cosmology include particle physics experiments and theory  theoretical and observational astrophysics  general relativity  quantum mechanics  and plasma physics     Modern cosmology developed along tandem tracks of theory and observation  In 1916  Albert Einstein published his theory of general relativity  which provided a unified description of gravity as a geometric property of space and time  4  At the time  Einstein believed in a static universe  but found that his original formulation of the theory did not permit it  5  This is because masses distributed throughout the universe gravitationally attract  and move toward each other over time  6  However  he realized that his equations permitted the introduction of a constant term which could counteract the attractive force of gravity on the cosmic scale  Einstein published his first paper on relativistic cosmology in 1917  in which he added this cosmological constant to his field equations in order to force them to model a static universe  7  The Einstein model describes a static universe  space is finite and unbounded  analogous to the surface of a sphere  which has a finite area but no edges   However  this so-called Einstein model is unstable to small perturbationsit will eventually start to expand or contract  5  It was later realized that Einstein  s model was just one of a larger set of possibilities  all of which were consistent with general relativity and the cosmological principle  The cosmological solutions of general relativity were found by Alexander Friedmann in the early 1920s  8  His equations describe the FriedmannLematreRobertsonWalker universe  which may expand or contract  and whose geometry may be open  flat  or closed     In the 1910s  Vesto Slipher  and later Carl Wilhelm Wirtz  interpreted the red shift of spiral nebulae as a Doppler shift that indicated they were receding from Earth  12  13  However  it is difficult to determine the distance to astronomical objects  One way is to compare the physical size of an object to its angular size  but a physical size must be assumed to do this  Another method is to measure the brightness of an object and assume an intrinsic luminosity  from which the distance may be determined using the inverse-square law  Due to the difficulty of using these methods  they did not realize that the nebulae were actually galaxies outside our own Milky Way  nor did they speculate about the cosmological implications  In 1927  the Belgian Roman Catholic priest Georges Lematre independently derived the FriedmannLematreRobertsonWalker equations and proposed  on the basis of the recession of spiral nebulae  that the universe began with the \"explosion\" of a \"primeval atom\" 14 which was later called the Big Bang  In 1929  Edwin Hubble provided an observational basis for Lematre  s theory  Hubble showed that the spiral nebulae were galaxies by determining their distances using measurements of the brightness of Cepheid variable stars  He discovered a relationship between the redshift of a galaxy and its distance  He interpreted this as evidence that the galaxies are receding from Earth in every direction at speeds proportional to their distance  15  This fact is now known as Hubble  s law  though the numerical factor Hubble found relating recessional velocity and distance was off by a factor of ten  due to not knowing about the types of Cepheid variables     Given the cosmological principle  Hubble  s law suggested that the universe was expanding  Two primary explanations were proposed for the expansion  One was Lematre  s Big Bang theory  advocated and developed by George Gamow  The other explanation was Fred Hoyle  s steady state model in which new matter is created as the galaxies move away from each other  In this model  the universe is roughly the same at any point in time  16  17     For a number of years  support for these theories was evenly divided  However  the observational evidence began to support the idea that the universe evolved from a hot dense state  The discovery of the cosmic microwave background in 1965 lent strong support to the Big Bang model  17  and since the precise measurements of the cosmic microwave background by the Cosmic Background Explorer in the early 1990s  few cosmologists have seriously proposed other theories of the origin and evolution of the cosmos  One consequence of this is that in standard general relativity  the universe began with a singularity  as demonstrated by Roger Penrose and Stephen Hawking in the 1960s  18     An alternative view to extend the Big Bang model  suggesting the universe had no beginning or singularity and the age of the universe is infinite  has been presented  19  20  21     The lightest chemical elements  primarily hydrogen and helium  were created during the Big Bang through the process of nucleosynthesis  22  In a sequence of stellar nucleosynthesis reactions  smaller atomic nuclei are then combined into larger atomic nuclei  ultimately forming stable iron group elements such as iron and nickel  which have the highest nuclear binding energies  23  The net process results in a later energy release  meaning subsequent to the Big Bang  24  Such reactions of nuclear particles can lead to sudden energy releases from cataclysmic variable stars such as novae  Gravitational collapse of matter into black holes also powers the most energetic processes  generally seen in the nuclear regions of galaxies  forming quasars and active galaxies     Cosmologists cannot explain all cosmic phenomena exactly  such as those related to the accelerating expansion of the universe  using conventional forms of energy  Instead  cosmologists propose a new form of energy called dark energy that permeates all space  25  One hypothesis is that dark energy is just the vacuum energy  a component of empty space that is associated with the virtual particles that exist due to the uncertainty principle  26     There is no clear way to define the total energy in the universe using the most widely accepted theory of gravity  general relativity  Therefore  it remains controversial whether the total energy is conserved in an expanding universe  For instance  each photon that travels through intergalactic space loses energy due to the redshift effect  This energy is not obviously transferred to any other system  so seems to be permanently lost  On the other hand  some cosmologists insist that energy is conserved in some sense  this follows the law of conservation of energy  27     Different forms of energy may dominate the cosmos  relativistic particles which are referred to as radiation  or non-relativistic particles referred to as matter  Relativistic particles are particles whose rest mass is zero or negligible compared to their kinetic energy  and so move at the speed of light or very close to it  non-relativistic particles have much higher rest mass than their energy and so move much slower than the speed of light     As the universe expands  both matter and radiation become diluted  However  the energy densities of radiation and matter dilute at different rates  As a particular volume expands  mass-energy density is changed only by the increase in volume  but the energy density of radiation is changed both by the increase in volume and by the increase in the wavelength of the photons that make it up  Thus the energy of radiation becomes a smaller part of the universe  s total energy than that of matter as it expands  The very early universe is said to have been   radiation dominated   and radiation controlled the deceleration of expansion  Later  as the average energy per photon becomes roughly 10 eV and lower  matter dictates the rate of deceleration and the universe is said to be   matter dominated    The intermediate case is not treated well analytically  As the expansion of the universe continues  matter dilutes even further and the cosmological constant becomes dominant  leading to an acceleration in the universe  s expansion     The history of the universe is a central issue in cosmology  The history of the universe is divided into different periods called epochs  according to the dominant forces and processes in each period  The standard cosmological model is known as the Lambda-CDM model     Within the standard cosmological model  the equations of motion governing the universe as a whole are derived from general relativity with a small  positive cosmological constant  28  The solution is an expanding universe  due to this expansion  the radiation and matter in the universe cool down and become diluted  At first  the expansion is slowed down by gravitation attracting the radiation and matter in the universe  However  as these become diluted  the cosmological constant becomes more dominant and the expansion of the universe starts to accelerate rather than decelerate  In our universe this happened billions of years ago  29     During the earliest moments of the universe  the average energy density was very high  making knowledge of particle physics critical to understanding this environment  Hence  scattering processes and decay of unstable elementary particles are important for cosmological models of this period     As a rule of thumb  a scattering or a decay process is cosmologically important in a certain epoch if the time scale describing that process is smaller than  or comparable to  the time scale of the expansion of the universe  clarification needed  The time scale that describes the expansion of the universe is                     1                  /                H                 displaystyle 1/H    with                     H                 displaystyle H    being the Hubble parameter  which varies with time  The expansion timescale                     1                  /                H                 displaystyle 1/H    is roughly equal to the age of the universe at each point in time     Observations suggest that the universe began around 13 8 billion years ago  30  Since then  the evolution of the universe has passed through three phases  The very early universe  which is still poorly understood  was the split second in which the universe was so hot that particles had energies higher than those currently accessible in particle accelerators on Earth  Therefore  while the basic features of this epoch have been worked out in the Big Bang theory  the details are largely based on educated guesses  nFollowing this  in the early universe  the evolution of the universe proceeded according to known high energy physics  This is when the first protons  electrons and neutrons formed  then nuclei and finally atoms  With the formation of neutral hydrogen  the cosmic microwave background was emitted  Finally  the epoch of structure formation began  when matter started to aggregate into the first stars and quasars  and ultimately galaxies  clusters of galaxies and superclusters formed  The future of the universe is not yet firmly known  but according to the CDM model it will continue expanding forever     Below  some of the most active areas of inquiry in cosmology are described  in roughly chronological order  This does not include all of the Big Bang cosmology  which is presented in Timeline of the Big Bang     The early  hot universe appears to be well explained by the Big Bang from roughly 1033 seconds onwards  but there are several problems  One is that there is no compelling reason  using current particle physics  for the universe to be flat  homogeneous  and isotropic  see the cosmological principle   Moreover  grand unified theories of particle physics suggest that there should be magnetic monopoles in the universe  which have not been found  These problems are resolved by a brief period of cosmic inflation  which drives the universe to flatness  smooths out anisotropies and inhomogeneities to the observed level  and exponentially dilutes the monopoles  31  The physical model behind cosmic inflation is extremely simple  but it has not yet been confirmed by particle physics  and there are difficult problems reconciling inflation and quantum field theory  vague  Some cosmologists think that string theory and brane cosmology will provide an alternative to inflation  32     Another major problem in cosmology is what caused the universe to contain far more matter than antimatter  Cosmologists can observationally deduce that the universe is not split into regions of matter and antimatter  If it were  there would be X-rays and gamma rays produced as a result of annihilation  but this is not observed  Therefore  some process in the early universe must have created a small excess of matter over antimatter  and this  currently not understood  process is called baryogenesis  Three required conditions for baryogenesis were derived by Andrei Sakharov in 1967  and requires a violation of the particle physics symmetry  called CP-symmetry  between matter and antimatter  33  However  particle accelerators measure too small a violation of CP-symmetry to account for the baryon asymmetry  Cosmologists and particle physicists look for additional violations of the CP-symmetry in the early universe that might account for the baryon asymmetry  34     Both the problems of baryogenesis and cosmic inflation are very closely related to particle physics  and their resolution might come from high energy theory and experiment  rather than through observations of the universe  speculation?     Big Bang nucleosynthesis is the theory of the formation of the elements in the early universe  It finished when the universe was about three minutes old and its temperature dropped below that at which nuclear fusion could occur  Big Bang nucleosynthesis had a brief period during which it could operate  so only the very lightest elements were produced  Starting from hydrogen ions  protons   it principally produced deuterium  helium-4  and lithium  Other elements were produced in only trace abundances  The basic theory of nucleosynthesis was developed in 1948 by George Gamow  Ralph Asher Alpher  and Robert Herman  35  It was used for many years as a probe of physics at the time of the Big Bang  as the theory of Big Bang nucleosynthesis connects the abundances of primordial light elements with the features of the early universe  22  Specifically  it can be used to test the equivalence principle  36  to probe dark matter  and test neutrino physics  37  Some cosmologists have proposed that Big Bang nucleosynthesis suggests there is a fourth \"sterile\" species of neutrino  38     The CDM  Lambda cold dark matter  or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant  denoted by Lambda  Greek    associated with dark energy  and cold dark matter  abbreviated CDM   It is frequently referred to as the standard model of Big Bang cosmology  39  40     The cosmic microwave background is radiation left over from decoupling after the epoch of recombination when neutral atoms first formed  At this point  radiation produced in the Big Bang stopped Thomson scattering from charged ions  The radiation  first observed in 1965 by Arno Penzias and Robert Woodrow Wilson  has a perfect thermal black-body spectrum  It has a temperature of 2 7 kelvins today and is isotropic to one part in 105  Cosmological perturbation theory  which describes the evolution of slight inhomogeneities in the early universe  has allowed cosmologists to precisely calculate the angular power spectrum of the radiation  and it has been measured by the recent satellite experiments  COBE and WMAP  41  and many ground and balloon-based experiments  such as Degree Angular Scale Interferometer  Cosmic Background Imager  and Boomerang   42  One of the goals of these efforts is to measure the basic parameters of the Lambda-CDM model with increasing accuracy  as well as to test the predictions of the Big Bang model and look for new physics  The results of measurements made by WMAP  for example  have placed limits on the neutrino masses  43     Newer experiments  such as QUIET and the Atacama Cosmology Telescope  are trying to measure the polarization of the cosmic microwave background  44  These measurements are expected to provide further confirmation of the theory as well as information about cosmic inflation  and the so-called secondary anisotropies  45  such as the Sunyaev-Zel  dovich effect and Sachs-Wolfe effect  which are caused by interaction between galaxies and clusters with the cosmic microwave background  46  47     On 17 March 2014  astronomers of the BICEP2 Collaboration announced the apparent detection of B-mode polarization of the CMB  considered to be evidence of primordial gravitational waves that are predicted by the theory of inflation to occur during the earliest phase of the Big Bang  9  10  11  48  However  later that year the Planck collaboration provided a more accurate measurement of cosmic dust  concluding that the B-mode signal from dust is the same strength as that reported from BICEP2  49  50  On 30 January 2015  a joint analysis of BICEP2 and Planck data was published and the European Space Agency announced that the signal can be entirely attributed to interstellar dust in the Milky Way  51     Understanding the formation and evolution of the largest and earliest structures  i e   quasars  galaxies  clusters and superclusters  is one of the largest efforts in cosmology  Cosmologists study a model of hierarchical structure formation in which structures form from the bottom up  with smaller objects forming first  while the largest objects  such as superclusters  are still assembling  52  One way to study structure in the universe is to survey the visible galaxies  in order to construct a three-dimensional picture of the galaxies in the universe and measure the matter power spectrum  This is the approach of the Sloan Digital Sky Survey and the 2dF Galaxy Redshift Survey  53  54     Another tool for understanding structure formation is simulations  which cosmologists use to study the gravitational aggregation of matter in the universe  as it clusters into filaments  superclusters and voids  Most simulations contain only non-baryonic cold dark matter  which should suffice to understand the universe on the largest scales  as there is much more dark matter in the universe than visible  baryonic matter  More advanced simulations are starting to include baryons and study the formation of individual galaxies  Cosmologists study these simulations to see if they agree with the galaxy surveys  and to understand any discrepancy  55     Other  complementary observations to measure the distribution of matter in the distant universe and to probe reionization include     These will help cosmologists settle the question of when and how structure formed in the universe     Evidence from Big Bang nucleosynthesis  the cosmic microwave background  structure formation  and galaxy rotation curves suggests that about 23% of the mass of the universe consists of non-baryonic dark matter  whereas only 4% consists of visible  baryonic matter  The gravitational effects of dark matter are well understood  as it behaves like a cold  non-radiative fluid that forms haloes around galaxies  Dark matter has never been detected in the laboratory  and the particle physics nature of dark matter remains completely unknown  Without observational constraints  there are a number of candidates  such as a stable supersymmetric particle  a weakly interacting massive particle  a gravitationally-interacting massive particle  an axion  and a massive compact halo object  Alternatives to the dark matter hypothesis include a modification of gravity at small accelerations  MOND  or an effect from brane cosmology  59     If the universe is flat  there must be an additional component making up 73%  in addition to the 23% dark matter and 4% baryons  of the energy density of the universe  This is called dark energy  In order not to interfere with Big Bang nucleosynthesis and the cosmic microwave background  it must not cluster in haloes like baryons and dark matter  There is strong observational evidence for dark energy  as the total energy density of the universe is known through constraints on the flatness of the universe  but the amount of clustering matter is tightly measured  and is much less than this  The case for dark energy was strengthened in 1999  when measurements demonstrated that the expansion of the universe has begun to gradually accelerate  60     Apart from its density and its clustering properties  nothing is known about dark energy  Quantum field theory predicts a cosmological constant  CC  much like dark energy  but 120 orders of magnitude larger than that observed  61  Steven Weinberg and a number of string theorists  see string landscape  have invoked the   weak anthropic principle    i e  the reason that physicists observe a universe with such a small cosmological constant is that no physicists  or any life  could exist in a universe with a larger cosmological constant  Many cosmologists find this an unsatisfying explanation  perhaps because while the weak anthropic principle is self-evident  given that living observers exist  there must be at least one universe with a cosmological constant which allows for life to exist  it does not attempt to explain the context of that universe  62  For example  the weak anthropic principle alone does not distinguish between     Other possible explanations for dark energy include quintessence 63  or a modification of gravity on the largest scales  64  The effect on cosmology of the dark energy that these models describe is given by the dark energy  s equation of state  which varies depending upon the theory  The nature of dark energy is one of the most challenging problems in cosmology     A better understanding of dark energy is likely to solve the problem of the ultimate fate of the universe  In the current cosmological epoch  the accelerated expansion due to dark energy is preventing structures larger than superclusters from forming  It is not known whether the acceleration will continue indefinitely  perhaps even increasing until a big rip  or whether it will eventually reverse  lead to a big freeze  or follow some other scenario  65     Gravitational waves  are ripples in the curvature of spacetime that propagate as waves at the speed of light  generated in certain gravitational interactions that propagate outward from their source  Gravitational-wave astronomy is an emerging branch of observational astronomy which aims to use gravitational waves to collect observational data about sources of detectable gravitational waves such as binary star systems composed of white dwarfs  neutron stars  and black holes  and events such as supernovae  and the formation of the early universe shortly after the Big Bang  66     In 2016  the LIGO Scientific Collaboration and Virgo Collaboration teams announced that they had made the first observation of gravitational waves  originating from a pair of merging black holes using the Advanced LIGO detectors  67  68  69  On 15 June 2016  a second detection of gravitational waves from coalescing black holes was announced  70  Besides LIGO  many other gravitational-wave observatories  detectors  are under construction  71     Cosmologists also study      n    The Internet  or internet   is the global system of interconnected computer networks that uses the Internet protocol suite  TCP/IP  to communicate between networks and devices  It is a network of networks that consists of private  public  academic  business  and government networks of local to global scope  linked by a broad array of electronic  wireless  and optical networking technologies  The Internet carries a vast range of information resources and services  such as the inter-linked hypertext documents and applications of the World Wide Web  WWW   electronic mail  telephony  and file sharing     The origins of the Internet date back to the development of packet switching and research commissioned by the United States Department of Defense in the 1960s to enable time-sharing of computers  1  The primary precursor network  the ARPANET  initially served as a backbone for interconnection of regional academic and military networks in the 1970s  The funding of the National Science Foundation Network as a new backbone in the 1980s  as well as private funding for other commercial extensions  led to worldwide participation in the development of new networking technologies  and the merger of many networks  2  The linking of commercial networks and enterprises by the early 1990s marked the beginning of the transition to the modern Internet  3  and generated a sustained exponential growth as generations of institutional  personal  and mobile computers were connected to the network  Although the Internet was widely used by academia in the 1980s  commercialization incorporated its services and technologies into virtually every aspect of modern life     Most traditional communication media  including telephony  radio  television  paper mail and newspapers are reshaped  redefined  or even bypassed by the Internet  giving birth to new services such as email  Internet telephony  Internet television  online music  digital newspapers  and video streaming websites  Newspaper  book  and other print publishing are adapting to website technology  or are reshaped into blogging  web feeds and online news aggregators  The Internet has enabled and accelerated new forms of personal interactions through instant messaging  Internet forums  and social networking services  Online shopping has grown exponentially for major retailers  small businesses  and entrepreneurs  as it enables firms to extend their \"brick and mortar\" presence to serve a larger market or even sell goods and services entirely online  Business-to-business and financial services on the Internet affect supply chains across entire industries     The Internet has no single centralized governance in either technological implementation or policies for access and usage  each constituent network sets its own policies  4  The overreaching definitions of the two principal name spaces in the Internet  the Internet Protocol address  IP address  space and the Domain Name System  DNS   are directed by a maintainer organization  the Internet Corporation for Assigned Names and Numbers  ICANN   The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force  IETF   a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise  5  In November 2006  the Internet was included on USA Today  s list of New Seven Wonders  6     The word internetted was used as early as 1849  meaning interconnected or interwoven  7  The word Internet was used in 1974 as the shorthand form of Internetwork  8  Today  the term Internet most commonly refers to the global system of interconnected computer networks  though it may also refer to any group of smaller networks  9      When it came into common use  most publications treated the word Internet as a capitalized proper noun  this has become less common  9  This reflects the tendency in English to capitalize new terms and move to lowercase as they become familiar  9  10  The word is sometimes still capitalized to distinguish the global internet from smaller networks  though many publications  including the AP Stylebook  recommend the lowercase form in every case  9  10  In 2016  the Oxford English Dictionary found that  based on a study of around 2 5 billion printed and online sources  \"Internet\" was capitalized in 54% of cases  11     The terms internet and World Wide Web are often used interchangeably  it is common to speak of \"going on the Internet\" when using a web browser to view web pages  However  the World Wide Web or the Web is only one of a large number of Internet services  12  a collection of documents  web pages  and other web resources  linked by hyperlinks and URLs  13     In the 1960s  the Advanced Research Projects Agency  ARPA  of the United States Department of Defense funded research into time-sharing of computers  14  15  16  Research into packet switching  one of the fundamental Internet technologies  started in the work of Paul Baran in the early 1960s and  independently  Donald Davies in 1965  1  17  After the Symposium on Operating Systems Principles in 1967  packet switching from the proposed NPL network was incorporated into the design for the ARPANET and other resource sharing networks such as the Merit Network and CYCLADES  which were developed in the late 1960s and early 1970s  18     ARPANET development began with two network nodes which were interconnected between the Network Measurement Center at the University of California  Los Angeles  UCLA  Henry Samueli School of Engineering and Applied Science directed by Leonard Kleinrock  and the NLS system at SRI International  SRI  by Douglas Engelbart in Menlo Park  California  on 29 October 1969  19  The third site was the Culler-Fried Interactive Mathematics Center at the University of California  Santa Barbara  followed by the University of Utah Graphics Department  In a sign of future growth  15 sites were connected to the young ARPANET by the end of 1971  20  21  These early years were documented in the 1972 film Computer Networks  The Heralds of Resource Sharing  22     Early international collaborations for the ARPANET were rare  Connections were made in 1973 to the Norwegian Seismic Array  NORSAR  via a satellite station in Tanum  Sweden  and to Peter Kirstein  s research group at University College London which provided a gateway to British academic networks  23  24  The ARPA projects and international working groups led to the development of various protocols and standards by which multiple separate networks could become a single network or \"a network of networks\"  25  In 1974  Vint Cerf and Bob Kahn used the term internet as a shorthand for internetwork in RFC xa0675  26  and later RFCs repeated this use  27  Cerf and Khan credit Louis Pouzin with important influences on TCP/IP design  28  Commercial PTT providers were concerned with developing X 25 public data networks  29     Access to the ARPANET was expanded in 1981 when the National Science Foundation  NSF  funded the Computer Science Network  CSNET   In 1982  the Internet Protocol Suite  TCP/IP  was standardized  which permitted worldwide proliferation of interconnected networks  TCP/IP network access expanded again in 1986 when the National Science Foundation Network  NSFNet  provided access to supercomputer sites in the United States for researchers  first at speeds of 56 kbit/s and later at 1 5 Mbit/s and 45 Mbit/s  30  The NSFNet expanded into academic and research organizations in Europe  Australia  New Zealand and Japan in 198889  31  32  33  34  Although other network protocols such as UUCP had global reach well before this time  this marked the beginning of the Internet as an intercontinental network  Commercial Internet service providers  ISPs  emerged in 1989 in the United States and Australia  35  The ARPANET was decommissioned in 1990  36     Steady advances in semiconductor technology and optical networking created new economic opportunities for commercial involvement in the expansion of the network in its core and for delivering services to the public  In mid-1989  MCI Mail and Compuserve established connections to the Internet  delivering email and public access products to the half million users of the Internet  37  Just months later  on 1 January 1990  PSInet launched an alternate Internet backbone for commercial use  one of the networks that added to the core of the commercial Internet of later years  In March 1990  the first high-speed T1  1 5 Mbit/s  link between the NSFNET and Europe was installed between Cornell University and CERN  allowing much more robust communications than were capable with satellites  38  Six months later Tim Berners-Lee would begin writing WorldWideWeb  the first web browser  after two years of lobbying CERN management  By Christmas 1990  Berners-Lee had built all the tools necessary for a working Web  the HyperText Transfer Protocol  HTTP  0 9  39  the HyperText Markup Language  HTML   the first Web browser  which was also a HTML editor and could access Usenet newsgroups and FTP files   the first HTTP server software  later known as CERN httpd   the first web server  40  and the first Web pages that described the project itself  In 1991 the Commercial Internet eXchange was founded  allowing PSInet to communicate with the other commercial networks CERFnet and Alternet  Stanford Federal Credit Union was the first financial institution to offer online Internet banking services to all of its members in October 1994  41  In 1996  OP Financial Group  also a cooperative bank  became the second online bank in the world and the first in Europe  42  By 1995  the Internet was fully commercialized in the U S  when the NSFNet was decommissioned  removing the last restrictions on use of the Internet to carry commercial traffic  43     As technology advanced and commercial opportunities fueled reciprocal growth  the volume of Internet traffic started experiencing similar characteristics as that of the scaling of MOS transistors  exemplified by Moore  s law  doubling every 18 months  This growth  formalized as Edholm  s law  was catalyzed by advances in MOS technology  laser light wave systems  and noise performance  46     Since 1995  the Internet has tremendously impacted culture and commerce  including the rise of near instant communication by email  instant messaging  telephony  Voice over Internet Protocol or VoIP   two-way interactive video calls  and the World Wide Web 47  with its discussion forums  blogs  social networking services  and online shopping sites  Increasing amounts of data are transmitted at higher and higher speeds over fiber optic networks operating at 1-Gbit/s  10-Gbit/s  or more  The Internet continues to grow  driven by ever greater amounts of online information and knowledge  commerce  entertainment and social networking services  48  During the late 1990s  it was estimated that traffic on the public Internet grew by 100 percent per year  while the mean annual growth in the number of Internet users was thought to be between 20% and 50%  49  This growth is often attributed to the lack of central administration  which allows organic growth of the network  as well as the non-proprietary nature of the Internet protocols  which encourages vendor interoperability and prevents any one company from exerting too much control over the network  50  As of 31 xa0March xa02011 update   the estimated total number of Internet users was 2 095 xa0billion  30 2% of world population   51  It is estimated that in 1993 the Internet carried only 1% of the information flowing through two-way telecommunication  By 2000 this figure had grown to 51%  and by 2007 more than 97% of all telecommunicated information was carried over the Internet  52     The Internet is a global network that comprises many voluntarily interconnected autonomous networks  It operates without a central governing body  The technical underpinning and standardization of the core protocols  IPv4 and IPv6  is an activity of the Internet Engineering Task Force  IETF   a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise  To maintain interoperability  the principal name spaces of the Internet are administered by the Internet Corporation for Assigned Names and Numbers  ICANN   ICANN is governed by an international board of directors drawn from across the Internet technical  business  academic  and other non-commercial communities  ICANN coordinates the assignment of unique identifiers for use on the Internet  including domain names  IP addresses  application port numbers in the transport protocols  and many other parameters  Globally unified name spaces are essential for maintaining the global reach of the Internet  This role of ICANN distinguishes it as perhaps the only central coordinating body for the global Internet  53     Regional Internet registries  RIRs  were established for five regions of the world  The African Network Information Center  AfriNIC  for Africa  the American Registry for Internet Numbers  ARIN  for North America  the Asia-Pacific Network Information Centre  APNIC  for Asia and the Pacific region  the Latin American and Caribbean Internet Addresses Registry  LACNIC  for Latin America and the Caribbean region  and the Rseaux IP Europens  Network Coordination Centre  RIPE NCC  for Europe  the Middle East  and Central Asia were delegated to assign IP address blocks and other Internet parameters to local registries  such as Internet service providers  from a designated pool of addresses set aside for each region     The National Telecommunications and Information Administration  an agency of the United States Department of Commerce  had final approval over changes to the DNS root zone until the IANA stewardship transition on 1 October 2016  54  55  56  57  The Internet Society  ISOC  was founded in 1992 with a mission to \"assure the open development  evolution and use of the Internet for the benefit of all people throughout the world\"  58  Its members include individuals  anyone may join  as well as corporations  organizations  governments  and universities  Among other activities ISOC provides an administrative home for a number of less formally organized groups that are involved in developing and managing the Internet  including  the IETF  Internet Architecture Board  IAB   Internet Engineering Steering Group  IESG   Internet Research Task Force  IRTF   and Internet Research Steering Group  IRSG   On 16 November 2005  the United Nations-sponsored World Summit on the Information Society in Tunis established the Internet Governance Forum  IGF  to discuss Internet-related issues     The communications infrastructure of the Internet consists of its hardware components and a system of software layers that control various aspects of the architecture  As with any computer network  the Internet physically consists of routers  media  such as cabling and radio links   repeaters  modems etc  However  as an example of internetworking  many of the network nodes are not necessarily internet equipment per se  the internet packets are carried by other full-fledged networking protocols with the Internet acting as a homogeneous networking standard  running across heterogeneous hardware  with the packets guided to their destinations by IP routers     Internet service providers  ISPs  establish the worldwide connectivity between individual networks at various levels of scope  End-users who only access the Internet when needed to perform a function or obtain information  represent the bottom of the routing hierarchy  At the top of the routing hierarchy are the tier 1 networks  large telecommunication companies that exchange traffic directly with each other via very high speed fibre optic cables and governed by peering agreements  Tier 2 and lower-level networks buy Internet transit from other providers to reach at least some parties on the global Internet  though they may also engage in peering  An ISP may use a single upstream provider for connectivity  or implement multihoming to achieve redundancy and load balancing  Internet exchange points are major traffic exchanges with physical connections to multiple ISPs  Large organizations  such as academic institutions  large enterprises  and governments  may perform the same function as ISPs  engaging in peering and purchasing transit on behalf of their internal networks  Research networks tend to interconnect with large subnetworks such as GEANT  GLORIAD  Internet2  and the UK  s national research and education network  JANET     Common methods of Internet access by users include dial-up with a computer modem via telephone circuits  broadband over coaxial cable  fiber optics or copper wires  Wi-Fi  satellite  and cellular telephone technology  e g  3G  4G   The Internet may often be accessed from computers in libraries and Internet cafes  Internet access points exist in many public places such as airport halls and coffee shops  Various terms are used  such as public Internet kiosk  public access terminal  and Web payphone  Many hotels also have public terminals that are usually fee-based  These terminals are widely accessed for various usages  such as ticket booking  bank deposit  or online payment  Wi-Fi provides wireless access to the Internet via local computer networks  Hotspots providing such access include Wi-Fi cafes  where users need to bring their own wireless devices such as a laptop or PDA  These services may be free to all  free to customers only  or fee-based     Grassroots efforts have led to wireless community networks  Commercial Wi-Fi services that cover large areas are available in many cities  such as New York  London  Vienna  Toronto  San Francisco  Philadelphia  Chicago and Pittsburgh  where the Internet can then be accessed from places such as a park bench  59  Experiments have also been conducted with proprietary mobile wireless networks like Ricochet  various high-speed data services over cellular networks  and fixed wireless services  Modern smartphones can also access the Internet through the cellular carrier network  For Web browsing  these devices provide applications such as Google Chrome  Safari  and Firefox and a wide variety of other Internet software may be installed from app-stores  Internet usage by mobile and tablet devices exceeded desktop worldwide for the first time in October 2016  60       World Trends in Freedom of Expression and Media Development Global Report 2017/2018    The International Telecommunication Union  ITU  estimated that  by the end of 2017  48% of individual users regularly connect to the Internet  up from 34% in 2012  61  Mobile Internet connectivity has played an important role in expanding access in recent years especially in Asia and the Pacific and in Africa  62  The number of unique mobile cellular subscriptions increased from 3 89 xa0billion in 2012 to 4 83 xa0billion in 2016  two-thirds of the world  s population  with more than half of subscriptions located in Asia and the Pacific  The number of subscriptions is predicted to rise to 5 69 xa0billion users in 2020  63  As of 2016 update   almost 60% of the world  s population had access to a 4G broadband cellular network  up from almost 50% in 2015 and 11% in 2012  disputed   discuss  63  The limits that users face on accessing information via mobile applications coincide with a broader process of fragmentation of the Internet  Fragmentation restricts access to media content and tends to affect poorest users the most  62     Zero-rating  the practice of Internet service providers allowing users free connectivity to access specific content or applications without cost  has offered opportunities to surmount economic hurdles  but has also been accused by its critics as creating a two-tiered Internet  To address the issues with zero-rating  an alternative model has emerged in the concept of   equal rating   and is being tested in experiments by Mozilla and Orange in Africa  Equal rating prevents prioritization of one type of content and zero-rates all content up to a specified data cap  A study published by Chatham House  15 out of 19 countries researched in Latin America had some kind of hybrid or zero-rated product offered  Some countries in the region had a handful of plans to choose from  across all mobile network operators  while others  such as Colombia  offered as many as 30 pre-paid and 34 post-paid plans  64     A study of eight countries in the Global South found that zero-rated data plans exist in every country  although there is a great range in the frequency with which they are offered and actually used in each  65  The study looked at the top three to five carriers by market share in Bangladesh  Colombia  Ghana  India  Kenya  Nigeria  Peru and Philippines  Across the 181 plans examined  13 per cent were offering zero-rated services  Another study  covering Ghana  Kenya  Nigeria and South Africa  found Facebook  s Free Basics and Wikipedia Zero to be the most commonly zero-rated content  66     The Internet standards describe a framework known as the Internet protocol suite  also called TCP/IP  based on the first two components   This is a suite of protocols that are ordered into a set of four conceptional layers by the scope of their operation  originally documented in RFC xa01122 and RFC xa01123  At the top is the application layer  where communication is described in terms of the objects or data structures most appropriate for each application  For example  a web browser operates in a clientserver application model and exchanges information with the Hypertext Transfer Protocol  HTTP  and an application-germane data structure  such as the Hypertext Markup Language  HTML      Below this top layer  the transport layer connects applications on different hosts with a logical channel through the network  It provides this service with a variety of possible characteristics  such as ordered  reliable delivery  TCP   and an unreliable datagram service  UDP      Underlying these layers are the networking technologies that interconnect networks at their borders and exchange traffic across them  The Internet layer implements the Internet Protocol  IP  which enables computers to identify and locate each other by IP address  and route their traffic via intermediate  transit  networks  67  The internet protocol layer code is independent of the type of network that it is physically running over     At the bottom of the architecture is the link layer  which connects nodes on the same physical link  and contains protocols that do not require routers for traversal to other links  The protocol suite does not explicitly specify hardware methods to transfer bits  or protocols to manage such hardware  but assumes that appropriate technology is available  Examples of that technology include Wi-Fi  Ethernet  and DSL     The most prominent component of the Internet model is the Internet Protocol  IP   IP enables internetworking and  in essence  establishes the Internet itself  Two versions of the Internet Protocol exist  IPV4 and IPV6     For locating individual computers on the network  the Internet provides IP addresses  IP addresses are used by the Internet infrastructure to direct internet packets to their destinations  They consist of fixed-length numbers  which are found within the packet  IP addresses are generally assigned to equipment either automatically via DHCP  or are configured     However  the network also supports other addressing systems  Users generally enter domain names  e g  \"en wikipedia org\"  instead of IP addresses because they are easier to remember  they are converted by the Domain Name System  DNS  into IP addresses which are more efficient for routing purposes     Internet Protocol version 4  IPv4  defines an IP address as a 32-bit number  67  IPv4 is the initial version used on the first generation of the Internet and is still in dominant use  It was designed to address up to 4 3 xa0billion  109  hosts  However  the explosive growth of the Internet has led to IPv4 address exhaustion  which entered its final stage in 2011  68  when the global IPv4 address allocation pool was exhausted     Because of the growth of the Internet and the depletion of available IPv4 addresses  a new version of IP IPv6  was developed in the mid-1990s  which provides vastly larger addressing capabilities and more efficient routing of Internet traffic  IPv6 uses 128 bits for the IP address and was standardized in 1998  69  70  71  IPv6 deployment has been ongoing since the mid-2000s and is currently in growing deployment around the world  since Internet address registries  RIRs  began to urge all resource managers to plan rapid adoption and conversion  72     IPv6 is not directly interoperable by design with IPv4  In essence  it establishes a parallel version of the Internet not directly accessible with IPv4 software  Thus  translation facilities must exist for internetworking or nodes must have duplicate networking software for both networks  Essentially all modern computer operating systems support both versions of the Internet Protocol  Network infrastructure  however  has been lagging in this development  Aside from the complex array of physical connections that make up its infrastructure  the Internet is facilitated by bi- or multi-lateral commercial contracts  e g   peering agreements  and by technical specifications or protocols that describe the exchange of data over the network  Indeed  the Internet is defined by its interconnections and routing policies     A subnetwork or subnet is a logical subdivision of an IP network  73  1 16 The practice of dividing a network into two or more networks is called subnetting     Computers that belong to a subnet are addressed with an identical most-significant bit-group in their IP addresses  This results in the logical division of an IP address into two fields  the network number or routing prefix and the rest field or host identifier  The rest field is an identifier for a specific host or network interface     The routing prefix may be expressed in Classless Inter-Domain Routing  CIDR  notation written as the first address of a network  followed by a slash character  /   and ending with the bit-length of the prefix  For example  198 51 100 0/24 is the prefix of the Internet Protocol version 4 network starting at the given address  having 24 bits allocated for the network prefix  and the remaining 8 bits reserved for host addressing  Addresses in the range 198 51 100 0 to 198 51 100 255 belong to this network  The IPv6 address specification 2001 db8  /32 is a large address block with 296 addresses  having a 32-bit routing prefix     For IPv4  a network may also be characterized by its subnet mask or netmask  which is the bitmask that when applied by a bitwise AND operation to any IP address in the network  yields the routing prefix  Subnet masks are also expressed in dot-decimal notation like an address  For example  255 255 255 0 is the subnet mask for the prefix 198 51 100 0/24     Traffic is exchanged between subnetworks through routers when the routing prefixes of the source address and the destination address differ  A router serves as a logical or physical boundary between the subnets    \"The benefits of subnetting an existing network vary with each deployment scenario  In the address allocation architecture of the Internet using CIDR and in large organizations  it is necessary to allocate address space efficiently  Subnetting may also enhance routing efficiency  or have advantages in network management when subnetworks are administratively controlled by different entities in a larger organization  Subnets may be arranged logically in a hierarchical architecture  partitioning an organization s network address space into a tree-like routing structure  n\"   Computers and routers use routing tables in their operating system to direct IP packets to reach a node on a different subnetwork  Routing tables are maintained by manual configuration or automatically by routing protocols  End-nodes typically use a default route that points toward an ISP providing transit  while ISP routers use the Border Gateway Protocol to establish the most efficient routing across the complex connections of the global Internet  The default gateway is the node that serves as the forwarding host  router  to other networks when no other route specification matches the destination IP address of a packet  74  75     While the hardware components in the Internet infrastructure can often be used to support other software systems  it is the design and the standardization process of the software that characterizes the Internet and provides the foundation for its scalability and success  The responsibility for the architectural design of the Internet software systems has been assumed by the Internet Engineering Task Force  IETF   76  The IETF conducts standard-setting work groups  open to any individual  about the various aspects of Internet architecture  The resulting contributions and standards are published as Request for Comments  RFC  documents on the IETF web site  The principal methods of networking that enable the Internet are contained in specially designated RFCs that constitute the Internet Standards  Other less rigorous documents are simply informative  experimental  or historical  or document the best current practices  BCP  when implementing Internet technologies     The Internet carries many applications and services  most prominently the World Wide Web  including social media  electronic mail  mobile applications  multiplayer online games  Internet telephony  file sharing  and streaming media services     Most servers that provide these services are today hosted in data centers  and content is often accessed through high-performance content delivery networks     The World Wide Web is a global collection of documents  images  multimedia  applications  and other resources  logically interrelated by hyperlinks and referenced with Uniform Resource Identifiers  URIs   which provide a global system of named references  URIs symbolically identify services  web servers  databases  and the documents and resources that they can provide  Hypertext Transfer Protocol  HTTP  is the main access protocol of the World Wide Web  Web services also use HTTP for communication between software systems for information transfer  sharing and exchanging business data and logistic and is one of many languages or protocols that can be used for communication on the Internet  77     World Wide Web browser software  such as Microsoft  s Internet Explorer/Edge  Mozilla Firefox  Opera  Apple  s Safari  and Google Chrome  lets users navigate from one web page to another via the hyperlinks embedded in the documents  These documents may also contain any combination of computer data  including graphics  sounds  text  video  multimedia and interactive content that runs while the user is interacting with the page  Client-side software can include animations  games  office applications and scientific demonstrations  Through keyword-driven Internet research using search engines like Yahoo!  Bing and Google  users worldwide have easy  instant access to a vast and diverse amount of online information  Compared to printed media  books  encyclopedias and traditional libraries  the World Wide Web has enabled the decentralization of information on a large scale     The Web has enabled individuals and organizations to publish ideas and information to a potentially large audience online at greatly reduced expense and time delay  Publishing a web page  a blog  or building a website involves little initial cost and many cost-free services are available  However  publishing and maintaining large  professional web sites with attractive  diverse and up-to-date information is still a difficult and expensive proposition  Many individuals and some companies and groups use web logs or blogs  which are largely used as easily updatable online diaries  Some commercial organizations encourage staff to communicate advice in their areas of specialization in the hope that visitors will be impressed by the expert knowledge and free information  and be attracted to the corporation as a result     Advertising on popular web pages can be lucrative  and e-commerce  which is the sale of products and services directly via the Web  continues to grow  Online advertising is a form of marketing and advertising which uses the Internet to deliver promotional marketing messages to consumers  It includes email marketing  search engine marketing  SEM   social media marketing  many types of display advertising  including web banner advertising   and mobile advertising  In 2011  Internet advertising revenues in the United States surpassed those of cable television and nearly exceeded those of broadcast television  78  19 Many common online advertising practices are controversial and increasingly subject to regulation     When the Web developed in the 1990s  a typical web page was stored in completed form on a web server  formatted in HTML  complete for transmission to a web browser in response to a request  Over time  the process of creating and serving web pages has become dynamic  creating a flexible design  layout  and content  Websites are often created using content management software with  initially  very little content  Contributors to these systems  who may be paid staff  members of an organization or the public  fill underlying databases with content using editing pages designed for that purpose while casual visitors view and read this content in HTML form  There may or may not be editorial  approval and security systems built into the process of taking newly entered content and making it available to the target visitors     Email is an important communications service available via the Internet  The concept of sending electronic text messages between parties  analogous to mailing letters or memos  predates the creation of the Internet  79  80  Pictures  documents  and other files are sent as email attachments  Email messages can be cc-ed to multiple email addresses     Internet telephony is a common communications service realized with the Internet  The name of the principle internetworking protocol  the Internet Protocol  lends its name to voice over Internet Protocol  VoIP   The idea began in the early 1990s with walkie-talkie-like voice applications for personal computers  VoIP systems now dominate many markets  and are as easy to use and as convenient as a traditional telephone  The benefit has been substantial cost savings over traditional telephone calls  especially over long distances  Cable  ADSL  and mobile data networks provide Internet access in customer premises 81  and inexpensive VoIP network adapters provide the connection for traditional analog telephone sets  The voice quality of VoIP often exceeds that of traditional calls  Remaining problems for VoIP include the situation that emergency services may not be universally available  and that devices rely on a local power supply  while older traditional phones are powered from the local loop  and typically operate during a power failure     File sharing is an example of transferring large amounts of data across the Internet  A computer file can be emailed to customers  colleagues and friends as an attachment  It can be uploaded to a website or File Transfer Protocol  FTP  server for easy download by others  It can be put into a \"shared location\" or onto a file server for instant use by colleagues  The load of bulk downloads to many users can be eased by the use of \"mirror\" servers or peer-to-peer networks  In any of these cases  access to the file may be controlled by user authentication  the transit of the file over the Internet may be obscured by encryption  and money may change hands for access to the file  The price can be paid by the remote charging of funds from  for example  a credit card whose details are also passedusually fully encryptedacross the Internet  The origin and authenticity of the file received may be checked by digital signatures or by MD5 or other message digests  These simple features of the Internet  over a worldwide basis  are changing the production  sale  and distribution of anything that can be reduced to a computer file for transmission  This includes all manner of print publications  software products  news  music  film  video  photography  graphics and the other arts  This in turn has caused seismic shifts in each of the existing industries that previously controlled the production and distribution of these products     Streaming media is the real-time delivery of digital media for the immediate consumption or enjoyment by end users  Many radio and television broadcasters provide Internet feeds of their live audio and video productions  They may also allow time-shift viewing or listening such as Preview  Classic Clips and Listen Again features  These providers have been joined by a range of pure Internet \"broadcasters\" who never had on-air licenses  This means that an Internet-connected device  such as a computer or something more specific  can be used to access on-line media in much the same way as was previously possible only with a television or radio receiver  The range of available types of content is much wider  from specialized technical webcasts to on-demand popular multimedia services  Podcasting is a variation on this theme  whereusually audiomaterial is downloaded and played back on a computer or shifted to a portable media player to be listened to on the move  These techniques using simple equipment allow anybody  with little censorship or licensing control  to broadcast audio-visual material worldwide     Digital media streaming increases the demand for network bandwidth  For example  standard image quality needs 1 Mbit/s link speed for SD 480p  HD 720p quality requires 2 5 Mbit/s  and the top-of-the-line HDX quality needs 4 5 Mbit/s for 1080p  82     Webcams are a low-cost extension of this phenomenon  While some webcams can give full-frame-rate video  the picture either is usually small or updates slowly  Internet users can watch animals around an African waterhole  ships in the Panama Canal  traffic at a local roundabout or monitor their own premises  live and in real time  Video chat rooms and video conferencing are also popular with many uses being found for personal webcams  with and without two-way sound  YouTube was founded on 15 February 2005 and is now the leading website for free streaming video with more than two billion users  83  It uses an HTML5 based web player by default to stream and show video files  84  Registered users may upload an unlimited amount of video and build their own personal profile  YouTube claims that its users watch hundreds of millions  and upload hundreds of thousands of videos daily     The Internet has enabled new forms of social interaction  activities  and social associations  This phenomenon has given rise to the scholarly study of the sociology of the Internet     From 2000 to 2009  the number of Internet users globally rose from 394 xa0million to 1 858 xa0billion  88  By 2010  22 percent of the world  s population had access to computers with 1 xa0billion Google searches every day  300 xa0million Internet users reading blogs  and 2 xa0billion videos viewed daily on YouTube  89  In 2014 the world  s Internet users surpassed 3 xa0billion or 43 6 percent of world population  but two-thirds of the users came from richest countries  with 78 0 percent of Europe countries population using the Internet  followed by 57 4 percent of the Americas  90  However  by 2018  Asia alone accounted for 51% of all Internet users  with 2 2 xa0billion out of the 4 3 xa0billion Internet users in the world coming from that region  The number of China  s Internet users surpassed a major milestone in 2018  when the country  s Internet regulatory authority  China Internet Network Information Centre  announced that China had 802 xa0million Internet users  91  By 2019  China was the world  s leading country in terms of Internet users  with more than 800 xa0million users  followed closely by India  with some 700 xa0million users  with the United States a distant third with 275 xa0million users  However  in terms of penetration  China has when?  a 38 4% penetration rate compared to India  s 40% and the United States  s 80%  92  As of 2020  it was estimated that 4 5 xa0billion people use the Internet  more than half of the world  s population  93  94     The prevalent language for communication via the Internet has always been English  This may be a result of the origin of the Internet  as well as the language  s role as a lingua franca and as a world language  Early computer systems were limited to the characters in the American Standard Code for Information Interchange  ASCII   a subset of the Latin alphabet     After English  27%   the most requested languages on the World Wide Web are Chinese  25%   Spanish  8%   Japanese  5%   Portuguese and German  4% each   Arabic  French and Russian  3% each   and Korean  2%   95  By region  42% of the world  s Internet users are based in Asia  24% in Europe  14% in North America  10% in Latin America and the Caribbean taken together  6% in Africa  3% in the Middle East and 1% in Australia/Oceania  96  The Internet  s technologies have developed enough in recent years  especially in the use of Unicode  that good facilities are available for development and communication in the world  s widely used languages  However  some glitches such as mojibake  incorrect display of some languages   characters  still remain     In an American study in 2005  the percentage of men using the Internet was very slightly ahead of the percentage of women  although this difference reversed in those under 30  Men logged on more often  spent more time online  and were more likely to be broadband users  whereas women tended to make more use of opportunities to communicate  such as email   Men were more likely to use the Internet to pay bills  participate in auctions  and for recreation such as downloading music and videos  Men and women were equally likely to use the Internet for shopping and banking  97  nMore recent studies indicate that in 2008  women significantly outnumbered men on most social networking services  such as Facebook and Myspace  although the ratios varied with age  98  In addition  women watched more streaming content  whereas men downloaded more  99  In terms of blogs  men were more likely to blog in the first place  among those who blog  men were more likely to have a professional blog  whereas women were more likely to have a personal blog  100     Splitting by country  in 2012 Iceland  Norway  Sweden  the Netherlands  and Denmark had the highest Internet penetration by the number of users  with 93% or more of the population with access  101     Several neologisms exist that refer to Internet users  Netizen  as in \"citizen of the net\"  102  refers to those actively involved in improving online communities  the Internet in general or surrounding political affairs and rights such as free speech  103  104  Internaut refers to operators or technically highly capable users of the Internet  105  106  digital citizen refers to a person using the Internet in order to engage in society  politics  and government participation  107     The Internet allows greater flexibility in working hours and location  especially with the spread of unmetered high-speed connections  The Internet can be accessed almost anywhere by numerous means  including through mobile Internet devices  Mobile phones  datacards  handheld game consoles and cellular routers allow users to connect to the Internet wirelessly  Within the limitations imposed by small screens and other limited facilities of such pocket-sized devices  the services of the Internet  including email and the web  may be available  Service providers may restrict the services offered and mobile data charges may be significantly higher than other access methods     Educational material at all levels from pre-school to post-doctoral is available from websites  Examples range from CBeebies  through school and high-school revision guides and virtual universities  to access to top-end scholarly literature through the likes of Google Scholar  For distance education  help with homework and other assignments  self-guided learning  whiling away spare time or just looking up more detail on an interesting fact  it has never been easier for people to access educational information at any level from anywhere  The Internet in general and the World Wide Web in particular are important enablers of both formal and informal education  Further  the Internet allows universities  in particular  researchers from the social and behavioral sciences  to conduct research remotely via virtual laboratories  with profound changes in reach and generalizability of findings as well as in communication between scientists and in the publication of results  111     The low cost and nearly instantaneous sharing of ideas  knowledge  and skills have made collaborative work dramatically easier  with the help of collaborative software  Not only can a group cheaply communicate and share ideas but the wide reach of the Internet allows such groups more easily to form  An example of this is the free software movement  which has produced  among other things  Linux  Mozilla Firefox  and OpenOffice org  later forked into LibreOffice   Internet chat  whether using an IRC chat room  an instant messaging system  or a social networking service  allows colleagues to stay in touch in a very convenient way while working at their computers during the day  Messages can be exchanged even more quickly and conveniently than via email  These systems may allow files to be exchanged  drawings and images to be shared  or voice and video contact between team members     Content management systems allow collaborating teams to work on shared sets of documents simultaneously without accidentally destroying each other  s work  Business and project teams can share calendars as well as documents and other information  Such collaboration occurs in a wide variety of areas including scientific research  software development  conference planning  political activism and creative writing  Social and political collaboration is also becoming more widespread as both Internet access and computer literacy spread     The Internet allows computer users to remotely access other computers and information stores easily from any access point  Access may be with computer security  i e  authentication and encryption technologies  depending on the requirements  This is encouraging new ways of working from home  collaboration and information sharing in many industries  An accountant sitting at home can audit the books of a company based in another country  on a server situated in a third country that is remotely maintained by IT specialists in a fourth  These accounts could have been created by home-working bookkeepers  in other remote locations  based on information emailed to them from offices all over the world  Some of these things were possible before the widespread use of the Internet  but the cost of private leased lines would have made many of them infeasible in practice  An office worker away from their desk  perhaps on the other side of the world on a business trip or a holiday  can access their emails  access their data using cloud computing  or open a remote desktop session into their office PC using a secure virtual private network  VPN  connection on the Internet  This can give the worker complete access to all of their normal files and data  including email and other applications  while away from the office  It has been referred to among system administrators as the Virtual Private Nightmare  112  because it extends the secure perimeter of a corporate network into remote locations and its employees   homes     By late 2010s Internet has been described as \"the main source of scientific information \"for the majority of the global North population\"  113  111    Many people use the World Wide Web to access news  weather and sports reports  to plan and book vacations and to pursue their personal interests  People use chat  messaging and email to make and stay in touch with friends worldwide  sometimes in the same way as some previously had pen pals  Social networking services such as Facebook have created new ways to socialize and interact  Users of these sites are able to add a wide variety of information to pages  pursue common interests  and connect with others  It is also possible to find existing acquaintances  to allow communication among existing groups of people  Sites like LinkedIn foster commercial and business connections  YouTube and Flickr specialize in users   videos and photographs  Social networking services are also widely used by businesses and other organizations to promote their brands  to market to their customers and to encourage posts to \"go viral\"  \"Black hat\" social media techniques are also employed by some organizations  such as spam accounts and astroturfing     A risk for both individuals and organizations writing posts  especially public posts  on social networking services  is that especially foolish or controversial posts occasionally lead to an unexpected and possibly large-scale backlash on social media from other Internet users  This is also a risk in relation to controversial offline behavior  if it is widely made known  The nature of this backlash can range widely from counter-arguments and public mockery  through insults and hate speech  to  in extreme cases  rape and death threats  The online disinhibition effect describes the tendency of many individuals to behave more stridently or offensively online than they would in person  A significant number of feminist women have been the target of various forms of harassment in response to posts they have made on social media  and Twitter in particular has been criticised in the past for not doing enough to aid victims of online abuse  114     For organizations  such a backlash can cause overall brand damage  especially if reported by the media  However  this is not always the case  as any brand damage in the eyes of people with an opposing opinion to that presented by the organization could sometimes be outweighed by strengthening the brand in the eyes of others  Furthermore  if an organization or individual gives in to demands that others perceive as wrong-headed  that can then provoke a counter-backlash     Some websites  such as Reddit  have rules forbidding the posting of personal information of individuals  also known as doxxing   due to concerns about such postings leading to mobs of large numbers of Internet users directing harassment at the specific individuals thereby identified  In particular  the Reddit rule forbidding the posting of personal information is widely understood to imply that all identifying photos and names must be censored in Facebook screenshots posted to Reddit  However  the interpretation of this rule in relation to public Twitter posts is less clear  and in any case  like-minded people online have many other ways they can use to direct each other  s attention to public social media posts they disagree with     Children also face dangers online such as cyberbullying and approaches by sexual predators  who sometimes pose as children themselves  Children may also encounter material which they may find upsetting  or material that their parents consider to be not age-appropriate  Due to naivety  they may also post personal information about themselves online  which could put them or their families at risk unless warned not to do so  Many parents choose to enable Internet filtering  and/or supervise their children  s online activities  in an attempt to protect their children from inappropriate material on the Internet  The most popular social networking services  such as Facebook and Twitter  commonly forbid users under the age of 13  However  these policies are typically trivial to circumvent by registering an account with a false birth date  and a significant number of children aged under 13 join such sites anyway  Social networking services for younger children  which claim to provide better levels of protection for children  also exist  115     The Internet has been a major outlet for leisure activity since its inception  with entertaining social experiments such as MUDs and MOOs being conducted on university servers  and humor-related Usenet groups receiving much traffic  citation needed  Many Internet forums have sections devoted to games and funny videos  citation needed  The Internet pornography and online gambling industries have taken advantage of the World Wide Web  Although many governments have attempted to restrict both industries   use of the Internet  in general  this has failed to stop their widespread popularity  116     Another area of leisure activity on the Internet is multiplayer gaming  117  This form of recreation creates communities  where people of all ages and origins enjoy the fast-paced world of multiplayer games  These range from MMORPG to first-person shooters  from role-playing video games to online gambling  While online gaming has been around since the 1970s  modern modes of online gaming began with subscription services such as GameSpy and MPlayer  118  Non-subscribers were limited to certain types of game play or certain games  Many people use the Internet to access and download music  movies and other works for their enjoyment and relaxation  Free and fee-based services exist for all of these activities  using centralized servers and distributed peer-to-peer technologies  Some of these sources exercise more care with respect to the original artists   copyrights than others     Internet usage has been correlated to users   loneliness  119  Lonely people tend to use the Internet as an outlet for their feelings and to share their stories with others  such as in the \"I am lonely will anyone speak to me\" thread     A 2017 book claimed that the Internet consolidates most aspects of human endeavor into singular arenas of which all of humanity are potential members and competitors  with fundamentally negative impacts on mental health as a result  While successes in each field of activity are pervasively visible and trumpeted  they are reserved for an extremely thin sliver of the world  s most exceptional  leaving everyone else behind  Whereas  before the Internet  expectations of success in any field were supported by reasonable probabilities of achievement at the village  suburb  city or even state level  the same expectations in the Internet world are virtually certain to bring disappointment today  there is always someone else  somewhere on the planet  who can do better and take the now one-and-only top spot  120     Cybersectarianism is a new organizational form which involves  \"highly dispersed small groups of practitioners that may remain largely anonymous within the larger social context and operate in relative secrecy  while still linked remotely to a larger network of believers who share a set of practices and texts  and often a common devotion to a particular leader  Overseas supporters provide funding and support  domestic practitioners distribute tracts  participate in acts of resistance  and share information on the internal situation with outsiders  Collectively  members and practitioners of such sects construct viable virtual communities of faith  exchanging personal testimonies and engaging in the collective study via email  on-line chat rooms  and web-based message boards \" 121  In particular  the British government has raised concerns about the prospect of young British Muslims being indoctrinated into Islamic extremism by material on the Internet  being persuaded to join terrorist groups such as the so-called \"Islamic State\"  and then potentially committing acts of terrorism on returning to Britain after fighting in Syria or Iraq     Cyberslacking can become a drain on corporate resources  the average UK employee spent 57 minutes a day surfing the Web while at work  according to a 2003 study by Peninsula Business Services  122  Internet addiction disorder is excessive computer use that interferes with daily life  Nicholas G  Carr believes that Internet use has other effects on individuals  for instance improving skills of scan-reading and interfering with the deep thinking that leads to true creativity  123     Electronic business  e-business  encompasses business processes spanning the entire value chain  purchasing  supply chain management  marketing  sales  customer service  and business relationship  E-commerce seeks to add revenue streams using the Internet to build and enhance relationships with clients and partners  According to International Data Corporation  the size of worldwide e-commerce  when global business-to-business and -consumer transactions are combined  equate to $16 xa0trillion for 2013  A report by Oxford Economics added those two together to estimate the total size of the digital economy at $20 4 xa0trillion  equivalent to roughly 13 8% of global sales  124     While much has been written of the economic advantages of Internet-enabled commerce  there is also evidence that some aspects of the Internet such as maps and location-aware services may serve to reinforce economic inequality and the digital divide  125  Electronic commerce may be responsible for consolidation and the decline of mom-and-pop  brick and mortar businesses resulting in increases in income inequality  126  127  128     Author Andrew Keen  a long-time critic of the social transformations caused by the Internet  has focused on the economic effects of consolidation from Internet businesses  Keen cites a 2013 Institute for Local Self-Reliance report saying brick-and-mortar retailers employ 47 people for every $10 xa0million in sales while Amazon employs only 14  Similarly  the 700-employee room rental start-up Airbnb was valued at $10 xa0billion in 2014  about half as much as Hilton Worldwide  which employs 152 000 people  At that time  Uber employed 1 000 full-time employees and was valued at $18 2 xa0billion  about the same valuation as Avis Rent a Car and The Hertz Corporation combined  which together employed almost 60 000 people  129     Telecommuting is the performance within a traditional worker and employer relationship when it is facilitated by tools such as groupware  virtual private networks  conference calling  videoconferencing  and VoIP so that work may be performed from any location  most conveniently the worker  s home  It can be efficient and useful for companies as it allows workers to communicate over long distances  saving significant amounts of travel time and cost  As broadband Internet connections become commonplace  more workers have adequate bandwidth at home to use these tools to link their home to their corporate intranet and internal communication networks     Wikis have also been used in the academic community for sharing and dissemination of information across institutional and international boundaries  130  In those settings  they have been found useful for collaboration on grant writing  strategic planning  departmental documentation  and committee work  131  The United States Patent and Trademark Office uses a wiki to allow the public to collaborate on finding prior art relevant to examination of pending patent applications  Queens  New York has used a wiki to allow citizens to collaborate on the design and planning of a local park  132  The English Wikipedia has the largest user base among wikis on the World Wide Web 133  and ranks in the top 10 among all Web sites in terms of traffic  134     The Internet has achieved new relevance as a political tool  The presidential campaign of Howard Dean in 2004 in the United States was notable for its success in soliciting donation via the Internet  Many political groups use the Internet to achieve a new method of organizing for carrying out their mission  having given rise to Internet activism  most notably practiced by rebels in the Arab Spring  135  136  The New York Times suggested that social media websites  such as Facebook and Twitter  helped people organize the political revolutions in Egypt  by helping activists organize protests  communicate grievances  and disseminate information  137     Many have understood the Internet as an extension of the Habermasian notion of the public sphere  observing how network communication technologies provide something like a global civic forum  However  incidents of politically motivated Internet censorship have now been recorded in many countries  including western democracies  citation needed     The spread of low-cost Internet access in developing countries has opened up new possibilities for peer-to-peer charities  which allow individuals to contribute small amounts to charitable projects for other individuals  Websites  such as DonorsChoose and GlobalGiving  allow small-scale donors to direct funds to individual projects of their choice  A popular twist on Internet-based philanthropy is the use of peer-to-peer lending for charitable purposes  Kiva pioneered this concept in 2005  offering the first web-based service to publish individual loan profiles for funding  Kiva raises funds for local intermediary microfinance organizations that post stories and updates on behalf of the borrowers  Lenders can contribute as little as $25 to loans of their choice  and receive their money back as borrowers repay  Kiva falls short of being a pure peer-to-peer charity  in that loans are disbursed before being funded by lenders and borrowers do not communicate with lenders themselves  138  139     Internet resources  hardware  and software components are the target of criminal or malicious attempts to gain unauthorized control to cause interruptions  commit fraud  engage in blackmail or access private information     Malware is malicious software used and distributed via the Internet  It includes computer viruses which are copied with the help of humans  computer worms which copy themselves automatically  software for denial of service attacks  ransomware  botnets  and spyware that reports on the activity and typing of users  Usually  these activities constitute cybercrime  Defense theorists have also speculated about the possibilities of hackers using cyber warfare using similar methods on a large scale  140     The vast majority of computer surveillance involves the monitoring of data and traffic on the Internet  141  In the United States for example  under the Communications Assistance For Law Enforcement Act  all phone calls and broadband Internet traffic  emails  web traffic  instant messaging  etc   are required to be available for unimpeded real-time monitoring by Federal law enforcement agencies  142  143  144  Packet capture is the monitoring of data traffic on a computer network  Computers communicate over the Internet by breaking up messages  emails  images  videos  web pages  files  etc   into small chunks called \"packets\"  which are routed through a network of computers  until they reach their destination  where they are assembled back into a complete \"message\" again  Packet Capture Appliance intercepts these packets as they are traveling through the network  in order to examine their contents using other programs  A packet capture is an information gathering tool  but not an analysis tool  That is it gathers \"messages\" but it does not analyze them and figure out what they mean  Other programs are needed to perform traffic analysis and sift through intercepted data looking for important/useful information  Under the Communications Assistance For Law Enforcement Act all U S  telecommunications providers are required to install packet sniffing technology to allow Federal law enforcement and intelligence agencies to intercept all of their customers   broadband Internet and VoIP traffic  145     The large amount of data gathered from packet capturing requires surveillance software that filters and reports relevant information  such as the use of certain words or phrases  the access of certain types of web sites  or communicating via email or chat with certain parties  146  Agencies  such as the Information Awareness Office  NSA  GCHQ and the FBI  spend billions of dollars per year to develop  purchase  implement  and operate systems for interception and analysis of data  147  Similar systems are operated by Iranian secret police to identify and suppress dissidents  The required hardware and software was allegedly installed by German Siemens AG and Finnish Nokia  148     Some governments  such as those of Burma  Iran  North Korea  Mainland China  Saudi Arabia and the United Arab Emirates  restrict access to content on the Internet within their territories  especially to political and religious content  with domain name and keyword filters  154     In Norway  Denmark  Finland  and Sweden  major Internet service providers have voluntarily agreed to restrict access to sites listed by authorities  While this list of forbidden resources is supposed to contain only known child pornography sites  the content of the list is secret  155  Many countries  including the United States  have enacted laws against the possession or distribution of certain material  such as child pornography  via the Internet  but do not mandate filter software  Many free or commercially available software programs  called content-control software are available to users to block offensive websites on individual computers or networks  in order to limit access by children to pornographic material or depiction of violence     As the Internet is a heterogeneous network  the physical characteristics  including for example the data transfer rates of connections  vary widely  It exhibits emergent phenomena that depend on its large-scale organization  156     The volume of Internet traffic is difficult to measure  because no single point of measurement exists in the multi-tiered  non-hierarchical topology  Traffic data may be estimated from the aggregate volume through the peering points of the Tier 1 network providers  but traffic that stays local in large provider networks may not be accounted for     An Internet blackout or outage can be caused by local signalling interruptions  Disruptions of submarine communications cables may cause blackouts or slowdowns to large areas  such as in the 2008 submarine cable disruption  Less-developed countries are more vulnerable due to a small number of high-capacity links  Land cables are also vulnerable  as in 2011 when a woman digging for scrap metal severed most connectivity for the nation of Armenia  157  Internet blackouts affecting almost entire countries can be achieved by governments as a form of Internet censorship  as in the blockage of the Internet in Egypt  whereby approximately 93% 158  of networks were without access in 2011 in an attempt to stop mobilization for anti-government protests  159     Estimates of the Internet  s electricity usage have been the subject of controversy  according to a 2014 peer-reviewed research paper that found claims differing by a factor of 20 000 published in the literature during the preceding decade  ranging from 0 0064 kilowatt hours per gigabyte transferred  kWh/GB  to 136 kWh/GB  160  The researchers attributed these discrepancies mainly to the year of reference  i e  whether efficiency gains over time had been taken into account  and to whether \"end devices such as personal computers and servers are included\" in the analysis  160     In 2011  academic researchers estimated the overall energy used by the Internet to be between 170 and 307 GW  less than two percent of the energy used by humanity  This estimate included the energy needed to build  operate  and periodically replace the estimated 750 xa0million laptops  a billion smart phones and 100 xa0million servers worldwide as well as the energy that routers  cell towers  optical switches  Wi-Fi transmitters and cloud storage devices use when transmitting Internet traffic  161  162  According to a non-peer reviewed study published in 2018 by The Shift Project  a French think tank funded by corporate sponsors   nearly 4% of global CO2 emissions could be attributed to global data transfer and the necessary infrastructure  163  The study also said that online video streaming alone accounted for 60% of this data transfer and therefore contributed to over 300 xa0million tons of CO2 emission per year  and argued for new \"digital sobriety\" regulations restricting the use and size of video files  164      Media related to Internet at Wikimedia Commons    A discrete-event simulation  DES  models the operation of a system as a  discrete  sequence of events in time  Each event occurs at a particular instant in time and marks a change of state in the system  1  Between consecutive events  no change in the system is assumed to occur  thus the simulation time can directly jump to the occurrence time of the next event  which is called next-event time progression     In addition to next-event time progression  there is also an alternative approach  called fixed-increment time progression  where time is broken up into small time slices and the system state is updated according to the set of events/activities happening in the time slice  2  Because not every time slice has to be simulated  a next-event time simulation can typically run much faster than a corresponding fixed-increment time simulation     Both forms of DES contrast with continuous simulation in which the system state is changed continuously over time on the basis of a set of differential equations defining the rates of change of state variables     A common exercise in learning how to build discrete-event simulations is to model a queue  such as customers arriving at a bank to be served by a teller   In this example  the system entities are Customer-queue and Tellers   The system events are Customer-Arrival and Customer-Departure    The event of Teller-Begins-Service can be part of the logic of the arrival and departure events   The system states  which are changed by these events  are Number-of-Customers-in-the-Queue  an integer from 0 to  and Teller-Status  busy or idle     The random variables that need to be characterized to model this system stochastically are Customer-Interarrival-Time and Teller-Service-Time  An agent-based framework for performance modeling of an optimistic parallel discrete event simulator is another example for a discrete event simulation  3     In addition to the logic of what happens when system events occur  discrete event simulations include the following     A system state is a set of variables that captures the salient properties of the system to be studied  The state trajectory over time S t  can be mathematically represented by a step function whose value can change whenever an event occurs    \"The simulation must keep track of the current simulation time  in whatever measurement units are suitable for the system being modeled  In discrete-event simulations  as opposed to continuous simulations  time  hops  because events are instantaneous  the clock skips to the next event start time as the simulation proceeds  n\"   The simulation maintains at least one list of simulation events   This is sometimes called the pending event set because it lists events that are pending as a result of previously simulated event but have yet to be simulated themselves  An event is described by the time at which it occurs and a type  indicating the code that will be used to simulate that event   It is common for the event code to be parametrized  in which case  the event description also contains parameters to the event code     When events are instantaneous  activities that extend over time are modeled as sequences of events   Some simulation frameworks allow the time of an event to be specified as an interval  giving the start time and the end time of each event     Single-threaded simulation engines based on instantaneous events have just one current event   In contrast  multi-threaded simulation engines and simulation engines supporting an interval-based event model may have multiple current events   In both cases  there are significant problems with synchronization between current events     The pending event set is typically organized as a priority queue  sorted by event time  4   That is  regardless of the order in which events are added to the event set  they are removed in strictly chronological order   Various priority queue implementations have been studied in the context of discrete event simulation  5  alternatives studied have included splay trees  skip lists  calendar queues  6  and ladder queues  7  8  nOn massively-parallel machines  such as multi-core or many-core CPUs  the pending event set can be implemented by relying on non-blocking algorithms  in order to reduce the cost of synchronization among the concurrent threads  9  10     Typically  events are scheduled dynamically as the simulation proceeds  For example  in the bank example noted above  the event CUSTOMER-ARRIVAL at time t would  if the CUSTOMER_QUEUE was empty and TELLER was idle  include the creation of the subsequent event CUSTOMER-DEPARTURE to occur at time t+s  where s is a number generated from the SERVICE-TIME distribution     The simulation needs to generate random variables of various kinds  depending on the system model  This is accomplished by one or more Pseudorandom number generators  The use of pseudo-random numbers as opposed to true random numbers is a benefit should a simulation need a rerun with exactly the same behavior     One of the problems with the random number distributions used in discrete-event simulation is that the steady-state distributions of event times may not be known in advance   As a result  the initial set of events placed into the pending event set will not have arrival times representative of the steady-state distribution   This problem is typically solved by bootstrapping the simulation model   Only a limited effort is made to assign realistic times to the initial set of pending events   These events  however  schedule additional events  and with time  the distribution of event times approaches its steady state   This is called bootstrapping the simulation model   In gathering statistics from the running model  it is important to either disregard events that occur before the steady state is reached or to run the simulation for long enough that the bootstrapping behavior is overwhelmed by steady-state behavior    This use of the term bootstrapping can be contrasted with its use in both statistics and computing      The simulation typically keeps track of the system  s statistics  which quantify the aspects of interest  In the bank example  it is of interest to track the mean waiting times  In a simulation model  performance metrics are not analytically derived from probability distributions  but rather as averages over replications  that is different runs of the model  Confidence intervals are usually constructed to help assess the quality of the output     Because events are bootstrapped  theoretically a discrete-event simulation could run forever  So the simulation designer must decide when the simulation will end   Typical choices are \"at time t\" or \"after processing number of events\" or  more generally  \"when statistical measure X reaches the value x\"    \"Pidd  1998  has proposed the three-phased approach to discrete event simulation  In this approach  the first phase is to jump to the next chronological event  The second phase is to execute all events that unconditionally occur at that time  these are called B-events   The third phase is to execute all events that conditionally occur at that time  these are called C-events   The three phase approach is a refinement of the event-based approach in which simultaneous events are ordered so as to make the most efficient use of computer resources   The three-phase approach is used by a number of commercial simulation software packages  but from the user s point of view  the specifics of the underlying simulation method are generally hidden  n\"   Simulation approaches are particularly well equipped to help users diagnose issues in complex environments   The theory of constraints illustrates the importance of understanding bottlenecks in a system  Identifying and removing bottlenecks allows improving processes and the overall system   For instance  in manufacturing enterprises bottlenecks may be created by excess inventory  overproduction  variability in processes and variability in routing or sequencing   By accurately documenting the system with the help of a simulation model it is possible to gain a birds eye view of the entire system     A working model of a system allows management to understand performance drivers   A simulation can be built to include any number of performance indicators such as worker utilization  on-time delivery rate  scrap rate  cash cycles  and so on     An operating theater is generally shared between several surgical disciplines   Through better understanding the nature of these procedures it may be possible to increase the patient throughput  nExample   If a heart surgery takes on average four hours  changing an operating room schedule from eight available hours to nine will not increase patient throughput   On the other hand  if a hernia procedure takes on average twenty minutes providing an extra hour may also not yield any increased throughput if the capacity and average time spent in the recovery room is not considered     Many systems improvement ideas are built on sound principles  proven methodologies  Lean  Six Sigma  TQM  etc   yet fail to improve the overall system   A simulation model allows the user to understand and test a performance improvement idea in the context of the overall system     Simulation modeling is commonly used to model potential investments   Through modeling investments decision-makers can make informed decisions and evaluate potential alternatives     Discrete event simulation is used in computer network to simulate new protocols  different system architectures  distributed  hierarchical  centralised  P2P  before actual deployment  It is possible to define different evaluation metrics  such as service time  bandwidth  dropped packets  resource consumption  and so on     System modeling approaches     Computational techniques     Software     Disciplines     Discrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous  In contrast to real numbers that have the property of varying \"smoothly\"  the objects studied in discrete mathematics  such as integers  graphs  and statements in logic 1  2   do not vary smoothly in this way  but have distinct  separated values  3  4  Discrete mathematics therefore excludes topics in \"continuous mathematics\" such as calculus or Euclidean geometry  Discrete objects can often be enumerated by integers  More formally  discrete mathematics has been characterized as the branch of mathematics dealing with countable sets 5   finite sets or sets with the same cardinality as the natural numbers   However  there is no exact definition of the term \"discrete mathematics \" 6  Indeed  discrete mathematics is described less by what is included than by what is excluded  continuously varying quantities and related notions     The set of objects studied in discrete mathematics can be finite or infinite  The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets  particularly those areas relevant to business     Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in discrete steps and store data in discrete bits  Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science  such as computer algorithms  programming languages  cryptography  automated theorem proving  and software development  Conversely  computer implementations are significant in applying ideas from discrete mathematics to real-world problems  such as in operations research     Although the main objects of study in discrete mathematics are discrete objects  analytic methods from continuous mathematics are often employed as well     In university curricula  \"Discrete Mathematics\" appeared in the 1980s  initially as a computer science support course  its contents were somewhat haphazard at the time  The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students  therefore  it is nowadays a prerequisite for mathematics majors in some universities as well  7  8  Some high-school-level discrete mathematics textbooks have appeared as well  9  At this level  discrete mathematics is sometimes seen as a preparatory course  not unlike precalculus in this respect  10     The Fulkerson Prize is awarded for outstanding papers in discrete mathematics     The history of discrete mathematics has involved a number of challenging problems which have focused attention within areas of the field  In graph theory  much research was motivated by attempts to prove the four color theorem  first stated in 1852  but not proved until 1976  by Kenneth Appel and Wolfgang Haken  using substantial computer assistance   11     In logic  the second problem on David Hilbert  s list of open problems presented in 1900 was to prove that the axioms of arithmetic are consistent  Gdel  s second incompleteness theorem  proved in 1931  showed that this was not possible  at least not within arithmetic itself  Hilbert  s tenth problem was to determine whether a given polynomial Diophantine equation with integer coefficients has an integer solution  In 1970  Yuri Matiyasevich proved that this could not be done     The need to break German codes in World War II led to advances in cryptography and theoretical computer science  with the first programmable digital electronic computer being developed at England  s Bletchley Park with the guidance of Alan Turing and his seminal work  On Computable Numbers  12  At the same time  military requirements motivated advances in operations research  The Cold War meant that cryptography remained important  with fundamental advances such as public-key cryptography being developed in the following decades  Operations research remained important as a tool in business and project management  with the critical path method being developed in the 1950s  The telecommunication industry has also motivated advances in discrete mathematics  particularly in graph theory and information theory  Formal verification of statements in logic has been necessary for software development of safety-critical systems  and advances in automated theorem proving have been driven by this need     Computational geometry has been an important part of the computer graphics incorporated into modern video games and computer-aided design tools     Several fields of discrete mathematics  particularly theoretical computer science  graph theory  and combinatorics  are important in addressing the challenging bioinformatics problems associated with understanding the tree of life  13     Currently  one of the most famous open problems in theoretical computer science is the P = NP problem  which involves the relationship between the complexity classes P and NP  The Clay Mathematics Institute has offered a $1 million USD prize for the first correct proof  along with prizes for six other mathematical problems  14     Theoretical computer science includes areas of discrete mathematics relevant to computing  It draws heavily on graph theory and mathematical logic  Included within theoretical computer science is the study of algorithms and data structures  Computability studies what can be computed in principle  and has close ties to logic  while complexity studies the time  space  and other resources taken by computations  Automata theory and formal language theory are closely related to computability  Petri nets and process algebras are used to model computer systems  and methods from discrete mathematics are used in analyzing VLSI electronic circuits  Computational geometry applies algorithms to geometrical problems  while computer image analysis applies them to representations of images  Theoretical computer science also includes the study of various continuous computational topics     Information theory involves the quantification of information  Closely related is coding theory which is used to design efficient and reliable data transmission and storage methods  Information theory also includes continuous topics such as  analog signals  analog coding  analog encryption     Logic is the study of the principles of valid reasoning and inference  as well as of consistency  soundness  and completeness  For example  in most systems of logic  but not in intuitionistic logic  Peirce  s law    PQ P P  is a theorem  For classical logic  it can be easily verified with a truth table  The study of mathematical proof is particularly important in logic  and has applications to automated theorem proving and formal verification of software     Logical formulas are discrete structures  as are proofs  which form finite trees 15  or  more generally  directed acyclic graph structures 16  17   with each inference step combining one or more premise branches to give a single conclusion   The truth values of logical formulas usually form a finite set  generally restricted to two values  true and false  but logic can also be continuous-valued  e g   fuzzy logic  Concepts such as infinite proof trees or infinite derivation trees have also been studied  18  e g  infinitary logic     Set theory is the branch of mathematics that studies sets  which are collections of objects  such as  blue  white  red  or the  infinite  set of all prime numbers  Partially ordered sets and sets with other relations have applications in several areas     In discrete mathematics  countable sets  including finite sets  are the main focus  The beginning of set theory as a branch of mathematics is usually marked by Georg Cantor  s work distinguishing between different kinds of infinite set  motivated by the study of trigonometric series  and further development of the theory of infinite sets is outside the scope of discrete mathematics  Indeed  contemporary work in descriptive set theory makes extensive use of traditional continuous mathematics     Combinatorics studies the way in which discrete structures can be combined or arranged  nEnumerative combinatorics concentrates on counting the number of certain combinatorial objects - e g  the twelvefold way provides a unified framework for counting permutations  combinations and partitions  nAnalytic combinatorics concerns the enumeration  i e   determining the number  of combinatorial structures using tools from complex analysis and probability theory  In contrast with enumerative combinatorics which uses explicit combinatorial formulae and generating functions to describe the results  analytic combinatorics aims at obtaining asymptotic formulae  nDesign theory is a study of combinatorial designs  which are collections of subsets with certain intersection properties  nPartition theory studies various enumeration and asymptotic problems related to integer partitions  and is closely related to q-series  special functions and orthogonal polynomials  Originally a part of number theory and analysis  partition theory is now considered a part of combinatorics or an independent field  nOrder theory is the study of partially ordered sets  both finite and infinite     Graph theory  the study of graphs and networks  is often considered part of combinatorics  but has grown large enough and distinct enough  with its own kind of problems  to be regarded as a subject in its own right  19  Graphs are one of the prime objects of study in discrete mathematics  They are among the most ubiquitous models of both natural and human-made structures  They can model many types of relations and process dynamics in physical  biological and social systems  In computer science  they can represent networks of communication  data organization  computational devices  the flow of computation  etc  In mathematics  they are useful in geometry and certain parts of topology  e g  knot theory  Algebraic graph theory has close links with group theory  There are also continuous graphs  however  for the most part  research in graph theory falls within the domain of discrete mathematics     Discrete probability theory deals with events that occur in countable sample spaces  For example  count observations such as the numbers of birds in flocks comprise only natural number values  0  1  2        On the other hand  continuous observations such as the weights of birds comprise real number values and would typically be modeled by a continuous probability distribution such as the normal  Discrete probability distributions can be used to approximate continuous ones and vice versa  For highly constrained situations such as throwing dice or experiments with decks of cards  calculating the probability of events is basically enumerative combinatorics     Number theory is concerned with the properties of numbers in general  particularly integers  It has applications to cryptography and cryptanalysis  particularly with regard to modular arithmetic  diophantine equations  linear and quadratic congruences  prime numbers and primality testing  Other discrete aspects of number theory include geometry of numbers  In analytic number theory  techniques from continuous mathematics are also used  Topics that go beyond discrete objects include transcendental numbers  diophantine approximation  p-adic analysis and function fields     Algebraic structures occur as both discrete examples and continuous examples  Discrete algebras include  boolean algebra used in logic gates and programming  relational algebra used in databases  discrete and finite versions of groups  rings and fields are important in algebraic coding theory  discrete semigroups and monoids appear in the theory of formal languages     A function defined on an interval of the integers is usually called a sequence  A sequence could be a finite sequence from a data source or an infinite sequence from a discrete dynamical system  Such a discrete function could be defined explicitly by a list  if its domain is finite   or by a formula for its general term  or it could be given implicitly by a recurrence relation or difference equation  Difference equations are similar to differential equations  but replace differentiation by taking the difference between adjacent terms  they can be used to approximate differential equations or  more often  studied in their own right  Many questions and methods concerning differential equations have counterparts for difference equations  For instance  where there are integral transforms in harmonic analysis for studying continuous functions or analogue signals  there are discrete transforms for discrete functions or digital signals  As well as the discrete metric there are more general discrete or finite metric spaces and finite topological spaces     Discrete geometry and combinatorial geometry are about combinatorial properties of discrete collections of geometrical objects  A long-standing topic in discrete geometry is tiling of the plane  Computational geometry applies algorithms to geometrical problems     Although topology is the field of mathematics that formalizes and generalizes the intuitive notion of \"continuous deformation\" of objects  it gives rise to many discrete topics  this can be attributed in part to the focus on topological invariants  which themselves usually take discrete values  nSee combinatorial topology  topological graph theory  topological combinatorics  computational topology  discrete topological space  finite topological space  topology  chemistry      Operations research provides techniques for solving practical problems in engineering  business  and other fields  problems such as allocating resources to maximize profit  and scheduling project activities to minimize risk  Operations research techniques include linear programming and other areas of optimization  queuing theory  scheduling theory  and network theory  Operations research also includes continuous topics such as continuous-time Markov process  continuous-time martingales  process optimization  and continuous and hybrid control theory     Decision theory is concerned with identifying the values  uncertainties and other issues relevant in a given decision  its rationality  and the resulting optimal decision     Utility theory is about measures of the relative economic satisfaction from  or desirability of  consumption of various goods and services     Social choice theory is about voting  A more puzzle-based approach to voting is ballot theory     Game theory deals with situations where success depends on the choices of others  which makes choosing the best course of action more complex  There are even continuous games  see differential game  Topics include auction theory and fair division     Discretization concerns the process of transferring continuous models and equations into discrete counterparts  often for the purposes of making calculations easier by using approximations  Numerical analysis provides an important example     There are many concepts in continuous mathematics which have discrete versions  such as discrete calculus  discrete probability distributions  discrete Fourier transforms  discrete geometry  discrete logarithms  discrete differential geometry  discrete exterior calculus  discrete Morse theory  difference equations  discrete dynamical systems  and discrete vector xa0measures     In applied mathematics  discrete modelling is the discrete analogue of continuous modelling  In discrete modelling  discrete formulae are fit to data  A common method in this form of modelling is to use recurrence relation     In algebraic geometry  the concept of a curve can be extended to discrete geometries by taking the spectra of polynomial rings over finite fields to be models of the affine spaces over that field  and letting subvarieties or spectra of other rings provide the curves that lie in that space  Although the space in which the curves appear has a finite number of points  the curves are not so much sets of points as analogues of curves in continuous settings  For example  every point of the form                     V                 x                c                         Spec         u2061        K                 x                 =                              A                                1                                   displaystyle V x-c   subset   operatorname  Spec  K x =  mathbb  A  ^ 1     for                     K                 displaystyle K    a field can be studied either as                     Spec         u2061        K                 x                           /                         x                c                         Spec         u2061        K                 displaystyle   operatorname  Spec  K x / x-c   cong   operatorname  Spec  K     a point  or as the spectrum                     Spec         u2061        K                 x                                                      x                        c                                                displaystyle   operatorname  Spec  K x _  x-c      of the local ring at  x-c   a point together with a neighborhood around it  Algebraic varieties also have a well-defined notion of tangent space called the Zariski tangent space  making many features of calculus applicable even in finite settings     The time scale calculus is a unification of the theory of difference equations with that of differential equations  which has applications to fields requiring simultaneous modelling of discrete and continuous data  Another way of modeling such a situation is the notion of hybrid dynamical systems     In computer aided engineering  CAE  a preprocessor is a program which provides a graphical user interface  GUI  to define physical properties   nThis data is used by the subsequent computer simulation     Steps that are followed in Pre-Processing    1> The geometry  physical bounds  of the problem is defined    2> The volume occupied by the fluid is divided into discrete cells  meshing     3> The physical modeling is defined - Eg  equations of motion + enthalpy + radiation + species conservation    4> Boundary conditions are defined  This involves specifying the fluid behavior and properties at the boundaries of the problem  For transient problems  the initial conditions are also defined     5> The simulation is started and the equations are solved iteratively as a steady state or transient    6> Finally a post-processor is used for the analysis and visualization of the resting solution        This computer graphicsrelated article is a stub  You can help Wikipedia by expanding it     In computer science  a deterministic algorithm is an algorithm that  given a particular input  will always produce the same output  with the underlying machine always passing through the same sequence of states  Deterministic algorithms are by far the most studied and familiar kind of algorithm  as well as one of the most practical  since they can be run on real machines efficiently     Formally  a deterministic algorithm computes a mathematical function  a function has a unique value for any input in its domain  and the algorithm is a process that produces this particular value as output     Deterministic algorithms can be defined in terms of a state machine  a state describes what a machine is doing at a particular instant in time  State machines pass in a discrete manner from one state to another  Just after we enter the input  the machine is in its initial state or start state  If the machine is deterministic  this means that from this point onwards  its current state determines what its next state will be  its course through the set of states is predetermined  Note that a machine can be deterministic and still never stop or finish  and therefore fail to deliver a result     Examples of particular abstract machines which are deterministic include the deterministic Turing machine and deterministic finite automaton     A variety of factors can cause an algorithm to behave in a way which is not deterministic  or non-deterministic     Although real programs are rarely purely deterministic  it is easier for humans as well as other programs to reason about programs that are  For this reason  most programming languages and especially functional programming languages make an effort to prevent the above events from happening except under controlled conditions     The prevalence of multi-core processors has resulted in a surge of interest in determinism in parallel programming and challenges of non-determinism have been well documented  1  2  A number of tools to help deal with the challenges have been proposed 3  4  5  6  to deal with deadlocks and race conditions     It is advantageous  in some cases  for a program to exhibit nondeterministic behavior  The behavior of a card shuffling program used in a game of blackjack  for example  should not be predictable by players  even if the source code of the program is visible  The use of a pseudorandom number generator is often not sufficient to ensure that players are unable to predict the outcome of a shuffle  A clever gambler might guess precisely the numbers the generator will choose and so determine the entire contents of the deck ahead of time  allowing him to cheat  for example  the Software Security Group at Reliable Software Technologies was able to do this for an implementation of Texas Hold   em Poker that is distributed by ASF Software  Inc  allowing them to consistently predict the outcome of hands ahead of time  7  These problems can be avoided  in part  through the use of a cryptographically secure pseudo-random number generator  but it is still necessary for an unpredictable random seed to be used to initialize the generator  For this purpose a source of nondeterminism is required  such as that provided by a hardware random number generator     Note that a negative answer to the P=NP problem would not imply that programs with nondeterministic output are theoretically more powerful than those with deterministic output  The complexity class NP  complexity  can be defined without any reference to nondeterminism using the verifier-based definition     This logic-functional programming language establishes different determinism categories for predicate modes as explained in the reference  8  9     Haskell provides several mechanisms     As seen in Standard ML  OCaml and Scala    In mathematics  a continuous function is a function that does not have any abrupt changes in value  known as discontinuities  More precisely  a function is continuous if arbitrarily small changes in its output can be assured by restricting to sufficiently small changes in its input  If not continuous  a function is said to be discontinuous  Up until the 19th century  mathematicians largely relied on intuitive notions of continuity  during which attempts such as the epsilondelta definition were made to formalize it     Continuity of functions is one of the core concepts of topology  which is treated in full generality below  The introductory portion of this article focuses on the special case where the inputs and outputs of functions are real numbers   A stronger form of continuity is uniform continuity  In addition  this article discusses the definition for the more general case of functions between two metric spaces  In order theory  especially in domain theory  one considers a notion of continuity known as Scott continuity  Other forms of continuity do exist but they are not discussed in this article     As an example  the function H t  denoting the height of a growing flower at time t would be considered continuous  In contrast  the function M t  denoting the amount of money in a bank account at time t would be considered discontinuous  since it \"jumps\" at each point in time when money is deposited or withdrawn     A form of the epsilondelta definition of continuity was first given by Bernard Bolzano in 1817  Augustin-Louis Cauchy defined continuity of                     y        =        f                 x                          displaystyle y=f x     as follows  an infinitely small increment                                      displaystyle   alpha     of the independent variable x always produces an infinitely small change                     f                 x        +                                 f                 x                          displaystyle f x+  alpha  -f x     of the dependent variable y  see e g  Cours d  Analyse  p  xa034   Cauchy defined infinitely small quantities in terms of variable quantities  and his definition of continuity closely parallels the infinitesimal definition used today  see microcontinuity   The formal definition and the distinction between pointwise continuity and uniform continuity were first given by Bolzano in the 1830s but the work wasn  t published until the 1930s  Like Bolzano  1  Karl Weierstrass 2  denied continuity of a function at a point c unless it was defined at and on both sides of c  but douard Goursat 3  allowed the function to be defined only at and on one side of c  and Camille Jordan 4  allowed it even if the function was defined only at c  All three of those nonequivalent definitions of pointwise continuity are still in use  5  Eduard Heine provided the first published definition of uniform continuity in 1872  but based these ideas on lectures given by Peter Gustav Lejeune Dirichlet in 1854  6     A real function  that is a function from real numbers to real numbers  can be represented by a graph in the Cartesian plane  such a function is continuous if  roughly speaking  the graph is a single unbroken curve whose domain is the entire real line   A more mathematically rigorous definition is given below  7     A rigorous definition of continuity of real functions is usually given in a first course in calculus in terms of the idea of a limit  First  a function f with variable x is said to be continuous at the point c on the real line  if the limit of                     f                 x                                   displaystyle f x      as x approaches that point c  is equal to the value                     f                 c                          displaystyle f c      and second  the function  as a whole  is said to be continuous  if it is continuous at every point  A function is said to be discontinuous  or to have a discontinuity  at some point when it is not continuous there  These points themselves are also addressed as discontinuities     There are several different definitions of continuity of a function   Sometimes a function is said to be continuous if it is continuous at every point in its domain   In this case  the function                     f                 x                 =        tan         u2061                 x                                   displaystyle f x =  tan x      with the domain of all real                     x                                                            2              n              +              1                        2                                            displaystyle x  neq   pi    frac  2n+1  2                           n                 displaystyle n    any integer  is continuous  Sometimes an exception is made for boundaries of the domain  For example  the graph of the function                     f                 x                 =                              x                                            displaystyle f x =   sqrt  x       with the domain of all non-negative reals  has a left-hand endpoint  In this case only the limit from the right is required to equal the value of the function  Under this definition f is continuous at the boundary                     x        =        0                 displaystyle x=0    and so for all non-negative arguments  The most common and restrictive definition is that a function is continuous if it is continuous at all real numbers   In this case  the previous two examples are not continuous  but every polynomial function is continuous  as are the sine  cosine  and exponential functions   Care should be exercised in using the word continuous  so that it is clear from the context which meaning of the word is intended     Using mathematical notation  there are several ways to define continuous functions in each of the three senses mentioned above     Let    This subset                     D                 displaystyle D    is the domain of f  Some possible choices include     In case of the domain                     D                 displaystyle D    being defined as an open interval                      a                 displaystyle a    and                     b                 displaystyle b    do not belong to                     D                 displaystyle D     and the values of                     f                 a                          displaystyle f a     and                     f                 b                          displaystyle f b     do not matter for continuity on                     D                 displaystyle D        The function f is continuous at some point c of its domain if the limit of                     f                 x                                   displaystyle f x      as x approaches c through the domain of f   exists and is equal to                     f                 c                                   displaystyle f c      8  In mathematical notation  this is written as     Here  we have assumed that the domain of f does not have any isolated points      A neighborhood of a point c is a set that contains  at least  all points within some fixed distance of c   Intuitively  a function is continuous at a point c if the range of f over the neighborhood of c shrinks to a single point                     f                 c                          displaystyle f c     as the width of the neighborhood around c shrinks to zero   More precisely  a function f is continuous at a point c of its domain if  for any neighborhood                               N                      1                                   f                 c                                   displaystyle N_ 1  f c      there is a neighborhood                               N                      2                                   c                          displaystyle N_ 2  c     in its domain such that                     f                 x                                   N                      1                                   f                 c                                   displaystyle f x   in N_ 1  f c      whenever                     x                          N                      2                                   c                                   displaystyle x  in N_ 2  c      n    This definition only requires that the domain and the codomain are topological spaces and is thus the most general definition   It follows from this definition that a function f is automatically continuous at every isolated point of its domain   As a specific example  every real valued function on the set of integers is continuous     One can instead require that for any sequence                                        x                      n                                                           n                                      N                                               displaystyle  x_ n  _ n  in   mathbb  N       of points in the domain which converges to c  the corresponding sequence                                                                      f                                             x                                  n                                                                                                      n                                      N                                               displaystyle   left f x_ n    right _ n  in   mathbb  N       converges to                     f                 c                                   displaystyle f c       In mathematical notation                                                 x                      n                                                           n                                      N                                              D                           lim                      n                                                            x                      n                          =        c                          lim                      n                                                  f                           x                      n                                   =        f                 c                                           displaystyle   forall  x_ n  _ n  in   mathbb  N     subset D   lim _ n  to   infty  x_ n =c  Rightarrow   lim _ n  to   infty  f x_ n  =f c         n    Explicitly including the definition of the limit of a function  we obtain a self-contained definition  nGiven a function                     f                 D                R                 displaystyle f D  to R    as above and an element                               x                      0                                   displaystyle x_ 0     of the domain D  f is said to be continuous at the point                               x                      0                                   displaystyle x_ 0     when the following holds  For any number                             >        0                          displaystyle   varepsilon >0     however small  there exists some number                             >        0                 displaystyle   delta >0    such that for all x in the domain of f with                               x                      0                                          <        x        <                  x                      0                          +                                  displaystyle x_ 0 -  delta <x<x_ 0 +  delta      the value of                     f                 x                          displaystyle f x     satisfies    Alternatively written  continuity of                     f                 D                R                 displaystyle f D  to R    at                               x                      0                                  D                 displaystyle x_ 0   in D    means that for every                             >        0                          displaystyle   varepsilon >0     there exists a                             >        0                 displaystyle   delta >0    such that for all                     x                D                 displaystyle x  in D        More intuitively  we can say that if we want to get all the                     f                 x                          displaystyle f x     values to stay in some small neighborhood around                     f                                         x                          0                                                                   displaystyle f  left x_ 0   right      we simply need to choose a small enough neighborhood for the x values around                               x                      0                                            displaystyle x_ 0      If we can do that no matter how small the                     f                 x                          displaystyle f x     neighborhood is  then f is continuous at                               x                      0                                            displaystyle x_ 0      n    In modern terms  this is generalized by the definition of continuity of a function with respect to a basis for the topology  here the metric topology     Weierstrass had required that the interval                               x                      0                                          <        x        <                  x                      0                          +                         displaystyle x_ 0 -  delta <x<x_ 0 +  delta     be entirely within the domain D  but Jordan removed that restriction     In proofs and numerical analysis we often need to know how fast limits are converging  or in other words  control of the remainder  We can formalise this to a definition of continuity   nA function                     C                          0                                                   0                                           displaystyle C  0   infty    to  0   infty      is called a control function if    A function                     f                 D                R                 displaystyle f D  to R    is C-continuous at                               x                      0                                   displaystyle x_ 0     if     A function is continuous in                               x                      0                                   displaystyle x_ 0     if it is C-continuous for some control function C     This approach leads naturally to refining the notion of  continuity by restricting the set of admissible control functions  For a given set of control functions                                           C                                   displaystyle    mathcal  C      a function is                                           C                                   displaystyle    mathcal  C      n-continuous if it is                     C                 displaystyle C    n-continuous for some                     C                                      C                                            displaystyle C  in    mathcal  C       For example  the Lipschitz and Hlder continuous functions of exponent   below are defined by the set of control functions     Continuity can also be defined in terms of oscillation  a function f is continuous at a point                               x                      0                                   displaystyle x_ 0     if and only if its oscillation at that point is zero  9  in symbols                                                      f                                             x                      0                                   =        0                  displaystyle   omega _ f  x_ 0  =0     A benefit of this definition is that it quantifies discontinuity  the oscillation gives how much the function is discontinuous at a point     This definition is useful in descriptive set theory to study the set of discontinuities and continuous points  the continuous points are the intersection of the sets where the oscillation is less than                                      displaystyle   varepsilon      hence a                               G                                                         displaystyle G_   delta      set   and gives a very quick proof of one direction of the Lebesgue integrability condition  10     The oscillation is equivalent to the                                                      displaystyle   varepsilon -  delta     definition by a simple re-arrangement  and by using a limit  lim sup  lim inf  to define oscillation  if  at a given point  for a given                                                     0                                   displaystyle   varepsilon _ 0     there is no                                      displaystyle   delta     that satisfies the                                                      displaystyle   varepsilon -  delta     definition  then the oscillation is at least                                                     0                                            displaystyle   varepsilon _ 0      and conversely if for every                                      displaystyle   varepsilon     there is a desired                                               displaystyle   delta      the oscillation is 0  The oscillation definition can be naturally generalized to maps from a topological space to a metric space     Cauchy defined continuity of a function in the following intuitive terms  an infinitesimal change in the independent variable corresponds to an infinitesimal change of the dependent variable  see Cours d  analyse  page 34   Non-standard analysis is a way of making this mathematically rigorous  The real line is augmented by the addition of infinite and infinitesimal numbers to form the hyperreal numbers   In nonstandard analysis  continuity can be defined as follows      see microcontinuity    In other words  an infinitesimal increment of the independent variable always produces to an infinitesimal change of the dependent variable  giving a modern expression to Augustin-Louis Cauchy  s definition of continuity     Checking the continuity of a given function can be simplified by checking one of the above defining properties for the building blocks of the given function  It is straightforward to show that the sum of two functions  continuous on some domain  is also continuous on this domain  Given    The same holds for the product of continuous functions     Combining the above preservations of continuity and the continuity of constant functions and of the identity function                     I                 x                 =        x                 displaystyle I x =x    on                               R                         displaystyle   mathbb  R       one arrives at the continuity of all polynomial functions on                               R                         displaystyle   mathbb  R       such as    In the same way it can be shown that the reciprocal of a continuous function    This implies that  excluding the roots of                     g                          displaystyle g     the quotient of continuous functions    For example  the function  pictured     Since the function sine is continuous on all reals  the sinc function                     G                 x                 =        sin         u2061                 x                           /                x                          displaystyle G x =  sin x /x     is defined and continuous for all real                     x                0                  displaystyle x  neq 0     However  unlike the previous example  G can be extended to a continuous function on all real numbers  by defining the value                     G                 0                          displaystyle G 0     to be 1  which is the limit of                     G                 x                                   displaystyle G x      when x approaches 0  i e      Thus  by setting    the sinc-function becomes a continuous function on all real numbers  The term removable singularity is used in such cases  when  re defining values of a function to coincide with the appropriate limits make a function continuous at specific points     A more involved construction of continuous functions is the function composition  Given two continuous functions    This construction allows stating  for example  that    An example of a discontinuous function is the Heaviside step function                     H                 displaystyle H     defined by    Pick for instance                             =        1                  /                2                 displaystyle   varepsilon =1/2     Then there is no                                      displaystyle   delta     n-neighborhood around                     x        =        0                 displaystyle x=0     i e  no open interval                                                                                         displaystyle  -  delta       delta      with                             >        0                          displaystyle   delta >0     that will force all the                     H                 x                          displaystyle H x     values to be within the                                      displaystyle   varepsilon     n-neighborhood of                     H                 0                          displaystyle H 0      i e  within                              1                  /                2                         3                  /                2                          displaystyle  1/2    3/2      Intuitively we can think of this type of discontinuity as a sudden jump in function values     Similarly  the signum or sign function    is discontinuous at                     x        =        0                 displaystyle x=0    but continuous everywhere else  Yet another example  the function    is continuous everywhere apart from                     x        =        0                 displaystyle x=0        Besides plausible continuities and discontinuities like above  there are also functions with a behavior  often coined pathological  for example  Thomae  s function     is continuous at all irrational numbers and discontinuous at all rational numbers  In a similar vein  Dirichlet  s function  the indicator function for the set of rational numbers     is nowhere continuous     Let                     f                 x                          displaystyle f x     be a function that is continuous at a point                               x                      0                                            displaystyle x_ 0      and                               y                      0                                   displaystyle y_ 0     be a value such                     f                                         x                          0                                                                   y                      0                                            displaystyle f  left x_ 0   right   neq y_ 0      Then                     f                 x                                   y                      0                                   displaystyle f x   neq y_ 0     throughout some neighbourhood of                               x                      0                                            displaystyle x_ 0      12     Proof  By the definition of continuity  take                             =                                                            |                                            y                                  0                                                          f                                             x                                  0                                                                           |                                      2                          >        0                 displaystyle   varepsilon =   frac  |y_ 0 -f x_ 0  |  2  >0      then there exists                             >        0                 displaystyle   delta >0    such that     The intermediate value theorem is an existence theorem  based on the real number property of completeness  and states    \"For example  if a child grows from 1 xa0m to 1 5 xa0m between the ages of two and six years  then  at some time between two and six years of age  the child s height must have been 1 25 xa0m  n\"   As a consequence  if f is continuous on                              a                 b                          displaystyle  a b     and                     f                 a                          displaystyle f a     and                     f                 b                          displaystyle f b     differ in sign  then  at some point                     c                         a                 b                                   displaystyle c  in  a b                          f                 c                          displaystyle f c     must equal zero     The extreme value theorem states that if a function f is defined on a closed interval                              a                 b                          displaystyle  a b      or any closed and bounded set  and is continuous there  then the function attains its maximum  i e  there exists                     c                         a                 b                          displaystyle c  in  a b     with                     f                 c                         f                 x                          displaystyle f c   geq f x     for all                     x                         a                 b                                   displaystyle x  in  a b      The same is true of the minimum of f  These statements are not  in general  true if the function is defined on an open interval                              a                 b                          displaystyle  a b      or any set that is not both closed and bounded   as  for example  the continuous function                     f                 x                 =                              1            x                                            displaystyle f x =   frac  1  x       defined on the open interval  0 1   does not attain a maximum  being unbounded above     Every differentiable function    is everywhere continuous  However  it is not differentiable at                     x        =        0                 displaystyle x=0     but is so everywhere else   Weierstrass  s function is also everywhere continuous but nowhere differentiable     The derivative                     f                 x                          displaystyle f x     of a differentiable function f x  need not be continuous  If f x  is continuous  f x  is said to be continuously differentiable  The set of such functions is denoted                               C                      1                                            1                 b                                            displaystyle C^ 1   1 b       More generally  the set of functions    Every continuous function    Given a sequence    A right-continuous function    A left-continuous function    Discontinuous functions may be discontinuous in a restricted way  giving rise to the concept of directional continuity  or right and left continuous functions  and semi-continuity  Roughly speaking  a function is right-continuous if no jump occurs when the limit point is approached from the right  Formally  f is said to be right-continuous at the point c if the following holds  For any number                             >        0                 displaystyle   varepsilon >0    however small  there exists some number                             >        0                 displaystyle   delta >0    such that for all x in the domain with                     c        <        x        <        c        +                                  displaystyle c<x<c+  delta      the value of                     f                 x                          displaystyle f x     will satisfy    This is the same condition as for continuous functions  except that it is required to hold for x strictly larger than c only  Requiring it instead for all x with                     c                        <        x        <        c                 displaystyle c-  delta <x<c    yields the notion of left-continuous functions  A function is continuous if and only if it is both right-continuous and left-continuous     A function f is lower semi-continuous if  roughly  any jumps that might occur only go down  but not up  That is  for any                             >        0                          displaystyle   varepsilon >0     there exists some number                             >        0                 displaystyle   delta >0    such that for all x in the domain with                               |                x                c                  |                <                                  displaystyle |x-c|<  delta      the value of                     f                 x                          displaystyle f x     satisfies    The concept of continuous real-valued functions can be generalized to functions between metric spaces  A metric space is a set X equipped with a function  called metric                                d                      X                                            displaystyle d_ X      that can be thought of as a measurement of the distance of any two elements in X  Formally  the metric is a function    The set of points at which a function between metric spaces is continuous is a                               G                                                         displaystyle G_   delta      set xa0 this follows from the                                                      displaystyle   varepsilon -  delta     definition of continuity     This notion of continuity is applied  for example  in functional analysis  A key statement in this area says that a linear operator    The concept of continuity for functions between metric spaces can be strengthened in various ways by limiting the way                                      displaystyle   delta     depends on                                      displaystyle   varepsilon     and c in the definition above  Intuitively  a function f as above is uniformly continuous if the                                      displaystyle   delta     does nnot depend on the point c  More precisely  it is required that for every real number                             >        0                 displaystyle   varepsilon >0    there exists                             >        0                 displaystyle   delta >0    such that for every                     c                 b                X                 displaystyle c b  in X    with                               d                      X                                   b                 c                 <                                  displaystyle d_ X  b c <  delta      we have that                               d                      Y                                   f                 b                          f                 c                          <                                  displaystyle d_ Y  f b  f c  <  varepsilon      Thus  any uniformly continuous function is continuous  The converse does not hold in general  but holds when the domain space X is compact  Uniformly continuous maps can be defined in the more general situation of uniform spaces  13     A function is Hlder continuous with exponent   a real number  if there is a constant K such that for all                     b                 c                X                          displaystyle b c  in X     the inequality    Another  more abstract  notion of continuity is continuity of functions between topological spaces in which there generally is no formal notion of distance  as there is in the case of metric spaces  A topological space is a set X together with a topology on X  which is a set of subsets of X satisfying a few requirements with respect to their unions and intersections that generalize the properties of the open balls in metric spaces while still allowing to talk about the neighbourhoods of a given point  The elements of a topology are called open subsets of X  with respect to the topology      A function    This is equivalent to the condition that the preimages of the closed sets  which are the complements of the open subsets  in Y are closed in X     An extreme example  if a set X is given the discrete topology  in which every subset is open   all functions    The translation in the language of neighborhoods of the                                                                         displaystyle    varepsilon    delta      n-definition of continuity leads to the following definition of the continuity at a point     This definition is equivalent to the same statement with neighborhoods restricted to open neighborhoods and can be restated in several ways by using preimages rather than images     Also  as every set that contains a neighborhood is also a neighborhood  and                               f                                  1                                   V                          displaystyle f^ -1  V     is the largest subset U of X such that                     f                 U                         V                          displaystyle f U   subseteq V     this definition may be simplified into     As an open set is a set that is a neighborhood of all its points  a function                     f                 X                Y                 displaystyle f X  to Y    is continuous at every point of X if and only if it is a continuous function     If X and Y are metric spaces  it is equivalent to consider the neighborhood system of open balls centered at x and f x  instead of all neighborhoods  This gives back the above                                                      displaystyle   varepsilon -  delta     definition of continuity in the context of metric spaces   In general topological spaces  there is no notion of nearness or distance  If however the target space is a Hausdorff space  it is still true that f is continuous at a if and only if the limit of f as x approaches a is f a   At an isolated point  every function is continuous     Given                     x                X                          displaystyle x  in X     a map                     f                 X                Y                 displaystyle f X  to Y    is continuous at                     x                 displaystyle x    if and only if whenever                                           B                                   displaystyle    mathcal  B      is a filter on                     X                 displaystyle X    that converges to                     x                 displaystyle x    in                     X                          displaystyle X     which is expressed by writing                                           B                                  x                          displaystyle    mathcal  B    to x     then necessarily                     f                                       B                                           f                 x                          displaystyle f    mathcal  B     to f x     in                     Y                          displaystyle Y      nIf                                           N                                   x                          displaystyle    mathcal  N   x     denotes the neighborhood filter at                     x                 displaystyle x    then                     f                 X                Y                 displaystyle f X  to Y    is continuous at                     x                 displaystyle x    if and only if                     f                                       N                                   x                                  f                 x                          displaystyle f    mathcal  N   x    to f x     in                     Y                          displaystyle Y     15  Moreover  this happens if and only if the prefilter                     f                                       N                                   x                                   displaystyle f    mathcal  N   x      is a filter base for the neighborhood filter of                     f                 x                          displaystyle f x     in                     Y                          displaystyle Y     15     Several equivalent definitions for a topological structure exist and thus there are several equivalent ways to define a continuous function     In several contexts  the topology of a space is conveniently  specified in terms of limit points   In many instances  this is accomplished by specifying when a point is the limit of a sequence  but for some spaces that are too large in some sense  one specifies also when a point is the limit of more general sets of points indexed by a directed set  known as nets   A function is  Heine- continuous only if it takes limits of sequences to limits of sequences   In the former case  preservation of limits is also sufficient  in the latter  a function may preserve all limits of sequences yet still fail to be continuous  and preservation of nets is a necessary and sufficient condition     In detail  a function                     f                 X                Y                 displaystyle f X  to Y    is sequentially continuous if whenever a sequence                                                      x                          n                                                          displaystyle   left x_ n   right     in X converges to a limit x  the sequence                                                      f                                                                      x                                  n                                                                                                       displaystyle   left f   left x_ n   right   right     converges to f x    Thus sequentially continuous functions \"preserve sequential limits\"   Every continuous function is sequentially continuous   If X is a first-countable space and countable choice holds  then the converse also holds  any function preserving sequential limits is continuous   In particular  if X is a metric space  sequential continuity and continuity are equivalent   For non first-countable spaces  sequential continuity might be strictly weaker than continuity   The spaces for which the two properties are equivalent are called sequential spaces   This motivates the consideration of nets instead of sequences in general topological spaces   Continuous functions preserve limits of nets  and in fact this property characterizes continuous functions     For instance  consider the case of real-valued functions of one real variable  16     Theorem xa0 xa0A function                     f                 A                          R                                  R                         displaystyle f A  subseteq   mathbb  R    to   mathbb  R      is continuous at                               x                      0                                   displaystyle x_ 0     if and only if it is sequentially continuous at that point     Proof  Assume that                     f                 A                          R                                  R                         displaystyle f A  subseteq   mathbb  R    to   mathbb  R      is continuous at                               x                      0                                   displaystyle x_ 0      in the sense of                                                      displaystyle   epsilon -  delta     continuity   Let                                                                      x                              n                                                                       n                        1                                   displaystyle   left x_ n   right _ n  geq 1     be a sequence converging at                               x                      0                                   displaystyle x_ 0      such a sequence always exists  e g                                x                      n                          =        x                         n                 displaystyle x_ n =x   forall n      since                     f                 displaystyle f    is continuous at                               x                      0                                   displaystyle x_ 0     n    In terms of the interior operator  a function                     f                 X                Y                 displaystyle f X  to Y    between topological spaces is continuous if and only if for every subset                     B                Y                          displaystyle B  subseteq Y         In terms of the closure operator                      f                 X                Y                 displaystyle f X  to Y    is continuous if and only if for every subset                     A                X                          displaystyle A  subseteq X         Instead of specifying topological spaces by their open subsets  any topology on                     X                 displaystyle X    can alternatively be determined by a closure operator or by an interior operator    nSpecifically  the map that sends a subset                     A                 displaystyle A    of a topological space                     X                 displaystyle X    to its topological closure                               cl                      X                           u2061        A                 displaystyle   operatorname  cl  _ X A    satisfies the Kuratowski closure axioms and conversely  for any closure operator                     A                cl         u2061        A                 displaystyle A  mapsto   operatorname  cl  A    there exists a unique topology                                      displaystyle   tau     on                     X                 displaystyle X     specifically                               =                 X                cl         u2061        A                 A                X                          displaystyle   tau  =   X  setminus   operatorname  cl  A A  subseteq X        such that for every subset                     A                X                          displaystyle A  subseteq X                         cl         u2061        A                 displaystyle   operatorname  cl  A    is equal to the topological closure                               cl                      X                           u2061        A                 displaystyle   operatorname  cl  _ X A    of                     A                 displaystyle A    in                              X                                                    displaystyle  X   tau       If the sets                     X                 displaystyle X    and                     Y                 displaystyle Y    are each associated with closure operators  both denoted by                     cl                 displaystyle   operatorname  cl       then a map                     f                 X                Y                 displaystyle f X  to Y    is continuous if and only if                     f                                         cl             u2061            A                                             cl         u2061                 f                 A                                   displaystyle f  left   operatorname  cl  A  right   subseteq   operatorname  cl   f A      for every subset                     A                X                          displaystyle A  subseteq X     n    Similarly  the map that sends a subset                     A                 displaystyle A    of                     X                 displaystyle X    to its topological interior                               int                      X                           u2061        A                 displaystyle   operatorname  int  _ X A    defines an interior operator and conversely  any interior operator                     A                int         u2061        A                 displaystyle A  mapsto   operatorname  int  A    induces a unique topology                                      displaystyle   tau     on                     X                 displaystyle X     specifically                               =                 int         u2061        A                 A                X                          displaystyle   tau  =     operatorname  int  A A  subseteq X        such that for every                     A                X                          displaystyle A  subseteq X                         int         u2061        A                 displaystyle   operatorname  int  A    is equal to the topological interior                               int                      X                           u2061        A                 displaystyle   operatorname  int  _ X A    of                     A                 displaystyle A    in                              X                                                    displaystyle  X   tau       If the sets                     X                 displaystyle X    and                     Y                 displaystyle Y    are each associated with interior operators  both denoted by                     int                 displaystyle   operatorname  int       then a map                     f                 X                Y                 displaystyle f X  to Y    is continuous if and only if                               f                                  1                                                           int             u2061            B                                             int         u2061                                                       f                                              1                                                   B                                                           displaystyle f^ -1   left   operatorname  int  B  right   subseteq   operatorname  int    left f^ -1  B   right     for every subset                     B                Y                          displaystyle B  subseteq Y     17     Continuity can also be characterized in terms of filters  A function                     f                 X                Y                 displaystyle f X  to Y    is continuous if and only if whenever a filter                                           B                                   displaystyle    mathcal  B      on                     X                 displaystyle X    converges in                     X                 displaystyle X    to a point                     x                X                          displaystyle x  in X     then the prefilter                     f                                       B                                            displaystyle f    mathcal  B       converges in                     Y                 displaystyle Y    to                     f                 x                                   displaystyle f x      This characterization remains true if the word \"filter\" is replaced by \"prefilter \" 15     If                     f                 X                Y                 displaystyle f X  to Y    and                     g                 Y                Z                 displaystyle g Y  to Z    are continuous  then so is the composition                     g                f                 X                Z                          displaystyle g  circ f X  to Z     If                     f                 X                Y                 displaystyle f X  to Y    is continuous and    The possible topologies on a fixed set X are partially ordered  a topology                                                     1                                   displaystyle   tau _ 1     is said to be coarser than another topology                                                     2                                   displaystyle   tau _ 2      notation                                                      1                                                                  2                                   displaystyle   tau _ 1   subseteq   tau _ 2      if every open subset with respect to                                                     1                                   displaystyle   tau _ 1     is also open with respect to                                                     2                                            displaystyle   tau _ 2      Then  the identity map    Symmetric to the concept of a continuous map is an open map  for which images of open sets are open  In fact  if an open map f has an inverse function  that inverse is continuous  and if a continuous map g has an inverse  that inverse is open  Given a bijective function f between two topological spaces  the inverse function                               f                                  1                                   displaystyle f^ -1     need not be continuous  A bijective continuous function with continuous inverse function is called a homeomorphism     If a continuous bijection has as its domain a compact space and its codomain is Hausdorff  then it is a homeomorphism     Given a function    Dually  for a function f from a set S to a topological space X  the initial topology on S is defined by designating as an open set every subset A of S such that                     A        =                  f                                  1                                   U                          displaystyle A=f^ -1  U     for some open subset U of X   If S has an existing topology  f is continuous with respect to this topology if and only if the existing topology is finer than the initial topology on S   Thus the initial topology can be characterized as the coarsest topology on S that makes f continuous   If f is injective  this topology is canonically identified with the subspace topology of S  viewed as a subset of X     A topology on a set S is uniquely determined by the class of all continuous functions                     S                X                 displaystyle S  to X    into all topological spaces X  Dually  a similar idea can be applied to maps                     X                S                          displaystyle X  to S     n    If                     f                 S                Y                 displaystyle f S  to Y    is a continuous function from some subset                     S                 displaystyle S    of a topological space                     X                 displaystyle X    then an continuous extension of                     f                 displaystyle f    to                     X                 displaystyle X    is any continuous function                     F                 X                Y                 displaystyle F X  to Y    such that                     F                 s                 =        f                 s                          displaystyle F s =f s     for every                     s                S                          displaystyle s  in S     which is a condition that often written as                     f        =        F                                            |                                            S                                            displaystyle f=F   big   vert  _ S      In words  it is any continuous function                     F                 X                Y                 displaystyle F X  to Y    that restricts to                     f                 displaystyle f    on                     S                          displaystyle S     This notion is used  for example  in the Tietze extension theorem and the HahnBanach theorem  Were                     f                 S                Y                 displaystyle f S  to Y    not continuous then it could not possibly have a continuous extension  If                     Y                 displaystyle Y    is a Hausdorff space and                     S                 displaystyle S    is a dense subset of                     X                 displaystyle X    then a continuous extension of                     f                 S                Y                 displaystyle f S  to Y    to                     X                          displaystyle X     if one exists  will be unique      Various other mathematical domains use the concept of continuity in different  but related meanings  For example  in order theory  an order-preserving function                     f                 X                Y                 displaystyle f X  to Y    between particular types of partially ordered sets X and Y is continuous if for each directed subset A of X  we have                     sup        f                 A                 =        f                 sup        A                                   displaystyle   sup f A =f   sup A      Here                             sup                         displaystyle      sup        is the supremum with respect to the orderings in X and Y  respectively  This notion of continuity is the same as topological continuity when the partially ordered sets are given the Scott topology  18  19     In category theory  a functor    A continuity space is a generalization of metric spaces and posets  20  21  which uses the concept of quantales  and that can be used to unify the notions of metric spaces and domains  22     In probability theory and related fields  a stochastic  /stokstk/  or random process is a mathematical object usually defined as a family of random variables  Stochastic processes are widely used as mathematical models of systems and phenomena that appear to vary in a random manner  Examples include the growth of a bacterial population  an electrical current fluctuating due to thermal noise  or the movement of a gas molecule  1  4  5  Stochastic processes have applications in many disciplines such as biology  6  chemistry  7  ecology  8  neuroscience  9  physics  10  image processing  signal processing  11  control theory  12  information theory  13  computer science  14  cryptography 15  and telecommunications  16  Furthermore  seemingly random changes in financial markets have motivated the extensive use of stochastic processes in finance  17  18  19     Applications and the study of phenomena have in turn inspired the proposal of new stochastic processes  Examples of such stochastic processes include the Wiener process or Brownian motion process  a  used by Louis Bachelier to study price changes on the Paris Bourse  22  and the Poisson process  used by A  K  Erlang to study the number of phone calls occurring in a certain period of time  23  These two stochastic processes are considered the most important and central in the theory of stochastic processes  1  4  24  and were discovered repeatedly and independently  both before and after Bachelier and Erlang  in different settings and countries  22  25     The term random function is also used to refer to a stochastic or random process  26  27  because a stochastic process can also be interpreted as a random element in a function space  28  29  The terms stochastic process and random process are used interchangeably  often with no specific mathematical space for the set that indexes the random variables  28  30  But often these two terms are used when the random variables are indexed by the integers or an interval of the real line  5  30  If the random variables are indexed by the Cartesian plane or some higher-dimensional Euclidean space  then the collection of random variables is usually called a random field instead  5  31  The values of a stochastic process are not always numbers and can be vectors or other mathematical objects  5  29     Based on their mathematical properties  stochastic processes can be grouped into various categories  which include random walks  32  martingales  33  Markov processes  34  Lvy processes  35  Gaussian processes  36  random fields  37  renewal processes  and branching processes  38  The study of stochastic processes uses mathematical knowledge and techniques from probability  calculus  linear algebra  set theory  and topology 39  40  41  as well as branches of mathematical analysis such as real analysis  measure theory  Fourier analysis  and functional analysis  42  43  44  The theory of stochastic processes is considered to be an important contribution to mathematics 45  and it continues to be an active topic of research for both theoretical reasons and applications  46  47  48     A stochastic or random process can be defined as a collection of random variables that is indexed by some mathematical set  meaning that each random variable of the stochastic process is uniquely associated with an element in the set  4  5  The set used to index the random variables is called the index set  Historically  the index set was some subset of the real line  such as the natural numbers  giving the index set the interpretation of time  1  Each random variable in the collection takes values from the same mathematical space known as the state space  This state space can be  for example  the integers  the real line or                     n                 displaystyle n    n-dimensional Euclidean space  1  5  An increment is the amount that a stochastic process changes between two index values  often interpreted as two points in time  49  50  A stochastic process can have many outcomes  due to its randomness  and a single outcome of a stochastic process is called  among other names  a sample function or realization  29  51     A stochastic process can be classified in different ways  for example  by its state space  its index set  or the dependence among the random variables  One common way of classification is by the cardinality of the index set and the state space  52  53  54     When interpreted as time  if the index set of a stochastic process has a finite or countable number of elements  such as a finite set of numbers  the set of integers  or the natural numbers  then the stochastic process is said to be in discrete time  55  56  If the index set is some interval of the real line  then time is said to be continuous  The two types of stochastic processes are respectively referred to as discrete-time and continuous-time stochastic processes  49  57  58  Discrete-time stochastic processes are considered easier to study because continuous-time processes require more advanced mathematical techniques and knowledge  particularly due to the index set being uncountable  59  60  If the index set is the integers  or some subset of them  then the stochastic process can also be called a random sequence  56     If the state space is the integers or natural numbers  then the stochastic process is called a discrete or integer-valued stochastic process  If the state space is the real line  then the stochastic process is referred to as a real-valued stochastic process or a process with continuous state space  If the state space is                     n                 displaystyle n    n-dimensional Euclidean space  then the stochastic process is called a                     n                 displaystyle n    n-dimensional vector process or                     n                 displaystyle n    n-vector process  52  53     The word stochastic in English was originally used as an adjective with the definition \"pertaining to conjecturing\"  and stemming from a Greek word meaning \"to aim at a mark  guess\"  and the Oxford English Dictionary gives the year 1662 as its earliest occurrence  61  In his work on probability Ars Conjectandi  originally published in Latin in 1713  Jakob Bernoulli used the phrase \"Ars Conjectandi sive Stochastice\"  which has been translated to \"the art of conjecturing or stochastics\"  62  This phrase was used  with reference to Bernoulli  by Ladislaus Bortkiewicz 63  who in 1917 wrote in German the word stochastik with a sense meaning random  The term stochastic process first appeared in English in a 1934 paper by Joseph Doob  61  For the term and a specific mathematical definition  Doob cited another 1934 paper  where the term stochastischer Proze was used in German by Aleksandr Khinchin  64  65  though the German term had been used earlier  for example  by Andrei Kolmogorov in 1931  66     According to the Oxford English Dictionary  early occurrences of the word random in English with its current meaning  which relates to chance or luck  date back to the 16th century  while earlier recorded usages started in the 14th century as a noun meaning \"impetuosity  great speed  force  or violence  in riding  running  striking  etc  \"  The word itself comes from a Middle French word meaning \"speed  haste\"  and it is probably derived from a French verb meaning \"to run\" or \"to gallop\"  The first written appearance of the term random process pre-dates stochastic process  which the Oxford English Dictionary also gives as a synonym  and was used in an article by Francis Edgeworth published in 1888  67     The definition of a stochastic process varies  68  but a stochastic process is traditionally defined as a collection of random variables indexed by some set  69  70  The terms random process and stochastic process are considered synonyms and are used interchangeably  without the index set being precisely specified  28  30  31  71  72  73  Both \"collection\"  29  71  or \"family\" are used 4  74  while instead of \"index set\"  sometimes the terms \"parameter set\" 29  or \"parameter space\" 31  are used     The term random function is also used to refer to a stochastic or random process  5  75  76  though sometimes it is only used when the stochastic process takes real values  29  74  This term is also used when the index sets are mathematical spaces other than the real line  5  77  while the terms stochastic process and random process are usually used when the index set is interpreted as time  5  77  78  and other terms are used such as random field when the index set is                     n                 displaystyle n    n-dimensional Euclidean space                                           R                                n                                   displaystyle   mathbb  R  ^ n     or a manifold  5  29  31     A stochastic process can be denoted  among other ways  by                              X                 t                                                  t                        T                                   displaystyle    X t    _ t  in T      57                                         X                      t                                                           t                        T                                   displaystyle    X_ t    _ t  in T      70                                         X                      t                                            displaystyle    X_ t        79                               X                 t                                   displaystyle    X t        or simply as                     X                 displaystyle X    or                     X                 t                          displaystyle X t      although                     X                 t                          displaystyle X t     is regarded as an abuse of function notation  80  For example                      X                 t                          displaystyle X t     or                               X                      t                                   displaystyle X_ t     are used to refer to the random variable with the index                     t                 displaystyle t     and not the entire stochastic process  79  If the index set is                     T        =                 0                                           displaystyle T= 0   infty       then one can write  for example                                         X                      t                                   t                0                          displaystyle  X_ t  t  geq 0     to denote the stochastic process  30     One of the simplest stochastic processes is the Bernoulli process  81  which is a sequence of independent and identically distributed  iid  random variables  where each random variable takes either the value one or zero  say one with probability                     p                 displaystyle p    and zero with probability                     1                p                 displaystyle 1-p     This process can be linked to repeatedly flipping a coin  where the probability of obtaining a head is                     p                 displaystyle p    and its value is one  while the value of a tail is zero  82  In other words  a Bernoulli process is a sequence of iid Bernoulli random variables  83  where each coin flip is an example of a Bernoulli trial  84     Random walks are stochastic processes that are usually defined as sums of iid random variables or random vectors in Euclidean space  so they are processes that change in discrete time  85  86  87  88  89  But some also use the term to refer to processes that change in continuous time  90  particularly the Wiener process used in finance  which has led to some confusion  resulting in its criticism  91  There are other various types of random walks  defined so their state spaces can be other mathematical objects  such as lattices and groups  and in general they are highly studied and have many applications in different disciplines  90  92     A classic example of a random walk is known as the simple random walk  which is a stochastic process in discrete time with the integers as the state space  and is based on a Bernoulli process  where each Bernoulli variable takes either the value positive one or negative one  In other words  the simple random walk takes place on the integers  and its value increases by one with probability  say                      p                 displaystyle p     or decreases by one with probability                     1                p                 displaystyle 1-p     so the index set of this random walk is the natural numbers  while its state space is the integers  If the                     p        =        0 5                 displaystyle p=0 5     this random walk is called a symmetric random walk  93  94     The Wiener process is a stochastic process with stationary and independent increments that are normally distributed based on the size of the increments  2  95  The Wiener process is named after Norbert Wiener  who proved its mathematical existence  but the process is also called the Brownian motion process or just Brownian motion due to its historical connection as a model for Brownian movement in liquids  96  97  98     Playing a central role in the theory of probability  the Wiener process is often considered the most important and studied stochastic process  with connections to other stochastic processes  1  2  3  99  100  101  102  Its index set and state space are the non-negative numbers and real numbers  respectively  so it has both continuous index set and states space  103  But the process can be defined more generally so its state space can be                     n                 displaystyle n    n-dimensional Euclidean space  92  100  104  If the mean of any increment is zero  then the resulting Wiener or Brownian motion process is said to have zero drift  If the mean of the increment for any two points in time is equal to the time difference multiplied by some constant                                      displaystyle   mu      which is a real number  then the resulting stochastic process is said to have drift                                      displaystyle   mu      105  106  107     Almost surely  a sample path of a Wiener process is continuous everywhere but nowhere differentiable  It can be considered as a continuous version of the simple random walk  50  106  The process arises as the mathematical limit of other stochastic processes such as certain random walks rescaled  108  109  which is the subject of Donsker  s theorem or invariance principle  also known as the functional central limit theorem  110  111  112     The Wiener process is a member of some important families of stochastic processes  including Markov processes  Lvy processes and Gaussian processes  2  50  The process also has many applications and is the main stochastic process used in stochastic calculus  113  114  It plays a central role in quantitative finance  115  116  where it is used  for example  in the BlackScholesMerton model  117  The process is also used in different fields  including the majority of natural sciences as well as some branches of social sciences  as a mathematical model for various random phenomena  3  118  119     The Poisson process is a stochastic process that has different forms and definitions  120  121  It can be defined as a counting process  which is a stochastic process that represents the random number of points or events up to some time  The number of points of the process that are located in the interval from zero to some given time is a Poisson random variable that depends on that time and some parameter  This process has the natural numbers as its state space and the non-negative numbers as its index set  This process is also called the Poisson counting process  since it can be interpreted as an example of a counting process  120     If a Poisson process is defined with a single positive constant  then the process is called a homogeneous Poisson process  120  122  The homogeneous Poisson process is a member of important classes of stochastic processes such as Markov processes and Lvy processes  50     The homogeneous Poisson process can be defined and generalized in different ways  It can be defined such that its index set is the real line  and this stochastic process is also called the stationary Poisson process  123  124  If the parameter constant of the Poisson process is replaced with some non-negative integrable function of                     t                 displaystyle t     the resulting process is called an inhomogeneous or nonhomogeneous Poisson process  where the average density of points of the process is no longer constant  125  Serving as a fundamental process in queueing theory  the Poisson process is an important process for mathematical models  where it finds applications for models of events randomly occurring in certain time windows  126  127     Defined on the real line  the Poisson process can be interpreted as a stochastic process  50  128  among other random objects  129  130  But then it can be defined on the                     n                 displaystyle n    n-dimensional Euclidean space or other mathematical spaces  131  where it is often interpreted as a random set or a random counting measure  instead of a stochastic process  129  130  In this setting  the Poisson process  also called the Poisson point process  is one of the most important objects in probability theory  both for applications and theoretical reasons  23  132  But it has been remarked that the Poisson process does not receive as much attention as it should  partly due to it often being considered just on the real line  and not on other mathematical spaces  132  133     A stochastic process is defined as a collection of random variables defined on a common probability space                                                                     F                                   P                          displaystyle    Omega     mathcal  F   P      where                                      displaystyle   Omega     is a sample space                                            F                                   displaystyle    mathcal  F      is a                                      displaystyle   sigma     n-algebra  and                     P                 displaystyle P    is a probability measure  and the random variables  indexed by some set                     T                 displaystyle T     all take values in the same mathematical space                     S                 displaystyle S     which must be measurable with respect to some                                      displaystyle   sigma     n-algebra                                      displaystyle   Sigma      29     In other words  for a given probability space                                                                     F                                   P                          displaystyle    Omega     mathcal  F   P     and a measurable space                              S                                           displaystyle  S   Sigma       a stochastic process is a collection of                     S                 displaystyle S    n-valued random variables  which can be written as  81     Historically  in many problems from the natural sciences a point                     t                T                 displaystyle t  in T    had the meaning of time  so                     X                 t                          displaystyle X t     is a random variable representing a value observed at time                     t                 displaystyle t     134  A stochastic process can also be written as                              X                 t                                           t                T                          displaystyle    X t   omega   t  in T       to reflect that it is actually a function of two variables                      t                T                 displaystyle t  in T    and                                                      displaystyle   omega   in   Omega      29  135     There are other ways to consider a stochastic process  with the above definition being considered the traditional one  69  70  For example  a stochastic process can be interpreted or defined as a                               S                      T                                   displaystyle S^ T     n-valued random variable  where                               S                      T                                   displaystyle S^ T     is the space of all the possible                     S                 displaystyle S    n-valued functions of                     t                T                 displaystyle t  in T    that map from the set                     T                 displaystyle T    into the space                     S                 displaystyle S     28  69     The set                     T                 displaystyle T    is called the index set 4  52  or parameter set 29  136  of the stochastic process  Often this set is some subset of the real line  such as the natural numbers or an interval  giving the set                     T                 displaystyle T    the interpretation of time  1  In addition to these sets  the index set                     T                 displaystyle T    can be another set with a total order or a more general set  1  55  such as the Cartesian plane                               R                      2                                   displaystyle R^ 2     or                     n                 displaystyle n    n-dimensional Euclidean space  where an element                     t                T                 displaystyle t  in T    can represent a point in space  49  137  That said  many results and theorems are only possible for stochastic processes with a totally ordered index set  138     The mathematical space                     S                 displaystyle S    of a stochastic process is called its state space  This mathematical space can be defined using integers  real lines                      n                 displaystyle n    n-dimensional Euclidean spaces  complex planes  or more abstract mathematical spaces  The state space is defined using elements that reflect the different values that the stochastic process can take  1  5  29  52  57     A sample function is a single outcome of a stochastic process  so it is formed by taking a single possible value of each random variable of the stochastic process  29  139  More precisely  if                              X                 t                                           t                T                          displaystyle    X t   omega   t  in T       is a stochastic process  then for any point                                                      displaystyle   omega   in   Omega      the mapping    is called a sample function  a realization  or  particularly when                     T                 displaystyle T    is interpreted as time  a sample path of the stochastic process                              X                 t                                           t                T                          displaystyle    X t   omega   t  in T        51  This means that for a fixed                                                      displaystyle   omega   in   Omega      there exists a sample function that maps the index set                     T                 displaystyle T    to the state space                     S                 displaystyle S     29  Other names for a sample function of a stochastic process include trajectory  path function 140  or path  141     An increment of a stochastic process is the difference between two random variables of the same stochastic process  For a stochastic process with an index set that can be interpreted as time  an increment is how much the stochastic process changes over a certain time period  For example  if                              X                 t                          t                T                          displaystyle    X t  t  in T       is a stochastic process with state space                     S                 displaystyle S    and index set                     T        =                 0                                           displaystyle T= 0   infty       then for any two non-negative numbers                               t                      1                                           0                                           displaystyle t_ 1   in  0   infty      and                               t                      2                                           0                                           displaystyle t_ 2   in  0   infty      such that                               t                      1                                            t                      2                                   displaystyle t_ 1   leq t_ 2      the difference                               X                                    t                              2                                                                      X                                    t                              1                                                             displaystyle X_ t_ 2  -X_ t_ 1      is a                     S                 displaystyle S    n-valued random variable known as an increment  49  50  When interested in the increments  often the state space                     S                 displaystyle S    is the real line or the natural numbers  but it can be                     n                 displaystyle n    n-dimensional Euclidean space or more abstract spaces such as Banach spaces  50     For a stochastic process                     X                                           S                      T                                   displaystyle X  colon   Omega   rightarrow S^ T     defined on the probability space                                                                     F                                   P                          displaystyle    Omega     mathcal  F   P      the law of stochastic process                     X                 displaystyle X    is defined as the image measure     where                     P                 displaystyle P    is a probability measure  the symbol                                      displaystyle   circ     denotes function composition and                               X                                  1                                   displaystyle X^ -1     is the pre-image of the measurable function or  equivalently  the                               S                      T                                   displaystyle S^ T     n-valued random variable                     X                 displaystyle X     where                               S                      T                                   displaystyle S^ T     is the space of all the possible                     S                 displaystyle S    n-valued functions of                     t                T                 displaystyle t  in T     so the law of a stochastic process is a probability measure  28  69  142  143     For a measurable subset                     B                 displaystyle B    of                               S                      T                                   displaystyle S^ T      the pre-image of                     X                 displaystyle X    gives    so the law of a                     X                 displaystyle X    can be written as  29     The law of a stochastic process or a random variable is also called the probability law  probability distribution  or the distribution  134  142  144  145  146     For a stochastic process                     X                 displaystyle X    with law                                      displaystyle   mu      its finite-dimensional distribution for                               t                      1                                                              t                      n                                  T                 displaystyle t_ 1    dots  t_ n   in T    is defined as     This measure                                                                   t                              1                                                                                                        t                              n                                                             displaystyle   mu _ t_ 1     t_ n      nis the joint distribution of the random vector                              X                                       t                          1                                                                         X                                       t                          n                                                                 displaystyle  X  t_ 1      dots  X  t_ n         it can be viewed as a \"projection\" of the law                                      displaystyle   mu     onto a finite subset of                     T                 displaystyle T     28  147     For any measurable subset                     C                 displaystyle C    of the                     n                 displaystyle n    n-fold Cartesian power                               S                      n                          =        S                                S                 displaystyle S^ n =S  times   dots   times S     the finite-dimensional distributions of a stochastic process                     X                 displaystyle X    can be written as  29     The finite-dimensional distributions of a stochastic process satisfy two mathematical conditions known as consistency conditions  58     Stationarity is a mathematical property that a stochastic process has when all the random variables of that stochastic process are identically distributed  In other words  if                     X                 displaystyle X    is a stationary stochastic process  then for any                     t                T                 displaystyle t  in T    the random variable                               X                      t                                   displaystyle X_ t     has the same distribution  which means that for any set of                     n                 displaystyle n    index set values                               t                      1                                                              t                      n                                   displaystyle t_ 1    dots  t_ n      the corresponding                     n                 displaystyle n    random variables    all have the same probability distribution  The index set of a stationary stochastic process is usually interpreted as time  so it can be the integers or the real line  148  149  But the concept of stationarity also exists for point processes and random fields  where the index set is not interpreted as time  148  150  151     When the index set                     T                 displaystyle T    can be interpreted as time  a stochastic process is said to be stationary if its finite-dimensional distributions are invariant under translations of time  This type of stochastic process can be used to describe a physical system that is in steady state  but still experiences random fluctuations  148  The intuition behind stationarity is that as time passes the distribution of the stationary stochastic process remains the same  152  A sequence of random variables forms a stationary stochastic process only if the random variables are identically distributed  148     A stochastic process with the above definition of stationarity is sometimes said to be strictly stationary  but there are other forms of stationarity  One example is when a discrete-time or continuous-time stochastic process                     X                 displaystyle X    is said to be stationary in the wide sense  then the process                     X                 displaystyle X    has a finite second moment for all                     t                T                 displaystyle t  in T    and the covariance of the two random variables                               X                      t                                   displaystyle X_ t     and                               X                      t            +            h                                   displaystyle X_ t+h     depends only on the number                     h                 displaystyle h    for all                     t                T                 displaystyle t  in T     152  153  Khinchin introduced the related concept of stationarity in the wide sense  which has other names including covariance stationarity or stationarity in the broad sense  153  154     A filtration is an increasing sequence of sigma-algebras defined in relation to some probability space and an index set that has some total order relation  such as in the case of the index set being some subset of the real numbers  More formally  if a stochastic process has an index set with a total order  then a filtration                                                                  F                                            t                                                           t                        T                                   displaystyle       mathcal  F  _ t    _ t  in T      on a probability space                                                                     F                                   P                          displaystyle    Omega     mathcal  F   P     is a family of sigma-algebras such that                                                         F                                            s                                                                      F                                            t                                                        F                                   displaystyle    mathcal  F  _ s   subseteq    mathcal  F  _ t   subseteq    mathcal  F      for all                     s                t                 displaystyle s  leq t     where                     t                 s                T                 displaystyle t s  in T    and                                      displaystyle   leq     denotes the total order of the index set                     T                 displaystyle T     52  With the concept of a filtration  it is possible to study the amount of information contained in a stochastic process                               X                      t                                   displaystyle X_ t     at                     t                T                 displaystyle t  in T     which can be interpreted as time                     t                 displaystyle t     52  155  The intuition behind a filtration                                                         F                                            t                                   displaystyle    mathcal  F  _ t     is that as time                     t                 displaystyle t    passes  more and more information on                               X                      t                                   displaystyle X_ t     is known or available  which is captured in                                                         F                                            t                                   displaystyle    mathcal  F  _ t      resulting in finer and finer partitions of                                      displaystyle   Omega      156  157     A modification of a stochastic process is another stochastic process  which is closely related to the original stochastic process  More precisely  a stochastic process                     X                 displaystyle X    that has the same index set                     T                 displaystyle T     set space                     S                 displaystyle S     and probability space                                                                     F                                   P                          displaystyle    Omega     cal  F   P     as another stochastic process                     Y                 displaystyle Y    is said to be a modification of                     Y                 displaystyle Y    if for all                     t                T                 displaystyle t  in T    the following    holds  Two stochastic processes that are modifications of each other have the same finite-dimensional law 158  and they are said to be stochastically equivalent or equivalent  159     Instead of modification  the term version is also used  150  160  161  162  however some authors use the term version when two stochastic processes have the same finite-dimensional distributions  but they may be defined on different probability spaces  so two processes that are modifications of each other  are also versions of each other  in the latter sense  but not the converse  163  142     If a continuous-time real-valued stochastic process meets certain moment conditions on its increments  then the Kolmogorov continuity theorem says that there exists a modification of this process that has continuous sample paths with probability one  so the stochastic process has a continuous modification or version  161  162  164  The theorem can also be generalized to random fields so the index set is                     n                 displaystyle n    n-dimensional Euclidean space 165  as well as to stochastic processes with metric spaces as their state spaces  166     Two stochastic processes                     X                 displaystyle X    and                     Y                 displaystyle Y    defined on the same probability space                                                                     F                                   P                          displaystyle    Omega     mathcal  F   P     with the same index set                     T                 displaystyle T    and set space                     S                 displaystyle S    are said be indistinguishable if the following    holds  142  158  If two                     X                 displaystyle X    and                     Y                 displaystyle Y    are modifications of each other and are almost surely continuous  then                     X                 displaystyle X    and                     Y                 displaystyle Y    are indistinguishable  167     Separability is a property of a stochastic process based on its index set in relation to the probability measure  The property is assumed so that functionals of stochastic processes or random fields with uncountable index sets can form random variables  For a stochastic process to be separable  in addition to other conditions  its index set must be a separable space  b  which means that the index set has a dense countable subset  150  168     More precisely  a real-valued continuous-time stochastic process                     X                 displaystyle X    with a probability space                                                                     F                                   P                          displaystyle    Omega     cal  F   P     is separable if its index set                     T                 displaystyle T    has a dense countable subset                     U                T                 displaystyle U  subset T    and there is a set                                                     0                                                   displaystyle   Omega _ 0   subset   Omega     of probability zero  so                     P                                                 0                                   =        0                 displaystyle P   Omega _ 0  =0     such that for every open set                     G                T                 displaystyle G  subset T    and every closed set                     F                          R          =                                                                                        displaystyle F  subset   textstyle R= -  infty    infty       the two events                                        X                      t                                  F                   xa0for all xa0                t                G                U                          displaystyle    X_ t   in F   text  for all   t  in G  cap U       and                                        X                      t                                  F                   xa0for all xa0                t                G                          displaystyle    X_ t   in F   text  for all   t  in G       differ from each other at most on a subset of                                                     0                                   displaystyle   Omega _ 0      169  170  171  nThe definition of separability c  can also be stated for other index sets and state spaces  174  such as in the case of random fields  where the index set as well as the state space can be                     n                 displaystyle n    n-dimensional Euclidean space  31  150     The concept of separability of a stochastic process was introduced by Joseph Doob   168  The underlying idea of separability is to make a countable set of points of the index set determine the properties of the stochastic process  172  Any stochastic process with a countable index set already meets the separability conditions  so discrete-time stochastic processes are always separable  175  A theorem by Doob  sometimes known as Doob  s separability theorem  says that any real-valued continuous-time stochastic process has a separable modification  168  170  176  Versions of this theorem also exist for more general stochastic processes with index sets and state spaces other than the real line  136     Two stochastic processes                     X                 displaystyle X    and                     Y                 displaystyle Y    defined on the same probability space                                                                     F                                   P                          displaystyle    Omega     mathcal  F   P     with the same index set                     T                 displaystyle T    are said be independent if for all                     n                          N                         displaystyle  in   mathbb  N      and for every choice of epochs                               t                      1                                                              t                      n                                  T                 displaystyle t_ 1    ldots  t_ n   in T     the random vectors                                                      X                                       t                              1                                                                                         X                                       t                              n                                                                                     displaystyle   left X t_ 1     ldots  X t_ n    right     and                                                      Y                                       t                              1                                                                                         Y                                       t                              n                                                                                     displaystyle   left Y t_ 1     ldots  Y t_ n    right     are independent  177  p  515    Two stochastic processes                                                      X                          t                                                          displaystyle   left   X_ t   right       and                                                      Y                          t                                                          displaystyle   left   Y_ t   right       are called uncorrelated if their cross-covariance                               K                                    X                                      Y                                       u2061                           t                      1                                             t                      2                                   =        E         u2061                                                                                      X                                                   t                                      1                                                                                                                                           X                                                                                     t                                      1                                                                                                                                                     Y                                                   t                                      2                                                                                                                                           Y                                                                                     t                                      2                                                                                                                                          displaystyle   operatorname  K  _   mathbf  X    mathbf  Y    t_ 1  t_ 2  =  operatorname  E    left   left X t_ 1  -  mu _ X  t_ 1    right   left Y t_ 2  -  mu _ Y  t_ 2    right   right     is zero for all times  178  p  142 Formally     If two stochastic processes                     X                 displaystyle X    and                     Y                 displaystyle Y    are independent  then they are also uncorrelated  178  p  151    Two stochastic processes                                                      X                          t                                                          displaystyle   left   X_ t   right       and                                                      Y                          t                                                          displaystyle   left   Y_ t   right       are called orthogonal if their cross-correlation                               R                                    X                                      Y                                       u2061                           t                      1                                             t                      2                                   =        E         u2061                 X                           t                      1                                                                       Y                                             t                                  2                                                                                                                 displaystyle   operatorname  R  _   mathbf  X    mathbf  Y    t_ 1  t_ 2  =  operatorname  E   X t_ 1     overline  Y t_ 2         is zero for all times  178  p  142 Formally     A Skorokhod space  also written as Skorohod space  is a mathematical space of all the functions that are right-continuous with left limits  defined on some interval of the real line such as                              0                 1                          displaystyle  0 1     or                              0                                           displaystyle  0   infty       and take values on the real line or on some metric space  179  180  181  Such functions are known as cdlg or cadlag functions  based on the acronym of the French phrase continue  droite  limite  gauche  179  182  A Skorokhod function space  introduced by Anatoliy Skorokhod  181  is often denoted with the letter                     D                 displaystyle D     179  180  181  182  so the function space is also referred to as space                     D                 displaystyle D     179  183  184  The notation of this function space can also include the interval on which all the cdlg functions are defined  so  for example                      D                 0                 1                          displaystyle D 0 1     denotes the space of cdlg functions defined on the unit interval                              0                 1                          displaystyle  0 1      182  184  185     Skorokhod function spaces are frequently used in the theory of stochastic processes because it often assumed that the sample functions of continuous-time stochastic processes belong to a Skorokhod space  181  183  Such spaces contain continuous functions  which correspond to sample functions of the Wiener process  But the space also has functions with discontinuities  which means that the sample functions of stochastic processes with jumps  such as the Poisson process  on the real line   are also members of this space  184  186     In the context of mathematical construction of stochastic processes  the term regularity is used when discussing and assuming certain conditions for a stochastic process to resolve possible construction issues  187  188  For example  to study stochastic processes with uncountable index sets  it is assumed that the stochastic process adheres to some type of regularity condition such as the sample functions being continuous  189  190     Markov processes are stochastic processes  traditionally in discrete or continuous time  that have the Markov property  which means the next value of the Markov process depends on the current value  but it is conditionally independent of the previous values of the stochastic process  In other words  the behavior of the process in the future is stochastically independent of its behavior in the past  given the current state of the process  191  192     The Brownian motion process and the Poisson process  in one dimension  are both examples of Markov processes 193  in continuous time  while random walks on the integers and the gambler  s ruin problem are examples of Markov processes in discrete time  194  195     A Markov chain is a type of Markov process that has either discrete state space or discrete index set  often representing time   but the precise definition of a Markov chain varies  196  For example  it is common to define a Markov chain as a Markov process in either discrete or continuous time with a countable state space  thus regardless of the nature of time   197  198  199  200  but it has been also common to define a Markov chain as having discrete time in either countable or continuous state space  thus regardless of the state space   196  It has been argued that the first definition of a Markov chain  where it has discrete time  now tends to be used  despite the second definition having been used by researchers like Joseph Doob and Kai Lai Chung  201     Markov processes form an important class of stochastic processes and have applications in many areas  40  202  For example  they are the basis for a general stochastic simulation method known as Markov chain Monte Carlo  which is used for simulating random objects with specific probability distributions  and has found application in Bayesian statistics  203  204     The concept of the Markov property was originally for stochastic processes in continuous and discrete time  but the property has been adapted for other index sets such as                     n                 displaystyle n    n-dimensional Euclidean space  which results in collections of random variables known as Markov random fields  205  206  207     A martingale is a discrete-time or continuous-time stochastic process with the property that  at every instant  given the current value and all the past values of the process  the conditional expectation of every future value is equal to the current value  In discrete time  if this property holds for the next value  then it holds for all future values  The exact mathematical definition of a martingale requires two other conditions coupled with the mathematical concept of a filtration  which is related to the intuition of increasing available information as time passes  Martingales are usually defined to be real-valued  208  209  155  but they can also be complex-valued 210  or even more general  211     A symmetric random walk and a Wiener process  with zero drift  are both examples of martingales  respectively  in discrete and continuous time  208  209  For a sequence of independent and identically distributed random variables                               X                      1                                             X                      2                                             X                      3                                                    displaystyle X_ 1  X_ 2  X_ 3    dots     with zero mean  the stochastic process formed from the successive partial sums                               X                      1                                             X                      1                          +                  X                      2                                             X                      1                          +                  X                      2                          +                  X                      3                                                    displaystyle X_ 1  X_ 1 +X_ 2  X_ 1 +X_ 2 +X_ 3    dots     is a discrete-time martingale  212  In this aspect  discrete-time martingales generalize the idea of partial sums of independent random variables  213     Martingales can also be created from stochastic processes by applying some suitable transformations  which is the case for the homogeneous Poisson process  on the real line  resulting in a martingale called the compensated Poisson process  209  Martingales can also be built from other martingales  212  For example  there are martingales based on the martingale the Wiener process  forming continuous-time martingales  208  214     Martingales mathematically formalize the idea of a fair game  215  and they were originally developed to show that it is not possible to win a fair game  216  But now they are used in many areas of probability  which is one of the main reasons for studying them  155  216  217  Many problems in probability have been solved by finding a martingale in the problem and studying it  218  Martingales will converge  given some conditions on their moments  so they are often used to derive convergence results  due largely to martingale convergence theorems  213  219  220     Martingales have many applications in statistics  but it has been remarked that its use and application are not as widespread as it could be in the field of statistics  particularly statistical inference  221  They have found applications in areas in probability theory such as queueing theory and Palm calculus 222  and other fields such as economics 223  and finance  18     Lvy processes are types of stochastic processes that can be considered as generalizations of random walks in continuous time  50  224  These processes have many applications in fields such as finance  fluid mechanics  physics and biology  225  226  The main defining characteristics of these processes are their stationarity and independence properties  so they were known as processes with stationary and independent increments  In other words  a stochastic process                     X                 displaystyle X    is a Lvy process if for                     n                 displaystyle n    non-negatives numbers                      0                          t                      1                                                            t                      n                                   displaystyle 0  leq t_ 1   leq   dots   leq t_ n      the corresponding                     n                1                 displaystyle n-1    increments    are all independent of each other  and the distribution of each increment only depends on the difference in time  50     A Lvy process can be defined such that its state space is some abstract mathematical space  such as a Banach space  but the processes are often defined so that they take values in Euclidean space  The index set is the non-negative numbers  so                     I        =                 0                                           displaystyle I= 0   infty       which gives the interpretation of time  Important stochastic processes such as the Wiener process  the homogeneous Poisson process  in one dimension   and subordinators are all Lvy processes  50  224     A random field is a collection of random variables indexed by a                     n                 displaystyle n    n-dimensional Euclidean space or some manifold  In general  a random field can be considered an example of a stochastic or random process  where the index set is not necessarily a subset of the real line  31  But there is a convention that an indexed collection of random variables is called a random field when the index has two or more dimensions  5  29  227  If the specific definition of a stochastic process requires the index set to be a subset of the real line  then the random field can be considered as a generalization of stochastic process  228     A point process is a collection of points randomly located on some mathematical space such as the real line                      n                 displaystyle n    n-dimensional Euclidean space  or more abstract spaces  Sometimes the term point process is not preferred  as historically the word process denoted an evolution of some system in time  so a point process is also called a random point field  229  There are different interpretations of a point process  such a random counting measure or a random set  230  231  Some authors regard a point process and stochastic process as two different objects such that a point process is a random object that arises from or is associated with a stochastic process  232  233  though it has been remarked that the difference between point processes and stochastic processes is not clear  233     Other authors consider a point process as a stochastic process  where the process is indexed by sets of the underlying space d  on which it is defined  such as the real line or                     n                 displaystyle n    n-dimensional Euclidean space  236  237  Other stochastic processes such as renewal and counting processes are studied in the theory of point processes  238  233     Probability theory has its origins in games of chance  which have a long history  with some games being played thousands of years ago  239  240  but very little analysis on them was done in terms of probability  239  241  The year 1654 is often considered the birth of probability theory when French mathematicians Pierre Fermat and Blaise Pascal had a written correspondence on probability  motivated by a gambling problem  239  242  243  But there was earlier mathematical work done on the probability of gambling games such as Liber de Ludo Aleae by Gerolamo Cardano  written in the 16th century but posthumously published later in 1663  239  244     After Cardano  Jakob Bernoulli e  wrote Ars Conjectandi  which is considered a significant event in the history of probability theory  239  Bernoulli  s book was published  also posthumously  in 1713 and inspired many mathematicians to study probability  239  246  247  But despite some renowned mathematicians contributing to probability theory  such as Pierre-Simon Laplace  Abraham de Moivre  Carl Gauss  Simon Poisson and Pafnuty Chebyshev  248  249  most of the mathematical community f  did not consider probability theory to be part of mathematics until the 20th century  248  250  251  252     In the physical sciences  scientists developed in the 19th century the discipline of statistical mechanics  where physical systems  such as containers filled with gases  can be regarded or treated mathematically as collections of many moving particles  Although there were attempts to incorporate randomness into statistical physics by some scientists  such as Rudolf Clausius  most of the work had little or no randomness  253  254  nThis changed in 1859 when James Clerk Maxwell contributed significantly to the field  more specifically  to the kinetic theory of gases  by presenting work where he assumed the gas particles move in random directions at random velocities  255  256  The kinetic theory of gases and statistical physics continued to be developed in the second half of the 19th century  with work done chiefly by Clausius  Ludwig Boltzmann and Josiah Gibbs  which would later have an influence on Albert Einstein  s mathematical model for Brownian movement  257     At the International Congress of Mathematicians in Paris in 1900  David Hilbert presented a list of mathematical problems  where his sixth problem asked for a mathematical treatment of physics and probability involving axioms  249  Around the start of the 20th century  mathematicians developed measure theory  a branch of mathematics for studying integrals of mathematical functions  where two of the founders were French mathematicians  Henri Lebesgue and mile Borel  In 1925 another French mathematician Paul Lvy published the first probability book that used ideas from measure theory  249     In 1920s fundamental contributions to probability theory were made in the Soviet Union by mathematicians such as Sergei Bernstein  Aleksandr Khinchin  g  and Andrei Kolmogorov  252  Kolmogorov published in 1929 his first attempt at presenting a mathematical foundation  based on measure theory  for probability theory  258  In the early 1930s Khinchin and Kolmogorov set up probability seminars  which were attended by researchers such as Eugene Slutsky and Nikolai Smirnov  259  and Khinchin gave the first mathematical definition of a stochastic process as a set of random variables indexed by the real line  64  260  h     In 1933 Andrei Kolmogorov published in German  his book on the foundations of probability theory titled Grundbegriffe der Wahrscheinlichkeitsrechnung  i  where Kolmogorov used measure theory to develop an axiomatic framework for probability theory  The publication of this book is now widely considered to be the birth of modern probability theory  when the theories of probability and stochastic processes became parts of mathematics  249  252     After the publication of Kolmogorov  s book  further fundamental work on probability theory and stochastic processes was done by Khinchin and Kolmogorov as well as other mathematicians such as Joseph Doob  William Feller  Maurice Frchet  Paul Lvy  Wolfgang Doeblin  and Harald Cramr  249  252  nDecades later Cramr referred to the 1930s as the \"heroic period of mathematical probability theory\"  252  World War II greatly interrupted the development of probability theory  causing  for example  the migration of Feller from Sweden to the United States of America 252  and the death of Doeblin  considered now a pioneer in stochastic processes  262     After World War II the study of probability theory and stochastic processes gained more attention from mathematicians  with significant contributions made in many areas of probability and mathematics as well as the creation of new areas  252  265  Starting in the 1940s  Kiyosi It published papers developing the field of stochastic calculus  which involves stochastic integrals and stochastic differential equations based on the Wiener or Brownian motion process  266     Also starting in the 1940s  connections were made between stochastic processes  particularly martingales  and the mathematical field of potential theory  with early ideas by Shizuo Kakutani and then later work by Joseph Doob  265  Further work  considered pioneering  was done by Gilbert Hunt in the 1950s  connecting Markov processes and potential theory  which had a significant effect on the theory of Lvy processes and led to more interest in studying Markov processes with methods developed by It  22  267  268     In 1953 Doob published his book Stochastic processes  which had a strong influence on the theory of stochastic processes and stressed the importance of measure theory in probability  265  264  Doob also chiefly developed the theory of martingales  with later substantial contributions by Paul-Andr Meyer  Earlier work had been carried out by Sergei Bernstein  Paul Lvy and Jean Ville  the latter adopting the term martingale for the stochastic process  269  270  Methods from the theory of martingales became popular for solving various probability problems  Techniques and theory were developed to study Markov processes and then applied to martingales  Conversely  methods from the theory of martingales were established to treat Markov processes  265     Other fields of probability were developed and used to study stochastic processes  with one main approach being the theory of large deviations  265  The theory has many applications in statistical physics  among other fields  and has core ideas going back to at least the 1930s  Later in the 1960s and 1970s fundamental work was done by Alexander Wentzell in the Soviet Union and Monroe D  Donsker and Srinivasa Varadhan in the United States of America  271  which would later result in Varadhan winning the 2007 Abel Prize  272  In the 1990s and 2000s the theories of SchrammLoewner evolution 273  and rough paths 142  were introduced and developed to study stochastic processes and other mathematical objects in probability theory  which respectively resulted in Fields Medals being awarded to Wendelin Werner 274  in 2008 and to Martin Hairer in 2014  275     The theory of stochastic processes still continues to be a focus of research  with yearly international conferences on the topic of stochastic processes  46  225     Although Khinchin gave mathematical definitions of stochastic processes in the 1930s  64  260  specific stochastic processes had already been discovered in different settings  such as the Brownian motion process and the Poisson process  22  25  Some families of stochastic processes such as point processes or renewal processes have long and complex histories  stretching back centuries  276     The Bernoulli process  which can serve as a mathematical model for flipping a biased coin  is possibly the first stochastic process to have been studied  82  The process is a sequence of independent Bernoulli trials  83  which are named after Jackob Bernoulli who used them to study games of chance  including probability problems proposed and studied earlier by Christiaan Huygens  277  Bernoulli  s work  including the Bernoulli process  were published in his book Ars Conjectandi in 1713  278     In 1905 Karl Pearson coined the term random walk while posing a problem describing a random walk on the plane  which was motivated by an application in biology  but such problems involving random walks had already been studied in other fields  Certain gambling problems that were studied centuries earlier can be considered as problems involving random walks  90  278  For example  the problem known as the Gambler  s ruin is based on a simple random walk  195  279  and is an example of a random walk with absorbing barriers  242  280  Pascal  Fermat and Huyens all gave numerical solutions to this problem without detailing their methods  281  and then more detailed solutions were presented by Jakob Bernoulli and Abraham de Moivre  282     For random walks in                     n                 displaystyle n    n-dimensional integer lattices  George Plya published in 1919 and 1921 work  where he studied the probability of a symmetric random walk returning to a previous position in the lattice  Plya showed that a symmetric random walk  which has an equal probability to advance in any direction in the lattice  will return to a previous position in the lattice an infinite number of times with probability one in one and two dimensions  but with probability zero in three or higher dimensions  283  284     The Wiener process or Brownian motion process has its origins in different fields including statistics  finance and physics  22  In 1880  Thorvald Thiele wrote a paper on the method of least squares  where he used the process to study the errors of a model in time-series analysis  285  286  287  The work is now considered as an early discovery of the statistical method known as Kalman filtering  but the work was largely overlooked  It is thought that the ideas in Thiele  s paper were too advanced to have been understood by the broader mathematical and statistical community at the time  287     The French mathematician Louis Bachelier used a Wiener process in his 1900 thesis 288  289  in order to model price changes on the Paris Bourse  a stock exchange  290  without knowing the work of Thiele  22  It has been speculated that Bachelier drew ideas from the random walk model of Jules Regnault  but Bachelier did not cite him  291  and Bachelier  s thesis is now considered pioneering in the field of financial mathematics  290  291     It is commonly thought that Bachelier  s work gained little attention and was forgotten for decades until it was rediscovered in the 1950s by the Leonard Savage  and then become more popular after Bachelier  s thesis was translated into English in 1964  But the work was never forgotten in the mathematical community  as Bachelier published a book in 1912 detailing his ideas  291  which was cited by mathematicians including Doob  Feller 291  and Kolmogorov  22  The book continued to be cited  but then starting in the 1960s the original thesis by Bachelier began to be cited more than his book when economists started citing Bachelier  s work  291     In 1905 Albert Einstein published a paper where he studied the physical observation of Brownian motion or movement to explain the seemingly random movements of particles in liquids by using ideas from the kinetic theory of gases  Einstein derived a differential equation  known as a diffusion equation  for describing the probability of finding a particle in a certain region of space  Shortly after Einstein  s first paper on Brownian movement  Marian Smoluchowski published work where he cited Einstein  but wrote that he had independently derived the equivalent results by using a different method  292     Einstein  s work  as well as experimental results obtained by Jean Perrin  later inspired Norbert Wiener in the 1920s 293  to use a type of measure theory  developed by Percy Daniell  and Fourier analysis to prove the existence of the Wiener process as a mathematical object  22     The Poisson process is named after Simon Poisson  due to its definition involving the Poisson distribution  but Poisson never studied the process  23  294  There are a number of claims for early uses or discoveries of the Poisson nprocess  23  25  nAt the beginning of the 20th century the Poisson process would arise independently in different situations  23  25  nIn Sweden 1903  Filip Lundberg published a thesis containing work  now considered fundamental and pioneering  where he proposed to model insurance claims with a homogeneous Poisson process  295  296     Another discovery occurred in Denmark in 1909 when A K  Erlang derived the Poisson distribution when developing a mathematical model for the number of incoming phone calls in a finite time interval  Erlang was not at the time aware of Poisson  s earlier work and assumed that the number phone calls arriving in each interval of time were independent to each other  He then found the limiting case  which is effectively recasting the Poisson distribution as a limit of the binomial distribution  23     In 1910 Ernest Rutherford and Hans Geiger published experimental results on counting alpha particles  Motivated by their work  Harry Bateman studied the counting problem and derived Poisson probabilities as a solution to a family of differential equations  resulting in the independent discovery of the Poisson process  23  After this time there were many studies and applications of the Poisson process  but its early history is complicated  which has been explained by the various applications of the process in numerous fields by biologists  ecologists  engineers and various physical scientists  23     Markov processes and Markov chains are named after Andrey Markov who studied Markov chains in the early 20th century  297  Markov was interested in studying an extension of independent random sequences  297  In his first paper on Markov chains  published in 1906  Markov showed that under certain conditions the average outcomes of the Markov chain would converge to a fixed vector of values  so proving a weak law of large numbers without the independence assumption  298  299  300  301  which had been commonly regarded as a requirement for such mathematical laws to hold  301  Markov later used Markov chains to study the distribution of vowels in Eugene Onegin  written by Alexander Pushkin  and proved a central limit theorem for such chains  298  299     In 1912 Poincar studied Markov chains on finite groups with an aim to study card shuffling  Other early uses of Markov chains include a diffusion model  introduced by Paul and Tatyana Ehrenfest in 1907  and a branching process  introduced by Francis Galton and Henry William Watson in 1873  preceding the work of Markov  299  300  After the work of Galton and Watson  it was later revealed that their branching process had been independently discovered and studied around three decades earlier by Irne-Jules Bienaym  302  Starting in 1928  Maurice Frchet became interested in Markov chains  eventually resulting in him publishing in 1938 a detailed study on Markov chains  299  303     Andrei Kolmogorov developed in a 1931 paper a large part of the early theory of continuous-time Markov processes  252  258  Kolmogorov was partly inspired by Louis Bachelier  s 1900 work on fluctuations in the stock market as well as Norbert Wiener  s work on Einstein  s model of Brownian movement  258  304  He introduced and studied a particular set of Markov processes known as diffusion processes  where he derived a set of differential equations describing the processes  258  305  Independent of Kolmogorov  s work  Sydney Chapman derived in a 1928 paper an equation  now called the ChapmanKolmogorov equation  in a less mathematically rigorous way than Kolmogorov  while studying Brownian movement  306  The differential equations are now called the Kolmogorov equations 307  or the KolmogorovChapman equations  308  Other mathematicians who contributed significantly to the foundations of Markov processes include William Feller  starting in the 1930s  and then later Eugene Dynkin  starting in the 1950s  252     Lvy processes such as the Wiener process and the Poisson process  on the real line  are named after Paul Lvy who started studying them in the 1930s  225  but they have connections to infinitely divisible distributions going back to the 1920s  224  In a 1932 paper Kolmogorov derived a characteristic function for random variables associated with Lvy processes  This result was later derived under more general conditions by Lvy in 1934  and then Khinchin independently gave an alternative form for this characteristic function in 1937  252  309  In addition to Lvy  Khinchin and Kolomogrov  early fundamental contributions to the theory of Lvy processes were made by Bruno de Finetti and Kiyosi It  224     In mathematics  constructions of mathematical objects are needed  which is also the case for stochastic processes  to prove that they exist mathematically  58  There are two main approaches for constructing a stochastic process  One approach involves considering a measurable space of functions  defining a suitable measurable mapping from a probability space to this measurable space of functions  and then deriving the corresponding finite-dimensional distributions  310     Another approach involves defining a collection of random variables to have specific finite-dimensional distributions  and then using Kolmogorov  s existence theorem j  to prove a corresponding stochastic process exists  58  310  This theorem  which is an existence theorem for measures on infinite product spaces  314  says that if any finite-dimensional distributions satisfy two conditions  known as consistency conditions  then there exists a stochastic process with those finite-dimensional distributions  58     When constructing continuous-time stochastic processes certain mathematical difficulties arise  due to the uncountable index sets  which do not occur with discrete-time processes  59  60  One problem is that is it possible to have more than one stochastic process with the same finite-dimensional distributions  For example  both the left-continuous modification and the right-continuous modification of a Poisson process have the same finite-dimensional distributions  315  This means that the distribution of the stochastic process does not  necessarily  specify uniquely the properties of the sample functions of the stochastic process  310  316     Another problem is that functionals of continuous-time process that rely upon an uncountable number of points of the index set may not be measurable  so the probabilities of certain events may not be well-defined  168  For example  the supremum of a stochastic process or random field is not necessarily a well-defined random variable  31  60  For a continuous-time stochastic process                     X                 displaystyle X     other characteristics that depend on an uncountable number of points of the index set                     T                 displaystyle T    include  168     To overcome these two difficulties  different assumptions and approaches are possible  70     One approach for avoiding mathematical construction issues of stochastic processes  proposed by Joseph Doob  is to assume that the stochastic process is separable  317  Separability ensures that infinite-dimensional distributions determine the properties of sample functions by requiring that sample functions are essentially determined by their values on a dense countable set of points in the index set  318  Furthermore  if a stochastic process is separable  then functionals of an uncountable number of points of the index set are measurable and their probabilities can be studied  168  318     Another approach is possible  originally developed by Anatoliy Skorokhod and Andrei Kolmogorov  319  for a continuous-time stochastic process with any metric space as its state space  For the construction of such a stochastic process  it is assumed that the sample functions of the stochastic process belong to some suitable function space  which is usually the Skorokhod space consisting of all right-continuous functions with left limits  This approach is now more used than the separability assumption  70  263  but such a stochastic process based on this approach will be automatically separable  320     Although less used  the separability assumption is considered more general because every stochastic process has a separable version  263  It is also used when it is not possible to construct a stochastic process in a Skorokhod space  173  For example  separability is assumed when constructing and studying random fields  where the collection of random variables is now indexed by sets other than the real line such as                     n                 displaystyle n    n-dimensional Euclidean space  31  321      nComputer simulation was pioneered as a scientific tool in meteorology nand nuclear physics in the period directly following World War II  and nsince then has become indispensable in a growing number of ndisciplines  The list of sciences that make extensive use of computer nsimulation has grown to include astrophysics  particle physics  nmaterials science  engineering  fluid mechanics  climate science  nevolutionary biology  ecology  economics  decision theory  medicine  nsociology  epidemiology  and many others  There are even a few ndisciplines  such as chaos theory and complexity theory  whose very nexistence has emerged alongside the development of the computational nmodels they study      nAfter a slow start  philosophers of science have begun to devote nmore attention to the role of computer simulation in nscience  Several areas of philosophical interest in ncomputer simulation have emerged  What is the structure of the nepistemology of computer simulation? What is the relationship nbetween computer simulation and experiment? Does computer nsimulation raise issues for the philosophy of science that are not nfully covered by recent work on models more generally? What does ncomputer simulation teach us about emergence? About the structure nof scientific theories? About the role  if any  of fictions in nscientific modeling?     No single definition of computer simulation is appropriate  In the nfirst place  the term is used in both a narrow and a broad sense  In nthe second place  one might want to understand the term from more than none point of view       nIn its narrowest sense  a computer simulation is a program that is nrun on a computer and that uses step-by-step methods to explore nthe approximate behavior of a mathematical model  Usually this is na model of a real-world system  although the system in question might nbe an imaginary or hypothetical one   Such a computer program is na computer simulation model  One run of the nprogram on the computer is a computer simulation of the system  The nalgorithm takes as its input a specification of the systems nstate  the value of all of its variables  at some time t   nIt then calculates the systems state at time t+1   nFrom the values characterizing that second state  it then calculates nthe systems state at time t+2  and so on  When nrun on a computer  the algorithm thus produces a numerical picture of nthe evolution of the systems state  as it is conceptualized in nthe model       nThis sequence of values for the model variables can be saved as a nlarge collection of data and is often viewed on a ncomputer screen using methods of visualization  Often  but ncertainly not always  the methods of visualization are designed to nmimic the output of some scientific instrumentso that the nsimulation appears to be measuring a system of interest      nSometimes the step-by-step methods of computer simulation are used nbecause the model of interest contains continuous  differential  nequations  which specify continuous rates of change in time  that ncannot be solved analyticallyeither in principle or perhaps only in npractice  This underwrites the spirit of the following ndefinition given by Paul Humphreys  any computer-implemented method nfor exploring the properties of mathematical models where analytic nmethods are not available  1991  500   But even as a narrow ndefinition  this one should be read carefully  and not be taken to nsuggest that simulations are only used when there are analytically nunsolvable equations in the model  Computer simulations are often nused either because the original model itself contains discrete nequationswhich can be directly implemented in an algorithm suitable nfor simulationor because the original model consists of something nbetter described as rules of evolution than as nequations      nIn the former case  when equations are being ndiscretized  the turning of equations that describe ncontinuous rates of change into discrete equations   it should be nemphasized that  although it is common to speak of simulations nsolving those equations  a discretization can at best nonly find something which approximates the solution of continuous nequations  to some desired degree of accuracy  Finally  nwhen speaking of a computer simulation in the narrowest nsense  we should be speaking of a particular implementation of the nalgorithm on a particular digital computer  written in a particular nlanguage  using a particular compiler  etc  There are cases in nwhich different results can be obtained as a result of variations in nany of these particulars      nMore broadly  we can think of computer simulation as a comprehensive nmethod for studying systems  In this broader sense of the term  it nrefers to an entire process  This process includes choosing na model  finding a way of implementing that model in a form that can be nrun on a computer  calculating the output of the algorithm  and nvisualizing and studying the resultant data  The method includes nthis entire processused to make inferences about the target nsystem that one tries to modelas well as the procedures used to nsanction those inferences  This is more or less the definition of ncomputer simulation studies in Winsberg 2003  111   nSuccessful simulation studies do more than compute numbers  They nmake use of a variety of techniques to draw inferences from these nnumbers  Simulations make creative use of calculational ntechniques that can only be motivated extra-mathematically and nextra-theoretically  As such  unlike simple computations that can be ncarried out on a computer  the results of simulations are not nautomatically reliable  Much effort and expertise goes into deciding nwhich simulation results are reliable and which are not   nWhen philosophers of science write about computer simulation  and make nclaims about what epistemological or methodological properties ncomputer simulations have  they usually mean the term to nbe understood in this broad sense of a computer simulation nstudy       nBoth of the above definitions take computer simulation to be nfundamentally about using a computer to solve  or to approximately nsolve  the mathematical equations of a model that is meant to represent nsome systemeither real or hypothetical  Another approach nis to try to define simulation independently of the nnotion of computer simulation  and then to define computer nsimulation compositionally  as a simulation that is carried out nby a programmed digital computer  On this approach  a simulation is any nsystem that is believed  or hoped  to have dynamical behavior that is nsimilar enough to some other system such that the former can be studied nto learn about the latter        nFor example  if we study some object because we believe it is nsufficiently dynamically similar to a basin of fluid for us to learn nabout basins of fluid by studying the it  then it provides a nsimulation of basins of fluid  This is in line with the definition of nsimulation we find in Hartmann  it is something that imitates none process by another process  In this definition the term nprocess refers solely to some object or system whose nstate changes in time  1996  83    Hughes  1999  objected that nHartmanns definition ruled out simulations that imitate a systems nstructure rather than its dynamics  Humphreys revised his definition nof simulation to accord with the remarks of Hartmann and Hughes as nfollows      n Note that Humphreys is here defining computer simulation  not nsimulation generally  but he is doing it in the spirit of defining a ncompositional term   It should be noted that Humphreys definitions nmake simulation out to be a success term  and that seems nunfortunate  A better definition would be one that  like the one in nthe last section  included a word like believed or nhoped to address this issue      nIn most philosophical discussions of computer simulation  the more nuseful concept is the one defined in 1 2  The exception is when it is nexplicitly the goal of the discussion to understand computer nsimulation as an example of simulation more generally  see section n5   Examples of simulations that are not computer simulations include nthe famous physical model of the San Francisco Bay  Huggins & nSchultz 1973    This is a working hydraulic scale model of the San nFrancisco Bay and Sacramento-San Joaquin River Delta System built in nthe 1950s by the Army Corps of engineers to study possible engineering ninterventions in the Bay  Another nice example  which is discussed nextensively in  Dardashti et al   2015  2019  is the use of acoustic ndumb holes made out of Bose-Einstein condensates to nstudy the behavior of Black Holes  Physicist Bill Unruh noted that in ncertain fluids  something akin to a black hole would arise if there nwere regions of the fluid that were moving so fast that waves would nhave to move faster than the speed of sound  something they cannot do  nin order to escape from them  Unruh 1981   Such regions would in neffect have sonic event horizons  Unruh called such a physical setup a ndumb hole  dumb as in mute  nand proposed that it could be studied in order to learn things we do nnot know about black holes  For some time  this proposal was viewed as nnothing more than a clever idea  but physicists have recently come to nrealize that  using Bose-Einstein condensates  they can actually build nand study dumb holes in the laboratory  It is clear why we should nthink of such a setup as a simulation  the dumb hole simulates the nblack hole  Instead of finding a computer program to simulate the nblack holes  physicists find a fluid dynamical setup for which they nbelieve they have a good model and for which that model has nfundamental mathematical similarities to the model of the systems of ninterest  They observe the behavior of the fluid setup in the nlaboratory in order to make inferences about the black holes  The npoint  then  of the definitions of simulation in this section is to ntry to understand in what sense computer simulation and these sorts of nactivities are species of the same genus  We might then be in a better nsituation to understand why a simulation in the sense of 1 3 which nhappens to be run on a computer overlaps with a simulation in the nsense of 1 2  We will come back to this in section 5      nBarberousse et al   2009   however  have been critical of this nanalogy  They point out that computer simulations do not nwork the way Unruhs simulation works  It is not the case that the ncomputer as a material object and the target system follow the same ndifferential equations  A  good reference about nsimulations that are not computer simulations is Trenholme 1994     Two types of computer simulation are often ndistinguished  equation-based simulations nand agent-based  or nindividual-based  simulations  Computer nSimulations of both types are used for three different general sorts of npurposes  prediction  both pointwise and global/qualitative   nunderstanding  and exploratory or heuristic purposes      nEquation-based simulations are most commonly used in the physical nsciences and other sciences where there is governing theory that can nguide the construction of mathematical models based on differential nequations  I use the term equation based here nto refer to simulations based on the kinds of global equations we nassociate with physical theoriesas opposed to rules of nevolution  which are discussed in the next section   Equation nbased simulations can either be particle-based  where there are many ndiscrete bodies and a set of differential equations governing their ninteraction  or they can be field-based  where there is a set of nequations governing the time evolution of a continuous medium or nfield  An example of the former is a simulation of galaxy nformation  in which the gravitational interaction between a finite ncollection of discrete bodies is discretized in time and space   nAn example of the latter is the simulation of a fluid  such as a nmeteorological system like a severe storm  Here the system is ntreated as a continuous mediuma fluidand a field representing its ndistribution of the relevant variables in space is discretized in space nand then updated in discrete intervals of time      nAgent-based simulations are most common in the social and behavioral nsciences  though we also find them in such disciplines as artificial nlife  epidemiology  ecology  and any discipline in which the networked ninteraction of many individuals is being studied   nAgent-based simulations are similar to particle-based simulations in nthat they represent the behavior of n-many discrete individuals   nBut unlike equation-particle-based simulations  there are no global ndifferential equations that govern the motions of the nindividuals  Rather  in agent-based simulations  the behavior of nthe individuals is dictated by their own local rules     nTo give one example  a famous and groundbreaking agent-based nsimulation was Thomas Schellings  1971  model of nsegregation  The agents in his simulation nwere individuals who lived on a chessboard   nThe individuals were divided into two groups in the society  e g  two ndifferent races  boys and girls  smokers and non-smokers  etc    nEach square on the board represented a house  with at most one person nper house  An individual is happy if he/she has a certain percent nof neighbors of his/her own group  Happy agents stay where they are  nunhappy agents move to free locations  Schelling found that the board nquickly evolved into a strongly segregated location pattern if the nagents happiness rules were specified so that nsegregation was heavily favored  Surprisingly  however  he also found nthat initially integrated boards tipped into full segregation even if nthe agents happiness rules expressed only a mild preference for having nneighbors of their own type      nIn section 2 1 we discussed equation-based models that are based on nparticle methods and those that are based on field methods   nBut some simulation models are hybrids of different kinds of modeling nmethods  Multiscale simulation models  in particular  ncouple together modeling elements from different scales of ndescription  A good example of this would be a model that nsimulates the dynamics of bulk matter by treating the material as a nfield undergoing stress and strain at a relatively coarse level of ndescription  but which zooms into particular regions of the material nwhere important small scale effects are taking place  and models those nsmaller regions with relatively more fine-grained modeling methods  nSuch methods might rely on molecular dynamics  or quantum mechanics  or nbotheach of which is a more fine-grained description of matter nthan is offered by treating the material as a field  Multiscale nsimulation methods can be further broken down into serial multiscale nand parallel multiscale methods  The more traditional nmethod is serial multi-scale modeling  The idea here is to nchoose a region  simulate it at the lower level of description  nsummarize the results into a set of parameters digestible by the higher nlevel model  and pass them up to into the part of the algorithm ncalculating at the higher level      nSerial multiscale methods are not effective when the different scales nare strongly coupled together  When the different scales interact nstrongly to produce the observed behavior  what is required is an napproach that simulates each region simultaneously  This is called nparallel multiscale modeling  Parallel multiscale modeling is the nfoundation of a nearly ubiquitous simulation method  so called nsub-grid modeling  Sub-grid modeling refers to the nrepresentation of important small-scale physical processes that occur nat length-scales that cannot be adequately resolved on the grid size nof a particular simulation   Remember that many simulations discretize ncontinuous equations  so they have a relatively arbitrary finite ngrid size   In the study of turbulence in fluids  for nexample  a common practical strategy for calculation is to account for nthe missing small-scale vortices  or eddies  that nfall inside the grid cells  This is done by adding to the large-scale nmotion an eddy viscosity that characterizes the transport and ndissipation of energy in the smaller-scale flowor any such nfeature that occurs at too small a scale to be captured by the ngrid      nIn climate science and kindred disciplines  sub-grid modeling is ncalled parameterization  This  again  refers to the nmethod of replacing processesones that are too small-scale or ncomplex to be physically represented in the model by a more nsimple mathematical description  This is as opposed to other nprocessese g   large-scale flow of the atmospherethat are ncalculated at the grid level in accordance with the basic theory  It is ncalled parameterization because various non-physical nparameters are needed to drive the highly approximative nalgorithms that compute the sub-grid values  Examples of nparameterization in climate simulations include the descent rate of nraindrops  the rate of atmospheric radiative transfer  and the rate of ncloud formation  For example  the average cloudiness over a 100 nkm2 grid box is not cleanly related to the average humidity nover the box  Nonetheless  as the average humidity increases  average ncloudiness will also increasehence there could be a parameter linking naverage cloudiness to average humidity inside a grid box  Even nthough modern-day parameterizations of cloud formation are more nsophisticated than this  the basic idea is well illustrated by the nexample  The use of nsub-grid modeling methods in simulation has important consequences for nunderstanding the structure of the epistemology of nsimulation  This will be discussed in greater detail in nsection 4      nSub-grid modelling methods can be contrasted with another kind of nparallel multiscale model where the sub-grid algorithms are more ntheoretically principled  but are motivated by a theory at a different nlevel of description  In the example of the simulation of bulk matter nmentioned above  for example  the algorithm driving the smaller level nof description is not built by the seat-of-the-pants  The algorithm ndriving the smaller level is actually more theoretically principled nthan the higher level in the sense that the physics is more nfundamental  quantum mechanics or molecular dynamics vs  continuum nmechanics  These kinds of multiscale models  in other words  cobble ntogether the resources of theories at different levels of ndescription  So they provide for interesting examples that nprovoke our thinking about intertheoretic relationships  and that nchallenge the widely-held view that an inconsistent set of laws can nhave no models       nIn the scientific literature  there is another large class of ncomputer simulations called Monte Carlo  MC  Simulations  MC nsimulations are computer algorithms that use randomness to calculate nthe properties of a mathematical model and where the randomness of the nalgorithm is not a feature of the target model  A nice nexample is the use of a random algorithm to calculate the value nof    If you draw a unit square on a piece of npaper and inscribe a circle in it  and then randomly drop a collection nof objects inside the square  the proportion of objects that land in nthe circle would be roughly equal to /4  A ncomputer simulation that simulated a procedure like that would be ncalled a MC simulation for calculating        nMany philosophers of science have deviated from ordinary scientific nlanguage here and have shied away from thinking of MC simulations as ngenuine simulations  Grne-Yanoff and Weirich  2010  offer the nfollowing reasoning  The Monte Carlo approach does not have a nmimetic purpose  It imitates the deterministic system not in order to nserve as a surrogate that is investigated in its stead but only in norder to offer an alternative computation of the deterministic nsystems properties  p 30   This shows that MC nsimulations do not fit any of the above definitions aptly  On the nother hand  the divide between philosophers and ordinary language can nperhaps be squared by noting that MC simulations simulate an imaginary nprocess that might be used for calculating something relevant to nstudying some other process  Suppose I am modeling a planetary orbit nand for my calculation I need to know the value of   If I do the nMC simulation mentioned in the last paragraph  I am simulating the nprocess of randomly dropping objects into a square  but what I am nmodeling is a planetary orbit  This is the sense in which MC nsimulations are simulations  but they are not nsimulations of the systems they are being used to study  nHowever  as Beisbart and Norton  2012  point nout  some MC simulations  viz  those that use MC techniques to solve nstochastic dynamical equations referring to a physical system  are in nfact simulations of the systems they study       nThere are three general categories of purposes to which computer nsimulations can be put  Simulations can be used for nheuristic purposes  for the purpose of predicting data that we do not nhave  and for generating understanding of data that we do already nhave      nUnder the category of heuristic models  simulations can be further nsubdivided into those used to communicate knowledge to others  and nthose used to represent information to ourselves  When Watson and Crick nplayed with tin plates and wire  they were doing the latter at first  nand the former when they showed the results to others  When the army ncorps built the model of the San Francisco Bay to convince the voting npopulation that a particular intervention was dangerous  they were nusing it for this kind of heuristic purpose  Computer simulations can nbe used for both of these kinds of purposesto explore features of npossible representational structures  or to communicate knowledge to nothers  For example  computer simulations of natural processes  such as nbacterial reproduction  tectonic shifting  chemical reactions  and nevolution have all been used in classroom settings to help students nvisualize hidden structure in phenomena and processes that are nimpractical  impossible  or costly to illustrate in a wet nlaboratory setting      nAnother broad class of purposes to which computer simulations can be nput is in telling us about how we should expect some system in the nreal world to behave under a particular set of circumstances  Loosely nspeaking  computer simulation can be used for prediction  We can use nmodels to predict the future  or to retrodict the past  we can use nthem to make precise predictions or loose and general ones  With nregard to the relative precision of the predictions we make with nsimulations  we can be slightly more fine-grained in our ntaxonomy  There are a  Point predictions  Where will the planet Mars nbe on October 21st  2300? b  Qualitative or global or nsystemic predictions  Is the orbit of this planet stable? What scaling nlaw emerges in these kinds of systems? What is the fractal dimension nof the attractor for systems of this kind? and c  Range predictions  nIt is 66% likely that the global mean surface temperature will nincrease by between 25 degrees C by the year 2100  it is nhighly likely that sea level will rise by at least two nfeet  it is implausible that the thermohaline will shut ndown in the next 50 years      nFinally  simulations can be used to understand systems and their nbehavior  If we already have data telling us how some system nbehaves  we can use computer simulation to answer questions about how nthese events could possibly have occurred  or about how those events nactually did occur      nWhen thinking about the topic of the next section  the epistemology nof computer simulations  we should also keep in mind that the nprocedures needed to sanction the results of simulations will often ndepend  in large part  on which of the above kind of purpose or npurposes the simulation will be put to      nAs computer simulation methods have gained importance in more and nmore disciplines  the issue of their trustworthiness for generating new nknowledge has grown  especially when simulations are expected to be ncounted as epistemic peers with experiments and traditional analytic ntheoretical methods  The relevant question is always whether or not the nresults of a particular computer simulation are accurate enough for ntheir intended purpose  If a simulation is being used to nforecast weather  does it predict the variables we are ninterested in to a degree of accuracy that is sufficient to meet the nneeds of its consumers? If a simulation of the atmosphere above a nMidwestern plain is being used to understand the structure of na severe thunderstorm  do we have confidence that the structures in the nflowthe ones that will play an explanatory role in our account of why nthe storm sometimes splits in two  or why it sometimes forms ntornadosare being depicted accurately enough to support our nconfidence in the explanation? If a simulation is being used in nengineering and design  are the predictions made by the simulation nreliable enough to sanction a particular choice of design parameters  nor to sanction our belief that a particular design of airplane wing nwill function? Assuming that the answer to these questions is sometimes nyes  i e  that these kinds of inferences are at least nsometimes justified  the central philosophical question is  what njustifies them? More generally  how can the claim that a simulation is ngood enough for its intended purpose be evaluated? These are the ncentral questions of the epistemology of computer simulation EOCS       nGiven that confirmation theory is one of the traditional ntopics in philosophy of science  it might seem obvious that the nlatter would have the resources to begin to approach these questions  nWinsberg  1999   however  argued that when it comes to topics related nto the credentialing of knowledge claims  philosophy of science has ntraditionally concerned itself with the justification of theories  not ntheir application  Most simulation  on the other hand  to the extent nthat it makes use of the theory  tends to make use of the nwell-established theory  EOCS  in other words  is rarely about testing nthe basic theories that may go into the simulation  and most often nabout establishing the credibility of the hypotheses that are  in part  nthe result of applications of those theories      nWinsberg  2001  argued that  unlike the epistemological issues that ntake center stage in traditional confirmation theory  an adequate EOCS nmust meet three conditions  In particular it must take account of nthe fact that the knowledge produced by computer simulations is the nresult of inferences that are downward  motley  and nautonomous     Downward  EOCS must reflect the fact that in a large nnumber of cases  accepted scientific theories are the starting point nfor the construction of computer simulation models and play an nimportant role in the justification of inferences from simulation nresults to conclusions about real-world target systems  The nword downward was meant to signal the fact that  unlike nmost scientific inferences that have traditionally interested nphilosophers  which move up from observation instances to ntheories  here we have inferences that are drawn  in part  from nhigh theory  down to particular features of phenomena      nMotley  EOCS must take into account that simulation results nnevertheless typically depend not just on theory but on many other nmodel ingredients and resources as well  including parameterizations discussed above   numerical solution methods  mathematical tricks  napproximations and idealizations  outright fictions  ad hoc nassumptions  function libraries  compilers and computer hardware  and nperhaps most importantly  the blood  sweat  and tears of much trial and nerror      nAutonomous  EOCS must take into account the autonomy nof the knowledge produced by simulation in the sense that the knowledge nproduced by simulation cannot be sanctioned entirely by comparison with nobservation  Simulations are usually employed to study phenomena where ndata are sparse  In these circumstances  simulations are meant to nreplace experiments and observations as sources of data about the world nbecause the relevant experiments or observations are out of reach  nfor principled  practical  or ethical reasons      nParker  2013  has made the point that the usefulness of nthese conditions is somewhat compromised by the fact that it is overly nfocused on simulation in the physical sciences  and other disciplines nwhere simulation is theory-driven and equation-based  This seems ncorrect  In the social and behavioral sciences  and other ndisciplines where agent-based simulation  see 2 2  are more the nnorm  and where models are built in the absence of established and nquantitative theories  EOCS probably ought to be characterized in other nterms      nFor instance  some social scientists who use agent-based simulation npursue a methodology in which social phenomena  for example an nobserved pattern like segregation  are explained  or accounted for  by ngenerating similar looking phenomena in their simulations  Epstein and nAxtell 1996  Epstein 1999   But this raises its own sorts of nepistemological questions  What exactly has been accomplished  what nkind of knowledge has been acquired  when an observed social nphenomenon is more or less reproduced by an agent-based simulation? nDoes this count as an explanation of the phenomenon?  A possible nexplanation?  see e g   Grne-Yanoff 2007    Giuseppe Primiero 2019  argues that there is a whole domain of artificial nsciences built around agent-based and multi-agent system based nsimulations  and that it requires its own epistemology--one where nvalidation cannot be defined by comparison with an existing real-world nsystem  but must be defined vis a vis an intended system      nIt is also fair to say  as Parker does  2013   that the nconditions outlined above pay insufficient attention to the various and ndiffering purposes for which simulations are used  as discussed in n2 4   If we are using a simulation to make detailed nquantitative predictions about the future behavior of a target system  nthe epistemology of such inferences might require more stringent nstandards than those that are involved when the inferences being made nare about the general  qualitative behavior of a whole class of nsystems  Indeed  it is also fair to say that much more work could nbe done in classifying the kinds of purposes to which computer nsimulations are put and the constraints those purposes place on the nstructure of their epistemology      nFrigg and Reiss  2009  argued that none of these three conditions nare new to computer simulation  They argued that ordinary npaper and pencil modeling incorporate these nfeatures  Indeed  they argued that computer simulation could not npossibly raise new epistemological issues because the epistemological nissues could be cleanly divided into the question of the nappropriateness of the model underlying the simulation  which is an nissue that is identical to the epistemological issues that arise in nordinary modeling  and the question of the correctness of the solution nto the model equations delivered by the simulation  which is a nmathematical question  and not one related to the epistemology of nscience  On the first point  Winsberg  2009b  replied that nit was the simultaneous confluence of all three features that was new nto simulation  We will return to the second point in section n4 3     nSome of the work on the EOCS has developed analogies between ncomputer simulation in order to draw on recent work in the epistemology nof experiment  particularly the work of Allan Franklin  nsee the entry on  experiments in physics      nIn his work on the epistemology of experiment  Franklin  1986  n1989  identified a number of strategies that experimenters use to nincrease rational confidence in their results  Weissart  1997   nand Parker  2008a  argued for various forms of analogy between these nstrategies and a number of strategies available to simulationists to nsanction their results  The most detailed analysis of these nrelationships is to be found in Parker 2008a  where she also uses nthese analogies to highlight weaknesses in current approaches to nsimulation model evaluation      nWinsberg  2003  also makes use of Ian Hackings  1983  1988  n1992  work on the philosophy of experiment  One of Hackings ncentral insights about experiment is captured in his slogan that nexperiments have a life of their own  1992  306   Hacking nintended to convey two things with this slogan  The first was a nreaction against the unstable picture of science that comes  for nexample  from Kuhn  Hacking  1992  suggests that experimental results ncan remain stable even in the face of dramatic changes in the other nparts of sciences  The second  related  point he intended to nconvey was that experiments are organic  develop  change  and nyet retain a certain long-term development which makes us talk about nrepeating and replicating experiments  1992  307   Some of the ntechniques that simulationists use to construct their models get ncredentialed in much the same way that Hacking says that instruments nand experimental procedures and methods do  the credentials develop nover an extended period of time and become deeply tradition-bound  In nHackings language  the techniques and sets of assumptions that nsimulationists use become self-vindicating  Perhaps a nbetter expression would be that they carry their own credentials  This nprovides a response to the problem posed in 4 1  of understanding how nsimulation could have a viable epistemology despite the motley and nautonomous nature of its inferences      nDrawing inspiration from another philosopher of experiment  Mayo n1996   Parker  2008b  suggests a remedy to some of the shortcomings in ncurrent approaches to simulation model evaluation  In this work  nParker suggests that Mayos error-statistical approach for nunderstanding the traditional experimentwhich makes use of the nnotion of a severe testcould shed light on the nepistemology of simulation  The central question of the epistemology nof simulation from an error-statistical perspective becomes  nWhat warrants our taking a computer simulation to be a severe ntest of some hypothesis about the natural world? That is  what nwarrants our concluding that the simulation would be unlikely to give nthe results that it in fact gave  if the hypothesis of interest were nfalse  2008b  380 ?  Parker believes that too much of what passes for nsimulation model evaluation lacks rigor and structure because it      nDrawing explicitly upon Mayos  1996  work  she argues that nwhat the epistemology of simulation ought to be doing  instead  is noffering some account of the canonical errors that can narise  as well as strategies for probing for their presence      nPractitioners of simulation  particularly in engineering contexts  nin weapons testing  and in climate science  tend to conceptualize nthe EOCS in terms of verification and validation   nVerification is said to be the process of determining whether nthe output of the simulation approximates the true solutions to the ndifferential equations of the original model  Validation  on nthe other hand  is said to be the process of determining whether the nchosen model is a good enough representation of the real-world system nfor the purpose of the simulation  The literature on verification and nvalidation from engineers and scientists is enormous and it is nbeginning to receive some attention from philosophers      nVerification can be divided into solution verification and code nverification  The former verifies that the output of the intended nalgorithm approximates the true solutions to the differential nequations of the original model  The latter verifies that the ncode  as written  carries out the intended algorithm  Code nverification has been mostly ignored by philosophers of science  nprobably because it has been seen as more of a problem in computer nscience than in empirical scienceperhaps a mistake  Part of solution nverification consists in comparing computed output with analytic nsolutions  so called benchmark solutions    nThough this method can of course help to make case for the results of a ncomputer simulation  it is by itself inadequate  since nsimulations are often used precisely because analytic solution is nunavailable for regions of solution space that are of interest  Other nindirect techniques are available  the most important of which is nprobably checking to see whether and at what rate computed output nconverges to a stable solution as the time and spatial resolution of nthe discretization grid gets finer      nThe principal strategy of validation involves comparing model output nwith observable data  Again  of course  this strategy is nlimited in most cases  where simulations are being run because nobservable data are sparse  But complex strategies can be nemployed  including comparing the output of subsystems of a simulation nto relevant experiments  Parker  2013  Oberkampf and Roy n2010       nThe concepts of verification and validation has drawn some criticism nfrom philosophers  Oreskes et al  1994  a very widely-cited article  nwas mostly critical of the terminology  arguing that nvalidity  in particular  is a property that only applies nto logical arguments  and that hence the term  when applied to models  nmight lead to overconfidence      nWinsberg  2010  2018  p 155  has argued that the conceptual division nbetween verification and validation can be misleading  if it is taken nto suggest that there is one set of methods which can  by itself  show nthat we have solved the equations right  and that there is another set nof methods  which can  by itself  show that weve got the right nequations  He also argued that it is misleading to think that the nepistemology of simulation is cleanly divided into an empirical part verification  and a mathematical  and computer science  part validation   But this misleading idea often follows discussion of nverification and validation  We find this both in the work of npractitioners and philosophers      nHere is the standard line from a practitioner  Roy  nVerification deals with mathematics and addresses the ncorrectness of the numerical solution to a given model  Validation  on nthe other hand  deals with physics and addresses the appropriateness nof the model in reproducing experimental data  Verification can be nthought of as solving the chosen equations correctly  while validation nis choosing the correct equations in the first place  Roy n2005       nSome philosophers have put this distinction to work in arguments nabout the philosophical novelty of simulation  We first nraised this issue in section 4 1  where Frigg and Reiss argued that nsimulation could have no epistemologically novel features  since it ncontained two distinct components  a component that is identical nto the epistemology of ordinary modeling  and a component that is nentirely mathematical  We should distinguish two ndifferent notions of reliability here  answering two different nquestions  First  are the solutions that the computer provides close nenough to the actual  but unavailable  solutions to be nuseful?this is a purely mathematical question and falls within nthe class of problems we have just mentioned  So  there is nothing new nhere from a philosophical point of view and the question is indeed one nof number crunching  Second  do the computational models that are the nbasis of the simulations represent the target system correctly? That nis  are the simulation results externally valid? This is a serious nquestion  but one that is independent of the rst problem  and none that equally arises in connection with models that do not involve nintractable mathematics and ordinary experiments  Frigg nand Reiss 2009       nBut verification and validation are not  strictly speaking  so cleanly nseparable  That is because most methods of validation  by themselves  nare much too weak to establish the validity of a simulation  And most nmodel equations chosen for simulation are not in any straightforward nsense the right equations  they are not the model nequations we would choose in an ideal world  We have good reason to nthink  in other words  that there are model equations out there that nenjoy better empirical support  in the abstract  The equations we nchoose often reflect a compromise between what we think best describes nthe phenomena and computational tractability  So the equations that nare chosen are rarely well validated on their own  If we nwant to understand why simulation results are taken to be credible  we nhave to look at the epistemology of simulation as an integrated whole  nnot as cleanly divided into verification and validationeach of nwhich  on its own  would look inadequate to the task      nSo one point is that verification and validation are not nindependently-successful and separable activities  But the other point nis that there are not two independent entities onto which these nactivities can be directed  a model chosen to discretized  and a method nfor discretizing it  Once one recognizes that the equations to be nsolved are sometimes chosen so as to cancel out ndiscretization errors  etc   Lenhard 2007 has a very nice example of nthis involving the Arakawa operator   this later distinction gets harder nto maintain  So success is achieved in simulation with a kind of nback-and-forth  trial-and-error  piecemeal adjustment between model and nmethod of calculation  And when this is the case  it is hard even to nknow what it means to say that a simulation is separately verified and nvalidated      nNo one has argued that V&V isnt a useful distinction  but nrather that scientists shouldnt overinflate a pragmatically nuseful distinction into a clean methodological dictate that nmisrepresents the messiness of their own practice  Collaterally  Frigg nand Reisss argument for the absence of epistemological novelty nin simulation fails for just this reason   It is not a purely nmathematical question whether the solutions that the computer nprovides close enough to the actual  but unavailable  solutions to be nuseful  At least not in this respect  it is not a question that can be nanswered  as a pragmatic matter  entirely using mathematical nmethods  And hence it is an empirical/epistemological issue that does nnot arise in ordinary modeling      nA major strand of ordinary  outside of the philosophy of science  nepistemology is to emphasize the degree to which it is a condition for nthe possibility of knowledge that we rely on our senses and the ntestimony of other people in a way that we cannot ourselves justify  According to Tyler Burge  1993 1998   belief in the results of nthese two processes are warranted but not justified  Rather  naccording to Burge  we are entitled to these beliefs   w e are entitled to rely  other things equal  on perception  nmemory  deductive and inductive reasoning  and onthe word of nothers  1993  p  458   Beliefs in which a believer is nentitled are those that are unsupported by evidence available to the nbeliever  but which the believer is nevertheless warranted in nbelieving      nSome work in EOCS has developed analogies between computer nsimulation and the kinds of knowledge producing practices Burge nassociates with entitlement   See especially Barberousse nand Vorms  2014  and Beisbart  2017   This is  in some ways  a nnatural outgrowth of Burges arguments that we view computer nassisted proofs in this way  1998   Computer simulations nare extremely complex  often the result of the epistemic labor of a ndiverse set of scientists and other experts  and perhaps most nimportantly  epistemically opaque  Humphreys  2004    Because of nthese features  Beisbart argues that it is reasonable to treat computer nsimulations in the same way that we treat our senses and the testimony nof others  simply as things that can be trusted on the assumption nthat everything is working smoothly   Beisbart  n2017        nSymons and Alvarado  2019  argue that there is a fundamental problem nwith this approach to EOCS and it has to do with a feature of ncomputer-aided proof that was crucial to Burges original naccount  that of a being a transparent conveyor  nIt is very important to note  for example  that Burges naccount of content preservation and transparent conveying requires nthat the recipient already has reason not to doubt the source p  13   But Symons and Alvarado point to many of the properties of ncomputer simulations  drawing from Winsberg 2010 and Ruphy 2015  in nvirtue of which they fail to have these properties  Lenhard and nKster 2019 is also relevant here  as they argue that there are nmany features of computer simulation that make them difficult to nreproduce and that therefore undermine some of the stability that nwould be required for them to be transparent conveyors  For these nreasons and others having to do with many of the features discussed in n4 2 and 4 3  Symons and Alvarado argue that it is implausible that we nshould view computer simulation as a basic epistemic practice on a par nwith sense perception  memory  testimony  or the like      nAnother approach to EOCS is to ground it in the practical aspects of nthe craft of modeling and simulation  According to this view  in nother words  the best account we can give of the reasons we have for nbelieving the results of computer simulation studies is to have trust nin the practical skills and craft of the modelers that use them   nA good example of this kind of account is  Hubig and Kaminski  n2017   The epistemological goal of this kind of work is to nidentify the locus of our trust in simulations in practical aspects of nthe craft of modeling and simulation  rather than in any nfeatures of the models themselves   Resch et al  2017  argue that na good part of the reason we should trust simulations is not because of nthe simulations themselves  but because of the interpretive artistry of nthose who employ their art and skill to interpret simulation noutputs  Symons and Alvarado  2019  are also critical of this napproach  arguing that Part of the task of the epistemology of ncomputer simulation is to explain the difference between the ncontemporary scientists position in relation to epistemically nopaque computer simulations    p 7  and the believers in a nmechanical oracles relation to their oracles   Pragmatic nand epistemic considerations  according to Symons and Alvarado  nco-exist  and they are not possible competitors for the correct nexplanation of our trust in simulations--the epistemic reasons are nultimate what explain and ground the pragmatic ones       nWorking scientists sometimes describe simulation studies in nexperimental terms  The connection between simulation and nexperiment probably goes back as far as von Neumann  who  when nadvocating very early on for the use of computers in physics  noted nthat many difficult experiments had to be conducted merely to determine nfacts that ought  in principle  to be derivable from theory  Once nvon Neumanns vision became a reality  and some of these nexperiments began to be replaced by simulations  it became somewhat nnatural to view them as versions of experiment  A nrepresentative passage can be found in a popular book on nsimulation      nThe idea of in silico experiments becomes even more nplausible when a simulation study is designed to learn what happens to na system as a result of various possible interventions  What would nhappen to the global climate if x amount of carbon were added nto the atmosphere? What will happen to this airplane wing if it is nsubjected to such-and-such strain? How would traffic patterns change nif an onramp is added at this location?     nPhilosophers  consequently  have begun to consider in what sense  if nany  computer simulations are like experiments and in what sense they ndiffer   A related issue is the question of when a process that nfundamentally involves computer simulation can counts as measurement Parker  2017  A number of views have emerged in the literature ncentered around defending and criticizing two theses     The identity thesis  Computer simulation studies are nliterally instances of experiments     The epistemological dependence thesis  The identity nthesis would  if it were true  be a good reason  weak version   or the nbest reason  stronger version   or the only reason  strongest version  nit is a necessary condition  to believe that simulations can provide nwarrants for belief in the hypotheses that they support  A nconsequence of the strongest version is that only if the identity nthesis is true is there reason to believe that simulations can confer nwarrant for believing in hypotheses      nThe central idea behind the epistemological dependence thesis is nthat experiments are the canonical entities that play a central role in nwarranting our belief in scientific hypotheses  and that therefore the ndegree to which we ought to think that simulations can also play a role nin warranting such beliefs depends on the extent to which they can be nidentified as a kind of experiment      nOne can find philosophers arguing for the identity thesis as early as nHumphreys 1995 and Hughes 1999  And there is at least implicit support nfor  the stronger  version of the epistemological dependence thesis in nHughes  The earliest explicit argument in favor of the epistemological ndependence thesis  however  is in Norton and Suppe 2001  According to nNorton and Suppe  simulations can warrant belief precisely because nthey literally are experiments  They have a detailed story to tell nabout in what sense they are experiments  and how this is all supposed nto work   According to Norton and Suppe  a valid simulation is one in nwhich certain formal relations  what they call nrealization  hold between a base model  the modeled nphysical system itself  and the computer running the algorithm  When nthe proper conditions are met  a simulation can be used as an ninstrument for probing or detecting real world phenomena  Empirical ndata about real phenomena are produced under conditions of nexperimental control  p  73       nOne problem with this story is that the formal conditions that they nset out are much too strict  It is unlikely that there are very many nreal examples of computer simulations that meet their strict standards  nSimulation is almost always a far more idealizing and approximating nenterprise  So  if simulations are experiments  it is probably nnot in the way that Norton and Suppe imagined      nMore generally  the identity thesis has drawn fire from other nquarters      nGilbert and Troitzsch argued that  t he major difference is nthat while in an experiment  one is controlling the actual object of ninterest  for example  in a chemistry experiment  the chemicals under ninvestigation   in a simulation one is experimenting with a model nrather than the phenomenon itself   Gilbert and Troitzsch 1999  n13   But this doesnt seem right  Many  Guala 2002  2008  Morgan 2003  nParker 2009a  Winsberg 2009a  have pointed to problems with the nclaim  If Gilbert and Troitzsch mean that simulationists manipulate nmodels in the sense of abstract objects  then the claim is difficult nto understandhow do we manipulate an abstract entity? If  on nthe other hand  they simply mean to point to the fact that the nphysical object that simulationists manipulatea digital ncomputeris not the actual object of interest  then it is not nclear why this differs from ordinary experiments       nIt is false that real experiments always manipulate exactly their ntargets of interest  In fact  in both real experiments and nsimulations  there is a complex relationship between what is nmanipulated in the investigation on the one hand  and the real-world nsystems that are the targets of the investigation on the other  In ncases of both experiment and simulation  therefore  it takes an nargument of some substance to establish the external nvalidity of the investigation  to establish that what is nlearned about the system being manipulated is applicable to the system nof interest  Mendel  for example  manipulated pea plants  but he was ninterested in learning about the phenomenon of heritability ngenerally  The idea of a model organism in biology makes this nidea perspicuous  We experiment on Caenorhabditis elegans nbecause we are interested in understanding how organism in general use ngenes to control development and genealogy  We experiment non Drosophila melanogaster  because it provides a nuseful model of mutations and genetic inheritance  But the idea is not nlimited to biology   Galileo experimented with inclined planes because nhe was interested in how objects fall and how they would behave in the nabsence of interfering forcesphenomena that the inclined plane nexperiments did not even actually instantiate      nOf course  this view about experiments is not uncontested  It is ntrue that  quite often  experimentalists infer something about a system ndistinct from the system they interfere with  However  it is not clear nwhether this inference is proper part of the original experiment  nPeschard  2010  mounts a criticism along these lines  and hence can be nseen as a defender of Gilbert and Troitzsch  Peschard argues that the nfundamental assumption of their criticsthat in experimentation  njust as in simulation  what is manipulated is a system standing in for na target systemis confused  It confuses  Peschard argues  the nepistemic target of an experiment with its epistemic nmotivation  She argues that while the epistemic motivation for ndoing experiments on C  elegans might be quite far-reaching  nthe proper epistemic target for any such experiment is the worm nitself   In a simulation  according to Peschard  however  the nepistemic target is never the digital computer itself  Thus  nsimulation is distinct from experiment  according to her  in that its nepistemic target  as opposed to merely its epistemic motivation  is ndistinct from the object being manipulated  Roush  2017  can also be nseen as a defender of the Gilbert and Troitzsch line  but Roush nappeals to sameness of natural kinds as the crucial feature that nseparates experiments and simulations  Other opponents of the identity nthesis include Giere  2009  and Beisbart and Norton  2012  Other nInternet Resources       nIt is not clear how to adjudicate this dispute  and it seems to nrevolve primarily around a difference of emphasis  One can nemphasize the difference between experiment and simulation  nfollowing Gilbert and Troitzsch and Peschard  by insisting that nexperiments teach us first about their epistemic targets and only nsecondarily allow inferences to the behavior of other systems   I e   experiments on worms teach us  in the first instance  about nworms  and only secondarily allow us to make inferences about genetic ncontrol more generally   This would make them conceptually ndifferent from computer simulations  which are not thought to teach us  nin the first instance  about the behavior of computers  and only in the nsecond instance about storms  or galaxies  or whatever      nOr one can emphasize similarity in the opposite way  One can nemphasize the degree to which experimental targets are always chosen nas surrogates for whats really of interest   Morrison  2009 is nprobably the most forceful defender of emphasizing this aspect of the nsimilarity of experiment and simulation   She argues that most nexperimental practice  and indeed most measurement practice  involve nthe same kinds of modeling practices as simulations   In any case  npace Peschard  nothing but a debate about nomenclatureand maybe nan appeal to the ordinary language use of scientists  not always the nmost compelling kind of argumentwould prevent us from saying nthat the epistemic target of a storm simulation is the computer  and nthat the storm is merely the epistemic motivation for studying the ncomputer      nBe that as it may  many philosophers of simulation  including those ndiscussed in this section  have chosen the latter pathpartly as na way of drawing attention to ways in which the message lurking behind nGilbert and Troitzschs quoted claim paints an overly simplistic npicture of experiment  It does seem overly simplistic to paint a npicture according to which experiment gets a direct grip on nthe world  whereas simulations situation is exactly nopposite  And this is the picture one seems to get from the nGilber and Troitzsch quotation  Peschards more sophisticated picture ninvolving a distinction between epistemic targets and epistemic nmotivations goes a long way towards smoothing over those concerns nwithout pushing us into the territory of thinking that simulation and nexperiment are exactly the same  in this regard      nStill  despite rejecting Gilbert and Troitzschs ncharacterization of the difference between simulation and experiment  nGuala and Morgan both reject the identity thesis  Drawing on the nwork of Simon  1969   Guala argues that simulations differ nfundamentally from experiments in that the object of manipulation in an nexperiment bears a material similarity to the target of interest  but nin a simulation  the similarity between object and target is merely nformal  Interestingly  while Morgan accepts this argument against the nidentity thesis  she seems to hold to a version of the nepistemological dependency thesis  She argues  in other words  nthat the difference between experiments and simulations identified by nGuala implies that simulations are epistemologically inferior to real nexperiments  that they have intrinsically less power to warrant nbelief in hypotheses about the real world because they are not nexperiments      nA defense of the epistemic power of simulations against Morgans 2002  argument could come in the form of a defense of the identity nthesis  or in the form of a rejection of the epistemological ndependency thesis  On the former front  there seem to be two problems nwith Gualas  2002  argument against the identity thesis  The nfirst is that the notion of material similarity here is too weak  and nthe second is that the notion of mere formal similarity is too vague  nto do the required work  Consider  for example  the fact that it is nnot uncommon  in the engineering sciences  to use simulation methods nto study the behavior of systems fabricated out of silicon  The nengineer wants to learn about the properties of different design npossibilities for a silicon device  so she develops a computational nmodel of the device and runs a simulation of its behavior on a digital ncomputer  There are deep material similarities between  and some of nthe same material causes are at work in  the central processor of the ncomputer and the silicon device being studied  On Gualas line nof reasoning  this should mark this as an example of a real nexperiment  but that seems wrong  The peculiarities of this example nillustrate the problem rather starkly  but the problem is in fact nquite general  any two systems bear some material similarities to each nother and some differences     Parke  2014  argues against the nepistemological dependency thesis by undermining two premises that she nbelieves support it  fist  that experiments generate greater ninferential power than simulations  and second  that simulations ncannot surprise us in the same way that experiments can      nOn the flip side  the idea that the existence of a formal similarity nbetween two material entities could mark anything interesting is nconceptually confused  Given any two sufficiently complex entities  nthere are many ways in which they are formally identical  not to nmention similar  There are also ways in which they are formally ncompletely different  Now  we can speak loosely  and say that two nthings bear a formal similarity  but what we really mean is that our nbest formal representations of the two entities have formal nsimilarities  In any case  there appear to be good grounds for rejecting both nthe Gilbert and Troitzsch and the Morgan and Guala grounds for ndistinguishing experiments and simulations       nReturning to the defense of the epistemic power of simulations  there nare also grounds for rejecting the epistemological dependence thesis  nAs Parker  2009a  points out  in both experiment and simulation  we ncan have relevant similarities between computer simulations and target nsystems  and thats what matters  When the relevant background nknowledge is in place  a simulation can provide more reliable nknowledge of a system than an experiment  A computer simulation of the nsolar system  based on our most sophisticated models of celestial ndynamics  will produce better representations of the planets norbits than any experiment        nParke  2014  argues against the epistemological dependency thesis by nundermining two premises that she believes support it  fist  that nexperiments generate greater inferential power than simulations  and nsecond  that simulations cannot surprise us in the same way that nexperiments can   The argument that simulations cannot surprise us ncomes from Morgan  2005    Pace Morgan  Parke argues that nsimulationists are often surprised by their simulations  both because nthey are not computationally omniscient  and because they are not nalways the sole creators of the models and code they use   She argues  nmoreover  that  d ifferences in researchers epistemic states  alone  nseem like the wrong grounds for tracking a distinction between nexperiment and simulation  258    Adrian Curry  2017  defends nMorgans original intuition by making two friendly amendments   He nargues that the distinction Morgan was really after was between two ndifferent kinds of surprise  and in particular to what the source of nsurprise is  surprise due to bringing out theoretical knowledge into ncontact with the world are distinctive of experiment   He also more ncarefully defines surprise in a non-psychological way such that it is na quality the attainment of which constitutes genuine epistemic nprogress  p  640       nPaul Humphreys  2004  has argued that computer simulations nhave profound implications for our understanding of the structure of ntheories  he argues that they reveal inadequacies with both the nsemantic and syntactic views of scientific theories  This claim has ndrawn sharp fire from Roman Frigg and Julian Reiss  2009   Frigg and nReiss argue that whether a model admits of analytic solution or not has nno bearing on how it relates to the world  They use the example of the ndouble pendulum to show this  Whether or not the pendulums inner nfulcrum is held fixed  a fact which will determine whether the relevant nmodel is analytically solvable  has no bearing on the semantics of the nelements of the model  From this  they conclude that the semantics of a nmodel  or how it relates to the world  is unaffected by whether or not nthe model is analytically solvable      n This was not responsive  however  nto the most charitable reading of what Humphreys was pointing at  The nsyntactic and semantic views of theories  after all  were not just naccounts of how our abstract scientific representations relate to the nworld  More particularly  they were not stories about the nrelation between particular models and the world  but rather about the nrelation between theories and the world  and the role  if any  nthat models played in that relation      nThey were also stories that had a lot to say about where the nphilosophically interesting action is when it comes to scientific ntheorizing  The syntactic view suggested that scientific practice could nbe adequately rationally reconstructed by thinking of theories as naxiomatic systems  and  more importantly  that logical deduction was a nuseful regulative ideal for thinking about how inferences from theory nto the world are drawn  The syntactic view also  by omission  made if nfairly clear that modeling played  if anything  only a nheuristic role in science   This was a feature of the nsyntactic view of theories that Frederick Suppe  one of its most ardent ncritics  often railed against   Theories themselves had nothing nto do with models  and theories could be compared directly to nthe world  without any important role for modeling to play      nThe semantic view of theories  on the other hand  did emphasize an nimportant role for models  but it also urged that theories were nnon-linguistic entities  It urged philosophers not to be distracted by nthe contingencies of the particular form of linguistic expression a ntheory might be found in  say  a particular textbook      nComputer simulations  however  do seem to illustrate that both of nthese themes were misguided  It was profoundly wrong to think that nlogical deduction was the right tool for rationally reconstructing the nprocess of theory application  Computer simulations show that there are nmethods of theory application that vastly outstrip the inferential npower of logical deduction  The space of solutions  for example  that nis available via logical deduction from the theory of fluids is nmicroscopic compared with the space of applications that can be nexplored via computer simulation  On the flip side  computer nsimulations seem to reveal that  as Humphreys  2004  has urged  syntax nmatters  It was wrong  it turns out  to suggest  as the semantic nview did  that the particular linguistic form in which a scientific ntheory is expressed is philosophically uninteresting  The syntax of the ntheorys expression will have a deep effect on what inferences ncan be drawn from it  what kinds of idealizations will work well with nit  etc  Humphreys put the point as follows  the nspecific syntactic representation used is often crucial to the nsolvability of the theorys equations  Humphreys 2009  np 620   The theory of fluids can be used to nemphasize this point  whether we express that theory in Eulerian or nLagrangian form will deeply affect what  in practice  we can calculate nand how  it will affect what idealizations  approximations  and ncalculational techniques will be effective and reliable in which ncircumstances  So the epistemology of computer simulation needs nto be sensitive to the particular syntactic formulation of a theory  nand how well that particular formulation has been credentialed   nHence  it does seem right to emphasize  as Humphreys  2004  did  that ncomputer simulations have revealed inadequacies with both the syntactic nand semantic theories      nPaul Humphreys  2004  and Mark Bedau  1997  2011  have argued that nphilosophers interested in the topic of emergence can learn a great ndeal by looking at computer simulation  Philosophers ninterested in this topic should consult the entry on  emergent properties  nwhere the contributions of all these philosophers have been ndiscussed       nThe connection between emergence and simulation was perhaps best narticulated by Bedau in his  2011   Bedau argued that any nconception of emergence must meet the twin hallmarks of explaining how nthe whole depends on its parts and how the whole is independent of its nparts  He argues that philosophers often focus on what he calls nstrong emergence  which posits brute downward causation that is nirreducible in principle  But he argues that this is a nmistake  He focuses instead  on what he calls weak nemergence  which allows for reducibility of wholes to parts nin principle but not in practice  Systems that nproduce emergent properties are mere mechanisms  but the mechanisms are nvery complex  they have very many independently interacting parts   As na result  there is no way to figure out exactly what will happen given na specific set of initial and boundary conditions  except to crawl the ncausal web  It is here that the connection to computer nsimulation arises  Weakly emergent properties are characteristic nof complex systems in nature  And it is also characteristic of ncomplex computer simulations that there is no way to predict what they nwill do except to let them run  Weak emergence explains  according to nBedau  why computer simulations play a central role in the nscience of complex systems  The best way to understand and predict how nreal complex systems behave is to simulate them by crawling the nmicro-causal web  and see what happens      Models of course involve idealizations  But it has been argued nthat some kinds of idealization  which play an especially prominent nrole in the kinds of modeling involved in computer simulation  are nspecialto the point that they deserve the title of nfiction  This section will discuss attempts to define nfictions and explore their role in computer simulation      nThere are two different lines of thinking about the role of fictions nin science  According to one  all models are fictions  This line of nthinking is motivated by considering the role  for example  of nthe ideal pendulum in science  Scientists  it is argued  noften make claims about these sorts of entities  e g   the nideal pendulum has a period proportional to the square-root of its nlength  but they are nowhere to be found in the real world  nhence they must be fictional entities  This line of argument about nfictional entities in science does not connect up in any special way nwith computer simulationreaders interested in this topic should nconsult the entry on scientific representation  forthcoming       nAnother line of thinking about fictions is concerned with the question nof what sorts of representations in science ought to be regarded as nfictional  Here  the concern is not so much about the ontology of nscientific model entities  but about the representational character of nvarious postulated model entities  Here  Winsberg  2009c  has argued nthat fictions do have a special connection to computer simulations  Or nrather  that some computer simulations contain elements that best ntypify what we might call fictional representations in science  even nif those representations are not uniquely present in simulations       nHe notes that the first conception of a fictionmentioned nabovewhich makes any representation that contradicts nreality a fiction  p  179   doesnt correspond to our ordinary nuse of the term  a rough map is not fiction  He then proposes an nalternative definition  nonfiction is offered as a good nenough guide to some part of the world  p  181   fiction is nnot  But the definition needs to be refined   Take the fable of the ngrasshopper and the ant  Although the fable offers lessons about how nthe world is  it is still fiction because it is a useful guide nto the way the world is in some general sense rather than a nspecific guide to the way a part of the world is  its prima nfacie representational target  a singing grasshopper and ntoiling ant  Nonfictions  on the other hand  point to a certain npart of the world and are a guide to that part of the world p  181       nThese kinds of fictional components of models are nparadigmatically exemplified in certain computer simulations  Two of nhis examples are the silogen atom and artificial nviscosity  Silogen atoms appear in certain nanomechanical models nof cracks in silicona species of the kind of multiscale models nthat blend quantum mechanics and molecular mechanics mentioned in nsection 2 3  The silogen containing models of crack propagation in nsilicon work by describing the crack itself using quantum mechanics and nthe region immediately surrounding the crack using classical molecular ndynamics  To bring together the modeling frameworks in the two regions  nthe boundary gets treated as if it contains silogen natoms  which have a mixture of the properties of silicon and those of nhydrogen  Silogen atoms are fictions  They are not offered as even a ngood enough description of the atoms at the nboundarytheir prima facie representational targets  But they are nused so that the overall model can be hoped to get things right  Thus nthe overall model is not a fiction  but one of its components is  nArtificial viscosity is a similar sort of example  Fluids with abrupt nshocks are difficult to model on a computational grid because the nabrupt shock hides inside a single grid cell  and cannot be resolved by nsuch an algorithm  Artificial viscosity is a technique that pretends nthat the fluid is highly viscousa fictionright were the nshock is  so that he shock becomes less abrupt  and blurs over several ngrid cells  Getting the viscosity  and hence the thickness of the nshock  wrong  helps to get the overall model to work well nenough  Again  the overall model of the fluid is not a fiction  nit is a reliable enough guide to the behavior of the fluid  But the ncomponent called artificial viscosity is a fictionit is not nbeing used to reliably model the shock  It is being incorporated into a nlarger modeling framework so as to make that larger framework  nreliable enough      nThis account has drawn two sorts of criticisms  Toon  2010  has nargued that this definition of a fiction is too narrow  He gives nexamples of historical fictions like I  Claudius  and nSchindlers Ark  which he argues are fictions  despite nthe fact that they are offered as good enough nguides to those people  places and events in certain respects and we nare entitled to take them as such   p  2867   Toon  presumably  nsupports a broader conception of the role of fictions in science  then  naccording to which they do not play a particularly prominent or nheightened role in computer simulation      nGordon Purves  forthcoming  argues that there are examples of nfictions in computational models  his example is so-called nimaginary cracks   and elsewhere  that do not meet the nstrict requirements discussed above  Unlike Toon  however  he also nwants to delineate fictional modeling elements from the non-fictional nones  His principal criticism is of the criterion of fictionhood in nterms of social norms of useand Purves argues that we ought to nbe able to settle whether or not some piece of modeling is a fiction in nthe absence of such norms  Thus  he wants to find an intrinsic ncharacterization of a scientific fiction  His proposal takes as nconstitutive of model fictions that they fail to have the ncharacteristic that Laymon  1985  called piecewise nimprovability  PI   PI is a characteristic of many models that nare idealizations  it says that as you de-idealize  your model becomes nmore and more accurate  But as you de-idealize a silogen atom  you do nnot get a more and more accurate simulation of a silicon crack  But nPurves takes this failure of PI to be constitutive of a fiction  rather nthan merely symptomatic of them      n biology  experiment in | computation  in physical systems | computer science  philosophy of | computing  modern history of | emergent properties | models in science | physics  experiment in | science  theory and observation in | scientific representation | scientific theories  structure of n     Copyright  2019 by n nEric Winsberg n<winsberg@usf edu>        View this site from another server         The Stanford Encyclopedia of Philosophy is copyright  2016 by The Metaphysics Research Lab  Center for the Study of Language and Information  CSLI   Stanford University    Library of Congress Catalog Data  ISSN 1095-5054     nModels are of central importance in many scientific contexts  The ncentrality of models such as inflationary models in cosmology  ngeneral-circulation models of the global climate  the double-helix nmodel of DNA  evolutionary models in biology  agent-based models in nthe social sciences  and general-equilibrium models of markets in ntheir respective domains is a case in point  the Other Internet Resources section at the end of this entry contains links to online resources nthat discuss these models   Scientists spend significant amounts of ntime building  testing  comparing  and revising models  and much njournal space is dedicated to interpreting and discussing the nimplications of models      nAs a result  models have attracted philosophers attention and nthere are now sizable bodies of literature about various aspects of nscientific modeling  A tangible result of philosophical engagement nwith models is a proliferation of model types recognized in the nphilosophical literature  Probing models  nphenomenological models  computational models  ndevelopmental models  explanatory models  nimpoverished models  testing models  idealized nmodels  theoretical models  scale models  nheuristic models  caricature models  exploratory nmodels  didactic models  fantasy models  nminimal models  toy models  imaginary nmodels  mathematical models  mechanistic nmodels  substitute models  iconic models  nformal models  analogue models  and instrumental nmodels are but some of the notions that are used to categorize nmodels  While at first glance this abundance is overwhelming  it can nbe brought under control by recognizing that these notions pertain to ndifferent problems that arise in connection with models  Models raise nquestions in semantics  how  if at all  do models represent?   nontology  what kind of things are models?   epistemology  how do we nlearn and explain with models?   and  of course  in other domains nwithin philosophy of science      nMany scientific models are representational models  they represent a nselected part or aspect of the world  which is the models ntarget system  Standard examples are the billiard ball model of a gas  nthe Bohr model of the atom  the LotkaVolterra model of predatorprey ninteraction  the MundellFleming model of an open economy  and the nscale model of a bridge      nThis raises the question what it means for a model to represent a ntarget system  This problem is rather involved and decomposes into nvarious subproblems  For an in-depth discussion of the issue of nrepresentation  see the entry on scientific representation  At this point  rather than addressing the issue of what it means for na model to represent  we focus on a number of different kinds of nrepresentation that play important roles in the practice of nmodel-based science  namely scale models  analogical models  idealized nmodels  toy models  minimal models  phenomenological models  nexploratory models  and models of data  These categories are not nmutually exclusive  and a given model can fall into several categories nat once      nScale models  Some models are down-sized or enlarged copies nof their target systems  Black 1962   A typical example is a small nwooden car that is put into a wind tunnel to explore the actual ncars aerodynamic properties  The intuition is that a scale nmodel is a naturalistic replica or a truthful mirror image of the ntarget  for this reason  scale models are sometimes also referred to nas true models  Achinstein 1968  Ch  xa07   However  there nis no such thing as a perfectly faithful scale model  faithfulness is nalways restricted to some respects  The wooden scale model of the car nprovides a faithful portrayal of the cars shape but not of its nmaterial  And even in the respects in which a model is a faithful nrepresentation  the relation between model-properties and ntarget-properties is usually not straightforward  When engineers use  nsay  a 1 100 scale model of a ship to investigate the resistance that nan actual ship experiences when moving through the water  they cannot nsimply measure the resistance the model experiences and then multiply nit with the scale  In fact  the resistance faced by the model does not ntranslate into the resistance faced by the actual ship in a nstraightforward manner  that is  one cannot simply scale the water nresistance with the scale of the model  the real ship need not have none hundred times the water resistance of its 1 100 model   The two nquantities stand in a complicated nonlinear relation with each other  nand the exact form of that relation is often highly nontrivial and nemerges as the result of a thoroughgoing study of the situation Sterrett 2006  forthcoming  Pincock forthcoming       nAnalogical models  Standard examples of analogical models ninclude the billiard ball model of a gas  the hydraulic model of an neconomic system  and the dumb hole model of a black hole  At the most nbasic level  two things are analogous if there are certain relevant nsimilarities between them  In a classic text  Hesse  1963  ndistinguishes different types of analogies according to the kinds of nsimilarity relations into which two objects enter  A simple type of nanalogy is one that is based on shared properties  There is an analogy nbetween the earth and the moon based on the fact that both are large  nsolid  opaque  spherical bodies that receive heat and light from the nsun  revolve around their axes  and gravitate towards other bodies  nBut sameness of properties is not a necessary condition  An analogy nbetween two objects can also be based on relevant similarities between ntheir properties  In this more liberal sense  we can say that there is nan analogy between sound and light because echoes are similar to nreflections  loudness to brightness  pitch to color  detectability by nthe ear to detectability by the eye  and so on      nAnalogies can also be based on the sameness or resemblance of nrelations between parts of two systems rather than on their monadic nproperties  It is in this sense that the relation of a father to his nchildren is asserted to be analogous to the relation of the state to nits citizens  The analogies mentioned so far have been what Hesse ncalls material analogies  We obtain a more formal notion nof analogy when we abstract from the concrete features of the systems nand only focus on their formal set-up  What the analogue model then nshares with its target is not a set of features  but the same pattern nof abstract relationships  i e   the same structure  where structure nis understood in a formal sense   This notion of analogy is closely nrelated to what Hesse calls formal analogy  Two items nare related by formal analogy if they are both interpretations of the nsame formal calculus  For instance  there is a formal analogy between na swinging pendulum and an oscillating electric circuit because they nare both described by the same mathematical equation      nA further important distinction due to Hesse is the one between npositive  negative  and neutral analogies  The positive analogy nbetween two items consists in the properties or relations they share both gas molecules and billiard balls have mass   the negative nanalogy consists in the properties they do not share  billiard balls nare colored  gas molecules are not   the neutral analogy comprises the nproperties of which it is not known  yet  whether they belong to the npositive or the negative analogy  do billiard balls and molecules have nthe same cross section in scattering processes?   Neutral analogies nplay an important role in scientific research because they give rise nto questions and suggest new hypotheses  For this reason several nauthors have emphasized the heuristic role that analogies play in ntheory and model construction  as well as in creative thought Bailer-Jones and Bailer-Jones 2002  Bailer-Jones 2009  Ch  xa03  Hesse n1974  Holyoak and Thagard 1995  Kroes 1989  Psillos 1995  and the nessays collected in Helman 1988   See also the entry on analogy and analogical reasoning      nIt has also been discussed whether using analogical models can in some ncases be confirmatory in a Bayesian sense  Hesse  1974  208219  nargues that this is possible if the analogy is a material analogy  nBartha  2010  2013  2019   disagrees and argues that analogical models ncannot be confirmatory in a Bayesian sense because the information nencapsulated in an analogical model is part of the relevant background nknowledge  which has the consequence that the posterior probability of na hypothesis about a target system cannot change as a result of nobserving the analogy  Analogical models can therefore only establish nthe plausibility of a conclusion in the sense of justifying a nnon-negligible prior probability assignment  Bartha 2010  n8 5       nMore recently  these questions have been discussed in the context of nso-called analogue experiments  which promise to provide knowledge nabout an experimentally inaccessible target system  e g   a black nhole  by manipulating another system  the source system  e g   a nBoseEinstein condensate   Dardashti  Thbault  and Winsberg 2017  and Dardashti  Hartmann et al   2019  have argued that  given ncertain conditions  an analogue simulation of one system by another nsystem can confirm claims about the target system  e g   that black nholes emit Hawking radiation   See Crowther et al   forthcoming  for a ncritical discussion  and also the entry on computer simulations in science      nIdealized models  Idealized models are models that involve a ndeliberate simplification or distortion of something complicated with nthe objective of making it more tractable or understandable  nFrictionless planes  point masses  completely isolated systems  nomniscient and fully rational agents  and markets in perfect nequilibrium are well-known examples  Idealizations are a crucial means nfor science to cope with systems that are too difficult to study in ntheir full complexity  Potochnik 2017       nPhilosophical debates over idealization have focused on two general nkinds of idealizations  so-called Aristotelian and Galilean nidealizations  Aristotelian idealization amounts to stripping naway  in our imagination  all properties from a concrete object nthat we believe are not relevant to the problem at hand  There is ndisagreement on how this is done  Jones  2005  and Godfrey-Smith 2009  offer an analysis of abstraction in terms of truth  while an nabstraction remains silent about certain features or aspects of the nsystem  it does not say anything false and still offers a true  albeit nrestricted  description  This allows scientists to focus on a limited nset of properties in isolation  An example is a classical-mechanics nmodel of the planetary system  which describes the position of an nobject as a function of time and disregards all other properties of nplanets  Cartwright  1989  Ch  xa05   Musgrave  1981   who uses the term nnegligibility assumptions  and Mki  1994   who nspeaks of the method of isolation  allow abstractions to nsay something false  for instance by neglecting a causally relevant nfactor      nGalilean idealizations are ones that involve deliberate distortions  nphysicists build models consisting of point masses moving on nfrictionless planes  economists assume that agents are omniscient  nbiologists study isolated populations  and so on  Using nsimplifications of this sort whenever a situation is too difficult to ntackle was characteristic of Galileos approach to science  For nthis reason it is common to refer to distortive nidealizations of this kind as Galilean idealizations McMullin 1985   An example for such an idealization is a model of nmotion on an ice rink that assumes the ice to be frictionless  when  nin reality  it has low but non-zero friction      nGalilean idealizations are sometimes characterized as controlled nidealizations  i e   as ones that allow for de-idealization by nsuccessive removal of the distorting assumptions  McMullin 1985  nWeisberg 2007   Thus construed  Galilean idealizations dont ncover all distortive idealizations  Batterman  2002  2011  and Rice 2015  2019  discuss distortive idealizations that are ineliminable in nthat they cannot be removed from the model without dismantling the nmodel altogether      nWhat does a model involving distortions tell us about reality? Laymon 1991  formulated a theory which understands idealizations as ideal nlimits  imagine a series of refinements of the actual situation which napproach the postulated limit  and then require that the closer the nproperties of a system come to the ideal limit  the closer its nbehavior has to come to the behavior of the system at the limit monotonicity   If this is the case  then scientists can study the nsystem at the limit and carry over conclusions from that system to nsystems distant from the limit  But these conditions need not always nhold  In fact  it can happen that the limiting system does not napproach the system at the limit  If this happens  we are faced with a nsingular limit  Berry 2002   In such cases the system at the limit can nexhibit behavior that is different from the behavior of systems ndistant from the limit  Limits of this kind appear in a number of ncontexts  most notably in the theory of phase transitions in nstatistical mechanics  There is  however  no agreement over the ncorrect interpretation of such limits  Batterman  2002  2011  sees nthem as indicative of emergent phenomena  while Butterfield  2011a b  nsees them as compatible with reduction  see also the entries on intertheory relations in physics and scientific reduction       nGalilean and Aristotelian idealizations are not mutually exclusive  nand many models exhibit both in that they take into account a narrow nset of properties and distort them  Consider again the nclassical-mechanics model of the planetary system  the model only ntakes a narrow set of properties into account and distorts them  for ninstance by describing planets as ideal spheres with a nrotation-symmetric mass distribution      nA concept that is closely related to idealization is approximation  In na broad sense  A can be called an approximation of B if nA is somehow close to B  This  however  is too broad nbecause it makes room for any likeness to qualify as an approximation  nRueger and Sharp  1998  limit approximations to quantitative ncloseness  and Portides  2007  frames it as an essentially nmathematical concept  On that notion A is an approximation of nB iff A is close to B in a specifiable nmathematical sense  where the relevant sense of close nwill be given by the context  An example is the approximation of one ncurve with another one  which can be achieved by expanding a function ninto a power series and only keeping the first two or three terms  In ndifferent situations we approximate an equation with another one by nletting a control parameter tend towards zero  Redhead 1980   This nraises the question of how approximations are different from nidealizations  which can also involve mathematical closeness  Norton 2012  sees the distinction between the two as referential  an napproximation is an inexact description of the target while an nidealization introduces a secondary system  real or fictitious  which nstands for the target system  while being distinct from it   If we say nthat the period of the pendulum on the wall is roughly two seconds  nthen this is an approximation  if we reason about the real pendulum by nassuming that the pendulum bob is a point mass and that the string is nmassless  i e   if we assume that the pendulum is a so-called ideal npendulum   then we use an idealization  Separating idealizations and napproximations in this way does not imply that there cannot be ninteresting relations between the two  For instance  an approximation ncan be justified by pointing out that it is the mathematical nexpression of an acceptable idealization  e g   when we neglect a ndissipative term in an equation of motion because we make the nidealizing assumption that the system is frictionless       nToy models  Toy models are extremely simplified and strongly ndistorted renderings of their targets  and often only represent a nsmall number of causal or explanatory factors  Hartmann 1995  nReutlinger et al  2018  Nguyen forthcoming   Typical examples are the nLotkaVolterra model in population ecology  Weisberg 2013  and the nSchelling model of segregation in the social sciences  Sugden n2000   Toy models usually ndo not perform well in terms of prediction and empirical adequacy  and nthey seem to serve other epistemic goals  more on these in Section 3   This raises the question whether they should be regarded as nrepresentational at all  Luczak 2017       nSome toy models are characterized as caricatures Gibbard and Varian 1978  Batterman and Rice 2014   Caricature models nisolate a small number of salient characteristics of a system and ndistort them into an extreme case  A classic example is nAkerlofs  1970  model of the car market  the market for nlemons   which explains the difference in price between new and nused cars solely in terms of asymmetric information  thereby ndisregarding all other factors that may influence the prices of cars see also Sugden 2000   nHowever  it is controversial whether such highly idealized models can nstill be regarded as informative representations of their target nsystems  For a discussion of caricature models  in particular in neconomics  see Reiss  2006       nMinimal models  Minimal models are closely related to toy nmodels in that they are also highly simplified  They are so simplified nthat some argue that they are non-representational  they lack any nsimilarity  isomorphism  or resemblance relation to the world Batterman and Rice 2014   It has been argued that many economic nmodels are of this kind  Grne-Yanoff 2009   Minimal economic models are nalso unconstrained by natural laws  and do not isolate any real nfactors  ibid    And yet  minimal models help us to learn nsomething about the world in the sense that they function as nsurrogates for a real system  scientists can study the model to learn nsomething about the target  It is  however  controversial whether nminimal models can assist scientists in learning something about the nworld if they do not represent anything  Fumagalli 2016   Minimal nmodels that purportedly lack any similarity or representation are also nused in different parts of physics to explain the macro-scale behavior nof various systems whose micro-scale behavior is extremely diverse Batterman and Rice 2014  Rice 2018  2019  Shech 2018   Typical nexamples are the features of phase transitions and the flow of fluids  nProponents of minimal models argue that what provides an explanation nof the macro-scale behavior of a system in these cases is not a nfeature that system and model have in common  but the fact that the nsystem and the model belong to the same universality class  a nclass of models that exhibit the same limiting behavior even though nthey show very different behavior at finite scales   It is  however  ncontroversial whether explanations of this kind are possible without nreference to at least some common features  Lange 2015  Reutlinger n2017       nPhenomenological models  Phenomenological models have been ndefined in different  although related  ways  A common definition ntakes them to be models that only represent observable properties of ntheir targets and refrain from postulating hidden mechanisms and the nlike  Bokulich 2011   Another approach  due to McMullin  1968   ndefines phenomenological models as models that are independent of ntheories  This  however  seems to be too strong  Many phenomenological nmodels  while failing to be derivable from a theory  incorporate nprinciples and laws associated with theories  The liquid-drop model of nthe atomic nucleus  for instance  portrays the nucleus as a liquid ndrop and describes it as having several properties  surface tension nand charge  among others  originating in different theories hydrodynamics and electrodynamics  respectively   Certain aspects of nthese theoriesalthough usually not the full theoriesare nthen used to determine both the static and dynamical properties of the nnucleus  Finally  it is tempting to identify phenomenological models nwith models of a phenomenon  Here  phenomenon nis an umbrella term covering all relatively stable and general nfeatures of the world that are interesting from a scientific point of nview  The weakening of sound as a function of the distance to the nsource  the decay of alpha particles  the chemical reactions that take nplace when a piece of limestone dissolves in an acid  the growth of a npopulation of rabbits  and the dependence of house prices on the base nrate of the Federal Reserve are phenomena in this sense  For further ndiscussion  see Bailer-Jones  2009  Ch  xa07   Bogen and Woodward  1988   nand the entry on theory and observation in science      nExploratory models  Exploratory models are models which are nnot proposed in the first place to learn something about a specific ntarget system or a particular experimentally established phenomenon  nExploratory models function as the starting point of further nexplorations in which the model is modified and refined  Gelfert 2016  points out that exploratory models can provide nproofs-of-principle and suggest how-possibly explanations  2016  Ch  xa04   As an example  Gelfert mentions early models in theoretical necology  such as the LotkaVolterra model of predatorprey ninteraction  which mimic the qualitative behavior of speed-up and nslow-down in population growth in an environment with limited nresources  2016  80   Such models do not give an accurate account of nthe behavior of any actual population  but they provide the starting npoint for the development of more realistic models  Massimi  2019  nnotes that exploratory models provide modal knowledge  Fisher  2006  nsees these models as tools for the examination of the features of a ngiven theory      nModels of data  A model of data  sometimes also data nmodel  is a corrected  rectified  regimented  and in many ninstances idealized version of the data we gain from immediate nobservation  the so-called raw data  Suppes 1962   Characteristically  none first eliminates errors  e g   removes points from the record that nare due to faulty observation  and then presents the data in a nneat way  for instance by drawing a smooth curve through na set of points  These two steps are commonly referred to as ndata reduction and curve fitting  When we ninvestigate  for instance  the trajectory of a certain planet  we nfirst eliminate points that are fallacious from the observation nrecords and then fit a smooth curve to the remaining ones  Models of ndata play a crucial role in confirming theories because it is the nmodel of data  and not the often messy and complex raw data  that ntheories are tested against      nThe construction of a model of data can be extremely complicated  It nrequires sophisticated statistical techniques and raises serious nmethodological as well as philosophical questions  How do we decide nwhich points on the record need to be removed? And given a clean set nof data  what curve do we fit to it? The first question has been dealt nwith mainly within the context of the philosophy of experiment  see  nfor instance  Galison 1997 and Staley 2004   At the heart of the nlatter question lies the so-called curve-fitting problem  which is nthat the data themselves dictate neither the form of the fitted curve nnor what statistical techniques scientists should use to construct a ncurve  The choice and rationalization of statistical techniques is the nsubject matter of the philosophy of statistics  and we refer the nreader to the entry Philosophy of Statistics and to Bandyopadhyay and Forster  2011  for a discussion of these nissues  Further discussions of models of data can be found in nBailer-Jones  2009  Ch  xa07   Brewer and Chinn  1994   Harris  2003   nHartmann  1995   Laymon  1982   Mayo  1996  2018   and Suppes 2007       nThe gathering  processing  dissemination  analysis  interpretation  and nstorage of data raise many important questions beyond the relatively nnarrow issues pertaining to models of data  Leonelli  2016  2019  ninvestigates the status of data in science  argues that data should nbe defined not by their provenance but by their evidential function  nand studies how data travel between different contexts      nWhat are models? That is  what kind of object are scientists dealing nwith when they work with a model? A number of authors have voiced nskepticism that this question has a meaningful answer  because models ndo not belong to a distinctive ontological category and anything can nbe a model  Callender and Cohen 2006  Giere 2010  Surez 2004  nSwoyer 1991  Teller 2001   Contessa  2010  replies that this is a nnon sequitur  Even if  from an ontological point of view  nanything can be a model and the class of things that are referred to nas models contains a heterogeneous collection of different things  it ndoes not follow that it is either impossible or pointless to develop nan ontology of models  This is because even if not all models are of a nparticular ontological kind  one can nevertheless ask to what nontological kinds the things that are de facto used as models nbelong  There may be several such kinds and each kind can be analyzed nin its own right  What sort of objects scientists use as models has nimportant repercussions for how models perform relevant functions such nas representation and explanation  and hence this issue cannot be ndismissed as just sociology      nThe objects that commonly serve as models indeed belong to different nontological kinds  physical objects  fictional objects  abstract nobjects  set-theoretic structures  descriptions  equations  or ncombinations of some of these  are frequently referred to as models  nand some models may fall into yet other classes of things  Following nContessas advice  the aim then is to develop an ontology for neach of these  Those with an interest in ontology may see this as a ngoal in its own right  It pays noting  however  that the question has nreverberations beyond ontology and bears on how one understands the nsemantics and the epistemology of models      nSome models are physical objects  Such models are commonly referred to nas material models  Standard examples of models of this nkind are scale models of objects like bridges and ships  see Section 1   Watson and Cricks metal model of DNA  Schaffner 1969   nPhillips and Newlyns hydraulic model of an economy  Morgan and nBoumans 2004   the US Army Corps of Engineers model of the San nFrancisco Bay  Weisberg 2013   Kendrews plasticine model of nmyoglobin  Frigg and Nguyen 2016   and model organisms in the life nsciences  Leonelli and Ankeny 2012  Leonelli 2010  Levy and Currie n2015   All these are material objects that serve as models  Material nmodels do not give rise to ontological difficulties over and above the nwell-known problems in connection with objects that metaphysicians ndeal with  for instance concerning the nature of properties  the nidentity of objects  parts and wholes  and so on      nHowever  many models are not material models  The Bohr model nof the atom  a frictionless pendulum  or an isolated population  for ninstance  are in the scientists mind rather than in the nlaboratory and they do not have to be physically realized and nexperimented upon to serve as models  These non-physical nmodels raise serious ontological questions  and how they are best nanalyzed is debated controversially  In the remainder of this section nwe review some of the suggestions that have attracted attention in the nrecent literature on models      nWhat has become known as the fiction view of models sees nmodels as akin to the imagined objects of literary fictionthat nis  as akin to fictional characters like Sherlock Holmes or fictional nplaces like Middle Earth  Godfrey-Smith 2007   So when Bohr introduced nhis model of the atom he introduced a fictional object of the same nkind as the object Conan Doyle introduced when he invented Sherlock nHolmes  This view squares well with scientific practice  where nscientists often talk about models as if they were objects and often ntake themselves to be describing imaginary atoms  populations  nor economies  It also squares well with philosophical views that see nthe construction and manipulation of models as essential aspects of nscientific investigation  Morgan 1999   even if models are not nmaterial objects  because these practices seem to be directed toward nsome kind of object      nWhat philosophical questions does this move solve? Fictional discourse nand fictional entities face well-known philosophical questions  and none may well argue that simply likening models to fictions amounts to nexplaining obscurum per obscurius  for a discussion of these nquestions  see the entry on fictional entities   One way to counter this objection and to motivate the fiction view of nmodels is to point to the views heuristic power  In this vein nFrigg  2010b  identifies five specific issues that an ontology of nmodels has to address and then notes that these issues arise in very nsimilar ways in the discussion about fiction  the issues are the nidentity conditions  property attribution  the semantics of ncomparative statements  truth conditions  and the epistemology of nimagined objects   Likening models to fiction then has heuristic value nbecause there is a rich literature on fiction that offers a number of nsolutions to these issues      nOnly a small portion of the options available in the extensive nliterature on fictions have actually been explored in the context of nscientific models  Contessa  2010  formulates what he calls the ndualist account  according to which a model is an nabstract object that stands for a possible concrete object  The nRutherford model of the atom  for instance  is an abstract object that nacts as a stand-in for one of the possible systems that contain an nelectron orbiting around a nucleus in a well-defined orbit  nBarberousse and Ludwig  2009  and Frigg  2010b  take a different route nand develop an account of models as fictions based on Waltons 1990  pretense theory of fiction  According to this view the nsentences of a passage of text introducing a model should be seen as a nprop in a game of make-believe  and the model is the product of an act nof pretense  This is an antirealist position in that it takes talk of nmodel objects to be figures of speech because ultimately nthere are no model objectsmodels only live in scientists nimaginations  Salis  forthcoming  reformulates this view to become nwhat she calls the the new fiction view of models  The ncore difference lies in the fact that what is considered as the model nare the model descriptions and their content rather than the nimaginings that they prescribe  This is a realist view of models  nbecause descriptions exist      nThe fiction view is not without critics  Giere  2009   Magnani  2012   nPincock  2012   Portides  2014   and Teller  2009  reject the fiction napproach and argue  in different ways  that models should not be nregarded as fictions  Weisberg  2013  argues for a middle position nwhich sees fictions as playing a heuristic role but denies that they nshould be regarded as forming part of a scientific model  The common ncore of these criticisms is that the fiction view misconstrues the nepistemic standing of models  To call something a fiction  so the ncharge goes  is tantamount to saying that it is false  and it is nunjustified to call an entire model a fictionand thereby claim nthat it fails to capture how the world isjust because the model ninvolves certain false assumptions or fictional elements  In other nwords  a representation isnt automatically counted as fiction njust because it has some inaccuracies  Proponents of the fiction view nagree with this point but deny that the notion of fiction should be nanalyzed in terms of falsity  What makes a work a fiction is not its nfalsity  or some ratio of false to true claims   neither is everything nthat is said in a novel untrue  Tolstoys War and Peace ncontains many true statements about Napoleons Franco-Russian nWar   nor does every text containing false claims qualify as fiction false news reports are just that  they are not fictions   The ndefining feature of a fiction is that readers are supposed to nimagine the events and characters described  not that they nare false  Frigg 2010a  Salis forthcoming       nGiere  1988  advocated the view that non-physical models nare abstract entities  However  there is little agreement on the nnature of abstract objects  and Hale  1988  8687  lists no less nthan twelve different possible characterizations  for a review of the navailable options  see the entry on abstract objects   In recent publications  Thomasson  2020  and Thomson-Jones 2020  develop what they call an artifactualist nview of models  which is based on Thomassons  1999  ntheory of abstract artifacts  This view agrees with the pretense ntheory that the content of text that introduces a fictional character nor a model should be understood as occurring in pretense  but at the nsame time insists that in producing such descriptions authors create nabstract cultural artifacts that then exist independently of either nthe author or the readers  Artifactualism agrees with Platonism that nabstract objects exist  but insists  contra Platonism  that nabstract objects are brought into existence through a creative act and nare not eternal   This allows the artifactualist to preserve the nadvantages of pretense theory while at the same time holding the nrealist view that fictional characters and models actually exist      nAn influential point of view takes models to be set-theoretic nstructures  This position can be traced back to Suppes  1960  and is nnow  with slight variants  held by most proponents of the so-called nsemantic view of theories  for a discussion of this view  see the nentry on the structure of scientific theories   There are differences between the versions of the semantic view  but nwith the exception of Giere  1988  all versions agree that models are nstructures of one sort or another  Da Costa and French 2000       nThis view of models has been criticized on various grounds  One npervasive criticism is that many types of models that play an nimportant role in science are not structures and cannot be naccommodated within the structuralist view of models  which can nneither account for how these models are constructed nor for how they nwork in the context of investigation  Cartwright 1999  Downes 1992  nMorrison 1999   Examples for such models are interpretative models and nmediating models  discussed later in Section 4 2  Another charge held against the set-theoretic approach is that nset-theoretic structures by themselves cannot be representational nmodelsat least if that requires them to share some structure nwith the targetbecause the ascription of a structure to a ntarget system which forms part of the physical world relies on a nsubstantive  non-structural  description of the target  which goes nbeyond what the structuralist approach can afford  Nguyen and Frigg nforthcoming       nA time-honored position has it that a model is a stylized description nof a target system  It has been argued that this is what scientists ndisplay in papers and textbooks when they present a model  Achinstein n1968  Black 1962   This view has not been subject to explicit ncriticism  However  some of the criticisms that have been marshaled nagainst the so-called syntactic view of theories equally threaten a nlinguistic understanding of models  for a discussion of this view  see the nentry on the structure of scientific theories   First  a standard criticism of the syntactic view is that by nassociating a theory with a particular formulation  the view nmisconstrues theory identity because any change in the formulation nresults in a new theory  Suppe 2000   A view that associates models nwith descriptions would seem to be open to the same criticism  Second  nmodels have different properties than descriptions  the Newtonian nmodel of the solar system consists of orbiting spheres  but it makes nno sense to say this about its description  Conversely  descriptions nhave properties that models do not have  a description can be written nin English and consist of 517 words  but the same cannot be said of a nmodel  One way around these difficulties is to associate the model nwith the content of a description rather than with the description nitself  For a discussion of a position on models that builds on the ncontent of a description  see Salis  forthcoming       nA contemporary version of descriptivism is Levys  2012  2015  nand Toons  2012  so-called direct-representation view  This nview shares with the fiction view of models  Section 2 2  the reliance on Waltons pretense theory  but uses it in a ndifferent way  The main difference is that the views discussed earlier nsee modeling as introducing a vehicle of representation  the model  nthat is distinct from the target  and they see the problem as nelucidating what kind of thing the model is  On the ndirect-representation view there are no models distinct from the ntarget  there are only model-descriptions and targets  with no models nin-between them  Modeling  on this view  consists in providing an nimaginative description of real things  A model-description prescribes nimaginings about the real system  the ideal pendulum  for instance  nprescribes model-users to imagine the real spring as perfectly elastic nand the bob as a point mass  This approach avoids the above problems nbecause the identity conditions for models are given by the conditions nfor games of make-believe  and not by the syntax of a description  and nproperty ascriptions take place in pretense  There are  however  nquestions about how this account deals with models that have no target like models of the ether or four-sex populations   and about how nmodels thus understood deal with idealizations  For a discussion of nthese points  see Frigg and Nguyen  2016   Poznic  2016   and Salis forthcoming       nA closely related approach sees models as equations  This is a version nof the view that models are descriptions  because equations are nsyntactic items that describe a mathematical structure  The issues nthat this view faces are similar to the ones we have already nencountered  First  one can describe the same situation using ndifferent kinds of coordinates and as a result obtain different nequations but without thereby also obtaining a different model  nSecond  the model and the equation have different properties  A npendulum contains a massless string  but the equation describing its nmotion does not  and an equation may be inhomogeneous  but the system nit describes is not  It is an open question whether these issues can nbe avoided by appeal to a pretense account      nOne of the main reasons why models play such an important role in nscience is that they perform a number of cognitive functions  For nexample  models are vehicles for learning about the world  Significant nparts of scientific investigation are carried out on models rather nthan on reality itself because by studying a model we can discover nfeatures of  and ascertain facts about  the system the model stands nfor  models allow for surrogative reasoning  Swoyer n1991   For instance  we study the nature of the hydrogen atom  the ndynamics of a population  or the behavior of a polymer by studying ntheir respective models  This cognitive function of models has been nwidely acknowledged in the literature  and some even suggest that nmodels give rise to a new style of reasoning  model-based nreasoning  according to which inferences are made by nmeans of creating models and manipulating  adapting  and evaluating nthem  Nersessian 2010  12  see also Magnani  Nersessian  and nThagard 1999  Magnani and Nersessian 2002  and Magnani and Casadio n2016       nLearning about a model happens in two places  in the construction of nthe model and in its manipulation  Morgan 1999   There are no fixed nrules or recipes for model building and so the very activity of nfiguring out what fits together  and how  affords an opportunity to nlearn about the model  Once the model is built  we do not learn about nits properties by looking at it  we have to use and manipulate the nmodel in order to elicit its secrets      nDepending on what kind of model we are dealing with  building and nmanipulating a model amount to different activities demanding ndifferent methodologies  Material models seem to be straightforward nbecause they are used in common experimental contexts  e g   we put nthe model of a car in the wind tunnel and measure its air resistance   nHence  as far as learning about the model is concerned  material nmodels do not give rise to questions that go beyond questions nconcerning experimentation more generally      nNot so with fictional and abstract models  What constraints are there nto the construction of fictional and abstract models  and how do we nmanipulate them? A natural response seems to be that we do this by nperforming a thought experiment  Different authors  e g   Brown 1991  nGendler 2000  Norton 1991  Reiss 2003  Sorensen 1992  have explored nthis line of argument  but they have reached very different and often nconflicting conclusions about how thought experiments are performed nand what the status of their outcomes is  for details  see the entry non thought experiments       nAn important class of models is computational in nature  For some nmodels it is possible to derive results or solve equations of a nmathematical model analytically  But quite often this is not the case  nIt is at this point that computers have a great impact  because they nallow us to solve problems that are otherwise intractable  Hence  ncomputational methods provide us with knowledge about  the nconsequences of  a model where analytical methods remain silent  Many nparts of current research in both the natural and social sciences rely non computer simulations  which help scientists to explore the nconsequences of models that cannot be investigated otherwise  The nformation and development of stars and galaxies  the dynamics of nhigh-energy heavy-ion reactions  the evolution of life  outbreaks of nwars  the progression of an economy  moral behavior  and the nconsequences of decision procedures in an organization are explored nwith computer simulations  to mention only a few examples      nComputer simulations are also heuristically important  They can nsuggest new theories  models  and hypotheses  for example  based on a nsystematic exploration of a models parameter space  Hartmann n1996   But computer simulations also bear methodological perils  For nexample  they may provide misleading results because  due to the ndiscrete nature of the calculations carried out on a digital computer  nthey only allow for the exploration of a part of the full parameter nspace  and this subspace need not reflect every important feature of nthe model  The severity of this problem is somewhat mitigated by the nincreasing power of modern computers  But the availability of more ncomputational power can also have adverse effects  it may encourage nscientists to swiftly come up with increasingly complex but nconceptually premature models  involving poorly understood assumptions nor mechanisms and too many additional adjustable parameters  for a ndiscussion of a related problem in the social sciences  see Braun and nSaam 2015  Ch  xa03   This can lead to an increase in empirical nadequacywhich may be welcome for certain forecasting ntasksbut not necessarily to a better understanding of the nunderlying mechanisms  As a result  the use of computer simulations ncan change the weight we assign to the various goals of science  nFinally  the availability of computer power may seduce scientists into nmaking calculations that do not have the degree of trustworthiness one nwould expect them to have  This happens  for instance  when computers nare used to propagate probability distributions forward in time  which ncan turn out to be misleading  see Frigg et al  2014   So it is nimportant not to be carried away by the means that new powerful ncomputers offer and lose sight of the actual goals of research  For a ndiscussion of further issues in connection with computer simulations  nwe refer the reader to the entry on computer simulations in science      nOnce we have knowledge about the model  this knowledge has to be ntranslated into knowledge about the target system  It is nat this point that the representational function of models becomes nimportant again  if a model represents  then it can instruct us about nreality because  at least some of  the models parts or aspects nhave corresponding parts or aspects in the world  But if learning is nconnected to representation and if there are different kinds of nrepresentations  analogies  idealizations  etc    then there are also ndifferent kinds of learning  If  for instance  we have a model we take nto be a realistic depiction  the transfer of knowledge from the model nto the target is accomplished in a different manner than when we deal nwith an analogue  or a model that involves idealizing assumptions  For na discussion of the different ways in which the representational nfunction of models can be exploited to learn about the target  we nrefer the reader to the entry Scientific Representation      nSome models explain  But how can they fulfill this function given that nthey typically involve idealizations? Do these models explain ndespite or because of the idealizations nthey involve? Does an explanatory use of models presuppose that they nrepresent  or can non-representational models also explain? And what nkind of explanation do models provide?     nThere is a long tradition requesting that the explanans of a nscientific explanation must be true  We find this requirement in the ndeductive-nomological model  Hempel 1965  as well as in the more nrecent literature  For instance  Strevens  2008  297  claims that no causal naccount of explanation  allows nonveridical models to nexplain  For further discussions  see also Colombo et al  2015       nAuthors working in this tradition deny that idealizations make a npositive contribution to explanation and explore how models can nexplain despite being idealized  McMullin  1968  1985  argues that a ncausal explanation based on an idealized model leaves out only nfeatures which are irrelevant for the respective explanatory task  see nalso Salmon 1984 and nPiccinini and Craver 2011 for a discussion of mechanism sketches   nFriedman  1974  argues nthat a more realistic  and hence less idealized  model explains better non the unification account  The idea is that idealizations can  at nleast in principle  be de-idealized  for a critical discussion of this nclaim in the context of the debate about scientific explanations  see nBatterman 2002  Bokulich 2011  Morrison 2005  2009  Jebeile and nKennedy 2015  and Rice 2015   Strevens  2008  argues that an explanatory ncausal model has to provide an accurate representation of the relevant ncausal relationships or processes which the model shares with the ntarget system  The idealized assumptions of a model do not make a ndifference for the phenomenon under consideration and are therefore nexplanatorily irrelevant  In contrast  both Potochnik  2017  and Rice 2015  argue that models that explain can directly distort nmany difference-making causes      nAccording to Woodwards  2003  theory  models are tools to find nout about the causal relations that hold between certain facts or nprocesses  and it is these relations that do the explanatory work  nMore specifically  explanations provide information about patterns of ncounterfactual dependence between the explanans and the explanandum nwhich      nenable us to see what sort of difference it would have made for the nexplanandum if the factors cited in the explanans had been different nin various possible ways   Woodward 2003  11       nAccounts of causal explanation have also led to various claims about nhow idealized models can provide explanations  exploring to what nextent idealization allows for the misrepresentation of irrelevant ncausal factors by the explanatory model  Elgin and Sober 2002  nStrevens 2004  2008  Potochnik 2007  Weisberg n2007  2013   However  having the causally relevant features in common nwith real systems continues to play the essential role in showing how nidealized models can be explanatory      nBut is it really the truth of the explanans that makes the model nexplanatory? Other authors pursue a more radical line and argue that nfalse models explain not only despite their falsity  but in nfact because of their falsity  Cartwright  1983  44  nmaintains that the truth doesnt explain much  In nher so-called simulacrum account of explanation  she nsuggests that we explain a phenomenon by constructing a model that nfits the phenomenon into the basic framework of a grand theory  1983  nCh  xa08   On this account  the model itself is the explanation we seek  nThis squares well with basic scientific intuitions  but it leaves us nwith the question of what notion of explanation is at work  see also nElgin and Sober 2002  and of what explanatory function idealizations nplay in model explanations  Rice 2018  2019   Wimsatt  2007  Ch  xa06  nstresses the role of false models as means to arrive at true theories  nBatterman and Rice  2014  argue that models explain because the ndetails that characterize specific systems do not matter for the nexplanation  Bokulich  2008  2009  2011  2012  pursues a similar line nof reasoning and sees the explanatory power of models as being closely nrelated to their fictional nature  Bokulich  2009  and Kennedy  2012  npresent non-representational accounts of model explanation  see also nJebeile and Kennedy 2015   Reiss  2012  and Woody  2004  provide general discussions of the relationship between nrepresentation and explanation      nMany authors have pointed out that understanding is one of the central ngoals of science  see  for instance  de Regt 2017  Elgin 2017  Khalifa n2017  Potochnik 2017   In some cases  we want to understand a certain nphenomenon  e g   why the sky is blue   in other cases  we want to nunderstand a specific scientific theory  e g   quantum mechanics  that naccounts for a phenomenon in question  Sometimes we gain understanding nof a phenomenon by understanding the corresponding theory or model  nFor instance  Maxwells theory of electromagnetism helps us nunderstand why the sky is blue  It is  however  controversial whether nunderstanding a phenomenon always presupposes an nunderstanding of the corresponding theory  de Regt 2009  26        nAlthough there are many different ways of gaining understanding  nmodels and the activity of scientific modeling are of particular nimportance here  de Regt et al  2009  Morrison 2009  Potochnik 2017  nRice 2016   This insight ncan be traced back at least to Lord Kelvin who  in his famous 1884 nBaltimore Lectures on Molecular Dynamics and the Wave Theory of nLight  maintained that the test of Do we or do we nnot understand a particular subject in physics? is Can nwe make a mechanical model of it?  Kelvin 1884  1987  111   see also Bailer-Jones 2009  Ch  xa02  and de Regt 2017  Ch  xa06        nBut why do models play such a crucial role in the understanding of a nsubject matter? Elgin  2017  argues that this is not despite  but nbecause  of models being literally false  She views false models as nfelicitous falsehoods that occupy center stage in the nepistemology of science  and mentions the ideal-gas model in nstatistical mechanics and the HardyWeinberg model in genetics as nexamples for literally false models that are central to their nrespective disciplines  Understanding is holistic and it concerns a ntopic  a discipline  or a subject matter  rather than isolated claims nor facts  Gaining understanding of a context means to have      nan epistemic commitment to a comprehensive  systematically linked body nof information that is grounded in fact  is duly responsive to reasons nor evidence  and enables nontrivial inference  argument  and perhaps naction regarding the topic the information pertains to  Elgin 2017  n44       nand models can play a crucial role in the pursuit of these epistemic ncommitments  For a discussion of Elgins account of models and nunderstanding  see Baumberger and Brun  2017  and Frigg and Nguyen forthcoming       nElgin  2017   Lipton  2009   and Rice  2016  all argue that models can be used to understand nindependently of their ability to provide an explanation  Other nauthors  among them Strevens  2008  2013   argue that understanding npresupposes a scientific explanation and that     nan individual has scientific understanding of a phenomenon just in ncase they grasp a correct scientific explanation of that phenomenon  Strevens 2013  510  see  nhowever  Sullivan and Khalifa 2019       nOn this account  understanding consists in a particular form of nepistemic access an individual scientist has to an explanation  For nStrevens this aspect is grasping  while for de Regt 2017  it is intelligibility  It is important to note nthat both Strevens and de Regt hold that such subjective naspects are a worthy topic for investigations in the philosophy of nscience  This contrasts with the traditional view  see  e g   Hempel n1965  that delegates them to the realm of psychology  See Friedman 1974   Trout  2002   and nReutlinger et al   2018  for further discussions of understanding      nBesides the functions already mentioned  it has been emphasized nvariously that models perform a number of other cognitive functions  nKnuuttila  2005  2011  argues that the epistemic value of models is nnot limited to their representational function  and develops an account nthat views models as epistemic artifacts which allow us to gather nknowledge in diverse ways  Nersessian  1999  2010  stresses the role nof analogue models in concept-formation and other cognitive processes  nHartmann  1995  and Leplin  1980  discuss models as tools for theory nconstruction and emphasize their heuristic and pedagogical value  nEpstein  2008  lists a number of specific functions of models in the nsocial sciences  Peschard  2011  investigates the way in which models nmay be used to construct other models and generate new target systems  nAnd Isaac  2013  discusses non-explanatory uses of models which do not nrely on their representational capacities      nAn important question concerns the relation between models and ntheories  There is a full spectrum of positions ranging from models nbeing subordinate to theories to models being independent of ntheories      nTo discuss the relation between models and theories in science it is nhelpful to briefly recapitulate the notions of a model and of a theory nin logic  A theory is taken to be a  usually deductively nclosed  set of sentences in a formal language  A model is a nstructure  in the sense introduced in Section 2 3  that makes all sentences of a theory true when its symbols are ninterpreted as referring to objects  relations  or functions of a nstructure  The structure is a model of the theory in the nsense that it is correctly described by the theory  see Bell and nMachover 1977 or Hodges 1997 for details   Logical models are nsometimes also referred to as models of theory to nindicate that they are interpretations of an abstract formal nsystem      nModels in science sometimes carry over from logic the idea of being nthe interpretation of an abstract calculus  Hesse 1967   This is nsalient in physics  where general lawssuch as Newtons nequation of motionlie at the heart of a theory  These laws are napplied to a particular systeme g   a pendulumby nchoosing a special force function  making assumptions about the mass ndistribution of the pendulum etc  The resulting model then is an ninterpretation  or realization  of the general law      nIt is important to keep the notions of a logical and a nrepresentational model separate  Thomson-Jones 2006   these are ndistinct concepts  Something can be a logical model without being a nrepresentational model  and vice versa  This  however  does nnot mean that something cannot be a model in both senses at once  In nfact  as Hesse  1967  points out  many models in science are both nlogical and representational models  Newtons model of planetary nmotion is a case in point  the model  consisting of two homogeneous nperfect spheres located in otherwise empty space that attract each nother gravitationally  is simultaneously a logical model  because it nmakes the axioms of Newtonian mechanics true when they are interpreted nas referring to the model  and a representational model  because it nrepresents the real sun and earth       nThere are two main conceptions of scientific theories  the so-called nsyntactic view of theories and the so-called semantic view of theories see the entry on the structure of scientific theories   On both conceptions models play a subsidiary role to theories  albeit nin very different ways  The syntactic view of theories  see entry nsection on the syntactic view  retains the logical notions of a model and a theory  It construes a ntheory as a set of sentences in an axiomatized logical system  and a nmodel as an alternative interpretation of a certain calculus Braithwaite 1953  Campbell 1920  1957   Nagel 1961  Spector 1965   nIf  for instance  we take the mathematics used in the kinetic theory nof gases and reinterpret the terms of this calculus in a way that nmakes them refer to billiard balls  the billiard balls are a model of nthe kinetic theory of gases in the sense that all sentences of the ntheory come out true  The model is meant to be something that we are nfamiliar with  and it serves the purpose of making an abstract formal ncalculus more palpable  A given theory can have different models  and nwhich model we choose depends both on our aims and our background nknowledge  Proponents of the syntactic view disagree about the nimportance of models  Carnap and Hempel thought that models only serve na pedagogic or aesthetic purpose and are ultimately dispensable nbecause all relevant information is contained in the theory  Carnap n1938  Hempel 1965  see also Bailer-Jones 1999   Nagel  1961  and nBraithwaite  1953   on the other hand  emphasize the nheuristic role of models  and Schaffner  1969  submits that ntheoretical terms get at least part of their meaning from models      nThe semantic view of theories  see entry section on the semantic view  dispenses with sentences in an axiomatized logical system and nconstrues a theory as a family of models  On this view  a theory nliterally is a class  cluster  or family of modelsmodels are the nbuilding blocks of which scientific theories are made up  Different nversions of the semantic view work with different notions of a model  nbut  as noted in Section 2 3  in the semantic view models are mostly construed as set-theoretic nstructures  For a discussion of the different options  we refer the nreader to the relevant entry in this encyclopedia  linked at the nbeginning of this paragraph       nIn both the syntactic and the semantic view of theories models are nseen as subordinate to theory and as playing no role outside the ncontext of a theory  This vision of models has been challenged in a nnumber of ways  with authors pointing out that models enjoy various ndegrees of freedom from theory and function autonomously in many ncontexts  Independence can take many forms  and large parts of the nliterature on models are concerned with investigating various forms of nindependence      nModels as completely independent of theory  The most radical ndeparture from a theory-centered analysis of models is the realization nthat there are models that are completely independent from any theory  nAn example of such a model is the LotkaVolterra model  The model ndescribes the interaction of two populations  a population of npredators and one of prey animals  Weisberg 2013   The model was nconstructed using only relatively commonsensical assumptions about npredators and prey and the mathematics of differential equations  nThere was no appeal to a theory of predatorprey interactions or a ntheory of population growth  and the model is independent of theories nabout its subject matter  If a model is constructed in a domain where nno theory is available  then the model is sometimes referred to as a nsubstitute model  Groenewold 1961   because the model nsubstitutes a theory      nModels as a means to explore theory  Models can also be used nto explore theories  Morgan and Morrison 1999   An obvious way in nwhich this can happen is when a model is a logical model of a theory see Section 4 1   A logical model is a set of objects and properties that make a formal nsentence true  and so one can see in the model how the axioms of the ntheory play out in a particular setting and what kinds of behavior nthey dictate  But not all models that are used to explore theories are nlogical models  and models can represent features of theories in other nways  As an example  consider chaos theory  The equations of nnon-linear systems  such as those describing the three-body nproblem  have solutions that are too complex to study with npaper-and-pencil methods  and even computer simulations are limited in nvarious ways  Abstract considerations about the qualitative behavior nof solutions show that there is a mechanism that has been dubbed nstretching and folding  see the entry Chaos   To obtain an idea of the complexity of the dynamics exhibiting nstretching and folding  Smale proposed to study a simple model of the nflownow known as the horseshoe map  Tabor 1989 which nprovides important insights into the nature of stretching and folding  nOther examples of models of that kind are the Kac ring model that is nused to study equilibrium properties of systems in statistical nmechanics  Lavis 2008  nand Nortons dome in Newtonian mechanics  Norton 2003       nModels as complements of theories  A theory may be nincompletely specified in the sense that it only imposes certain ngeneral constraints but remains silent about the details of concrete nsituations  which are provided by a model  Redhead 1980   A special ncase of this situation is when a qualitative theory is known and the nmodel introduces quantitative measures  Apostel 1961   Redheads nexample of a theory that is underdetermined in this way is axiomatic nquantum field theory  which only imposes certain general constraints non quantum fields but does not provide an account of particular nfields  Harr  2004  notes that models can complement theories by providing nmechanisms for processes that are left unspecified in the theory even nthough they are responsible for bringing about the observed nphenomena      nTheories may be too complicated to handle  In such cases a model can ncomplement a theory by providing a simplified version of the ntheoretical scenario that allows for a solution  Quantum nchromodynamics  for instance  cannot easily be used to investigate the nphysics of an atomic nucleus even though it is the relevant nfundamental theory  To get around this difficulty  physicists nconstruct tractable phenomenological models  such as the MIT bag nmodel  which effectively describe the relevant degrees of freedom of nthe system under consideration  Hartmann 1999  2001   The advantage of nthese models is that they yield results where theories remain silent  nTheir drawback is that it is often not clear how to understand the nrelationship between the model and the theory  as the two are  strictly nspeaking  contradictory      nModels as preliminary theories  The notion of a model as a nsubstitute for a theory is closely related to the notion of a ndevelopmental model  This term was coined by Leplin  1980   nwho pointed out how useful models were in the development of early nquantum theory  and it is now used as an umbrella notion covering ncases in which models are some sort of a preliminary exercise to ntheory      nAlso closely related is the notion of a probing model  or nstudy model   Models of this kind do not perform a nrepresentational function and are not expected to instruct us about nanything beyond the model itself  The purpose of these models is to ntest new theoretical tools that are used later on to build nrepresentational models  In field theory  for instance  the so-called n4-model was studied extensively  not because it was nbelieved to represent anything real  but because it served several nheuristic functions  the simplicity of the 4-model nallowed physicists to get a feeling for what quantum nfield theories are like and to extract some general features that this nsimple model shared with more complicated ones  Physicists could study ncomplicated techniques such as renormalization in a simple setting  nand it was possible to get acquainted with important nmechanismsin this case symmetry-breakingthat could later nbe used in different contexts  Hartmann 1995   This is true not only nfor physics  As Wimsatt  1987  2007  points out  a false model in ngenetics can perform many useful functions  among them the following  nthe false model can help answering questions about more realistic nmodels  provide an arena for answering questions about properties of nmore complex models  factor out phenomena that would not notherwise be seen  serve as a limiting case of a more general model or two false models may define the extremes of a continuum of cases non which the real case is supposed to lie   or lead to the nidentification of relevant variables and the estimation of their nvalues      nInterpretative models  Cartwright  1983  1999  argues that nmodels do not only aid the application of theories that are somehow nincomplete  she claims that models are also involved whenever na theory with an overarching mathematical structure is applied  The nmain theories in physicsclassical mechanics  electrodynamics  nquantum mechanics  and so onfall into this category  Theories of nthat kind are formulated in terms of abstract concepts that need to be nconcretized for the theory to provide a description of the target nsystem  and concretizing the relevant concepts  idealized objects and nprocesses are introduced  For instance  when applying classical nmechanics  the abstract concept of force has to be replaced with a nconcrete force such as gravity  To obtain tractable equations  this nprocedure has to be applied to a simplified scenario  for instance nthat of two perfectly spherical and homogeneous planets in otherwise nempty space  rather than to reality in its full complexity  The result nis an interpretative model  which grounds the application of nmathematical theories to real-world targets  Such models are nindependent from theory in that the theory does not determine their nform  and yet they are necessary for the application of the theory to na concrete problem      nModels as mediators  The relation between models and theories ncan be complicated and disorderly  The contributors to a programmatic ncollection of essays edited by Morgan and Morrison  1999  rally around nthe idea that models are instruments that mediate between theories and nthe world  Models are autonomous agents in that they are nindependent from both theories and their target systems  and it is nthis independence that allows them to mediate between the two  nTheories do not provide us with algorithms for the construction of a nmodel  they are not vending machines into which one can ninsert a problem and a model pops out  Cartwright 1999   The nconstruction of a model often requires detailed knowledge about nmaterials  approximation schemes  and the setup  and these are not nprovided by the corresponding theory  Furthermore  the inner workings nof a model are often driven by a number of different theories working ncooperatively  In contemporary climate modeling  for instance  nelements of different theoriesamong them fluid dynamics  nthermodynamics  electromagnetismare put to work cooperatively  nWhat delivers the results is not the stringent application of one ntheory  but the voices of different theories when put to use in chorus nwith each other in one model      nIn complex cases like the study of a laser system or the global nclimate  models and theories can get so entangled that it becomes nunclear where a line between the two should be drawn  where does the nmodel end and the theory begin? This is not only a problem for nphilosophical analysis  it also arises in scientific practice  nBailer-Jones  2002  interviewed a group of physicists about their nunderstanding of models and their relation to theories  and reports nwidely diverging views   i  xa0there is no substantive difference between nmodel and theory   ii  xa0models become theories when their ndegree of confirmation increases   iii  xa0models contain simplifications nand omissions  while theories are accurate and complete   iv  xa0theories nare more general than models  and modeling is about applying general ntheories to specific cases  The first suggestion seems to be too nradical to do justice to many aspects of practice  where a distinction nbetween models and theories is clearly made  The second view is in nline with common parlance  where the terms model and ntheory are sometimes used to express someones nattitude towards a particular hypothesis  The phrase its njust a model indicates that the hypothesis at stake is asserted nonly tentatively or is even known to be false  while something is nawarded the label theory if it has acquired some degree nof general acceptance  However  this use of model is ndifferent from the uses we have seen in Sections 1 to 3 and is therefore of no use if we aim to understand the relation nbetween scientific models and theories  and  incidentally  one can nequally dismiss speculative claims as being just a ntheory   The third proposal is correct in associating models nwith idealizations and simplifications  but it overshoots by nrestricting this to models  in fact  also theories can contain nidealizations and simplifications  The fourth view seems closely naligned with interpretative models and the idea that models are nmediators  but being more general is a gradual notion and hence does nnot provide a clear-cut criterion to distinguish between theories and nmodels      nThe debate over scientific models has important repercussions for nother issues in the philosophy of science  for a historical account of nthe philosophical discussion about models  see Bailer-Jones 1999   nTraditionally  the debates over  say  scientific realism  reductionism  nand laws of nature were couched in terms of theories  because theories nwere seen as the main carriers of scientific knowledge  Once models nare acknowledged as occupying an important place in the edifice of nscience  these issues have to be reconsidered with a focus on models  nThe question is whether  and if so how  discussions of these issues nchange when we shift focus from theories to models  Up to now  no ncomprehensive model-based account of any of these issues has emerged  nbut models have left important traces in the discussions of these ntopics      nAs we have seen in Section 1  models typically provide a distorted representation of their targets  nIf one sees science as primarily model-based  this could be taken to nsuggest an antirealist interpretation of science  Realists  however  ndeny that the presence of idealizations in models renders a realist napproach to science impossible and point out that a good model  while nnot literally true  is usually at least approximately true  and/or nthat it can be improved by de-idealization  Laymon 1985  McMullin n1985  Nowak 1979  Brzezinski and Nowak 1992       nApart from the usual worries about the elusiveness of the notion of napproximate truth  for a discussion  see the entry on truthlikeness   antirealists have taken issue with this reply for two  related  nreasons  First  as Cartwright  1989  points out  there is no reason to nassume that one can always improve a model by adding de-idealizing ncorrections  Second  it seems that de-idealization is not in naccordance with scientific practice because it is unusual that nscientists invest work in repeatedly de-idealizing an existing model  nRather  they shift to a different modeling framework once the nadjustments to be made get too involved  Hartmann 1998   The various nmodels of the atomic nucleus are a case in point  once it was realized nthat shell effects are important to understand various subatomic nphenomena  the  collective  liquid-drop model was put aside and the single-particle  shell model was developed to account for the ncorresponding findings  A further difficulty with de-idealization is nthat most idealizations are not controlled  For example  nit is not clear in what way one could de-idealize the MIT bag model to neventually arrive at quantum chromodynamics  the supposedly correct nunderlying theory      nA further antirealist argument  the incompatible-models nargument  takes as its starting point the observation that nscientists often successfully use several incompatible models of none and the same target system for predictive purposes Morrison 2000   These models seemingly contradict each other  as they nascribe different properties to the same target system  In nuclear nphysics  for instance  the liquid-drop model explores the analogy of nthe atomic nucleus with a  charged  fluid drop  while the shell model ndescribes nuclear properties in terms of the properties of protons and nneutrons  the constituents of an atomic nucleus  This practice appears nto cause a problem for scientific realism  Realists typically hold nthat there is a close connection between the predictive success of a ntheory and its being at least approximately true  But if several nmodels of the same system are predictively successful and if these nmodels are mutually inconsistent  then it is difficult to maintain nthat they are all approximately true      nRealists can react to this argument in various ways  First  they can nchallenge the claim that the models in question are indeed npredictively successful  If the models are not good predictors  then nthe argument is blocked  Second  they can defend a version of nperspectival realism  Giere 2006  Massimi 2017  Rueger n2005   Proponents of this position  which is sometimes also called nperspectivism  situate it somewhere between nstandard scientific realism and antirealism  and where nexactly the right middle position lies is the subject matter of active ndebate  Massimi 2018a b  Saatsi 2016  Teller 2018  and the ncontributions to Massimi and McCoy 2019   Third  realists can deny nthat there is a problem in the first place  because scientific models  nwhich are always idealized and therefore strictly speaking false  are njust the wrong vehicle to make a point about realism  which should be ndiscussed in terms of theories       nA particular focal point of the realism debate are laws of nature  nwhere the questions arise what laws are and whether they are ntruthfully reflected in our scientific representations  According to nthe two currently dominant accounts  the best-systems approach and the nnecessitarian approach  laws of nature are understood to be universal nin scope  meaning that they apply to everything that there is in the nworld  for discussion of laws  see the entry on laws of nature   This take on laws does not seem to sit well with a view that places nmodels at the center of scientific research  What role do general laws nplay in science if it is models that represent what is happening in nthe world? And how are models and laws related?     nOne possible response to these questions is to argue that laws of nnature govern entities and processes in a model rather than nin the world  Fundamental laws  on this approach  do not state facts nabout the world but hold true of entities and processes in nthe model  This view has been advocated in different variants  nCartwright  1983  argues that all laws are ceteris paribus nlaws  Cartwright  1999  makes use of capacities  which nshe considers to be prior to laws  and introduces the notion of a nnomological machine  This is      na fixed  enough  arrangement of components  or factors  with stable enough  capacities that in the right sort of stable  enough  nenvironment will  with repeated operation  give rise to the kind of nregular behavior that we represent in our scientific laws   1999  50  nsee also the entry on ceteris paribus laws       nGiere  1999  argues that the laws of a theory are better thought of  nnot as encoding general truths about the world  but rather as nopen-ended statements that can be filled in various ways in the nprocess of building more specific scientific models  Similar positions nhave also been defended by Teller  2001  and van Fraassen  1989       nThe multiple-models problem mentioned in Section 5 1 also raises the question of how different models are related  nEvidently  multiple models for the same target system do not generally nstand in a deductive relationship  as they often contradict each nother  Some  Cartwright 1999  Hacking 1983  have suggested a picture nof science according to which there are no systematic relations that nhold between different models  Some models are tied together because nthey represent the same target system  but this does not imply that nthey enter into any further relationships  deductive or otherwise   We nare confronted with a patchwork of models  all of which hold nceteris paribus in their specific domains of napplicability      nSome argue that this picture is at least partially incorrect because nthere are various interesting relations that hold between different nmodels or theories  These relations range from thoroughgoing reductive nrelations  Scheibe 1997  1999  2001  esp  Chs  V 23 and V 24  and ncontrolled approximations over singular limit relations  Batterman n2001  2016   to structural relations  Ghde 1997  and rather nloose relations called stories  Hartmann 1999  see also nBokulich 2003  Teller 2002  and the essays collected in Part xa0III of nHartmann et al  2008   These suggestions have been made on the basis nof case studies  and it remains to be seen whether a more general naccount of these relations can be given and whether a deeper njustification for them can be provided  for instance  within a nBayesian framework  first steps towards a Bayesian understanding of nreductive relations can be found in Dizadji-Bahmani et al  2011  nLiefke and Hartmann 2018  and Tei 2019       nModels also figure in the debate about reduction and emergence in nphysics  Here  some authors argue that the modern approach to nrenormalization challenges Nagels  1961  model of reduction or nthe broader doctrine of reductions  for a critical discussion  see  nfor instance  Batterman 2002  2010  2011  Morrison 2012  and Saatsi and Reutlinger 2018   nDizadji-Bahmani et al   2010  provide a defense of the NagelSchaffner nmodel of reduction  and Butterfield  2011a b  2014  argues that nrenormalization is consistent with Nagelian reduction  Palacios  2019  nshows that phase transitions are compatible with reductionism  and nHartmann  2001  argues that the effective-field-theories research nprogram is consistent with reductionism  see also Bain 2013 and nFranklin forthcoming   Rosaler  2015  argues for a local nform of reduction which sees the fundamental relation of reduction nholding between models  not theories  which is  however  compatible nwith the NagelSchaffner model of reduction  See also the entries on intertheory relations in physics and scientific reduction      nIn the social sciences  agent-based models  ABMs  are increasingly nused  Klein et al  2018   These models show how surprisingly complex nbehavioral patterns at the macro-scale can emerge from a small number nof simple behavioral rules for the individual agents and their ninteractions  This raises questions similar to the questions mentioned nabove about reduction and emergence in physics  but so far one only nfinds scattered remarks about reduction in the literature  See nWeisberg and Muldoon  2009  and Zollman  2007  for the application of nABMs to the epistemology and the social structure of science  and nColyvan  2013  for a discussion of methodological questions raised by nnormative models in general      n analogy and analogical reasoning | laws of nature | science  unity of | scientific explanation | scientific realism | scientific representation | scientific theories  structure of | simulations in science | thought experiments n     nWe would like to thank Joe Dewhurst  James Nguyen  Alexander nReutlinger  Collin Rice  Dunja eelja  and Paul Teller nfor helpful comments on the drafts of the revised version in 2019  nWhen writing the original version back in 2006 we benefitted from ncomments and suggestions by Nancy Cartwright  Paul Humphreys  Julian nReiss  Elliott Sober  Chris Swoyer  and Paul Teller      Copyright  2020 by n nRoman Frigg n<r p frigg@lse ac uk> nStephan Hartmann n<stephan hartmann@lrz uni-muenchen de>        View this site from another server         The Stanford Encyclopedia of Philosophy is copyright  2020 by The Metaphysics Research Lab  Center for the Study of Language and Information  CSLI   Stanford University    Library of Congress Catalog Data  ISSN 1095-5054  \n"
     ]
    }
   ],
   "source": [
    "#Extracting unwanted characters from the texts in the list and collecting them under a single string\n",
    "unwanted = ['\\\\','\\\\n','[',']','(',')','.',',',':',';', \"'\",' n ', '{','}']\n",
    "i = \"\"\n",
    "\n",
    "#Clean up characters that do not have a place in the ascii table and give errors when printing\n",
    "for item in liste:\n",
    "    i += item['text'].encode(\"ascii\", errors=\"ignore\").decode()\n",
    "\n",
    "for character in unwanted:\n",
    "        i = i.replace(character, \" \")\n",
    "        \n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f3933e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate the i string word by word and clear the stop-words with nltk and wordcloud libraries\n",
    "text_tokens = word_tokenize(i)\n",
    "tokens_without_sw = [word for word in text_tokens if word not in STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13cef81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the word cloud\n",
    "wordcloud = WordCloud().generate(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e1fd916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the word cloud with matplotlib\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf3b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
